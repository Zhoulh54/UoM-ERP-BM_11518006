{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61d09a3d-9bc9-4c40-aeab-c2e44e6e346e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.7\n",
      "cycler('color', ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "# Basic packages\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict\n",
    "# Helper modules\n",
    "from numpy import load\n",
    "import calendar\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "plt.rcdefaults()\n",
    "print(matplotlib.rcParams['axes.prop_cycle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e00436eb-f689-49e8-a22d-e8691333b666",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wj/7nbyftys3g77fcp37r3t0_lm0000gn/T/ipykernel_15133/3882517259.py:6: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_train = pd.read_csv(train_path)\n",
      "/var/folders/wj/7nbyftys3g77fcp37r3t0_lm0000gn/T/ipykernel_15133/3882517259.py:7: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_test = pd.read_csv(test_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training dataset shape: (4880065, 11)\n",
      "\n",
      "Data types:\n",
      "PERMNO      int64\n",
      "date       object\n",
      "EXCHCD      int64\n",
      "SICCD       int64\n",
      "TICKER     object\n",
      "COMNAM     object\n",
      "PERMCO      int64\n",
      "PRC       float64\n",
      "VOL       float64\n",
      "RET        object\n",
      "SHROUT    float64\n",
      "dtype: object\n",
      "\n",
      "Missing value ratio:\n",
      "PRC       0.025926\n",
      "VOL       0.025921\n",
      "TICKER    0.025124\n",
      "RET       0.000804\n",
      "SHROUT    0.000409\n",
      "PERMNO    0.000000\n",
      "date      0.000000\n",
      "EXCHCD    0.000000\n",
      "SICCD     0.000000\n",
      "COMNAM    0.000000\n",
      "PERMCO    0.000000\n",
      "dtype: float64\n",
      "\n",
      "Unique value count per column:\n",
      "EXCHCD          9\n",
      "SICCD         285\n",
      "PERMCO       3208\n",
      "PERMNO       3265\n",
      "TICKER       3620\n",
      "COMNAM       3913\n",
      "date         4029\n",
      "SHROUT      63473\n",
      "PRC         97215\n",
      "RET        315833\n",
      "VOL       1096425\n",
      "dtype: int64\n",
      "\n",
      "Sample unique values for object columns:\n",
      "date: ['2000-01-03' '2000-01-04' '2000-01-05' '2000-01-06' '2000-01-07']\n",
      "TICKER: ['DGSE' 'DGC' 'GGUY' 'XIOX' 'ATCM']\n",
      "COMNAM: ['DALLAS GOLD & SILVER EXCHANGE IN' 'D G S E COMPANIES INC'\n",
      " 'GOOD GUYS INC' 'XIOX CORP' 'AT COMM CORP']\n",
      "RET: ['0.000000' '-0.062500' '0.066667' '-0.020833' '-0.031915']\n",
      "\n",
      "Summary statistics for numeric columns:\n",
      "             PERMNO        EXCHCD         SICCD        PERMCO           PRC  \\\n",
      "count  4.880065e+06  4.880065e+06  4.880065e+06  4.880065e+06  4.753545e+06   \n",
      "mean   7.399982e+04  2.303937e+00  6.631573e+03  2.234132e+04  1.949726e+01   \n",
      "std    2.336461e+04  9.616432e-01  9.826164e+02  1.485108e+04  3.765689e+01   \n",
      "min    1.002800e+04 -2.000000e+00  5.000000e+03  3.300000e+01 -1.076910e+03   \n",
      "25%    7.671100e+04  1.000000e+00  5.651000e+03  1.219700e+04  3.640000e+00   \n",
      "50%    8.369300e+04  3.000000e+00  7.363000e+03  1.625700e+04  1.098000e+01   \n",
      "75%    8.741400e+04  3.000000e+00  7.372000e+03  3.462200e+04  2.507000e+01   \n",
      "max    9.343200e+04  3.300000e+01  7.999000e+03  5.624300e+04  1.469560e+03   \n",
      "\n",
      "                VOL        SHROUT  \n",
      "count  4.753568e+06  4.878067e+06  \n",
      "mean   7.692282e+05  8.171834e+04  \n",
      "std    3.229937e+06  3.516659e+05  \n",
      "min    0.000000e+00  2.000000e+00  \n",
      "25%    2.045000e+04  1.183700e+04  \n",
      "50%    1.183000e+05  2.605100e+04  \n",
      "75%    4.750202e+05  5.564400e+04  \n",
      "max    5.929250e+08  1.088065e+07  \n",
      "\n",
      "Test dataset shape: (1746341, 11)\n",
      "\n",
      "Data types:\n",
      "PERMNO      int64\n",
      "date       object\n",
      "EXCHCD      int64\n",
      "SICCD       int64\n",
      "TICKER     object\n",
      "COMNAM     object\n",
      "PERMCO      int64\n",
      "PRC       float64\n",
      "VOL       float64\n",
      "RET        object\n",
      "SHROUT    float64\n",
      "dtype: object\n",
      "\n",
      "Missing value ratio:\n",
      "PRC       0.005761\n",
      "VOL       0.005761\n",
      "TICKER    0.004654\n",
      "RET       0.001107\n",
      "SHROUT    0.000198\n",
      "PERMNO    0.000000\n",
      "date      0.000000\n",
      "EXCHCD    0.000000\n",
      "SICCD     0.000000\n",
      "COMNAM    0.000000\n",
      "PERMCO    0.000000\n",
      "dtype: float64\n",
      "\n",
      "Unique value count per column:\n",
      "EXCHCD          6\n",
      "SICCD         209\n",
      "PERMCO       1334\n",
      "PERMNO       1346\n",
      "TICKER       1448\n",
      "COMNAM       1518\n",
      "date         2264\n",
      "SHROUT      31859\n",
      "PRC        102322\n",
      "RET        275951\n",
      "VOL       1045324\n",
      "dtype: int64\n",
      "\n",
      "Sample unique values for object columns:\n",
      "date: ['2016-01-04' '2016-01-05' '2016-01-06' '2016-01-07' '2016-01-08']\n",
      "TICKER: ['DGSE' 'ELA' 'FKWL' 'ORCL' 'MSFT']\n",
      "COMNAM: ['D G S E COMPANIES INC' 'ENVELA CORP' 'FRANKLIN WIRELESS CORP'\n",
      " 'ORACLE CORP' 'MICROSOFT CORP']\n",
      "RET: ['-0.054545' '0.089744' '-0.073529' '0.031746' '-0.015385']\n",
      "\n",
      "Summary statistics for numeric columns:\n",
      "             PERMNO        EXCHCD         SICCD        PERMCO           PRC  \\\n",
      "count  1.746341e+06  1.746341e+06  1.746341e+06  1.746341e+06  1.736280e+06   \n",
      "mean   5.520087e+04  1.899508e+00  6.605422e+03  3.487926e+04  6.253243e+01   \n",
      "std    3.330143e+04  9.864910e-01  9.893968e+02  1.974400e+04  1.732782e+02   \n",
      "min    1.002800e+04 -1.000000e+00  5.010000e+03  3.700000e+01 -1.348540e+03   \n",
      "25%    1.687500e+04  1.000000e+00  5.621000e+03  1.488800e+04  7.260000e+00   \n",
      "50%    7.591200e+04  1.000000e+00  7.359000e+03  4.182000e+04  2.320000e+01   \n",
      "75%    8.686600e+04  3.000000e+00  7.373000e+03  5.462800e+04  6.047000e+01   \n",
      "max    9.342900e+04  5.000000e+00  7.999000e+03  6.009500e+04  5.300340e+03   \n",
      "\n",
      "                VOL        SHROUT  \n",
      "count  1.736280e+06  1.745996e+06  \n",
      "mean   1.536037e+06  1.524590e+05  \n",
      "std    6.002313e+06  4.777270e+05  \n",
      "min    0.000000e+00  5.500000e+01  \n",
      "25%    9.346400e+04  2.262275e+04  \n",
      "50%    3.649470e+05  4.903600e+04  \n",
      "75%    1.208558e+06  1.169820e+05  \n",
      "max    1.253254e+09  1.051501e+07  \n"
     ]
    }
   ],
   "source": [
    "train_path = \"CRSP_ConsumerDiscretionary_2000_2015.csv\"\n",
    "test_path = \"CRSP_ConsumerDiscretionary_2016_2024.csv\"\n",
    "\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "\n",
    "def inspect_dataset(df, name):\n",
    "    print(f\"\\n{name} dataset shape: {df.shape}\")\n",
    "\n",
    "    print(\"\\nData types:\")\n",
    "    print(df.dtypes)\n",
    "\n",
    "    print(\"\\nMissing value ratio:\")\n",
    "    print(df.isnull().mean().sort_values(ascending=False))\n",
    "\n",
    "    print(\"\\nUnique value count per column:\")\n",
    "    print(df.nunique().sort_values())\n",
    "\n",
    "    print(\"\\nSample unique values for object columns:\")\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        print(f\"{col}: {df[col].dropna().unique()[:5]}\")\n",
    "\n",
    "    print(\"\\nSummary statistics for numeric columns:\")\n",
    "    print(df.describe(include=[float, int]))\n",
    "\n",
    "inspect_dataset(df_train, \"Training\")\n",
    "inspect_dataset(df_test, \"Test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0704bcca-013f-43b5-bb18-1129545bf6dc",
   "metadata": {},
   "source": [
    "# Trading Day Coverage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cf8120c-859c-4166-9b7c-8e35ada91aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wj/7nbyftys3g77fcp37r3t0_lm0000gn/T/ipykernel_15133/3474543099.py:7: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         avg_mkt_cap  trading_days  trading_coverage  PRC_missing_ratio  \\\n",
      "PERMNO                                                                    \n",
      "10107   2.867913e+08        4025.0          0.999007           0.000000   \n",
      "55976   2.215097e+08        4025.0          0.999007           0.000000   \n",
      "14542   1.984949e+08         440.0          0.109208           0.000000   \n",
      "22111   1.785950e+08          80.0          0.019856           0.000000   \n",
      "12490   1.685508e+08         491.0          0.121866           0.000000   \n",
      "90319   1.358228e+08        2862.0          0.710350           0.000000   \n",
      "10104   1.175756e+08        4025.0          0.999007           0.000000   \n",
      "13407   1.155092e+08         910.0          0.225862           0.000000   \n",
      "40483   9.506039e+07         261.0          0.064780           0.003831   \n",
      "92611   9.400759e+07        1172.0          0.290891           0.000000   \n",
      "\n",
      "        VOL_missing_ratio  SHROUT_missing_ratio  RET_missing_ratio  \n",
      "PERMNO                                                              \n",
      "10107            0.000000              0.000000           0.000000  \n",
      "55976            0.000000              0.000000           0.000000  \n",
      "14542            0.000000              0.000000           0.000000  \n",
      "22111            0.000000              0.000000           0.000000  \n",
      "12490            0.000000              0.000000           0.000000  \n",
      "90319            0.000000              0.000000           0.000000  \n",
      "10104            0.000000              0.000000           0.000000  \n",
      "13407            0.000000              0.000000           0.000000  \n",
      "40483            0.003831              0.003831           0.003831  \n",
      "92611            0.000000              0.000000           0.000000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wj/7nbyftys3g77fcp37r3t0_lm0000gn/T/ipykernel_15133/3474543099.py:29: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  summary = top_df.groupby('PERMNO').apply(lambda x: pd.Series({\n"
     ]
    }
   ],
   "source": [
    "def summarize_top_mktcap_quality(path, date_start, date_end, top_n=50):\n",
    "    df = pd.read_csv(path)\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df = df[(df['date'] >= date_start) & (df['date'] <= date_end)]\n",
    "    \n",
    "    df = df[~df['RET'].isin(['C', 'B'])]\n",
    "    df['RET'] = df['RET'].astype(float)\n",
    "    df['PRC'] = df['PRC'].astype(float).abs()\n",
    "    df['mkt_cap'] = df['PRC'] * df['SHROUT']\n",
    "\n",
    "    top_permnos = df.groupby('PERMNO')['mkt_cap'].mean().sort_values(ascending=False).head(top_n).index\n",
    "    top_df = df[df['PERMNO'].isin(top_permnos)].copy()\n",
    "    total_days = df['date'].nunique()\n",
    "\n",
    "    summary = top_df.groupby('PERMNO').apply(lambda x: pd.Series({\n",
    "        'avg_mkt_cap': x['mkt_cap'].mean(),\n",
    "        'trading_days': x['date'].nunique(),\n",
    "        'trading_coverage': x['date'].nunique() / total_days,\n",
    "        'PRC_missing_ratio': x['PRC'].isnull().mean(),\n",
    "        'VOL_missing_ratio': x['VOL'].isnull().mean(),\n",
    "        'SHROUT_missing_ratio': x['SHROUT'].isnull().mean(),\n",
    "        'RET_missing_ratio': x['RET'].isnull().mean()\n",
    "    }))\n",
    "\n",
    "    return summary.sort_values(by='avg_mkt_cap', ascending=False)\n",
    "\n",
    "summary_df = summarize_top_mktcap_quality(\n",
    "    path=\"CRSP_ConsumerDiscretionary_2000_2015.csv\",\n",
    "    date_start=\"2000-01-01\",\n",
    "    date_end=\"2015-12-31\",\n",
    "    top_n=50\n",
    ")\n",
    "\n",
    "print(summary_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1965e79d-9837-4db7-8276-33ffff22ca5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wj/7nbyftys3g77fcp37r3t0_lm0000gn/T/ipykernel_15133/3064706608.py:14: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of raw stocks before cleaning: 3265\n",
      "\n",
      "Missing values before dropping:\n",
      "PRC       253\n",
      "VOL       256\n",
      "SHROUT      6\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wj/7nbyftys3g77fcp37r3t0_lm0000gn/T/ipykernel_15133/3064706608.py:14: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of raw stocks before cleaning: 1346\n",
      "\n",
      "Missing values before dropping:\n",
      "PRC       1550\n",
      "VOL       1550\n",
      "SHROUT       0\n",
      "dtype: int64\n",
      "Train stocks: 338 Test stocks: 395\n",
      "\n",
      "Train dataset check\n",
      "\n",
      "Missing values:\n",
      "PERMNO    0\n",
      "date      0\n",
      "EXCHCD    0\n",
      "SICCD     0\n",
      "TICKER    0\n",
      "COMNAM    0\n",
      "PERMCO    0\n",
      "PRC       0\n",
      "VOL       0\n",
      "RET       0\n",
      "SHROUT    0\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "PERMNO             int64\n",
      "date      datetime64[ns]\n",
      "EXCHCD             int64\n",
      "SICCD              int64\n",
      "TICKER            object\n",
      "COMNAM            object\n",
      "PERMCO             int64\n",
      "PRC              float64\n",
      "VOL              float64\n",
      "RET              float64\n",
      "SHROUT           float64\n",
      "dtype: object\n",
      "\n",
      "Duplicated rows: 0\n",
      "Duplicated [PERMNO, date] pairs: 0\n",
      "\n",
      "Test dataset check\n",
      "\n",
      "Missing values:\n",
      "PERMNO    0\n",
      "date      0\n",
      "EXCHCD    0\n",
      "SICCD     0\n",
      "TICKER    0\n",
      "COMNAM    0\n",
      "PERMCO    0\n",
      "PRC       0\n",
      "VOL       0\n",
      "RET       0\n",
      "SHROUT    0\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "PERMNO             int64\n",
      "date      datetime64[ns]\n",
      "EXCHCD             int64\n",
      "SICCD              int64\n",
      "TICKER            object\n",
      "COMNAM            object\n",
      "PERMCO             int64\n",
      "PRC              float64\n",
      "VOL              float64\n",
      "RET              float64\n",
      "SHROUT           float64\n",
      "dtype: object\n",
      "\n",
      "Duplicated rows: 0\n",
      "Duplicated [PERMNO, date] pairs: 0\n",
      "\n",
      "Total flagged stocks (Train): 0\n",
      "Series([], Name: RET, dtype: float64)\n",
      "\n",
      "Total flagged stocks (Test): 0\n",
      "Series([], Name: RET, dtype: float64)\n"
     ]
    }
   ],
   "source": [
    "def clean_crsp_dataset(path, date_start, date_end, trading_coverage_threshold=0.99):\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    num_raw_stocks = df['PERMNO'].nunique()\n",
    "    print(\"Number of raw stocks before cleaning:\", num_raw_stocks)\n",
    "\n",
    "    df = df[(df['date'] >= date_start) & (df['date'] <= date_end)]\n",
    "\n",
    "    df = df[~df['RET'].isin(['C', 'B'])]\n",
    "    df['RET'] = df['RET'].astype(float)\n",
    "\n",
    "    df['PRC'] = df['PRC'].astype(float).abs()\n",
    "\n",
    "    total_days = df['date'].nunique()\n",
    "\n",
    "    stock_days = df.groupby('PERMNO')['date'].nunique()\n",
    "\n",
    "    valid_permnos = stock_days[stock_days >= total_days * trading_coverage_threshold].index\n",
    "    df = df[df['PERMNO'].isin(valid_permnos)]\n",
    "    \n",
    "    missing_summary = df[['PRC', 'VOL', 'SHROUT']].isnull().sum()\n",
    "    print(\"\\nMissing values before dropping:\")\n",
    "    print(missing_summary)\n",
    "    \n",
    "    df = df.dropna(subset=['PRC', 'VOL', 'SHROUT'])\n",
    "\n",
    "    df = df[df['EXCHCD'].isin([1, 2, 3])]\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_clean = clean_crsp_dataset(\n",
    "    path=\"CRSP_ConsumerDiscretionary_2000_2015.csv\",\n",
    "    date_start=\"2000-01-01\",\n",
    "    date_end=\"2015-12-31\"\n",
    ")\n",
    "\n",
    "test_clean = clean_crsp_dataset(\n",
    "    path=\"CRSP_ConsumerDiscretionary_2016_2024.csv\",\n",
    "    date_start=\"2016-01-01\",\n",
    "    date_end=\"2024-12-31\"\n",
    ")\n",
    "\n",
    "train_clean.to_csv(\"CRSP_2000_2015_cleaned.csv\", index=False)\n",
    "test_clean.to_csv(\"CRSP_2016_2024_cleaned.csv\", index=False)\n",
    "print(\"Train stocks:\", train_clean['PERMNO'].nunique(), \"Test stocks:\", test_clean['PERMNO'].nunique())\n",
    "\n",
    "train_clean = pd.read_csv(\"CRSP_2000_2015_cleaned.csv\", parse_dates=[\"date\"])\n",
    "test_clean = pd.read_csv(\"CRSP_2016_2024_cleaned.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "def final_check(df, name):\n",
    "    print(f\"\\n{name} dataset check\")\n",
    "\n",
    "    print(\"\\nMissing values:\")\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "    print(\"\\nData types:\")\n",
    "    print(df.dtypes)\n",
    "\n",
    "    print(\"\\nDuplicated rows:\", df.duplicated().sum())\n",
    "\n",
    "    duplicates = df.duplicated(subset=['PERMNO', 'date']).sum()\n",
    "    print(\"Duplicated [PERMNO, date] pairs:\", duplicates)\n",
    "\n",
    "    ret_zero_ratio = df.groupby(\"PERMNO\")[\"RET\"].apply(lambda x: (x == 0).mean())\n",
    "\n",
    "    threshold = 0.3\n",
    "    flagged_stocks = ret_zero_ratio[ret_zero_ratio > threshold]\n",
    "    return flagged_stocks\n",
    "\n",
    "train_flagged = final_check(train_clean, \"Train\")\n",
    "test_flagged = final_check(test_clean, \"Test\")\n",
    "\n",
    "print(f\"\\nTotal flagged stocks (Train): {len(train_flagged)}\")\n",
    "print(train_flagged.sort_values(ascending=False).head(10))\n",
    "\n",
    "print(f\"\\nTotal flagged stocks (Test): {len(test_flagged)}\")\n",
    "print(test_flagged.sort_values(ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635aad3d-5e80-4a9b-9a08-06606f96a420",
   "metadata": {},
   "source": [
    "# Stock Selection Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f20b08e3-2cc0-48ef-81b2-c054fd1e3da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] 市值排名样本数: 336\n",
      "[Result] 最终成功匹配的股票数量：50\n",
      " Successfully matched 50 consistent stocks across train & test (≥99% coverage).\n"
     ]
    }
   ],
   "source": [
    "def select_top50_from_train_and_match_test(\n",
    "    train_df, test_df, cutoff_date=\"2015-12-31\", top_n=50, coverage_threshold=0.99\n",
    "):\n",
    "    \"\"\"\n",
    "    Steps:\n",
    "    1. Select the top_n stocks by market cap on the last day of the training set.\n",
    "    2. Check if these stocks meet the coverage threshold in the test set.\n",
    "    3. If not enough stocks meet the threshold, continue to add more until top_n is reached.\n",
    "\n",
    "    Returns:\n",
    "    - train_top: Training set data (only top_n stocks)\n",
    "    - test_top: Test set data (only top_n stocks)\n",
    "    - final_permnos: List of selected PERMNOs (length = top_n)\n",
    "    \"\"\"\n",
    "\n",
    "    train_df['date'] = pd.to_datetime(train_df['date'])\n",
    "    test_df['date'] = pd.to_datetime(test_df['date'])\n",
    "\n",
    "    last_day = pd.Timestamp(cutoff_date)\n",
    "    cutoff_df = train_df[train_df['date'] == last_day].copy()\n",
    "    cutoff_df['mkt_cap'] = cutoff_df['PRC'].abs() * cutoff_df['SHROUT']\n",
    "    ranked_permnos = (\n",
    "        cutoff_df.groupby('PERMNO')['mkt_cap']\n",
    "        .mean()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "\n",
    "    print(f\"[Info] Number of stocks ranked by market cap: {len(ranked_permnos)}\")\n",
    "\n",
    "    test_total_days = test_df['date'].nunique()\n",
    "    test_days_by_permno = test_df.groupby(\"PERMNO\")['date'].nunique()\n",
    "\n",
    "    final_permnos = []\n",
    "    for permno in ranked_permnos.index:\n",
    "        coverage = test_days_by_permno.get(permno, 0) / test_total_days\n",
    "        if coverage >= coverage_threshold:\n",
    "            final_permnos.append(permno)\n",
    "        if len(final_permnos) >= top_n:\n",
    "            break\n",
    "\n",
    "    print(f\"[Result] Number of successfully matched stocks: {len(final_permnos)}\")\n",
    "    \n",
    "    train_top = train_df[train_df[\"PERMNO\"].isin(final_permnos)].copy()\n",
    "    test_top = test_df[test_df[\"PERMNO\"].isin(final_permnos)].copy()\n",
    "\n",
    "    return train_top, test_top, final_permnos\n",
    "\n",
    "train_clean = pd.read_csv(\"CRSP_2000_2015_cleaned.csv\", parse_dates=[\"date\"])\n",
    "test_clean = pd.read_csv(\"CRSP_2016_2024_cleaned.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "train_top50, test_top50, top_permnos = select_top50_from_train_and_match_test(\n",
    "    train_clean, test_clean, cutoff_date=\"2015-12-31\", top_n=50\n",
    ")\n",
    "\n",
    "train_top50.to_csv(\"CRSP_2000_2015_top50_cleaned.csv\", index=False)\n",
    "test_top50.to_csv(\"CRSP_2016_2024_top50_cleaned.csv\", index=False)\n",
    "\n",
    "print(f\"Successfully matched {len(top_permnos)} consistent stocks across train & test (>=99% coverage).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a23d39-8e6a-42a4-9c58-060db6cfb161",
   "metadata": {},
   "source": [
    "# Verify stock matching results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d09b0b-5710-4ef2-9eda-7df17f8cebb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Final Top 50 Stock Alignment Summary (Sorted by Train MktCap):\n",
      "\n",
      "       Train Ticker Test Ticker   Train MktCap Found in Test?  Rank\n",
      "PERMNO                                                             \n",
      "10107          MSFT        MSFT 286,791,277.76            Yes     1\n",
      "55976           WMT         WMT 221,509,682.96            Yes     2\n",
      "10104          ORCL        ORCL 117,575,646.85            Yes     3\n",
      "66181            HD          HD  84,937,210.31            Yes     4\n",
      "43449           MCD         MCD  62,168,463.49            Yes     5\n",
      "84788          AMZN        AMZN  59,388,985.09            Yes     6\n",
      "17005           CVS         CVS  43,649,507.02            Yes     7\n",
      "19502           WAG         WBA  42,213,913.37            Yes     8\n",
      "86356          EBAY        EBAY  40,972,449.07            Yes     9\n",
      "49154            DH         TGT  39,019,445.95            Yes    10\n",
      "61399           LOW         LOW  38,923,986.97            Yes    11\n",
      "87055          COST        COST  29,844,583.58            Yes    12\n",
      "44644           AUD         ADP  27,929,243.40            Yes    13\n",
      "77702          SBUX        SBUX  25,994,724.95            Yes    14\n",
      "40539           TJX         TJX  19,028,945.37            Yes    15\n",
      "52038           SYY         SYY  18,979,771.92            Yes    16\n",
      "85348           YUM         YUM  18,146,649.50            Yes    17\n",
      "75510          ADBE        ADBE  18,066,454.86            Yes    18\n",
      "16678            KR          KR  17,308,303.55            Yes    19\n",
      "77606           KSS         KSS  16,095,370.82            Yes    20\n",
      "59010           GPS         GPS  16,089,185.61            Yes    21\n",
      "30681           OMC         OMC  14,616,503.49            Yes    22\n",
      "78975          INTU        INTU  12,750,499.69            Yes    23\n",
      "75607          SYMC        SYMC  12,702,368.63            Yes    24\n",
      "86158          CTSH        CTSH  12,519,728.29            Yes    25\n",
      "85913           MAR         MAR  12,519,418.83            Yes    26\n",
      "77462            FD           M  12,333,319.55            Yes    27\n",
      "64282           LTD          LB  10,914,943.17            Yes    28\n",
      "75828          ERTS          EA  10,673,243.28            Yes    29\n",
      "76605           AZO         AZO   9,906,193.29            Yes    30\n",
      "10696          FISV        FISV   9,126,347.41            Yes    31\n",
      "83639          CHKP        CHKP   8,498,879.78            Yes    32\n",
      "52695           GWW         GWW   8,421,745.26            Yes    33\n",
      "57817           JWN         JWN   7,948,061.55            Yes    34\n",
      "46674           GPC         GPC   7,942,035.51            Yes    35\n",
      "91556          ROST        ROST   7,212,109.70            Yes    36\n",
      "85753          VRSN        VRSN   7,197,109.08            Yes    37\n",
      "11618          FAST        FAST   7,135,878.13            Yes    38\n",
      "85631          ADSK        ADSK   6,802,653.48            Yes    39\n",
      "86144           DOX         DOX   6,688,727.48            Yes    40\n",
      "79103          ORLY        ORLY   6,552,637.53            Yes    41\n",
      "53065           IPG         IPG   6,288,125.82            Yes    42\n",
      "81481          DLTR        DLTR   6,021,825.87            Yes    43\n",
      "87299          AKAM        AKAM   5,442,044.09            Yes    44\n",
      "81655           DRI         DRI   5,144,144.14            Yes    45\n",
      "52476           EFX         EFX   5,133,508.17            Yes    46\n",
      "82581          HSIC        HSIC   4,997,925.29            Yes    47\n",
      "15456             Z          FL   3,555,946.56            Yes    48\n",
      "80286          TSCO        TSCO   3,433,997.06            Yes    49\n",
      "83621          ANSS        ANSS   3,178,229.52            Yes    50\n"
     ]
    }
   ],
   "source": [
    "def print_final_top50_by_train_mktcap(train_df, test_df, top_permnos):\n",
    "    \"\"\"\n",
    "    Print the final Top 50 stock information (sorted by train set market cap).\n",
    "    Output columns:\n",
    "    PERMNO, Train Ticker, Test Ticker, Train MktCap, Found in Test?, Rank\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate market cap\n",
    "    train_df['mkt_cap'] = train_df['PRC'] * train_df['SHROUT']\n",
    "    test_df['mkt_cap'] = test_df['PRC'] * test_df['SHROUT']\n",
    "\n",
    "    # Average market cap and TICKER\n",
    "    train_mktcap = train_df.groupby(\"PERMNO\")['mkt_cap'].mean()\n",
    "    train_ticker = train_df.groupby(\"PERMNO\")['TICKER'].first()\n",
    "    test_ticker = test_df.groupby(\"PERMNO\")['TICKER'].first()\n",
    "\n",
    "    # Merge tables (only use train's market cap for sorting)\n",
    "    df = pd.DataFrame({\n",
    "        \"Train Ticker\": train_ticker,\n",
    "        \"Test Ticker\": test_ticker,\n",
    "        \"Train MktCap\": train_mktcap\n",
    "    }).loc[top_permnos]\n",
    "\n",
    "    # Whether successfully matched in test set\n",
    "    df[\"Found in Test?\"] = df[\"Test Ticker\"].notnull().replace({True: \"Yes\", False: \"No\"})\n",
    "\n",
    "    # Sort by train market cap and assign rank\n",
    "    df = df.sort_values(\"Train MktCap\", ascending=False)\n",
    "    df[\"Rank\"] = range(1, len(df) + 1)\n",
    "\n",
    "    # Output\n",
    "    print(\"\\n Final Top 50 Stock Alignment Summary (Sorted by Train MktCap):\\n\")\n",
    "    print(df[[\"Train Ticker\", \"Test Ticker\", \"Train MktCap\", \"Found in Test?\", \"Rank\"]]\n",
    "          .to_string(formatters={\"Train MktCap\": \"{:,.2f}\".format}))\n",
    "\n",
    "print_final_top50_by_train_mktcap(train_top50, test_top50, top_permnos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7d43b6-3385-46a9-bcd2-68e4884cf789",
   "metadata": {},
   "source": [
    "# Final Dataset Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31777dfd-d0b8-49ea-9387-da0be9a8f001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Dataset Check: Train Top 50\n",
      "- Shape: (201218, 11)\n",
      "- Date range: 2000-01-03 to 2015-12-31\n",
      "- Missing values:\n",
      "PERMNO    0\n",
      "date      0\n",
      "EXCHCD    0\n",
      "SICCD     0\n",
      "TICKER    0\n",
      "COMNAM    0\n",
      "PERMCO    0\n",
      "PRC       0\n",
      "VOL       0\n",
      "RET       0\n",
      "SHROUT    0\n",
      "dtype: int64\n",
      "- Total rows with any NaN: 0\n",
      "- Duplicate rows: 0\n",
      "- PERMNO + date duplicates: 0\n",
      "- Unique PERMNOs: 50\n",
      "- Unique TICKERs: 57\n",
      "\n",
      " Dataset Check: Test Top 50\n",
      "- Shape: (113200, 11)\n",
      "- Date range: 2016-01-04 to 2024-12-31\n",
      "- Missing values:\n",
      "PERMNO    0\n",
      "date      0\n",
      "EXCHCD    0\n",
      "SICCD     0\n",
      "TICKER    0\n",
      "COMNAM    0\n",
      "PERMCO    0\n",
      "PRC       0\n",
      "VOL       0\n",
      "RET       0\n",
      "SHROUT    0\n",
      "dtype: int64\n",
      "- Total rows with any NaN: 0\n",
      "- Duplicate rows: 0\n",
      "- PERMNO + date duplicates: 0\n",
      "- Unique PERMNOs: 50\n",
      "- Unique TICKERs: 55\n"
     ]
    }
   ],
   "source": [
    "train_top50 = pd.read_csv(\"CRSP_2000_2015_top50_cleaned.csv\", parse_dates=[\"date\"])\n",
    "test_top50 = pd.read_csv(\"CRSP_2016_2024_top50_cleaned.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "def check_dataset(df, name):\n",
    "    print(f\"\\n Dataset Check: {name}\")\n",
    "    print(f\"- Shape: {df.shape}\")\n",
    "    print(f\"- Date range: {df['date'].min().date()} to {df['date'].max().date()}\")\n",
    "    print(\"- Missing values:\")\n",
    "    print(df.isnull().sum())\n",
    "    print(\"- Total rows with any NaN:\", df.isnull().any(axis=1).sum())\n",
    "    print(\"- Duplicate rows:\", df.duplicated().sum())\n",
    "    print(\"- PERMNO + date duplicates:\", df.duplicated(subset=[\"PERMNO\", \"date\"]).sum())\n",
    "    print(\"- Unique PERMNOs:\", df['PERMNO'].nunique())\n",
    "    print(\"- Unique TICKERs:\", df['TICKER'].nunique())\n",
    "\n",
    "check_dataset(train_top50, \"Train Top 50\")\n",
    "check_dataset(test_top50, \"Test Top 50\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12a89a8-d950-463b-835b-e2df56474731",
   "metadata": {},
   "source": [
    "# Factor Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70511381-584f-4e27-a1fb-dc124df01232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factor dataset check:\n",
      "- Shape: (6289, 8)\n",
      "- Columns: ['date', 'mktrf', 'smb', 'hml', 'rmw', 'cma', 'rf', 'umd']\n",
      "- Date range: 2000-01-03 to 2024-12-31\n",
      "\n",
      "Missing values per column:\n",
      "date     0\n",
      "mktrf    0\n",
      "smb      0\n",
      "hml      0\n",
      "rmw      0\n",
      "cma      0\n",
      "rf       0\n",
      "umd      0\n",
      "dtype: int64\n",
      "\n",
      "Duplicated dates: 0\n",
      "\n",
      "Preview of the first 5 rows:\n",
      "        date   mktrf     smb     hml     rmw     cma       rf     umd\n",
      "0 2000-01-03 -0.0071 -0.0006 -0.0141 -0.0150 -0.0064  0.00021 -0.0008\n",
      "1 2000-01-04 -0.0406  0.0033  0.0206  0.0047  0.0145  0.00021 -0.0191\n",
      "2 2000-01-05 -0.0009  0.0033  0.0016  0.0041  0.0111  0.00021 -0.0049\n",
      "3 2000-01-06 -0.0073 -0.0004  0.0126  0.0065  0.0121  0.00021 -0.0149\n",
      "4 2000-01-07  0.0321 -0.0094 -0.0142 -0.0088 -0.0096  0.00021  0.0056\n",
      "2000-01-01 is: Saturday\n",
      "2000-01-02 is: Sunday\n"
     ]
    }
   ],
   "source": [
    "def analyze_factors(file_path: str) -> pd.DataFrame:\n",
    "    factors = pd.read_csv(file_path, parse_dates=[\"date\"])\n",
    "\n",
    "    print(\"Factor dataset check:\")\n",
    "    print(\"- Shape:\", factors.shape)\n",
    "    print(\"- Columns:\", list(factors.columns))\n",
    "    print(\"- Date range:\", f\"{factors['date'].min().date()} to {factors['date'].max().date()}\")\n",
    "\n",
    "    print(\"\\nMissing values per column:\")\n",
    "    print(factors.isnull().sum())\n",
    "\n",
    "    dup_dates = factors.duplicated(subset=\"date\").sum()\n",
    "    print(\"\\nDuplicated dates:\", dup_dates)\n",
    "\n",
    "    print(\"\\nPreview of the first 5 rows:\")\n",
    "    print(factors.head())\n",
    "\n",
    "    for test_date in [\"2000-01-01\", \"2000-01-02\"]:\n",
    "        day = calendar.day_name[pd.to_datetime(test_date).weekday()]\n",
    "        print(f\"{test_date} is: {day}\")\n",
    "\n",
    "    return factors\n",
    "\n",
    "df = analyze_factors(\"5_Factors_Plus_Momentum.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9878105-c3f7-453e-97ec-cbf66e9a5e70",
   "metadata": {},
   "source": [
    "# Risk-Free Rate Data Alignment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96788638-68fe-494b-aa2f-61ab36636001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "缺失 rf 的交易日（前 10 个）:\n",
      "总共缺失 0 个日期\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"CRSP_2016_2024_top50_cleaned.csv\", parse_dates=[\"date\"])\n",
    "factors = pd.read_csv(\"5_Factors_Plus_Momentum.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "test_df[\"date\"] = pd.to_datetime(test_df[\"date\"]).dt.normalize()\n",
    "factors[\"date\"] = pd.to_datetime(factors[\"date\"]).dt.normalize()\n",
    "\n",
    "missing_dates = test_df[~test_df[\"date\"].isin(factors[\"date\"])][\"date\"].unique()\n",
    "\n",
    "print(\"Missing rf trading days (first 10):\")\n",
    "for dt in missing_dates[:10]:\n",
    "    print(f\"{dt.date()} (weekday={dt.weekday()})\")\n",
    "\n",
    "print(f\"Total missing dates: {len(missing_dates)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8250729e-4a6a-40ca-9590-099252ac96ca",
   "metadata": {},
   "source": [
    "# Final Training and Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5d2a7c5-ef00-479f-9073-c72ef4dcbaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing rf in train: 0, in test: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wj/7nbyftys3g77fcp37r3t0_lm0000gn/T/ipykernel_15133/4272901896.py:61: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train_cleaned = train_merged.groupby(\"PERMNO\").apply(remove_outliers).reset_index(drop=True)\n",
      "/var/folders/wj/7nbyftys3g77fcp37r3t0_lm0000gn/T/ipykernel_15133/4272901896.py:62: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test_cleaned = test_merged.groupby(\"PERMNO\").apply(remove_outliers).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets merged and excess returns calculated successfully.\n",
      "Original train size: 201218, cleaned train size: 197120\n",
      "Original test size: 113200, cleaned test size: 110900\n"
     ]
    }
   ],
   "source": [
    "def prepare_excess_return_data(\n",
    "    train_file: str,\n",
    "    test_file: str,\n",
    "    factor_file: str,\n",
    "    output_train_file: str,\n",
    "    output_test_file: str,\n",
    "    output_train_cleaned_file: str,\n",
    "    output_test_cleaned_file: str\n",
    ") -> None:\n",
    "\n",
    "    train_df = pd.read_csv(train_file, parse_dates=[\"date\"])\n",
    "    test_df = pd.read_csv(test_file, parse_dates=[\"date\"])\n",
    "    factors = pd.read_csv(factor_file, parse_dates=[\"date\"])\n",
    "\n",
    "    train_df[\"date\"] = pd.to_datetime(train_df[\"date\"]).dt.normalize()\n",
    "    test_df[\"date\"] = pd.to_datetime(test_df[\"date\"]).dt.normalize()\n",
    "    factors[\"date\"] = pd.to_datetime(factors[\"date\"]).dt.normalize()\n",
    "\n",
    "    factors = factors[[\"date\", \"rf\"]]\n",
    "\n",
    "    train_merged = pd.merge(train_df, factors, on=\"date\", how=\"left\")\n",
    "    test_merged = pd.merge(test_df, factors, on=\"date\", how=\"left\")\n",
    "\n",
    "    missing_train = train_merged[\"rf\"].isna().sum()\n",
    "    missing_test = test_merged[\"rf\"].isna().sum()\n",
    "    print(f\"Missing rf in train: {missing_train}, in test: {missing_test}\")\n",
    "\n",
    "    train_merged = train_merged.dropna(subset=[\"rf\"])\n",
    "    test_merged = test_merged.dropna(subset=[\"rf\"])\n",
    "\n",
    "    train_merged[\"EXRET\"] = train_merged[\"RET\"] - train_merged[\"rf\"]\n",
    "    test_merged[\"EXRET\"] = test_merged[\"RET\"] - test_merged[\"rf\"]\n",
    "\n",
    "    train_merged = train_merged.dropna(subset=[\"EXRET\"])\n",
    "    test_merged = test_merged.dropna(subset=[\"EXRET\"])\n",
    "    \n",
    "    train_merged = train_merged.sort_values(by=[\"PERMNO\", \"date\"]).reset_index(drop=True)\n",
    "    test_merged = test_merged.sort_values(by=[\"PERMNO\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "    train_merged.to_csv(output_train_file, index=False)\n",
    "    test_merged.to_csv(output_test_file, index=False)\n",
    "\n",
    "    def remove_outliers(group):\n",
    "        lower = group[\"EXRET\"].quantile(0.01)\n",
    "        upper = group[\"EXRET\"].quantile(0.99)\n",
    "        return group[(group[\"EXRET\"] >= lower) & (group[\"EXRET\"] <= upper)]\n",
    "\n",
    "    train_cleaned = train_merged.groupby(\"PERMNO\").apply(remove_outliers).reset_index(drop=True)\n",
    "    test_cleaned = test_merged.groupby(\"PERMNO\").apply(remove_outliers).reset_index(drop=True)\n",
    "\n",
    "    train_cleaned.to_csv(output_train_cleaned_file, index=False)\n",
    "    test_cleaned.to_csv(output_test_cleaned_file, index=False)\n",
    "\n",
    "    print(\"Datasets merged and excess returns calculated successfully.\")\n",
    "    print(f\"Original train size: {len(train_merged)}, cleaned train size: {len(train_cleaned)}\")\n",
    "    print(f\"Original test size: {len(test_merged)}, cleaned test size: {len(test_cleaned)}\")\n",
    "\n",
    "\n",
    "prepare_excess_return_data(\n",
    "    \"CRSP_2000_2015_top50_cleaned.csv\",\n",
    "    \"CRSP_2016_2024_top50_cleaned.csv\",\n",
    "    \"5_Factors_Plus_Momentum.csv\",\n",
    "    \"CRSP_2000_2015_top50_with_exret.csv\",\n",
    "    \"CRSP_2016_2024_top50_with_exret.csv\",\n",
    "    \"CRSP_2000_2015_top50_with_exret_cleaned.csv\",\n",
    "    \"CRSP_2016_2024_top50_with_exret_cleaned.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12e078b1-197a-4cf8-9a82-b938f48a9ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weekday\n",
      "1    22904\n",
      "2    22578\n",
      "4    22378\n",
      "3    22346\n",
      "0    20694\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"CRSP_2016_2024_top50_with_exret_cleaned.csv\", parse_dates=[\"date\"])\n",
    "df[\"weekday\"] = df[\"date\"].dt.dayofweek\n",
    "print(df[\"weekday\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443b0455-e51e-451b-81a4-269783187d70",
   "metadata": {},
   "source": [
    "# Sliding Window Dataset Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283be9d3",
   "metadata": {},
   "source": [
    "# X Standardization with Original Y Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a6e5981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FULL | Window = 5 | X shape = (307770, 5) | y shape = (307770,)\n",
      " FULL | Window = 21 | X shape = (306970, 21) | y shape = (306970,)\n",
      " FULL | Window = 252 | X shape = (295420, 252) | y shape = (295420,)\n",
      " FULL | Window = 512 | X shape = (282420, 512) | y shape = (282420,)\n",
      "FINISH  Window=5  Train=(196920, 5)  Test=(110850, 5)\n",
      "FINISH  Window=21  Train=(196120, 21)  Test=(110850, 21)\n",
      "FINISH  Window=252  Train=(184570, 252)  Test=(110850, 252)\n",
      "FINISH  Window=512  Train=(171570, 512)  Test=(110850, 512)\n",
      "\n",
      "All datasets saved safely to: all_window_datasets.npz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_datasets_for_multiple_windows(\n",
    "    df: pd.DataFrame,\n",
    "    window_sizes: List[int] = [5, 21, 252, 512],\n",
    "    prefix: str = \"train\",\n",
    "):\n",
    "    \"\"\"Generate rolling window datasets for backtesting (past *win* days → predict *t+1* day).\n",
    "\n",
    "    *Only* the definition of the target y is modified:\n",
    "        X_t   = [EXRET_{t-win}, …, EXRET_{t-1}]\n",
    "        y_t+1 =  EXRET_{t+1}\n",
    "\n",
    "  \n",
    "    \"\"\"\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"PRC\"] = df[\"PRC\"].abs()\n",
    "\n",
    "    df_by_date = {d: g for d, g in df.groupby(\"date\")}\n",
    "\n",
    "    for win in window_sizes:\n",
    "        X_list, y_list, meta_list = [], [], []\n",
    "\n",
    "        for permno, group in df.groupby(\"PERMNO\"):\n",
    "            group = group.sort_values(\"date\")\n",
    "            exret = group[\"EXRET\"].values\n",
    "            dates = group[\"date\"].values\n",
    "            shroud = group[\"SHROUT\"].values\n",
    "            prc = group[\"PRC\"].values\n",
    "            mktcap = prc * shroud\n",
    "\n",
    "            for i in range(win-1, len(group) - 1):\n",
    "                \n",
    "                signal_date = pd.Timestamp(dates[i])\n",
    "                ret_date    = pd.Timestamp(dates[i + 1])\n",
    "                current_date  = signal_date\n",
    "                current_mktcap = mktcap[i]\n",
    "\n",
    "                peers = df_by_date[current_date]\n",
    "                all_mktcap = peers[\"PRC\"].abs().values * peers[\"SHROUT\"].values\n",
    "                rank = (all_mktcap <= current_mktcap).sum()\n",
    "                mktcap_percentile = rank / len(all_mktcap)\n",
    "\n",
    "                X_list.append(exret[i - win + 1 : i + 1])\n",
    "                y_list.append(exret[i + 1])\n",
    "\n",
    "                meta_list.append(\n",
    "                    {\n",
    "                        \"PERMNO\": permno,\n",
    "                        \"date\": signal_date,\n",
    "                        \"ret_date\"   : ret_date,\n",
    "                        \"MKTCAP\": current_mktcap,\n",
    "                        \"MKTCAP_PERCENTILE\": mktcap_percentile,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        X_arr = np.array(X_list)\n",
    "        y_arr = np.array(y_list)\n",
    "        meta_df = pd.DataFrame(meta_list)\n",
    "        meta_df[\"signal_month\"] = meta_df[\"date\"].dt.to_period(\"M\")\n",
    "        meta_df[\"ret_month\"]    = meta_df[\"ret_date\"].dt.to_period(\"M\")\n",
    "        order = meta_df[\"ret_date\"].argsort().values\n",
    "        X_arr, y_arr = X_arr[order], y_arr[order]\n",
    "        meta_df = meta_df.iloc[order].reset_index(drop=True)\n",
    "\n",
    "        results[f\"X_{prefix}_{win}\"] = X_arr\n",
    "        results[f\"y_{prefix}_{win}\"] = y_arr\n",
    "        results[f\"meta_{prefix}_{win}\"] = meta_df\n",
    "\n",
    "        print(\n",
    "            f\" {prefix.upper()} | Window = {win} | X shape = {X_arr.shape} | y shape = {y_arr.shape}\"\n",
    "        )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def unpack_window_datasets(\n",
    "    train_datasets: Dict[str, object],\n",
    "    test_datasets: Dict[str, object],\n",
    "    window_sizes: List[int] = [5, 21, 252, 512],\n",
    "):\n",
    "    for win in window_sizes:\n",
    "        globals()[f\"X_train_{win}\"] = train_datasets[f\"X_train_{win}\"]\n",
    "        globals()[f\"y_train_{win}\"] = train_datasets[f\"y_train_{win}\"]\n",
    "        globals()[f\"meta_train_{win}\"] = train_datasets.get(f\"meta_train_{win}\", None)\n",
    "\n",
    "        globals()[f\"X_test_{win}\"] = test_datasets[f\"X_test_{win}\"]\n",
    "        globals()[f\"y_test_{win}\"] = test_datasets[f\"y_test_{win}\"]\n",
    "        globals()[f\"meta_test_{win}\"] = test_datasets.get(f\"meta_test_{win}\", None)\n",
    "\n",
    "        print(\n",
    "            f\" Unpacked → X_train_{win}, y_train_{win}, X_test_{win}, y_test_{win}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def prepare_and_save_window_data(\n",
    "    train_csv_path: str,\n",
    "    test_csv_path: str,\n",
    "    window_sizes: List[int] = [5, 21, 252, 512],\n",
    "    output_npz_path: str = \"all_window_datasets.npz\",\n",
    "    split_date: str = \"2016-01-01\",\n",
    "):\n",
    "    train_df = pd.read_csv(train_csv_path, parse_dates=[\"date\"])\n",
    "    test_df  = pd.read_csv(test_csv_path , parse_dates=[\"date\"])\n",
    "    full_df  = pd.concat([train_df, test_df], ignore_index=True).sort_values([\"PERMNO\",\"date\"])\n",
    "\n",
    "    full_sets = generate_datasets_for_multiple_windows(full_df, window_sizes, \"full\")\n",
    "\n",
    "    save_dict = {}\n",
    "\n",
    "    for win in window_sizes:\n",
    "        X_full   = full_sets[f\"X_full_{win}\"]\n",
    "        y_full   = full_sets[f\"y_full_{win}\"]\n",
    "        meta_full = full_sets[f\"meta_full_{win}\"]\n",
    "\n",
    "        mask_train = meta_full[\"ret_date\"] < split_date\n",
    "\n",
    "        for lab, m in [(\"train\", mask_train), (\"test\", ~mask_train)]:\n",
    "            X_part, y_part, meta_part = X_full[m], y_full[m], meta_full[m].reset_index(drop=True)\n",
    "            save_dict[f\"X_{lab}_{win}\"]   = X_part\n",
    "            save_dict[f\"y_{lab}_{win}\"]   = y_part\n",
    "            save_dict[f\"meta_{lab}_{win}\"] = meta_part.to_dict()\n",
    "            save_dict[f\"market_caps_{lab}_{win}\"] = meta_part[\"MKTCAP\"].values\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(save_dict[f\"X_train_{win}\"])\n",
    "        X_test_scaled  = scaler.transform(save_dict[f\"X_test_{win}\"])\n",
    "\n",
    "        save_dict[f\"X_train_{win}\"] = X_train_scaled\n",
    "        save_dict[f\"X_test_{win}\"]  = X_test_scaled\n",
    "        joblib.dump(scaler, f\"scaler_window_{win}.pkl\")\n",
    "\n",
    "        print(f\"FINISH  Window={win}  Train={X_train_scaled.shape}  Test={X_test_scaled.shape}\")\n",
    "\n",
    "    np.savez(output_npz_path, **save_dict)\n",
    "    print(f\"\\nAll datasets saved safely to: {output_npz_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prepare_and_save_window_data(\n",
    "        train_csv_path=\"CRSP_2000_2015_top50_with_exret_cleaned.csv\",\n",
    "        test_csv_path=\"CRSP_2016_2024_top50_with_exret_cleaned.csv\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa89b1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Train Window 5 ===\n",
      "X_train_5 shape: (196920, 5)\n",
      "y_train_5 shape: (196920,)\n",
      "meta_train_5 head:\n",
      "   PERMNO       date   ret_date        MKTCAP  MKTCAP_PERCENTILE signal_month  \\\n",
      "0   44644 2000-01-07 2000-01-10  3.336514e+07           0.883721      2000-01   \n",
      "1   15456 2000-01-07 2000-01-10  9.109508e+05           0.116279      2000-01   \n",
      "2   81481 2000-01-07 2000-01-10  3.198164e+06           0.255814      2000-01   \n",
      "3   77702 2000-01-07 2000-01-10  4.554635e+06           0.348837      2000-01   \n",
      "4   16678 2000-01-07 2000-01-10  1.575560e+07           0.674419      2000-01   \n",
      "\n",
      "  ret_month  \n",
      "0   2000-01  \n",
      "1   2000-01  \n",
      "2   2000-01  \n",
      "3   2000-01  \n",
      "4   2000-01  \n",
      "--------------------------------------------------\n",
      "=== Test Window 5 ===\n",
      "X_test_5 shape: (110850, 5)\n",
      "y_test_5 shape: (110850,)\n",
      "meta_test_5 head:\n",
      "   PERMNO       date   ret_date       MKTCAP  MKTCAP_PERCENTILE signal_month  \\\n",
      "0   75510 2016-01-04 2016-01-05  45783493.73           0.775510      2016-01   \n",
      "1   81655 2016-01-04 2016-01-05   8040722.25           0.040816      2016-01   \n",
      "2   61399 2016-01-04 2016-01-05  68889104.40           0.816327      2016-01   \n",
      "3   75607 2016-01-04 2016-01-05  13691880.00           0.387755      2016-01   \n",
      "4   59010 2016-01-04 2016-01-05  10252520.02           0.204082      2016-01   \n",
      "\n",
      "  ret_month  \n",
      "0   2016-01  \n",
      "1   2016-01  \n",
      "2   2016-01  \n",
      "3   2016-01  \n",
      "4   2016-01  \n",
      "--------------------------------------------------\n",
      "0   2016-01-04\n",
      "1   2016-01-04\n",
      "2   2016-01-04\n",
      "3   2016-01-04\n",
      "4   2016-01-04\n",
      "Name: date, dtype: datetime64[ns]\n",
      "[0.004023 0.018167 0.002125 0.001919 0.030184]\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"all_window_datasets.npz\", allow_pickle=True)\n",
    "\n",
    "X_train_5 = data[\"X_train_5\"]\n",
    "y_train_5 = data[\"y_train_5\"]\n",
    "meta_train_5 = pd.DataFrame(data[\"meta_train_5\"].item())\n",
    "\n",
    "X_test_5 = data[\"X_test_5\"]\n",
    "y_test_5 = data[\"y_test_5\"]\n",
    "meta_test_5 = pd.DataFrame(data[\"meta_test_5\"].item())\n",
    "\n",
    "print(\"=== Train Window 5 ===\")\n",
    "print(\"X_train_5 shape:\", X_train_5.shape)\n",
    "print(\"y_train_5 shape:\", y_train_5.shape)\n",
    "print(\"meta_train_5 head:\")\n",
    "print(meta_train_5.head())\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"=== Test Window 5 ===\")\n",
    "print(\"X_test_5 shape:\", X_test_5.shape)\n",
    "print(\"y_test_5 shape:\", y_test_5.shape)\n",
    "print(\"meta_test_5 head:\")\n",
    "print(meta_test_5.head())\n",
    "print(\"-\" * 50)\n",
    "print(meta_test_5['date'][:5])\n",
    "print(y_test_5[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf20a635",
   "metadata": {},
   "source": [
    "# Unscaled Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3018d596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FULL | Window = 5 | X shape = (307770, 5) | y shape = (307770,)\n",
      " FULL | Window = 21 | X shape = (306970, 21) | y shape = (306970,)\n",
      " FULL | Window = 252 | X shape = (295420, 252) | y shape = (295420,)\n",
      " FULL | Window = 512 | X shape = (282420, 512) | y shape = (282420,)\n",
      "FINISH  Window=5  Train=(196920, 5)  Test=(110850, 5)\n",
      "FINISH  Window=21  Train=(196120, 21)  Test=(110850, 21)\n",
      "FINISH  Window=252  Train=(184570, 252)  Test=(110850, 252)\n",
      "FINISH  Window=512  Train=(171570, 512)  Test=(110850, 512)\n",
      "\n",
      "All datasets saved safely to: all_window_datasets_unscaled.npz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_datasets_for_multiple_windows(\n",
    "    df: pd.DataFrame,\n",
    "    window_sizes: List[int] = [5, 21, 252, 512],\n",
    "    prefix: str = \"train\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate rolling window datasets for backtesting (past *win* days → predict *t+1* day).\n",
    "\n",
    "    Only the definition of target y is changed:\n",
    "        X_t   = [EXRET_{t-win}, ..., EXRET_{t-1}]\n",
    "        y_t+1 =  EXRET_{t+1}\n",
    "\n",
    "    Other fields and output structure remain the same as previous version.\n",
    "    \"\"\"\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"PRC\"] = df[\"PRC\"].abs()  # Take absolute value of price to avoid negative sign\n",
    "\n",
    "    # Cache the table for each date in advance for market cap percentile calculation\n",
    "    df_by_date = {d: g for d, g in df.groupby(\"date\")}\n",
    "\n",
    "    for win in window_sizes:\n",
    "        X_list, y_list, meta_list = [], [], []\n",
    "\n",
    "        for permno, group in df.groupby(\"PERMNO\"):\n",
    "            group = group.sort_values(\"date\")\n",
    "            exret = group[\"EXRET\"].values\n",
    "            dates = group[\"date\"].values\n",
    "            shroud = group[\"SHROUT\"].values\n",
    "            prc = group[\"PRC\"].values\n",
    "            mktcap = prc * shroud\n",
    "\n",
    "            for i in range(win-1, len(group) - 1):\n",
    "                signal_date = pd.Timestamp(dates[i])\n",
    "                ret_date    = pd.Timestamp(dates[i + 1])\n",
    "                current_date  = signal_date\n",
    "                current_mktcap = mktcap[i]\n",
    "\n",
    "                peers = df_by_date[current_date]\n",
    "                all_mktcap = peers[\"PRC\"].abs().values * peers[\"SHROUT\"].values\n",
    "                rank = (all_mktcap <= current_mktcap).sum()\n",
    "                mktcap_percentile = rank / len(all_mktcap)\n",
    "\n",
    "                X_list.append(exret[i - win + 1 : i + 1])\n",
    "                y_list.append(exret[i + 1])\n",
    "\n",
    "                meta_list.append(\n",
    "                    {\n",
    "                        \"PERMNO\": permno,\n",
    "                        \"date\": signal_date,\n",
    "                        \"ret_date\": ret_date,\n",
    "                        \"MKTCAP\": current_mktcap,\n",
    "                        \"MKTCAP_PERCENTILE\": mktcap_percentile,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        X_arr = np.array(X_list)\n",
    "        y_arr = np.array(y_list)\n",
    "        meta_df = pd.DataFrame(meta_list)\n",
    "        meta_df[\"signal_month\"] = meta_df[\"date\"].dt.to_period(\"M\")\n",
    "        meta_df[\"ret_month\"]    = meta_df[\"ret_date\"].dt.to_period(\"M\")\n",
    "        order = meta_df[\"ret_date\"].argsort().values\n",
    "        X_arr, y_arr = X_arr[order], y_arr[order]\n",
    "        meta_df = meta_df.iloc[order].reset_index(drop=True)\n",
    "\n",
    "        results[f\"X_{prefix}_{win}\"] = X_arr\n",
    "        results[f\"y_{prefix}_{win}\"] = y_arr\n",
    "        results[f\"meta_{prefix}_{win}\"] = meta_df\n",
    "\n",
    "        print(\n",
    "            f\"{prefix.upper()} | Window = {win} | X shape = {X_arr.shape} | y shape = {y_arr.shape}\"\n",
    "        )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def unpack_window_datasets(\n",
    "    train_datasets: Dict[str, object],\n",
    "    test_datasets: Dict[str, object],\n",
    "    window_sizes: List[int] = [5, 21, 252, 512],\n",
    "):\n",
    "    for win in window_sizes:\n",
    "        globals()[f\"X_train_{win}\"] = train_datasets[f\"X_train_{win}\"]\n",
    "        globals()[f\"y_train_{win}\"] = train_datasets[f\"y_train_{win}\"]\n",
    "        globals()[f\"meta_train_{win}\"] = train_datasets.get(f\"meta_train_{win}\", None)\n",
    "\n",
    "        globals()[f\"X_test_{win}\"] = test_datasets[f\"X_test_{win}\"]\n",
    "        globals()[f\"y_test_{win}\"] = test_datasets[f\"y_test_{win}\"]\n",
    "        globals()[f\"meta_test_{win}\"] = test_datasets.get(f\"meta_test_{win}\", None)\n",
    "\n",
    "        print(\n",
    "            f\"Unpacked: X_train_{win}, y_train_{win}, X_test_{win}, y_test_{win}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def prepare_and_save_window_data(\n",
    "    train_csv_path: str,\n",
    "    test_csv_path: str,\n",
    "    window_sizes: List[int] = [5, 21, 252, 512],\n",
    "    output_npz_path: str = \"all_window_datasets_unscaled.npz\",\n",
    "    split_date: str = \"2016-01-01\",\n",
    "):\n",
    "    train_df = pd.read_csv(train_csv_path, parse_dates=[\"date\"])\n",
    "    test_df = pd.read_csv(test_csv_path, parse_dates=[\"date\"])\n",
    "    full_df = pd.concat([train_df, test_df], ignore_index=True).sort_values([\"PERMNO\", \"date\"])\n",
    "\n",
    "    full_sets = generate_datasets_for_multiple_windows(full_df, window_sizes, \"full\")\n",
    "    save_dict = {}\n",
    "\n",
    "    for win in window_sizes:\n",
    "        X_full = full_sets[f\"X_full_{win}\"]\n",
    "        y_full = full_sets[f\"y_full_{win}\"]\n",
    "        meta_full = full_sets[f\"meta_full_{win}\"]\n",
    "\n",
    "        mask_train = meta_full[\"ret_date\"] < split_date\n",
    "\n",
    "        for lab, m in [(\"train\", mask_train), (\"test\", ~mask_train)]:\n",
    "            X_part = X_full[m]\n",
    "            y_part = y_full[m]\n",
    "            meta_part = meta_full[m].reset_index(drop=True)\n",
    "\n",
    "            save_dict[f\"X_{lab}_{win}\"] = X_part\n",
    "            save_dict[f\"y_{lab}_{win}\"] = y_part\n",
    "            save_dict[f\"meta_{lab}_{win}\"] = meta_part.to_dict()\n",
    "            save_dict[f\"market_caps_{lab}_{win}\"] = meta_part[\"MKTCAP\"].values\n",
    "\n",
    "        print(f\"FINISH Window={win} Train={X_full[mask_train].shape} Test={X_full[~mask_train].shape}\")\n",
    "\n",
    "    np.savez(output_npz_path, **save_dict)\n",
    "    print(f\"\\nAll datasets saved safely to: {output_npz_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prepare_and_save_window_data(\n",
    "        train_csv_path=\"CRSP_2000_2015_top50_with_exret_cleaned.csv\",\n",
    "        test_csv_path=\"CRSP_2016_2024_top50_with_exret_cleaned.csv\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e96fef",
   "metadata": {},
   "source": [
    "# Scaled Data for N-Beats and Autoformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad7b2b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FULL | Window = 5 | X shape = (307770, 5) | y shape = (307770,)\n",
      " FULL | Window = 21 | X shape = (306970, 21) | y shape = (306970,)\n",
      " FULL | Window = 252 | X shape = (295420, 252) | y shape = (295420,)\n",
      " FULL | Window = 512 | X shape = (282420, 512) | y shape = (282420,)\n",
      "[Saved] Window=5 | Train=(196920, 5) | Test=(110850, 5)\n",
      "[Saved] Window=21 | Train=(196120, 21) | Test=(110850, 21)\n",
      "[Saved] Window=252 | Train=(184570, 252) | Test=(110850, 252)\n",
      "[Saved] Window=512 | Train=(171570, 512) | Test=(110850, 512)\n",
      "\n",
      "[FINISH] All datasets saved to: all_window_datasets_scaled.npz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_datasets_for_multiple_windows(\n",
    "    df: pd.DataFrame,\n",
    "    window_sizes: List[int] = [5, 21, 252, 512],\n",
    "    prefix: str = \"train\",\n",
    "):\n",
    "    \n",
    "\n",
    "    results = {}\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"PRC\"] = df[\"PRC\"].abs()\n",
    "\n",
    "    df_by_date = {d: g for d, g in df.groupby(\"date\")}\n",
    "\n",
    "    for win in window_sizes:\n",
    "        X_list, y_list, meta_list = [], [], []\n",
    "\n",
    "        for permno, group in df.groupby(\"PERMNO\"):\n",
    "            group = group.sort_values(\"date\")\n",
    "            exret = group[\"EXRET\"].values\n",
    "            dates = group[\"date\"].values\n",
    "            shroud = group[\"SHROUT\"].values\n",
    "            prc = group[\"PRC\"].values\n",
    "            mktcap = prc * shroud\n",
    "\n",
    "            for i in range(win-1, len(group) - 1):\n",
    "                \n",
    "                signal_date = pd.Timestamp(dates[i])      \n",
    "                ret_date    = pd.Timestamp(dates[i + 1])\n",
    "                current_date  = signal_date\n",
    "                current_mktcap = mktcap[i]\n",
    "\n",
    "                peers = df_by_date[current_date]\n",
    "                all_mktcap = peers[\"PRC\"].abs().values * peers[\"SHROUT\"].values\n",
    "                rank = (all_mktcap <= current_mktcap).sum()\n",
    "                mktcap_percentile = rank / len(all_mktcap)\n",
    "\n",
    "                # 输入窗口: t-win … t\n",
    "                X_list.append(exret[i - win + 1 : i + 1])\n",
    "                # 目标值: t+1\n",
    "                y_list.append(exret[i + 1])\n",
    "\n",
    "                meta_list.append(\n",
    "                    {\n",
    "                        \"PERMNO\": permno,\n",
    "                        \"date\": signal_date,\n",
    "                        \"ret_date\"   : ret_date,\n",
    "                        \"MKTCAP\": current_mktcap,\n",
    "                        \"MKTCAP_PERCENTILE\": mktcap_percentile,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        X_arr = np.array(X_list)\n",
    "        y_arr = np.array(y_list)\n",
    "        meta_df = pd.DataFrame(meta_list)\n",
    "        meta_df[\"signal_month\"] = meta_df[\"date\"].dt.to_period(\"M\")\n",
    "        meta_df[\"ret_month\"]    = meta_df[\"ret_date\"].dt.to_period(\"M\")\n",
    "        order = meta_df[\"ret_date\"].argsort().values\n",
    "        X_arr, y_arr = X_arr[order], y_arr[order]\n",
    "        meta_df = meta_df.iloc[order].reset_index(drop=True)\n",
    "\n",
    "        results[f\"X_{prefix}_{win}\"] = X_arr\n",
    "        results[f\"y_{prefix}_{win}\"] = y_arr\n",
    "        results[f\"meta_{prefix}_{win}\"] = meta_df\n",
    "\n",
    "        print(\n",
    "            f\" {prefix.upper()} | Window = {win} | X shape = {X_arr.shape} | y shape = {y_arr.shape}\"\n",
    "        )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def unpack_window_datasets(\n",
    "    train_datasets: Dict[str, object],\n",
    "    test_datasets: Dict[str, object],\n",
    "    window_sizes: List[int] = [5, 21, 252, 512],\n",
    "):\n",
    "    for win in window_sizes:\n",
    "        globals()[f\"X_train_{win}\"] = train_datasets[f\"X_train_{win}\"]\n",
    "        globals()[f\"y_train_{win}\"] = train_datasets[f\"y_train_{win}\"]\n",
    "        globals()[f\"meta_train_{win}\"] = train_datasets.get(f\"meta_train_{win}\", None)\n",
    "\n",
    "        globals()[f\"X_test_{win}\"] = test_datasets[f\"X_test_{win}\"]\n",
    "        globals()[f\"y_test_{win}\"] = test_datasets[f\"y_test_{win}\"]\n",
    "        globals()[f\"meta_test_{win}\"] = test_datasets.get(f\"meta_test_{win}\", None)\n",
    "\n",
    "        print(\n",
    "            f\" Unpacked → X_train_{win}, y_train_{win}, X_test_{win}, y_test_{win}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def prepare_and_save_scaled_data(\n",
    "    train_csv_path: str,\n",
    "    test_csv_path: str,\n",
    "    window_sizes: List[int] = [5, 21, 252, 512],\n",
    "    output_npz_path: str = \"all_window_datasets_scaled.npz\",\n",
    "    split_date: str = \"2016-01-01\",\n",
    "):\n",
    "    train_df = pd.read_csv(train_csv_path, parse_dates=[\"date\"])\n",
    "    test_df = pd.read_csv(test_csv_path, parse_dates=[\"date\"])\n",
    "    full_df = pd.concat([train_df, test_df], ignore_index=True).sort_values([\"PERMNO\", \"date\"])\n",
    "\n",
    "    full_sets = generate_datasets_for_multiple_windows(full_df, window_sizes, \"full\")\n",
    "    save_dict = {}\n",
    "\n",
    "    for win in window_sizes:\n",
    "        X_full = full_sets[f\"X_full_{win}\"]\n",
    "        y_full = full_sets[f\"y_full_{win}\"]\n",
    "        meta_full = full_sets[f\"meta_full_{win}\"]\n",
    "\n",
    "        mask_train = meta_full[\"ret_date\"] < split_date\n",
    "\n",
    "        X_train_raw = X_full[mask_train]\n",
    "        X_test_raw  = X_full[~mask_train]\n",
    "        y_train_raw = y_full[mask_train].reshape(-1, 1)\n",
    "        y_test_raw  = y_full[~mask_train].reshape(-1, 1)\n",
    "\n",
    "        scaler_x = StandardScaler()\n",
    "        scaler_y = StandardScaler()\n",
    "        X_train = scaler_x.fit_transform(X_train_raw)\n",
    "        X_test  = scaler_x.transform(X_test_raw)\n",
    "        y_train = scaler_y.fit_transform(y_train_raw).flatten()\n",
    "        y_test  = scaler_y.transform(y_test_raw).flatten()\n",
    "\n",
    "        meta_train = meta_full[mask_train].reset_index(drop=True)\n",
    "        meta_test  = meta_full[~mask_train].reset_index(drop=True)\n",
    "\n",
    "        save_dict[f\"X_train_{win}\"] = X_train\n",
    "        save_dict[f\"X_test_{win}\"]  = X_test\n",
    "        save_dict[f\"y_train_{win}\"] = y_train\n",
    "        save_dict[f\"y_test_{win}\"]  = y_test\n",
    "        save_dict[f\"meta_train_{win}\"] = meta_train.to_dict()\n",
    "        save_dict[f\"meta_test_{win}\"] = meta_test.to_dict()\n",
    "\n",
    "        save_dict[f\"market_caps_train_{win}\"] = meta_train[\"MKTCAP\"].values\n",
    "        save_dict[f\"market_caps_test_{win}\"] = meta_test[\"MKTCAP\"].values\n",
    "\n",
    "        joblib.dump(scaler_x, f\"scaler_X_window_{win}.pkl\")\n",
    "        joblib.dump(scaler_y, f\"scaler_y_window_{win}.pkl\")\n",
    "\n",
    "        print(f\"[Saved] Window={win} | Train={X_train.shape} | Test={X_test.shape}\")\n",
    "\n",
    "    np.savez(output_npz_path, **save_dict)\n",
    "    print(f\"\\n[FINISH] All datasets saved to: {output_npz_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prepare_and_save_scaled_data(\n",
    "        train_csv_path=\"CRSP_2000_2015_top50_with_exret_cleaned.csv\",\n",
    "        test_csv_path=\"CRSP_2016_2024_top50_with_exret_cleaned.csv\",\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-mac)",
   "language": "python",
   "name": "tf-mac"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
