{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bd36a91b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd36a91b",
        "outputId": "00bea0ed-4e3e-4129-b836-264eb2ed239b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)\n",
            "fatal: destination path 'chronos-forecasting' already exists and is not an empty directory.\n",
            "/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos-forecasting\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m128.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.65)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.12)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.8)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.18.2)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.13.4)\n",
            "Requirement already satisfied: curl_cffi>=0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.13.0)\n",
            "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (5.29.5)\n",
            "Requirement already satisfied: websockets>=13.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (15.0.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.14.1)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from curl_cffi>=0.7->yfinance) (1.17.1)\n",
            "Requirement already satisfied: certifi>=2024.2.2 in /usr/local/lib/python3.11/dist-packages (from curl_cffi>=0.7->yfinance) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2.5.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create project directory and change working directory\n",
        "!mkdir -p '/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)'\n",
        "%cd '/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)'\n",
        "\n",
        "# Clone source code repository if not exists\n",
        "!git clone https://github.com/amazon-science/chronos-forecasting\n",
        "%cd chronos-forecasting\n",
        "\n",
        "# Install required packages\n",
        "%pip install torch transformers datasets accelerate scikit-learn tqdm joblib\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "import warnings\n",
        "from typing import Dict, Any, Optional, Tuple, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "! pip install yfinance\n",
        "import yfinance as yf\n",
        "import statsmodels.api as sm\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "from transformers.trainer_utils import IntervalStrategy\n",
        "from transformers.data.data_collator import default_data_collator\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, PeftModel, TaskType\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from scipy.stats import f as f_dist\n",
        "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
        "\n",
        "\n",
        "# ========================= Basic Settings =========================\n",
        "def ensure_chronos_import() -> None:\n",
        "    \"\"\"\n",
        "    Try to import chronos; if failed, add local/Colab repo src to sys.path.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import chronos  # noqa: F401\n",
        "        return\n",
        "    except Exception:\n",
        "        pass\n",
        "    candidates = [\n",
        "        \"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/3_Benchmark/5_Foundation_Models/Chronos/chronos-forecasting/src\",\n",
        "        \"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos-forecasting/src\",\n",
        "        \"/content/chronos-forecasting/src\",\n",
        "    ]\n",
        "    for repo_src in candidates:\n",
        "        if os.path.isdir(repo_src) and repo_src not in sys.path:\n",
        "            sys.path.append(repo_src)\n",
        "            try:\n",
        "                import chronos  # noqa: F401\n",
        "                return\n",
        "            except Exception:\n",
        "                continue\n",
        "    raise ImportError(\"chronos import failed. Please clone chronos-forecasting and ensure its 'src' is on sys.path.\")\n",
        "\n",
        "def set_seed(seed: int = 42) -> None:\n",
        "    \"\"\"\n",
        "    Set random seed for reproducibility (CPU, CUDA, MPS).\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    if torch.backends.mps.is_available():\n",
        "        try:\n",
        "            torch.mps.manual_seed(seed)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "def get_device() -> torch.device:\n",
        "    \"\"\"\n",
        "    Prefer MPS (Apple Silicon), otherwise use CUDA, otherwise CPU.\n",
        "    \"\"\"\n",
        "    if torch.backends.mps.is_available():\n",
        "        return torch.device(\"mps\")\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")\n",
        "    return torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "28313cbb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28313cbb",
        "outputId": "8c24dece-a4f9-4211-d250-9e2f8465de6d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2075427930.py:16: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  px = yf.download(\"^GSPC\", start=\"2016-01-01\", end=\"2024-12-31\")[\"Close\"]\n",
            "[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] S&P500 Excess Sharpe (2016–24) = 0.652\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "/tmp/ipython-input-2075427930.py:18: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  rf_align = rf_series.reindex(sp_ret.index).fillna(method=\"ffill\")\n"
          ]
        }
      ],
      "source": [
        "def annual_sharpe(rets, freq=252):\n",
        "    mu = float(np.mean(rets)) * freq\n",
        "    sd = float(np.std(rets, ddof=1)) * np.sqrt(freq)\n",
        "    return mu / sd if sd > 0 else 0\n",
        "\n",
        "# Load risk-free rate & calculate S&P500 Excess Sharpe\n",
        "\n",
        "rf_file = \"/content/drive/MyDrive/ERP Data/CRSP_2016_2024_top50_with_exret.csv\"\n",
        "try:\n",
        "    rf_df = pd.read_csv(rf_file, usecols=[\"date\", \"rf\"])\n",
        "    rf_df[\"date\"] = pd.to_datetime(rf_df[\"date\"])\n",
        "    rf_df = rf_df.drop_duplicates(\"date\").set_index(\"date\").sort_index()\n",
        "    rf_series = rf_df[\"rf\"].astype(float)\n",
        "\n",
        "    px = yf.download(\"^GSPC\", start=\"2016-01-01\", end=\"2024-12-31\")[\"Close\"]\n",
        "    sp_ret = px.pct_change().dropna()\n",
        "    rf_align = rf_series.reindex(sp_ret.index).fillna(method=\"ffill\")\n",
        "    sp_excess = sp_ret.values - rf_align.values\n",
        "\n",
        "    SR_MKT_EX = annual_sharpe(sp_excess)\n",
        "    print(f\"[INFO] S&P500 Excess Sharpe (2016–24) = {SR_MKT_EX:.3f}\")\n",
        "except Exception as e:\n",
        "    print(f\"Warning: Could not load risk-free rate data: {e}\")\n",
        "    SR_MKT_EX = 0.5  # Use default value\n",
        "\n",
        "def delta_sharpe(r2_zero: float, sr_base: float):\n",
        "    \"\"\"\n",
        "    If r2_zero <= 0   → ΔSharpe = 0, Sharpe* = sr_base\n",
        "    If r2_zero >= 1   → ΔSharpe = 0, Sharpe* = sr_base (extreme case fallback)\n",
        "    Otherwise, calculate according to the original formula\n",
        "    \"\"\"\n",
        "    if (r2_zero <= 0) or (r2_zero >= 1):\n",
        "        return 0.0, sr_base\n",
        "    sr_star = np.sqrt(sr_base ** 2 + r2_zero) / np.sqrt(1 - r2_zero)\n",
        "    return sr_star - sr_base, sr_star\n",
        "\n",
        "# Zero-based R²\n",
        "def r2_zero(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate zero-based R² (baseline is 0)\n",
        "    y_true: array of true values (N,)\n",
        "    y_pred: array of predicted values (N,)\n",
        "    \"\"\"\n",
        "    rss = np.sum((y_true - y_pred)**2)\n",
        "    tss = np.sum(y_true**2)\n",
        "    return 1 - rss / tss\n",
        "\n",
        "def calc_ic_daily(df, method='spearman'):\n",
        "    \"\"\"\n",
        "    Calculate daily cross-sectional RankIC\n",
        "    df: must contain ['signal_date','y_true','y_pred']\n",
        "    \"\"\"\n",
        "    ics = (df.groupby('signal_date')\n",
        "             .apply(lambda g: g['y_pred'].corr(g['y_true'], method=method))\n",
        "             .dropna())\n",
        "    mean_ic = ics.mean()\n",
        "    std_ic  = ics.std(ddof=1)\n",
        "    t_ic    = mean_ic / (std_ic / np.sqrt(len(ics))) if std_ic > 0 else np.nan\n",
        "    pos_ratio = (ics > 0).mean()\n",
        "    return mean_ic, t_ic, pos_ratio, ics\n",
        "\n",
        "def calc_directional_metrics(y_true, y_pred, permnos=None):\n",
        "    \"\"\"\n",
        "    Improved version:\n",
        "    - Sample-level sign prediction\n",
        "    - If grouped by stock, calculate Overall, Up, Down for each stock and then average\n",
        "    \"\"\"\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "\n",
        "    if permnos is None:\n",
        "        s_true = np.sign(y_true)\n",
        "        s_pred = np.sign(y_pred)\n",
        "        mask = s_true != 0\n",
        "        s_true = s_true[mask]\n",
        "        s_pred = s_pred[mask]\n",
        "\n",
        "        overall_acc = np.mean(s_true == s_pred)\n",
        "\n",
        "        up_mask = s_true > 0\n",
        "        down_mask = s_true < 0\n",
        "        up_acc = np.mean(s_true[up_mask] == s_pred[up_mask]) if np.any(up_mask) else 0\n",
        "        down_acc = np.mean(s_true[down_mask] == s_pred[down_mask]) if np.any(down_mask) else 0\n",
        "\n",
        "    else:\n",
        "        df = pd.DataFrame({\"permno\": permnos, \"yt\": y_true, \"yp\": y_pred})\n",
        "        overall_accs = []\n",
        "        up_accs = []\n",
        "        down_accs = []\n",
        "\n",
        "        for _, g in df.groupby(\"permno\"):\n",
        "            s_true = np.sign(g[\"yt\"].values)\n",
        "            s_pred = np.sign(g[\"yp\"].values)\n",
        "            mask = s_true != 0\n",
        "            s_true = s_true[mask]\n",
        "            s_pred = s_pred[mask]\n",
        "            if len(s_true) == 0:\n",
        "                continue\n",
        "            overall_accs.append(np.mean(s_true == s_pred))\n",
        "\n",
        "            up_mask = s_true > 0\n",
        "            down_mask = s_true < 0\n",
        "            up_accs.append(np.mean(s_true[up_mask] == s_pred[up_mask]) if np.any(up_mask) else np.nan)\n",
        "            down_accs.append(np.mean(s_true[down_mask] == s_pred[down_mask]) if np.any(down_mask) else np.nan)\n",
        "\n",
        "        overall_acc = np.nanmean(overall_accs)\n",
        "        up_acc = np.nanmean(up_accs)\n",
        "        down_acc = np.nanmean(down_accs)\n",
        "\n",
        "    return overall_acc, up_acc, down_acc\n",
        "\n",
        "def regression_metrics(y_true, y_pred, k, meta=None, permnos=None):\n",
        "    \"\"\"\n",
        "    Includes:\n",
        "    - Regression metrics\n",
        "    - Pointwise directional accuracy\n",
        "    - Market cap group metrics\n",
        "    \"\"\"\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    n = len(y_true)\n",
        "\n",
        "    r2 = r2_zero(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "\n",
        "    dir_acc, up_acc, down_acc = calc_directional_metrics(y_true, y_pred, permnos)\n",
        "\n",
        "    metrics = {\n",
        "        \"R²_zero\": r2,\n",
        "        \"RMSE\": rmse,\n",
        "        \"MAE\": mae,\n",
        "        \"MSE\": mse,\n",
        "        \"Directional Accuracy\": dir_acc,\n",
        "        \"Up_Directional_Acc\": up_acc,\n",
        "        \"Down_Directional_Acc\": down_acc\n",
        "    }\n",
        "\n",
        "    if meta is not None and \"MKTCAP_PERCENTILE\" in meta:\n",
        "        top_mask = meta[\"MKTCAP_PERCENTILE\"] >= 0.75\n",
        "        bottom_mask = meta[\"MKTCAP_PERCENTILE\"] <= 0.25\n",
        "\n",
        "        if np.any(top_mask):\n",
        "            yt_top = y_true[top_mask]\n",
        "            yp_top = y_pred[top_mask]\n",
        "            perm_top = permnos[top_mask] if permnos is not None else None\n",
        "            r2_top = r2_zero(yt_top, yp_top)\n",
        "            rmse_top = np.sqrt(mean_squared_error(yt_top, yp_top))\n",
        "            mae_top = mean_absolute_error(yt_top, yp_top)\n",
        "            mse_top = mean_squared_error(yt_top, yp_top)\n",
        "            dir_top, up_top, down_top = calc_directional_metrics(yt_top, yp_top, perm_top)\n",
        "            metrics.update({\n",
        "                \"Top25_R2_zero\": r2_top,\n",
        "                \"Top25_RMSE\": rmse_top,\n",
        "                \"Top25_MAE\": mae_top,\n",
        "                \"Top25_MSE\": mse_top,\n",
        "                \"Top25_Dir_Acc\": dir_top,\n",
        "                \"Top25_Up_Acc\": up_top,\n",
        "                \"Top25_Down_Acc\": down_top\n",
        "            })\n",
        "\n",
        "        if np.any(bottom_mask):\n",
        "            yt_bot = y_true[bottom_mask]\n",
        "            yp_bot = y_pred[bottom_mask]\n",
        "            perm_bot = permnos[bottom_mask] if permnos is not None else None\n",
        "            r2_bot = r2_zero(yt_bot, yp_bot)\n",
        "            rmse_bot = np.sqrt(mean_squared_error(yt_bot, yp_bot))\n",
        "            mae_bot = mean_absolute_error(yt_bot, yp_bot)\n",
        "            mse_bot = mean_squared_error(yt_bot, yp_bot)\n",
        "            dir_bot, up_bot, down_bot = calc_directional_metrics(yt_bot, yp_bot, perm_bot)\n",
        "            metrics.update({\n",
        "                \"Bottom25_R2_zero\": r2_bot,\n",
        "                \"Bottom25_RMSE\": rmse_bot,\n",
        "                \"Bottom25_MAE\": mae_bot,\n",
        "                \"Bottom25_MSE\": mse_bot,\n",
        "                \"Bottom25_Dir_Acc\": dir_bot,\n",
        "                \"Bottom25_Up_Acc\": up_bot,\n",
        "                \"Bottom25_Down_Acc\": down_bot\n",
        "            })\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def f_statistic(y_true, y_pred, k):\n",
        "    \"\"\"Return F statistic and corresponding p-value\"\"\"\n",
        "    n   = len(y_true)\n",
        "    rss = np.sum((y_true - y_pred) ** 2)\n",
        "    tss = np.sum(y_true ** 2)\n",
        "    r2  = 1 - rss / tss\n",
        "    if (r2 <= 0) or (n <= k):\n",
        "        return 0.0, 1.0\n",
        "    F = (r2 / k) / ((1 - r2) / (n - k))\n",
        "    p = f_dist.sf(F, k, n - k)\n",
        "    return F, p\n",
        "\n",
        "def overall_interval_metrics_method1(y_all, yhat_all, k, permnos_all=None, meta_all=None):\n",
        "    \"\"\"\n",
        "    Method 1: Calculate metrics for the entire interval at once (concatenate all samples from 2016-2024)\n",
        "    Returns: a dict, can be directly used for save_metrics()\n",
        "    \"\"\"\n",
        "    base = regression_metrics(\n",
        "        y_true=y_all,\n",
        "        y_pred=yhat_all,\n",
        "        k=k,\n",
        "        meta=meta_all,\n",
        "        permnos=permnos_all\n",
        "    )\n",
        "    F, p = f_statistic(y_all, yhat_all, k)\n",
        "    base[\"F_stat\"]     = F\n",
        "    base[\"F_pvalue\"]   = p\n",
        "    base[\"N_obs\"] = len(y_all)\n",
        "\n",
        "    delta_cash, sr_star_cash = delta_sharpe(base[\"R²_zero\"], sr_base=0)\n",
        "    base[\"ΔSharpe_cash\"]      = delta_cash\n",
        "    base[\"Sharpe*_cash\"]      = sr_star_cash\n",
        "\n",
        "    delta_mkt , sr_star_mkt  = delta_sharpe(base[\"R²_zero\"], sr_base=SR_MKT_EX)\n",
        "    base[\"ΔSharpe_mkt\"]       = delta_mkt\n",
        "    base[\"Sharpe*_mkt\"]       = sr_star_mkt\n",
        "\n",
        "    return base\n",
        "\n",
        "def sortino_ratio(rets, freq=252):\n",
        "    \"\"\"Calculate Sortino Ratio\"\"\"\n",
        "    downside = rets[rets < 0]\n",
        "    if len(downside) == 0:\n",
        "        return np.inf\n",
        "    mu = rets.mean() * freq\n",
        "    sigma = np.sqrt((downside ** 2).mean()) * np.sqrt(freq)\n",
        "    return mu / sigma\n",
        "\n",
        "def cvar(rets, alpha=0.95):\n",
        "    \"\"\"Calculate CVaR\"\"\"\n",
        "    q = np.quantile(rets, 1 - alpha)\n",
        "    return rets[rets <= q].mean()\n",
        "\n",
        "def save_metrics(metrics_dict, name, window, path=\"portfolio_metrics.csv\"):\n",
        "    \"\"\"Save metrics to CSV file\"\"\"\n",
        "    row = {'Model': name, 'Window': window}\n",
        "    row.update(metrics_dict)\n",
        "\n",
        "    if os.path.exists(path):\n",
        "        df = pd.read_csv(path)\n",
        "        df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
        "    else:\n",
        "        df = pd.DataFrame([row])\n",
        "\n",
        "    df.to_csv(path, index=False)\n",
        "    print(f\"Metrics saved for {name}_w{window} to {path}\")\n",
        "\n",
        "TC_GRID = [0.0005, 0.001, 0.002, 0.003, 0.004]  # 5, 10, 20, 30, 40 bps\n",
        "TC_TAG  = {\n",
        "    0.0005: \"tc5\",\n",
        "    0.001:  \"tc10\",\n",
        "    0.002:  \"tc20\",\n",
        "    0.003:  \"tc30\",\n",
        "    0.004:  \"tc40\"\n",
        "}\n",
        "\n",
        "class PortfolioBacktester:\n",
        "    def __init__(self):\n",
        "        self.results = {}\n",
        "\n",
        "    def calc_turnover(self, w_t, r_t, w_tp1):\n",
        "        \"\"\"Calculate turnover using the standard formula\"\"\"\n",
        "        if w_t is None:\n",
        "            return np.sum(np.abs(w_tp1))\n",
        "\n",
        "        gross_ret = np.sum(w_t * r_t)\n",
        "        if abs(1 + gross_ret) < 1e-8:\n",
        "            return np.sum(np.abs(w_tp1))\n",
        "\n",
        "        passive_weight = w_t * (1 + r_t) / (1 + gross_ret)\n",
        "        turnover = np.sum(np.abs(w_tp1 - passive_weight))\n",
        "        return turnover\n",
        "\n",
        "    def create_portfolios_with_permno_tracking(self, signals, market_caps, permnos, top_pct=0.1, bottom_pct=0.1, weight_scheme=\"VW\"):\n",
        "        \"\"\"\n",
        "        Create portfolio weights based on signals, strictly tracking permno alignment\n",
        "        weight_scheme: 'VW' value-weighted, 'EW' equal-weighted\n",
        "        \"\"\"\n",
        "        n_stocks = len(signals)\n",
        "        top_n    = max(1, int(round(n_stocks * top_pct)))\n",
        "        bottom_n = max(1, int(round(n_stocks * bottom_pct)))\n",
        "\n",
        "        sorted_idx = np.argsort(signals)[::-1]\n",
        "\n",
        "        top_idx = sorted_idx[:top_n]\n",
        "        bottom_idx = sorted_idx[-bottom_n:]\n",
        "\n",
        "        portfolio_data = {}\n",
        "\n",
        "        long_weights = np.zeros(n_stocks)\n",
        "        if len(top_idx) > 0:\n",
        "            if weight_scheme == \"VW\":\n",
        "                top_market_caps = market_caps[top_idx]\n",
        "                if np.sum(top_market_caps) > 0:\n",
        "                    long_weights[top_idx] = top_market_caps / np.sum(top_market_caps)\n",
        "            else:\n",
        "                long_weights[top_idx] = 1.0 / len(top_idx)\n",
        "\n",
        "        portfolio_data['long_only'] = {\n",
        "            'weights': long_weights,\n",
        "            'permnos': permnos.copy(),\n",
        "            'selected_permnos': permnos[top_idx] if len(top_idx) > 0 else np.array([])\n",
        "        }\n",
        "\n",
        "        short_weights = np.zeros(n_stocks)\n",
        "        if len(bottom_idx) > 0:\n",
        "            if weight_scheme == \"VW\":\n",
        "                bottom_market_caps = market_caps[bottom_idx]\n",
        "                if np.sum(bottom_market_caps) > 0:\n",
        "                    short_weights[bottom_idx] = -bottom_market_caps / np.sum(bottom_market_caps)\n",
        "            else:\n",
        "                short_weights[bottom_idx] = -1.0 / len(bottom_idx)\n",
        "\n",
        "        portfolio_data['short_only'] = {\n",
        "            'weights': short_weights,\n",
        "            'permnos': permnos.copy(),\n",
        "            'selected_permnos': permnos[bottom_idx] if len(bottom_idx) > 0 else np.array([])\n",
        "        }\n",
        "\n",
        "        ls_raw = long_weights + short_weights\n",
        "\n",
        "        gross_target = 2.0\n",
        "        current_gross = np.sum(np.abs(long_weights)) + np.sum(np.abs(short_weights))\n",
        "        scale = gross_target / current_gross if current_gross > 1e-8 else 0.0\n",
        "        ls_weights = scale * ls_raw\n",
        "\n",
        "        ls_selected_permnos = np.concatenate([\n",
        "            permnos[top_idx] if len(top_idx) > 0 else np.array([]),\n",
        "            permnos[bottom_idx] if len(bottom_idx) > 0 else np.array([])\n",
        "        ])\n",
        "\n",
        "        portfolio_data['long_short'] = {\n",
        "            'weights': ls_weights,\n",
        "            'permnos': permnos.copy(),\n",
        "            'selected_permnos': ls_selected_permnos\n",
        "        }\n",
        "\n",
        "        return portfolio_data\n",
        "\n",
        "    def calculate_aligned_portfolio_return(self, portfolio_weights, portfolio_permnos, actual_returns, actual_permnos):\n",
        "        \"\"\"Calculate portfolio return strictly aligned by permno\"\"\"\n",
        "        aligned_returns = np.zeros(len(portfolio_permnos))\n",
        "\n",
        "        return_dict = dict(zip(actual_permnos, actual_returns))\n",
        "\n",
        "        for i, permno in enumerate(portfolio_permnos):\n",
        "            if permno in return_dict:\n",
        "                aligned_returns[i] = return_dict[permno]\n",
        "\n",
        "        portfolio_return = np.sum(portfolio_weights * aligned_returns)\n",
        "        return portfolio_return, aligned_returns\n",
        "\n",
        "    def calculate_metrics(self, returns, turnover_series=None):\n",
        "        \"\"\"Calculate portfolio metrics - returns summary metrics only, not full series\"\"\"\n",
        "        returns = np.array(returns)\n",
        "\n",
        "        annual_return = np.mean(returns) * 252\n",
        "        annual_vol = np.std(returns, ddof=1) * np.sqrt(252)\n",
        "        sharpe = annual_return / annual_vol if annual_vol > 0 else 0\n",
        "\n",
        "        log_cum = np.cumsum(np.log1p(returns))\n",
        "        peak_log = np.maximum.accumulate(log_cum)\n",
        "        dd_log = peak_log - log_cum\n",
        "        max_drawdown = 1 - np.exp(-dd_log.max())\n",
        "        max_1d_loss = np.min(returns)\n",
        "\n",
        "        avg_turnover = np.mean(turnover_series) if turnover_series is not None else 0\n",
        "\n",
        "        sortino = sortino_ratio(returns)\n",
        "        cvar95  = cvar(returns, alpha=0.95)\n",
        "\n",
        "        result = {\n",
        "            'annual_return': annual_return,\n",
        "            'annual_vol': annual_vol,\n",
        "            'sharpe': sharpe,\n",
        "            'max_drawdown': max_drawdown,\n",
        "            'max_1d_loss': max_1d_loss,\n",
        "            'avg_turnover': avg_turnover,\n",
        "            'sortino': sortino,\n",
        "            'cvar95': cvar95\n",
        "        }\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f45fe1ca",
      "metadata": {
        "id": "f45fe1ca"
      },
      "outputs": [],
      "source": [
        "# ===== Additional: Utility functions for backtesting (local version) =====\n",
        "\n",
        "def load_datasets(npz_path: str):\n",
        "    return np.load(npz_path, allow_pickle=True)\n",
        "\n",
        "def get_batch_size(window: int) -> int:\n",
        "    return get_dynamic_batch_size(window, base=512)\n",
        "\n",
        "@torch.no_grad()\n",
        "def chronos_rolling_prediction(\n",
        "    pipeline,\n",
        "    X_data: np.ndarray,\n",
        "    batch_size: int = 256,\n",
        "    prediction_length: int = 1,\n",
        "    num_samples: int = 10\n",
        ") -> np.ndarray:\n",
        "    preds: List[float] = []\n",
        "    for i in range(0, len(X_data), batch_size):\n",
        "        ctx_list = [torch.from_numpy(seq.astype(np.float32)) for seq in X_data[i:i + batch_size]]\n",
        "        fr = pipeline.predict(\n",
        "            context=ctx_list,\n",
        "            prediction_length=prediction_length,\n",
        "            num_samples=num_samples\n",
        "        )\n",
        "        if isinstance(fr, torch.Tensor):\n",
        "            # Only take the mean of the first step (t=0) along the samples dimension\n",
        "            means = fr[:, 0, :].mean(dim=1).cpu().numpy()\n",
        "        else:\n",
        "            means = np.array([np.asarray(f)[0].mean() for f in fr])\n",
        "        preds.extend(means.tolist())\n",
        "    return np.array(preds, dtype=np.float32)\n",
        "\n",
        "# ========================= Dataset and time-based split =========================\n",
        "class ChronosWindowDataset(Dataset):\n",
        "    \"\"\"Convert windowed sequences (X, y) to Chronos tokens (order preserved).\"\"\"\n",
        "    def __init__(self, X: np.ndarray, y: np.ndarray, chronos_tokenizer, prediction_length: int = 1) -> None:\n",
        "        self.X = X.astype(np.float32)\n",
        "        self.y = y.astype(np.float32).reshape(-1)\n",
        "        self.tokenizer = chronos_tokenizer\n",
        "        self.pred_len = int(prediction_length)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        context = torch.from_numpy(self.X[idx])\n",
        "        input_ids, attention_mask, scale = self.tokenizer.context_input_transform(context.unsqueeze(0))\n",
        "        future_target = torch.tensor(self.y[idx: idx + 1]).unsqueeze(0)\n",
        "        labels, labels_mask = self.tokenizer.label_input_transform(future_target, scale)\n",
        "        labels[labels_mask == 0] = -100\n",
        "        return {\n",
        "            \"input_ids\": input_ids.squeeze(0),\n",
        "            \"attention_mask\": attention_mask.squeeze(0),\n",
        "            \"labels\": labels.squeeze(0),\n",
        "        }\n",
        "\n",
        "\n",
        "def load_npz_dataset(npz_path: str) -> Dict[str, Any]:\n",
        "    \"\"\"Load .npz data (contains X, y, meta for different windows and splits).\"\"\"\n",
        "    data = np.load(npz_path, allow_pickle=True)\n",
        "    return {k: data[k] for k in data.files}\n",
        "\n",
        "\n",
        "def extract_split(data: Dict[str, Any], window: int, split: str) -> Tuple[np.ndarray, np.ndarray, pd.DataFrame]:\n",
        "    \"\"\"Extract X, y, meta DataFrame for a given window and split (train/test).\"\"\"\n",
        "    X = data[f\"X_{split}_{window}\"]\n",
        "    y = data[f\"y_{split}_{window}\"]\n",
        "    meta_raw = data.get(f\"meta_{split}_{window}\")\n",
        "    if meta_raw is None:\n",
        "        meta = pd.DataFrame({\"PERMNO\": np.arange(len(X))})\n",
        "    else:\n",
        "        if hasattr(meta_raw, \"item\"):\n",
        "            meta = pd.DataFrame(meta_raw.item())\n",
        "        else:\n",
        "            meta = pd.DataFrame(meta_raw)\n",
        "    return X, y, meta\n",
        "\n",
        "\n",
        "def time_based_val_split(X: np.ndarray, y: np.ndarray, meta: pd.DataFrame, val_ratio: float = 0.2) -> Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray], Tuple[pd.DataFrame, pd.DataFrame]]:\n",
        "    \"\"\"Split train/val sets in time order (no shuffle).\"\"\"\n",
        "    n = len(X)\n",
        "    val_start = int(math.floor(n * (1.0 - val_ratio)))\n",
        "    X_tr, y_tr = X[:val_start], y[:val_start]\n",
        "    X_va, y_va = X[val_start:], y[val_start:]\n",
        "    meta_tr, meta_va = meta.iloc[:val_start].reset_index(drop=True), meta.iloc[val_start:].reset_index(drop=True)\n",
        "    return (X_tr, y_tr), (X_va, y_va), (meta_tr, meta_va)\n",
        "\n",
        "\n",
        "def get_dynamic_batch_size(window: int, base: int = 512) -> int:\n",
        "    \"\"\"Set batch size dynamically based on window length (longer sequence, smaller batch).\"\"\"\n",
        "    if window <= 5:\n",
        "        return base\n",
        "    elif window <= 21:\n",
        "        return 128\n",
        "    elif window <= 252:\n",
        "        return 64\n",
        "    elif window <= 512:\n",
        "        return 32\n",
        "    return max(base // 16, 16)\n",
        "\n",
        "# ========================= Ordered Trainer + LoRA config =========================\n",
        "class OrderedTrainer(Trainer):\n",
        "    \"\"\"Trainer that disables shuffling for train/eval, strictly preserves time order.\"\"\"\n",
        "    def get_train_dataloader(self) -> DataLoader:  # type: ignore[override]\n",
        "        num_workers = getattr(self.args, \"dataloader_num_workers\", 0)\n",
        "        dl_kwargs = dict(\n",
        "            dataset=self.train_dataset,\n",
        "            batch_size=self.args.per_device_train_batch_size,\n",
        "            shuffle=False,\n",
        "            collate_fn=self.data_collator,\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=getattr(self.args, \"dataloader_pin_memory\", False),\n",
        "            drop_last=getattr(self.args, \"dataloader_drop_last\", False),\n",
        "        )\n",
        "        if num_workers and num_workers > 0:\n",
        "            dl_kwargs[\"persistent_workers\"] = True\n",
        "            dl_kwargs[\"prefetch_factor\"] = 4\n",
        "        return DataLoader(**dl_kwargs)\n",
        "\n",
        "    def get_eval_dataloader(self, eval_dataset=None) -> DataLoader:  # type: ignore[override]\n",
        "        dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
        "        num_workers = getattr(self.args, \"dataloader_num_workers\", 0)\n",
        "        dl_kwargs = dict(\n",
        "            dataset=dataset,\n",
        "            batch_size=self.args.per_device_eval_batch_size,\n",
        "            shuffle=False,\n",
        "            collate_fn=self.data_collator,\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=getattr(self.args, \"dataloader_pin_memory\", False),\n",
        "            drop_last=False,\n",
        "        )\n",
        "        if num_workers and num_workers > 0:\n",
        "            dl_kwargs[\"persistent_workers\"] = True\n",
        "            dl_kwargs[\"prefetch_factor\"] = 4\n",
        "        return DataLoader(**dl_kwargs)\n",
        "\n",
        "\n",
        "def build_lora_config(r: int = 8, alpha: int = 16, dropout: float = 0.05, target_modules: Optional[List[str]] = None) -> LoraConfig:\n",
        "    \"\"\"Build LoRA config; by default applies to T5 attention/FFN key projections.\"\"\"\n",
        "    if target_modules is None:\n",
        "        target_modules = [\"q\", \"k\", \"v\", \"o\", \"wi\", \"wo\"]\n",
        "    return LoraConfig(\n",
        "        r=r,\n",
        "        lora_alpha=alpha,\n",
        "        lora_dropout=dropout,\n",
        "        bias=\"none\",\n",
        "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "        target_modules=target_modules,\n",
        "    )\n",
        "\n",
        "\n",
        "def bind_strip_num_items(model):\n",
        "    \"\"\"Wrap forward to remove incompatible kwargs injected by chronos, only keep accepted args.\"\"\"\n",
        "    import inspect\n",
        "    original_forward = model.forward\n",
        "    sig = inspect.signature(original_forward)\n",
        "    accepted = set(sig.parameters.keys())\n",
        "    def wrapped_forward(*args, **kwargs):\n",
        "        for bad in [\n",
        "            'num_items',\n",
        "            'num_items_in_batch',\n",
        "            'num_samples',\n",
        "            'prediction_length',\n",
        "            'context',\n",
        "            'scales',\n",
        "        ]:\n",
        "            kwargs.pop(bad, None)\n",
        "        kwargs = {k: v for k, v in kwargs.items() if k in accepted}\n",
        "        return original_forward(*args, **kwargs)\n",
        "    model.forward = wrapped_forward\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "def get_precision_and_optim() -> tuple[bool, bool, bool, str]:\n",
        "    use_bf16 = False\n",
        "    use_fp16 = False\n",
        "    allow_tf32 = False\n",
        "    optim_name = \"adamw_torch\"\n",
        "    if torch.cuda.is_available():\n",
        "        try:\n",
        "            major, _ = torch.cuda.get_device_capability(0)\n",
        "        except Exception:\n",
        "            major = 0\n",
        "        allow_tf32 = major >= 8\n",
        "        try:\n",
        "            use_bf16 = torch.cuda.is_bf16_supported()\n",
        "        except Exception:\n",
        "            use_bf16 = major >= 8\n",
        "        use_fp16 = not use_bf16\n",
        "        try:\n",
        "            optim_name = \"adamw_torch_fused\"\n",
        "        except Exception:\n",
        "            optim_name = \"adamw_torch\"\n",
        "    return use_bf16, use_fp16, allow_tf32, optim_name\n",
        "\n",
        "\n",
        "def configure_torch_backends(allow_tf32: bool) -> None:\n",
        "    if torch.cuda.is_available():\n",
        "        try:\n",
        "            torch.backends.cuda.matmul.allow_tf32 = bool(allow_tf32)\n",
        "        except Exception:\n",
        "            pass\n",
        "        try:\n",
        "            torch.backends.cudnn.benchmark = True\n",
        "        except Exception:\n",
        "            pass\n",
        "        try:\n",
        "            from torch.backends.cuda import sdp_kernel\n",
        "            sdp_kernel.enable_flash(True)\n",
        "            sdp_kernel.enable_mem_efficient(True)\n",
        "            sdp_kernel.enable_math(False)\n",
        "        except Exception:\n",
        "            pass\n",
        "    try:\n",
        "        torch.set_float32_matmul_precision(\"high\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "\n",
        "def create_trainer(\n",
        "    model,\n",
        "    train_ds: Dataset,\n",
        "    val_ds: Dataset,\n",
        "    output_dir: str,\n",
        "    per_device_train_batch_size: int,\n",
        "    per_device_eval_batch_size: int,\n",
        "    learning_rate: float,\n",
        "    weight_decay: float,\n",
        "    num_train_epochs: int,\n",
        "    patience: int,\n",
        "    logging_steps: int = 200,\n",
        "    eval_strategy: str = \"epoch\",\n",
        "    save_strategy: str = \"epoch\",\n",
        "    dataloader_num_workers: int = 0,\n",
        "    data_collator=None,\n",
        "    warmup_ratio: float = 0.10,\n",
        ") -> Trainer:\n",
        "    \"\"\"Build Trainer (with early stopping), no shuffle, and enable mixed precision and high performance settings.\"\"\"\n",
        "    use_bf16, use_fp16, allow_tf32, optim_name = get_precision_and_optim()\n",
        "    configure_torch_backends(allow_tf32)\n",
        "\n",
        "    num_workers = dataloader_num_workers if dataloader_num_workers > 0 else (4 if torch.cuda.is_available() else 0)\n",
        "    pin_memory = True if torch.cuda.is_available() else False\n",
        "\n",
        "    try:\n",
        "        eval_accum = 64 if (torch.cuda.is_available() and \"a100\" in torch.cuda.get_device_name(0).lower()) else 32\n",
        "    except Exception:\n",
        "        eval_accum = 32\n",
        "\n",
        "    try:\n",
        "        args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            per_device_train_batch_size=per_device_train_batch_size,\n",
        "            per_device_eval_batch_size=per_device_eval_batch_size,\n",
        "            learning_rate=learning_rate,\n",
        "            weight_decay=weight_decay,\n",
        "            num_train_epochs=num_train_epochs,\n",
        "            warmup_ratio=warmup_ratio,\n",
        "            max_grad_norm=1.0,\n",
        "            logging_dir=os.path.join(output_dir, \"logs\"),\n",
        "            logging_steps=logging_steps,\n",
        "            eval_strategy=IntervalStrategy(eval_strategy),\n",
        "            save_strategy=IntervalStrategy(save_strategy),\n",
        "            save_total_limit=1,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"eval_loss\",\n",
        "            greater_is_better=False,\n",
        "            report_to=[],\n",
        "            remove_unused_columns=False,\n",
        "            dataloader_num_workers=num_workers,\n",
        "            dataloader_pin_memory=pin_memory,\n",
        "            dataloader_drop_last=False,\n",
        "            fp16=use_fp16,\n",
        "            bf16=use_bf16,\n",
        "            tf32=allow_tf32,\n",
        "            gradient_accumulation_steps=1,\n",
        "            gradient_checkpointing=True,\n",
        "            group_by_length=False,\n",
        "            optim=optim_name,\n",
        "            eval_accumulation_steps=eval_accum,\n",
        "        )\n",
        "    except TypeError:\n",
        "        args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            per_device_train_batch_size=per_device_train_batch_size,\n",
        "            per_device_eval_batch_size=per_device_eval_batch_size,\n",
        "            learning_rate=learning_rate,\n",
        "            weight_decay=weight_decay,\n",
        "            num_train_epochs=num_train_epochs,\n",
        "            warmup_ratio=warmup_ratio,\n",
        "            max_grad_norm=1.0,\n",
        "            logging_dir=os.path.join(output_dir, \"logs\"),\n",
        "            logging_steps=logging_steps,\n",
        "            evaluation_strategy=IntervalStrategy(eval_strategy),\n",
        "            save_strategy=IntervalStrategy(save_strategy),\n",
        "            save_total_limit=1,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"eval_loss\",\n",
        "            greater_is_better=False,\n",
        "            report_to=[],\n",
        "            remove_unused_columns=False,\n",
        "            dataloader_num_workers=num_workers,\n",
        "            dataloader_pin_memory=pin_memory,\n",
        "            dataloader_drop_last=False,\n",
        "            fp16=use_fp16,\n",
        "            bf16=use_bf16,\n",
        "            gradient_accumulation_steps=1,\n",
        "            gradient_checkpointing=True,\n",
        "            group_by_length=False,\n",
        "            optim=optim_name,\n",
        "            eval_accumulation_steps=eval_accum,\n",
        "        )\n",
        "\n",
        "    trainer = OrderedTrainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=val_ds,\n",
        "        data_collator=(data_collator or default_data_collator),\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=patience)],\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        trainer.label_names = [\"labels\"]\n",
        "    except Exception:\n",
        "        try:\n",
        "            trainer.label_names = []\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    try:\n",
        "        model.gradient_checkpointing_enable()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return trainer\n",
        "\n",
        "\n",
        "def is_a100() -> bool:\n",
        "    if not torch.cuda.is_available():\n",
        "        return False\n",
        "    try:\n",
        "        name = torch.cuda.get_device_name(0).lower()\n",
        "        if \"a100\" in name:\n",
        "            return True\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        major, _ = torch.cuda.get_device_capability(0)\n",
        "        total_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "        return (major >= 8) and (total_gb >= 35)\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "\n",
        "def maybe_torch_compile(model):\n",
        "    \"\"\"Only try torch.compile if explicitly enabled; default off to avoid Inductor/Quantization errors.\"\"\"\n",
        "    USE_TORCH_COMPILE = os.environ.get(\"USE_TORCH_COMPILE\", \"0\") == \"1\"\n",
        "    if not (USE_TORCH_COMPILE and is_a100()):\n",
        "        return model\n",
        "    try:\n",
        "        import torch._dynamo\n",
        "        torch._dynamo.config.suppress_errors = True\n",
        "        compiled = torch.compile(model, mode=\"reduce-overhead\", fullgraph=False, backend=\"inductor\")\n",
        "        return compiled\n",
        "    except Exception:\n",
        "        return model\n",
        "\n",
        "import math\n",
        "\n",
        "def get_colab_memory_hint(default_bs: int) -> int:\n",
        "    \"\"\"Scale batch size based on GPU availability and memory.\n",
        "    - No GPU: return default\n",
        "    - T4/V100(≈16GB): ×1.0~1.5\n",
        "    - A100(40GB): ×2\n",
        "    \"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        return default_bs\n",
        "    try:\n",
        "        total = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "    except Exception:\n",
        "        total = 16\n",
        "    if total >= 35:\n",
        "        mult = 2.0\n",
        "    elif total >= 22:\n",
        "        mult = 1.5\n",
        "    else:\n",
        "        mult = 1.0\n",
        "    return int(max(1, round(default_bs * mult)))\n",
        "\n",
        "def adapt_per_window_cfg_for_colab(cfg_dict: dict) -> dict:\n",
        "    \"\"\"Scale batch size for GPU/Colab (A100≈40GB doubles), keep unchanged for CPU.\"\"\"\n",
        "    try:\n",
        "        if not torch.cuda.is_available():\n",
        "            return cfg_dict\n",
        "        new_cfg = {}\n",
        "        for w, cfg in cfg_dict.items():\n",
        "            base_train = int(cfg.get('train_bs', 256))\n",
        "            base_eval  = int(cfg.get('eval_bs', max(256, base_train)))\n",
        "            train_bs = get_colab_memory_hint(base_train)\n",
        "            eval_bs  = min(1024, max(base_eval, train_bs * 2))\n",
        "            ncfg = dict(cfg)\n",
        "            ncfg['train_bs'] = train_bs\n",
        "            ncfg['eval_bs']  = eval_bs\n",
        "            new_cfg[w] = ncfg\n",
        "        return new_cfg\n",
        "    except Exception:\n",
        "        return cfg_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a13d92e8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272,
          "referenced_widgets": [
            "77db9577636045b3af64ad20c4d4e708",
            "64539b8b90a64dda85e330f13ccd5398",
            "705f095a8bd64cb1ad054005d2242428",
            "9f63936d8d6c4bd6be2b8fc026de3d62",
            "92a2a444e4fe41e18c66933772dd8386",
            "302731da7c72494897d640436c66ad8c",
            "de51f545826f4d76a5e14418b6d2831a",
            "b2bba446142c4bf489641957f85c2b5b",
            "4903ec51bcbe493fb4a379e6cba0a3b6",
            "d90ffe4b8d1c411ba6e769207d753f35",
            "659e95fd144f415683ad9f4eaf25b730",
            "979e19bd39424ae38224ccb66faa9c26",
            "8b86556d2e84408c8c8a0fa614dcc73a",
            "2d8b2d281bec4e848c61c9e3fb2bb011",
            "a810085f6e7e490c93d8b1ea15afc117",
            "259df72b69744eba9ae9223ede1e7ddf",
            "26f72fa87d274f42a783ac8675394b16",
            "9888f124f5fd490aa5e3ffe525fa06b7",
            "8aac5f4987ae470fbb0739f50fa82d00",
            "1ca64de2015b45499886841236e81da8",
            "5fbc070393744c63819eb1190b197442",
            "1b1e8ad2e8914b91b36a563f11c7bfc8",
            "d1a69df425184c67a3d3db5af21e9be7",
            "8088dea0c3cb49e984aae7102ce1a5e5",
            "42386fd45f874130b3970ad0cbbde08c",
            "1175f52a956a45cb9f429c373d2c9c6f",
            "4ec05d8c11ba42d6b432b6be5f7bea11",
            "337e949e516b411f93168d9d7390cb97",
            "f1634d486b07408c93f6d037f3a7fa27",
            "660edc6a9e1a459d873dc44ae1d33bf2",
            "c1043f7033704ad9b607c2fb9ba9b599",
            "aec085822f50466e80e1fbac61f749a5",
            "b70874d232d74693bf036281edb7c97f"
          ]
        },
        "id": "a13d92e8",
        "outputId": "c687bea6-b600-4157-ac2a-8f176f1c8c9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77db9577636045b3af64ad20c4d4e708",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "979e19bd39424ae38224ccb66faa9c26",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.84G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d1a69df425184c67a3d3db5af21e9be7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset: /content/drive/MyDrive/ERP Data/all_window_datasets_unscaled.npz\n"
          ]
        }
      ],
      "source": [
        "# ========================= Global Config & Tokenizer/Collator =========================\n",
        "set_seed(42)\n",
        "ensure_chronos_import()\n",
        "\n",
        "from chronos import BaseChronosPipeline, ChronosConfig\n",
        "\n",
        "# Model path and base model\n",
        "base_model_id = \"amazon/chronos-t5-large\"\n",
        "windows = [5, 21, 252, 512]\n",
        "\n",
        "# Data path (time order not shuffled)\n",
        "default_data_path = \"/content/drive/MyDrive/ERP Data/all_window_datasets_unscaled.npz\"\n",
        "assert os.path.exists(default_data_path), \"Dataset not found. Please ensure all_window_datasets_unscaled.npz exists on Drive.\"\n",
        "data_path = os.environ.get(\"ERP_DATA_PATH\", default_data_path)\n",
        "\n",
        "# Save directories\n",
        "root_dir = \"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results\"\n",
        "save_models_dir = os.path.join(root_dir, \"models\")\n",
        "save_preds_dir = os.path.join(root_dir, \"predictions\")\n",
        "save_results_dir = root_dir\n",
        "for d in [save_models_dir, save_preds_dir, save_results_dir]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "# Device and base pipeline (for inferring tokenizer config)\n",
        "device = get_device()\n",
        "print(f\"Device: {device}\")\n",
        "pipeline = BaseChronosPipeline.from_pretrained(\n",
        "    base_model_id,\n",
        "    device_map=\"mps\" if (device.type == \"mps\") else (\"auto\" if torch.cuda.is_available() else None),\n",
        "    torch_dtype=torch.float32,\n",
        ")\n",
        "\n",
        "# Build training config from base config (prediction_length=1)\n",
        "base_cfg: ChronosConfig = pipeline.model.config  # type: ignore\n",
        "train_cfg = ChronosConfig(\n",
        "    tokenizer_class=base_cfg.tokenizer_class,\n",
        "    tokenizer_kwargs=base_cfg.tokenizer_kwargs,\n",
        "    context_length=base_cfg.context_length,\n",
        "    prediction_length=1,\n",
        "    n_tokens=base_cfg.n_tokens,\n",
        "    n_special_tokens=base_cfg.n_special_tokens,\n",
        "    pad_token_id=base_cfg.pad_token_id,\n",
        "    eos_token_id=base_cfg.eos_token_id,\n",
        "    use_eos_token=base_cfg.use_eos_token,\n",
        "    model_type=base_cfg.model_type,\n",
        "    num_samples=base_cfg.num_samples,\n",
        "    temperature=base_cfg.temperature,\n",
        "    top_k=base_cfg.top_k,\n",
        "    top_p=base_cfg.top_p,\n",
        ")\n",
        "train_tokenizer = train_cfg.create_tokenizer()\n",
        "\n",
        "# Collator: right pad input_ids in batch, adjust labels length\n",
        "class ChronosPadCollator:\n",
        "    def __init__(self, pad_token_id: int, label_len_expected: int):\n",
        "        self.pad_token_id = pad_token_id\n",
        "        self.label_len_expected = int(label_len_expected)\n",
        "    def __call__(self, features: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
        "        max_inp = max(f[\"input_ids\"].shape[0] for f in features)\n",
        "        input_ids, attention_masks, labels = [], [], []\n",
        "        for f in features:\n",
        "            Li = f[\"input_ids\"].shape[0]\n",
        "            pad_inp = max_inp - Li\n",
        "            input_ids.append(torch.nn.functional.pad(f[\"input_ids\"], (0, pad_inp), value=self.pad_token_id))\n",
        "            attention_masks.append(torch.nn.functional.pad(f[\"attention_mask\"].to(torch.long), (0, pad_inp), value=0))\n",
        "            lbl = f[\"labels\"]\n",
        "            Lexp = self.label_len_expected\n",
        "            if lbl.shape[0] > Lexp:\n",
        "                lbl = lbl[-Lexp:]\n",
        "            elif lbl.shape[0] < Lexp:\n",
        "                lbl = torch.nn.functional.pad(lbl, (0, Lexp - lbl.shape[0]), value=-100)\n",
        "            labels.append(lbl)\n",
        "        return {\n",
        "            \"input_ids\": torch.stack(input_ids, dim=0),\n",
        "            \"attention_mask\": torch.stack(attention_masks, dim=0),\n",
        "            \"labels\": torch.stack(labels, dim=0),\n",
        "        }\n",
        "\n",
        "label_len_expected = train_cfg.prediction_length + (1 if (train_cfg.use_eos_token and train_cfg.model_type==\"seq2seq\") else 0)\n",
        "pad_collator = ChronosPadCollator(pad_token_id=train_tokenizer.config.pad_token_id, label_len_expected=label_len_expected)\n",
        "\n",
        "print(f\"Loading dataset: {data_path}\")\n",
        "data = load_npz_dataset(data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8GhEFa4R5q9K",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8GhEFa4R5q9K",
        "outputId": "528401f8-7895-45e7-87ea-a7ff151ff60d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using per-window empirical hyperparameters.\n",
            "Using Colab-adaptive batch sizes:\n",
            "window=5: train_bs=1024, eval_bs=1024\n",
            "window=21: train_bs=1024, eval_bs=1024\n",
            "window=252: train_bs=256, eval_bs=512\n",
            "window=512: train_bs=128, eval_bs=256\n",
            "\n",
            "===== Training LoRA for window=5 =====\n",
            "trainable params: 8,650,752 || all params: 717,614,080 || trainable%: 1.2055\n",
            "Start fine-tuning (single window)...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='770' max='770' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [770/770 10:16, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.391183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>4.659300</td>\n",
              "      <td>3.386115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.380100</td>\n",
              "      <td>3.385463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.375300</td>\n",
              "      <td>3.385317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.375300</td>\n",
              "      <td>3.386373</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Saved] LoRA adapter: /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/models/chronos_t5_large_lora_w5/chronos_t5_lora_adapter\n",
            "\n",
            "===== Training LoRA for window=21 =====\n",
            "trainable params: 8,650,752 || all params: 717,614,080 || trainable%: 1.2055\n",
            "Start fine-tuning (single window)...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='770' max='770' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [770/770 19:08, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.326613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>4.721500</td>\n",
              "      <td>3.321280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.315400</td>\n",
              "      <td>3.320620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.312900</td>\n",
              "      <td>3.320474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.312900</td>\n",
              "      <td>3.321683</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Saved] LoRA adapter: /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/models/chronos_t5_large_lora_w21/chronos_t5_lora_adapter\n",
            "\n",
            "===== Training LoRA for window=252 =====\n",
            "trainable params: 17,301,504 || all params: 726,264,832 || trainable%: 2.3823\n",
            "Start fine-tuning (single window)...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2885' max='2885' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2885/2885 3:27:50, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.345300</td>\n",
              "      <td>3.309152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.312400</td>\n",
              "      <td>3.306160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.293500</td>\n",
              "      <td>3.305461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.289900</td>\n",
              "      <td>3.304948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.283600</td>\n",
              "      <td>3.304564</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Saved] LoRA adapter: /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/models/chronos_t5_large_lora_w252/chronos_t5_lora_adapter\n",
            "\n",
            "===== Training LoRA for window=512 =====\n",
            "trainable params: 17,301,504 || all params: 726,264,832 || trainable%: 2.3823\n",
            "Start fine-tuning (single window)...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5365' max='5365' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5365/5365 8:04:07, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.254800</td>\n",
              "      <td>3.293710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.185800</td>\n",
              "      <td>3.289315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.262900</td>\n",
              "      <td>3.287551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.233000</td>\n",
              "      <td>3.286320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.139300</td>\n",
              "      <td>3.286705</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Saved] LoRA adapter: /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/models/chronos_t5_large_lora_w512/chronos_t5_lora_adapter\n",
            "[Save] Metrics saved to: /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/chronos_t5_large_lora_per_window_metrics.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ========================= Empirical hyperparameters (per window) =========================\n",
        "per_window_cfg = {\n",
        "    5:   dict(epochs=5, patience=2, lr=5e-4, wd=0.005, r=8,  alpha=16,\n",
        "              dropout=0.05, warmup_ratio=0.10, train_bs=512, eval_bs=512),\n",
        "    21:  dict(epochs=5, patience=2, lr=6e-4, wd=0.005, r=8,  alpha=16,\n",
        "              dropout=0.05, warmup_ratio=0.10, train_bs=512, eval_bs=512),\n",
        "    252: dict(epochs=5, patience=2, lr=3e-4, wd=0.010, r=16, alpha=32,\n",
        "              dropout=0.07, warmup_ratio=0.10, train_bs=128, eval_bs=256),\n",
        "    512: dict(epochs=5, patience=2, lr=2e-4, wd=0.010, r=16, alpha=32,\n",
        "              dropout=0.08, warmup_ratio=0.10, train_bs=64,  eval_bs=128),\n",
        "}\n",
        "per_window_cfg = adapt_per_window_cfg_for_colab(per_window_cfg)\n",
        "\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "DO_TEST_EVAL = False\n",
        "\n",
        "results_summary: List[Dict[str, Any]] = []\n",
        "\n",
        "for ws in windows:\n",
        "    cfg = per_window_cfg[ws]\n",
        "\n",
        "    X_train, y_train, meta_train = extract_split(data, ws, split=\"train\")\n",
        "    X_test, y_test, meta_test = extract_split(data, ws, split=\"test\")\n",
        "    (X_tr, y_tr), (X_va, y_va), _ = time_based_val_split(X_train, y_train, meta_train, val_ratio=0.2)\n",
        "\n",
        "    base_cfg: ChronosConfig = pipeline.model.config  # type: ignore\n",
        "    ws_cfg = ChronosConfig(\n",
        "        tokenizer_class=base_cfg.tokenizer_class,\n",
        "        tokenizer_kwargs=base_cfg.tokenizer_kwargs,\n",
        "        context_length=base_cfg.context_length,\n",
        "        prediction_length=1,\n",
        "        n_tokens=base_cfg.n_tokens,\n",
        "        n_special_tokens=base_cfg.n_special_tokens,\n",
        "        pad_token_id=base_cfg.pad_token_id,\n",
        "        eos_token_id=base_cfg.eos_token_id,\n",
        "        use_eos_token=base_cfg.use_eos_token,\n",
        "        model_type=base_cfg.model_type,\n",
        "        num_samples=base_cfg.num_samples,\n",
        "        temperature=base_cfg.temperature,\n",
        "        top_k=base_cfg.top_k,\n",
        "        top_p=base_cfg.top_p,\n",
        "    )\n",
        "    ws_tokenizer = ws_cfg.create_tokenizer()\n",
        "\n",
        "    ds_tr = ChronosWindowDataset(X_tr, y_tr, ws_tokenizer, prediction_length=1)\n",
        "    ds_va = ChronosWindowDataset(X_va, y_va, ws_tokenizer, prediction_length=1)\n",
        "\n",
        "    base = AutoModelForSeq2SeqLM.from_pretrained(base_model_id)\n",
        "    base = bind_strip_num_items(base)\n",
        "    lora_cfg = build_lora_config(r=cfg[\"r\"], alpha=cfg[\"alpha\"], dropout=cfg[\"dropout\"], target_modules=[\"q\",\"k\",\"v\",\"o\",\"wi\",\"wo\"])\n",
        "    peft_model = get_peft_model(base, lora_cfg)\n",
        "    try:\n",
        "        peft_model.print_trainable_parameters()\n",
        "    except Exception:\n",
        "        pass\n",
        "    num_trainable = sum(p.requires_grad for p in peft_model.parameters())\n",
        "    if num_trainable == 0:\n",
        "        fallback_targets = [\"q\", \"k\", \"v\", \"o\", \"wi_0\", \"wi_1\", \"wo\"]\n",
        "        lora_cfg_fb = build_lora_config(r=cfg[\"r\"], alpha=cfg[\"alpha\"], dropout=cfg[\"dropout\"], target_modules=fallback_targets)\n",
        "        peft_model = get_peft_model(base, lora_cfg_fb)\n",
        "        try:\n",
        "            peft_model.print_trainable_parameters()\n",
        "        except Exception:\n",
        "            pass\n",
        "    try:\n",
        "        peft_model.enable_input_require_grads()\n",
        "    except Exception:\n",
        "        pass\n",
        "    peft_model = maybe_torch_compile(peft_model)\n",
        "    try:\n",
        "        peft_model.config.use_cache = False\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    out_dir_ws = os.path.join(save_models_dir, f\"chronos_t5_large_lora_w{ws}\")\n",
        "    os.makedirs(out_dir_ws, exist_ok=True)\n",
        "\n",
        "    trainer_ws = create_trainer(\n",
        "        model=peft_model,\n",
        "        train_ds=ds_tr,\n",
        "        val_ds=ds_va,\n",
        "        output_dir=out_dir_ws,\n",
        "        per_device_train_batch_size=cfg[\"train_bs\"],\n",
        "        per_device_eval_batch_size=cfg[\"eval_bs\"],\n",
        "        learning_rate=cfg[\"lr\"],\n",
        "        weight_decay=cfg[\"wd\"],\n",
        "        num_train_epochs=cfg[\"epochs\"],\n",
        "        patience=cfg[\"patience\"],\n",
        "        logging_steps=200,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        dataloader_num_workers=0,\n",
        "        data_collator=pad_collator,\n",
        "    )\n",
        "\n",
        "    train_output = trainer_ws.train()\n",
        "\n",
        "    adapter_dir = os.path.join(out_dir_ws, \"chronos_t5_lora_adapter\"); os.makedirs(adapter_dir, exist_ok=True)\n",
        "    peft_model.save_pretrained(adapter_dir)\n",
        "\n",
        "    if DO_TEST_EVAL:\n",
        "        base_infer = AutoModelForSeq2SeqLM.from_pretrained(base_model_id)\n",
        "        peft_infer = PeftModel.from_pretrained(base_infer, adapter_dir).to(device)\n",
        "        peft_infer = maybe_torch_compile(peft_infer)\n",
        "        peft_infer.eval()\n",
        "        try:\n",
        "            peft_infer.config.use_cache = True\n",
        "        except Exception:\n",
        "            pass\n",
        "        pipeline.model.model = peft_infer\n",
        "\n",
        "        @torch.no_grad()\n",
        "        def batch_predict_with_pipeline(X: np.ndarray, pipeline, batch_size: int = 256, num_samples: int = 10, prediction_length: int = 1) -> np.ndarray:\n",
        "            preds: List[float] = []\n",
        "            for i in tqdm(range(0, len(X), batch_size), desc=\"Batch Inference\"):\n",
        "                ctx_list = [torch.from_numpy(seq.astype(np.float32)) for seq in X[i:i + batch_size]]\n",
        "                fr = pipeline.predict(context=ctx_list, prediction_length=prediction_length, num_samples=num_samples)\n",
        "                means = fr.mean(dim=(1, 2)).cpu().numpy() if isinstance(fr, torch.Tensor) else np.array([np.array(f).mean() for f in fr])\n",
        "                preds.extend(means.tolist())\n",
        "            return np.array(preds, dtype=np.float32)\n",
        "\n",
        "        y_pred = batch_predict_with_pipeline(X_test, pipeline, batch_size=cfg[\"eval_bs\"], num_samples=10, prediction_length=1).reshape(-1)\n",
        "        permnos = meta_test[\"PERMNO\"].values if \"PERMNO\" in meta_test.columns else None\n",
        "        k_features = X_test.shape[1]\n",
        "        metrics = regression_metrics(y_true=y_test.reshape(-1), y_pred=y_pred.reshape(-1), k=k_features, meta=meta_test, permnos=permnos)\n",
        "\n",
        "        pred_df = pd.DataFrame({\n",
        "            \"PERMNO\": meta_test.get(\"PERMNO\", pd.Series([np.nan] * len(y_pred))),\n",
        "            \"DATE\": meta_test.get(\"DATE\", meta_test.get(\"date\", pd.Series([np.nan] * len(y_pred)))),\n",
        "            \"y_true\": y_test.reshape(-1),\n",
        "            \"y_pred\": y_pred,\n",
        "        })\n",
        "        csv_path = os.path.join(save_preds_dir, f\"chronos_t5_large_lora_w{ws}.csv\")\n",
        "        pred_df.to_csv(csv_path, index=False)\n",
        "\n",
        "        results_summary.append({\"Window\": ws, **metrics, \"pred_path\": csv_path, \"adapter_dir\": adapter_dir})\n",
        "\n",
        "metrics_csv = os.path.join(save_results_dir, \"chronos_t5_large_lora_per_window_metrics.csv\")\n",
        "pd.DataFrame(results_summary).to_csv(metrics_csv, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "550d2a27",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "550d2a27",
        "outputId": "f10045be-c724-4e13-d8c1-0b42f9b55641"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Chronos T5-Large Portfolio Backtesting...\n",
            "Starting Daily Rebalance Portfolio Backtesting Simulation\n",
            "Processing window size: 5\n",
            "[INFO] Loaded LoRA adapter for window=5 from: /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/models/chronos_t5_large_lora_w5/chronos_t5_lora_adapter\n",
            "[INFO] Backtest inference model: LoRA (window=5)\n",
            "  Model: chronos large, Scheme: VW\n",
            "  Processing year: 2016\n",
            "  Processing year: 2017\n",
            "  Processing year: 2018\n",
            "  Processing year: 2019\n",
            "  Processing year: 2020\n",
            "  Processing year: 2021\n",
            "  Processing year: 2022\n",
            "  Processing year: 2023\n",
            "  Processing year: 2024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2075427930.py:55: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda g: g['y_pred'].corr(g['y_true'], method=method))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics saved for chronos large_w5 to /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_metrics.csv\n",
            "  Model: chronos large, Scheme: EW\n",
            "  Processing year: 2016\n",
            "  Processing year: 2017\n",
            "  Processing year: 2018\n",
            "  Processing year: 2019\n",
            "  Processing year: 2020\n",
            "  Processing year: 2021\n",
            "  Processing year: 2022\n",
            "  Processing year: 2023\n",
            "  Processing year: 2024\n",
            "Processing window size: 21\n",
            "[INFO] Loaded LoRA adapter for window=21 from: /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/models/chronos_t5_large_lora_w21/chronos_t5_lora_adapter\n",
            "[INFO] Backtest inference model: LoRA (window=21)\n",
            "  Model: chronos large, Scheme: VW\n",
            "  Processing year: 2016\n",
            "  Processing year: 2017\n",
            "  Processing year: 2018\n",
            "  Processing year: 2019\n",
            "  Processing year: 2020\n",
            "  Processing year: 2021\n",
            "  Processing year: 2022\n",
            "  Processing year: 2023\n",
            "  Processing year: 2024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2075427930.py:55: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda g: g['y_pred'].corr(g['y_true'], method=method))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics saved for chronos large_w21 to /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_metrics.csv\n",
            "  Model: chronos large, Scheme: EW\n",
            "  Processing year: 2016\n",
            "  Processing year: 2017\n",
            "  Processing year: 2018\n",
            "  Processing year: 2019\n",
            "  Processing year: 2020\n",
            "  Processing year: 2021\n",
            "  Processing year: 2022\n",
            "  Processing year: 2023\n",
            "  Processing year: 2024\n",
            "Processing window size: 252\n",
            "[INFO] Loaded LoRA adapter for window=252 from: /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/models/chronos_t5_large_lora_w252/chronos_t5_lora_adapter\n",
            "[INFO] Backtest inference model: LoRA (window=252)\n",
            "  Model: chronos large, Scheme: VW\n",
            "  Processing year: 2016\n",
            "  Processing year: 2017\n",
            "  Processing year: 2018\n",
            "  Processing year: 2019\n",
            "  Processing year: 2020\n",
            "  Processing year: 2021\n",
            "  Processing year: 2022\n",
            "  Processing year: 2023\n",
            "  Processing year: 2024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2075427930.py:55: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda g: g['y_pred'].corr(g['y_true'], method=method))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics saved for chronos large_w252 to /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_metrics.csv\n",
            "  Model: chronos large, Scheme: EW\n",
            "  Processing year: 2016\n",
            "  Processing year: 2017\n",
            "  Processing year: 2018\n",
            "  Processing year: 2019\n",
            "  Processing year: 2020\n",
            "  Processing year: 2021\n",
            "  Processing year: 2022\n",
            "  Processing year: 2023\n",
            "  Processing year: 2024\n",
            "Processing window size: 512\n",
            "[INFO] Loaded LoRA adapter for window=512 from: /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/models/chronos_t5_large_lora_w512/chronos_t5_lora_adapter\n",
            "[INFO] Backtest inference model: LoRA (window=512)\n",
            "  Model: chronos large, Scheme: VW\n",
            "  Processing year: 2016\n",
            "  Processing year: 2017\n",
            "  Processing year: 2018\n",
            "  Processing year: 2019\n",
            "  Processing year: 2020\n",
            "  Processing year: 2021\n",
            "  Processing year: 2022\n",
            "  Processing year: 2023\n",
            "  Processing year: 2024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2075427930.py:55: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda g: g['y_pred'].corr(g['y_true'], method=method))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics saved for chronos large_w512 to /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_metrics.csv\n",
            "  Model: chronos large, Scheme: EW\n",
            "  Processing year: 2016\n",
            "  Processing year: 2017\n",
            "  Processing year: 2018\n",
            "  Processing year: 2019\n",
            "  Processing year: 2020\n",
            "  Processing year: 2021\n",
            "  Processing year: 2022\n",
            "  Processing year: 2023\n",
            "  Processing year: 2024\n",
            "VW results saved to /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_results_daily_rebalance_VW.csv\n",
            "EW results saved to /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_results_daily_rebalance_EW.csv\n",
            "VW results saved to /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_daily_series_VW.csv\n",
            "EW results saved to /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_daily_series_EW.csv\n",
            "Saved 443400 prediction rows to predictions_daily.csv\n",
            "Generated 24 portfolio summary records\n",
            "Generated 52272 daily series records\n",
            "\\n============================================================\n",
            "CHRONOS T5-BASE PORTFOLIO BACKTESTING RESULTS\n",
            "============================================================\n",
            "\\nSummary Results:\n",
            "   scheme          model  window portfolio_type  annual_return  annual_vol  \\\n",
            "0      VW  chronos large       5      long_only         0.1056      0.2037   \n",
            "1      VW  chronos large       5     short_only        -0.1431      0.1997   \n",
            "2      VW  chronos large       5     long_short        -0.0375      0.1887   \n",
            "3      EW  chronos large       5      long_only         0.0827      0.2134   \n",
            "4      EW  chronos large       5     short_only        -0.1874      0.1979   \n",
            "5      EW  chronos large       5     long_short        -0.1047      0.1642   \n",
            "6      VW  chronos large      21      long_only         0.1367      0.2074   \n",
            "7      VW  chronos large      21     short_only        -0.1460      0.1980   \n",
            "8      VW  chronos large      21     long_short        -0.0093      0.1903   \n",
            "9      EW  chronos large      21      long_only         0.1048      0.2198   \n",
            "10     EW  chronos large      21     short_only        -0.1505      0.1994   \n",
            "11     EW  chronos large      21     long_short        -0.0457      0.1663   \n",
            "12     VW  chronos large     252      long_only         0.1443      0.2047   \n",
            "13     VW  chronos large     252     short_only        -0.1053      0.1981   \n",
            "14     VW  chronos large     252     long_short         0.0390      0.2013   \n",
            "15     EW  chronos large     252      long_only         0.1765      0.2087   \n",
            "16     EW  chronos large     252     short_only        -0.1351      0.2082   \n",
            "17     EW  chronos large     252     long_short         0.0414      0.1792   \n",
            "18     VW  chronos large     512      long_only         0.1253      0.2085   \n",
            "19     VW  chronos large     512     short_only        -0.1073      0.1885   \n",
            "20     VW  chronos large     512     long_short         0.0179      0.1967   \n",
            "21     EW  chronos large     512      long_only         0.0923      0.2124   \n",
            "22     EW  chronos large     512     short_only        -0.1754      0.1914   \n",
            "23     EW  chronos large     512     long_short        -0.0831      0.1727   \n",
            "\n",
            "    sharpe  max_drawdown  max_1d_loss  avg_turnover  ...  tc20_sharpe  \\\n",
            "0   0.5183        0.2420      -0.0528        1.7573  ...      -3.8207   \n",
            "1  -0.7167        0.7807      -0.0734        1.7773  ...      -5.1987   \n",
            "2  -0.1989        0.5131      -0.0473        3.5348  ...      -9.5948   \n",
            "3   0.3877        0.3861      -0.0629        1.5827  ...      -3.3515   \n",
            "4  -0.9473        0.8508      -0.0473        1.7016  ...      -5.2790   \n",
            "5  -0.6377        0.6926      -0.0459        3.2848  ...     -10.7094   \n",
            "6   0.6591        0.3315      -0.0588        1.7471  ...      -3.5792   \n",
            "7  -0.7373        0.7757      -0.0487        1.7718  ...      -5.2261   \n",
            "8  -0.0490        0.4541      -0.0547        3.5188  ...      -9.3133   \n",
            "9   0.4768        0.3260      -0.0617        1.6045  ...      -3.1968   \n",
            "10 -0.7546        0.8120      -0.0488        1.6575  ...      -4.9377   \n",
            "11 -0.2746        0.5872      -0.0416        3.2620  ...     -10.0713   \n",
            "12  0.7048        0.2810      -0.0492        1.7327  ...      -3.5489   \n",
            "13 -0.5315        0.7513      -0.0734        1.7704  ...      -5.0202   \n",
            "14  0.1937        0.4955      -0.0816        3.5029  ...      -8.5328   \n",
            "15  0.8457        0.4122      -0.0611        1.6117  ...      -3.0389   \n",
            "16 -0.6489        0.7840      -0.0543        1.6338  ...      -4.5946   \n",
            "17  0.2312        0.5525      -0.0633        3.2455  ...      -8.8807   \n",
            "18  0.6009        0.3597      -0.0562        1.7643  ...      -3.6548   \n",
            "19 -0.5696        0.7144      -0.0449        1.7809  ...      -5.3254   \n",
            "20  0.0912        0.3273      -0.0596        3.5450  ...      -8.9511   \n",
            "21  0.4345        0.3841      -0.0568        1.6192  ...      -3.3996   \n",
            "22 -0.9161        0.8362      -0.0431        1.6214  ...      -5.1792   \n",
            "23 -0.4811        0.6768      -0.0475        3.2407  ...      -9.9117   \n",
            "\n",
            "    tc20_max_drawdown  tc30_annual_return  tc30_annual_vol  tc30_sharpe  \\\n",
            "0             -0.9990             -1.2229           0.2047      -5.9750   \n",
            "1             -0.9999             -1.4867           0.2002      -7.4264   \n",
            "2             -1.0000             -2.7099           0.1907     -14.2111   \n",
            "3             -0.9983             -1.1138           0.2135      -5.2169   \n",
            "4             -0.9999             -1.4739           0.1982      -7.4364   \n",
            "5             -1.0000             -2.5880           0.1650     -15.6892   \n",
            "6             -0.9987             -1.1841           0.2083      -5.6839   \n",
            "7             -0.9999             -1.4855           0.1995      -7.4466   \n",
            "8             -1.0000             -2.6695           0.1926     -13.8595   \n",
            "9             -0.9982             -1.1082           0.2205      -5.0249   \n",
            "10            -0.9998             -1.4035           0.2000      -7.0184   \n",
            "11            -1.0000             -2.5117           0.1690     -14.8617   \n",
            "12            -0.9985             -1.1656           0.2061      -5.6568   \n",
            "13            -0.9999             -1.4437           0.1993      -7.2443   \n",
            "14            -1.0000             -2.6092           0.2035     -12.8219   \n",
            "15            -0.9967             -1.0419           0.2097      -4.9698   \n",
            "16            -0.9998             -1.3702           0.2090      -6.5563   \n",
            "17            -1.0000             -2.4122           0.1802     -13.3893   \n",
            "18            -0.9989             -1.2085           0.2096      -5.7666   \n",
            "19            -0.9999             -1.4537           0.1891      -7.6875   \n",
            "20            -1.0000             -2.6621           0.1986     -13.4025   \n",
            "21            -0.9985             -1.1318           0.2133      -5.3056   \n",
            "22            -0.9999             -1.4011           0.1920      -7.2988   \n",
            "23            -1.0000             -2.5330           0.1739     -14.5651   \n",
            "\n",
            "    tc30_max_drawdown  tc40_annual_return  tc40_annual_vol  tc40_sharpe  \\\n",
            "0             -1.0000             -1.6658           0.2054      -8.1119   \n",
            "1             -1.0000             -1.9346           0.2008      -9.6367   \n",
            "2             -1.0000             -3.6006           0.1922     -18.7339   \n",
            "3             -0.9999             -1.5127           0.2138      -7.0748   \n",
            "4             -1.0000             -1.9027           0.1985      -9.5831   \n",
            "5             -1.0000             -3.4158           0.1659     -20.5918   \n",
            "6             -1.0000             -1.6244           0.2090      -7.7717   \n",
            "7             -1.0000             -1.9319           0.2003      -9.6432   \n",
            "8             -1.0000             -3.5563           0.1942     -18.3112   \n",
            "9             -0.9999             -1.5125           0.2210      -6.8431   \n",
            "10            -1.0000             -1.8212           0.2004      -9.0867   \n",
            "11            -1.0000             -3.3337           0.1705     -19.5476   \n",
            "12            -1.0000             -1.6023           0.2069      -7.7439   \n",
            "13            -1.0000             -1.8899           0.2001      -9.4468   \n",
            "14            -1.0000             -3.4919           0.2051     -17.0284   \n",
            "15            -0.9999             -1.4481           0.2102      -6.8883   \n",
            "16            -1.0000             -1.7820           0.2095      -8.5062   \n",
            "17            -1.0000             -3.2300           0.1811     -17.8374   \n",
            "18            -1.0000             -1.6531           0.2103      -7.8604   \n",
            "19            -1.0000             -1.9025           0.1897     -10.0302   \n",
            "20            -1.0000             -3.5555           0.2000     -17.7754   \n",
            "21            -1.0000             -1.5399           0.2139      -7.1999   \n",
            "22            -1.0000             -1.8097           0.1924      -9.4041   \n",
            "23            -1.0000             -3.3497           0.1750     -19.1438   \n",
            "\n",
            "    tc40_max_drawdown  \n",
            "0                -1.0  \n",
            "1                -1.0  \n",
            "2                -1.0  \n",
            "3                -1.0  \n",
            "4                -1.0  \n",
            "5                -1.0  \n",
            "6                -1.0  \n",
            "7                -1.0  \n",
            "8                -1.0  \n",
            "9                -1.0  \n",
            "10               -1.0  \n",
            "11               -1.0  \n",
            "12               -1.0  \n",
            "13               -1.0  \n",
            "14               -1.0  \n",
            "15               -1.0  \n",
            "16               -1.0  \n",
            "17               -1.0  \n",
            "18               -1.0  \n",
            "19               -1.0  \n",
            "20               -1.0  \n",
            "21               -1.0  \n",
            "22               -1.0  \n",
            "23               -1.0  \n",
            "\n",
            "[24 rows x 32 columns]\n"
          ]
        }
      ],
      "source": [
        "def run_chronos_portfolio_backtest(start_year=2016, end_year=2024, window_sizes=None, model_names=None,\n",
        "                                   npz_path=\"/content/drive/MyDrive/ERP Data/all_window_datasets_unscaled.npz\"):\n",
        "    \"\"\"\n",
        "    Portfolio simulation (daily prediction, next-day rebalance):\n",
        "        1. Use Chronos T5 Large model for zero-shot prediction\n",
        "        2. Daily prediction to daily signal\n",
        "        3. Daily portfolio construction (T+1 rebalance, strict permno alignment)\n",
        "        4. Separate summary metrics and time series data\n",
        "    \"\"\"\n",
        "    if window_sizes is None:\n",
        "        window_sizes = [5, 21, 252, 512]\n",
        "    if model_names is None:\n",
        "        model_names = [\"chronos large\"]\n",
        "\n",
        "    print(\"Starting Daily Rebalance Portfolio Backtesting Simulation\")\n",
        "\n",
        "    backtester = PortfolioBacktester()\n",
        "    datasets = load_datasets(npz_path)\n",
        "\n",
        "    summary_results = []\n",
        "    daily_series_data = []\n",
        "    pred_rows = []\n",
        "\n",
        "    WEIGHT_SCHEMES = [\"VW\", \"EW\"]\n",
        "\n",
        "    for window in window_sizes:\n",
        "        print(f\"Processing window size: {window}\")\n",
        "\n",
        "        try:\n",
        "            from transformers import AutoModelForSeq2SeqLM\n",
        "            adapter_dir = f\"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/models/chronos_t5_large_lora_w{window}/chronos_t5_lora_adapter\"\n",
        "            base_infer = AutoModelForSeq2SeqLM.from_pretrained(\"amazon/chronos-t5-large\")\n",
        "            peft_infer = PeftModel.from_pretrained(base_infer, adapter_dir).to(device).eval()\n",
        "            try:\n",
        "                peft_infer.config.use_cache = True\n",
        "            except Exception:\n",
        "                pass\n",
        "            pipeline.model.model = peft_infer\n",
        "            print(f\"[INFO] Loaded LoRA adapter for window={window} from: {adapter_dir}\")\n",
        "            print(f\"[INFO] Backtest inference model: LoRA (window={window})\")\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Could not load LoRA adapter for window={window}: {e}. Using base model.\")\n",
        "            print(f\"[INFO] Backtest inference model: BASE (window={window})\")\n",
        "\n",
        "        X_test = datasets[f\"X_test_{window}\"]\n",
        "        y_test = datasets[f\"y_test_{window}\"]\n",
        "        meta_test_dict = datasets[f\"meta_test_{window}\"].item()\n",
        "        meta_test = pd.DataFrame.from_dict(meta_test_dict)\n",
        "\n",
        "        permnos_test = meta_test[\"PERMNO\"].values\n",
        "        meta_test[\"signal_date\"]  = pd.to_datetime(meta_test[\"date\"])\n",
        "        meta_test[\"ret_date\"]     = pd.to_datetime(meta_test[\"ret_date\"])\n",
        "        market_caps = meta_test.get(\"MKTCAP\", np.ones(len(permnos_test)))\n",
        "\n",
        "        meta_test['date'] = pd.to_datetime(meta_test[\"date\"])\n",
        "        dates_test = meta_test['signal_date']\n",
        "\n",
        "        for model_name in model_names:\n",
        "            for scheme in WEIGHT_SCHEMES:\n",
        "                all_y_true   = []\n",
        "                all_y_pred   = []\n",
        "                all_permnos  = []\n",
        "                all_meta     = []\n",
        "                print(f\"  Model: {model_name}, Scheme: {scheme}\")\n",
        "\n",
        "                portfolio_daily_data = {\n",
        "                    'long_only': {'returns': [], 'turnovers': [], 'dates': []},\n",
        "                    'short_only': {'returns': [], 'turnovers': [], 'dates': []},\n",
        "                    'long_short': {'returns': [], 'turnovers': [], 'dates': []}\n",
        "                }\n",
        "\n",
        "                prev_portfolio_data = {'long_only': None, 'short_only': None, 'long_short': None}\n",
        "\n",
        "                signals_buf = {}\n",
        "\n",
        "                for year in range(start_year, min(end_year + 1, 2025)):\n",
        "                    print(f\"  Processing year: {year}\")\n",
        "\n",
        "                    year_mask = (dates_test.dt.year == year)\n",
        "                    if not np.any(year_mask):\n",
        "                        continue\n",
        "\n",
        "                    X_year = X_test[year_mask]\n",
        "                    y_year = y_test[year_mask]\n",
        "                    permnos_year = permnos_test[year_mask]\n",
        "                    market_caps_year = market_caps[year_mask]\n",
        "                    dates_year = dates_test[year_mask]\n",
        "                    ret_dates_year = meta_test.loc[year_mask, 'ret_date'].values\n",
        "\n",
        "                    batch_size = get_batch_size(window)\n",
        "                    predictions_year = chronos_rolling_prediction(\n",
        "                        pipeline=pipeline,\n",
        "                        X_data=X_year,\n",
        "                        batch_size=batch_size,\n",
        "                        prediction_length=1\n",
        "                    )\n",
        "\n",
        "                    df_quarter = pd.DataFrame({\n",
        "                        'signal_date': dates_year,\n",
        "                        'ret_date': ret_dates_year,\n",
        "                        'permno': permnos_year,\n",
        "                        'market_cap': market_caps_year,\n",
        "                        'actual_return': y_year,\n",
        "                        'prediction': predictions_year\n",
        "                    })\n",
        "\n",
        "                    if scheme == 'VW':\n",
        "                        df_q_save = df_quarter[['signal_date','ret_date','permno',\n",
        "                                                'actual_return','prediction','market_cap']].copy()\n",
        "                        df_q_save.rename(columns={'actual_return':'y_true',\n",
        "                                                  'prediction':'y_pred'}, inplace=True)\n",
        "                        df_q_save['model']  = model_name\n",
        "                        df_q_save['window'] = window\n",
        "                        pred_rows.append(df_q_save)\n",
        "\n",
        "                    all_y_true.append(df_quarter['actual_return'].values)\n",
        "                    all_y_pred.append(df_quarter['prediction'].values)\n",
        "                    all_permnos.append(df_quarter['permno'].values)\n",
        "                    all_meta.append(meta_test.loc[year_mask, :])\n",
        "\n",
        "                    for signal_date, sig_grp in df_quarter.groupby('signal_date'):\n",
        "\n",
        "                        daily_signals = (\n",
        "                            sig_grp.groupby('permno')['prediction'].mean()\n",
        "                                  .to_frame('prediction')\n",
        "                                  .join(sig_grp.groupby('permno')['market_cap'].mean())\n",
        "                        )\n",
        "                        signals_buf[signal_date] = daily_signals\n",
        "\n",
        "                        prev_date = signal_date - pd.tseries.offsets.BDay(1)\n",
        "                        if prev_date not in signals_buf:\n",
        "                            continue\n",
        "\n",
        "                        sigs = signals_buf.pop(prev_date)\n",
        "\n",
        "                        ret_grp = df_quarter[df_quarter['ret_date'] == signal_date]\n",
        "                        if len(ret_grp) == 0:\n",
        "                            continue\n",
        "\n",
        "                        daily_actual_returns = (\n",
        "                            ret_grp.groupby('permno')['actual_return']\n",
        "                                   .mean()\n",
        "                                   .reindex(sigs.index, fill_value=0)\n",
        "                                   .values\n",
        "                        )\n",
        "                        daily_permnos = sigs.index.values\n",
        "\n",
        "                        portfolios_data = backtester.create_portfolios_with_permno_tracking(\n",
        "                            signals      = sigs['prediction'].values,\n",
        "                            market_caps  = sigs['market_cap'].values,\n",
        "                            permnos      = daily_permnos,\n",
        "                            weight_scheme= scheme\n",
        "                        )\n",
        "\n",
        "                        for portfolio_type in ['long_only', 'short_only', 'long_short']:\n",
        "                            portfolio_info = portfolios_data[portfolio_type]\n",
        "\n",
        "                            portfolio_return, aligned_returns = backtester.calculate_aligned_portfolio_return(\n",
        "                                portfolio_weights=portfolio_info['weights'],\n",
        "                                portfolio_permnos=portfolio_info['permnos'],\n",
        "                                actual_returns=daily_actual_returns,\n",
        "                                actual_permnos=daily_permnos\n",
        "                            )\n",
        "\n",
        "                            if prev_portfolio_data[portfolio_type] is not None:\n",
        "                                prev_w_ser = pd.Series(\n",
        "                                    prev_portfolio_data[portfolio_type]['weights'],\n",
        "                                    index=prev_portfolio_data[portfolio_type]['permnos']\n",
        "                                )\n",
        "                                cur_w_ser = pd.Series(\n",
        "                                    portfolio_info['weights'],\n",
        "                                    index=portfolio_info['permnos']\n",
        "                                )\n",
        "\n",
        "                                prev_r_ser = pd.Series(\n",
        "                                    prev_portfolio_data[portfolio_type]['aligned_returns'],\n",
        "                                    index=prev_portfolio_data[portfolio_type]['permnos']\n",
        "                                )\n",
        "\n",
        "                                aligned_prev_w = prev_w_ser.reindex(cur_w_ser.index, fill_value=0).values\n",
        "                                aligned_prev_r = prev_r_ser.reindex(cur_w_ser.index, fill_value=0).values\n",
        "                                aligned_cur_w = cur_w_ser.values\n",
        "\n",
        "                                turnover = backtester.calc_turnover(\n",
        "                                    w_t  = aligned_prev_w,\n",
        "                                    r_t  = aligned_prev_r,\n",
        "                                    w_tp1= aligned_cur_w\n",
        "                                )\n",
        "                            else:\n",
        "                                turnover = np.sum(np.abs(portfolio_info['weights']))\n",
        "\n",
        "                            portfolio_daily_data[portfolio_type]['returns'].append(portfolio_return)\n",
        "                            portfolio_daily_data[portfolio_type]['turnovers'].append(turnover)\n",
        "                            portfolio_daily_data[portfolio_type]['dates'].append(signal_date)\n",
        "\n",
        "                            prev_portfolio_data[portfolio_type] = {\n",
        "                                'weights'        : portfolio_info['weights'],\n",
        "                                'permnos'        : portfolio_info['permnos'],\n",
        "                                'aligned_returns': aligned_returns\n",
        "                            }\n",
        "\n",
        "                for portfolio_type in ['long_only', 'short_only', 'long_short']:\n",
        "                    portfolio_data = portfolio_daily_data[portfolio_type]\n",
        "\n",
        "                    if len(portfolio_data['returns']) > 0:\n",
        "                        metrics = backtester.calculate_metrics(\n",
        "                            returns=portfolio_data['returns'],\n",
        "                            turnover_series=portfolio_data['turnovers']\n",
        "                        )\n",
        "\n",
        "                        rets = np.array(portfolio_data['returns'])\n",
        "                        tovs = np.array(portfolio_data['turnovers'])\n",
        "\n",
        "                        for tc in TC_GRID:\n",
        "                            tag = TC_TAG[tc]\n",
        "                            adj = rets - tovs * tc\n",
        "\n",
        "                            ann_ret = adj.mean() * 252\n",
        "                            ann_vol = adj.std(ddof=1) * np.sqrt(252)\n",
        "                            sharpe  = ann_ret / ann_vol if ann_vol > 0 else 0\n",
        "\n",
        "                            cum_adj = np.cumprod(1 + adj)\n",
        "                            mdd = ((cum_adj - np.maximum.accumulate(cum_adj)) /\n",
        "                                   np.maximum.accumulate(cum_adj)).min()\n",
        "\n",
        "                            metrics[f'{tag}_annual_return'] = ann_ret\n",
        "                            metrics[f'{tag}_annual_vol']    = ann_vol\n",
        "                            metrics[f'{tag}_sharpe']        = sharpe\n",
        "                            metrics[f'{tag}_max_drawdown']  = mdd\n",
        "\n",
        "                        summary_results.append({\n",
        "                            'scheme': scheme,\n",
        "                            'model': model_name,\n",
        "                            'window': window,\n",
        "                            'portfolio_type': portfolio_type,\n",
        "                            **metrics\n",
        "                        })\n",
        "\n",
        "                        rets_arr = np.array(portfolio_data['returns'])\n",
        "                        tovs_arr = np.array(portfolio_data['turnovers'])\n",
        "                        cum_no_tc = np.log1p(rets_arr).cumsum()\n",
        "\n",
        "                        tc_ret_dict = {}\n",
        "                        tc_cum_dict = {}\n",
        "                        for tc in TC_GRID:\n",
        "                            tag = TC_TAG[tc]\n",
        "                            r = rets_arr - tovs_arr * tc\n",
        "                            tc_ret_dict[tag] = r\n",
        "                            tc_cum_dict[tag] = np.log1p(r).cumsum()\n",
        "\n",
        "                        for i, date in enumerate(portfolio_data['dates']):\n",
        "                            row = {\n",
        "                                'scheme'        : scheme,\n",
        "                                'model'         : model_name,\n",
        "                                'window'        : window,\n",
        "                                'portfolio_type': portfolio_type,\n",
        "                                'date'          : str(date),\n",
        "                                'return'        : rets_arr[i],\n",
        "                                'turnover'      : tovs_arr[i],\n",
        "                                'cumulative'    : cum_no_tc[i],\n",
        "                            }\n",
        "                            for tag in TC_TAG.values():\n",
        "                                row[f'{tag}_return']     = tc_ret_dict[tag][i]\n",
        "                                row[f'{tag}_cumulative'] = tc_cum_dict[tag][i]\n",
        "\n",
        "                            daily_series_data.append(row)\n",
        "\n",
        "                if scheme == \"VW\" and len(all_y_true) > 0:\n",
        "                    y_all    = np.concatenate(all_y_true)\n",
        "                    yhat_all = np.concatenate(all_y_pred)\n",
        "                    perm_all = np.concatenate(all_permnos)\n",
        "                    meta_all = pd.concat(all_meta, ignore_index=True)\n",
        "\n",
        "                    k = X_test.shape[1]\n",
        "\n",
        "                    m1_metrics = overall_interval_metrics_method1(\n",
        "                        y_all, yhat_all, k,\n",
        "                        permnos_all=perm_all,\n",
        "                        meta_all=meta_all\n",
        "                    )\n",
        "\n",
        "                    full_pred_df = pd.concat(pred_rows, ignore_index=True)\n",
        "                    full_pred_df['signal_date'] = pd.to_datetime(full_pred_df['signal_date'], errors='coerce')\n",
        "\n",
        "                    cur = full_pred_df.loc[\n",
        "                        (full_pred_df['window'] == window) &\n",
        "                        (full_pred_df['model'] == model_name),\n",
        "                        ['signal_date', 'y_true', 'y_pred']\n",
        "                    ].dropna()\n",
        "\n",
        "                    if len(cur) >= 30:\n",
        "                        mean_ic, t_ic, pos_ic, _ = calc_ic_daily(cur, method='spearman')\n",
        "                    else:\n",
        "                        mean_ic, t_ic, pos_ic = np.nan, np.nan, np.nan\n",
        "\n",
        "                    m1_metrics['RankIC_mean']  = mean_ic\n",
        "                    m1_metrics['RankIC_t']     = t_ic\n",
        "                    m1_metrics['RankIC_pos%']  = pos_ic\n",
        "\n",
        "                    save_metrics(m1_metrics, name=model_name, window=window,\n",
        "                        path=\"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_metrics.csv\")\n",
        "\n",
        "    summary_df = pd.DataFrame(summary_results)\n",
        "    daily_df = pd.DataFrame(daily_series_data) if daily_series_data else pd.DataFrame()\n",
        "\n",
        "    tc_columns = [c for c in summary_df.columns if c.startswith('tc')]\n",
        "    summary_df[tc_columns] = summary_df[tc_columns].fillna(0.0)\n",
        "\n",
        "    def save_split_by_scheme(df, base_filename):\n",
        "        if df.empty:\n",
        "            print(f\"Warning: DataFrame is empty, skipping save for {base_filename}\")\n",
        "            return None, None\n",
        "\n",
        "        vw_df = df[df['scheme'] == 'VW']\n",
        "        ew_df = df[df['scheme'] == 'EW']\n",
        "\n",
        "        out_dir = \"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results\"\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "        vw_filename = os.path.join(out_dir, f\"{base_filename}_VW.csv\")\n",
        "        ew_filename = os.path.join(out_dir, f\"{base_filename}_EW.csv\")\n",
        "\n",
        "        vw_df.to_csv(vw_filename, index=False)\n",
        "        ew_df.to_csv(ew_filename, index=False)\n",
        "\n",
        "        print(f\"VW results saved to {vw_filename}\")\n",
        "        print(f\"EW results saved to {ew_filename}\")\n",
        "\n",
        "        return vw_filename, ew_filename\n",
        "\n",
        "    save_split_by_scheme(summary_df, \"portfolio_results_daily_rebalance\")\n",
        "\n",
        "    if not daily_df.empty:\n",
        "        save_split_by_scheme(daily_df, \"portfolio_daily_series\")\n",
        "\n",
        "    if pred_rows:\n",
        "        out_dir = \"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results\"\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "        pred_df = pd.concat(pred_rows, ignore_index=True)\n",
        "        pred_df.to_csv(os.path.join(out_dir, \"predictions_daily.csv\"), index=False)\n",
        "        print(f\"Saved {len(pred_df)} prediction rows to predictions_daily.csv\")\n",
        "\n",
        "    print(f\"Generated {len(summary_results)} portfolio summary records\")\n",
        "    print(f\"Generated {len(daily_series_data)} daily series records\")\n",
        "\n",
        "    return summary_df, daily_df, backtester\n",
        "\n",
        "print(\"Starting Chronos T5-Large Portfolio Backtesting...\")\n",
        "\n",
        "START_YEAR = globals().get(\"START_YEAR\", 2016)\n",
        "END_YEAR   = globals().get(\"END_YEAR\", 2024)\n",
        "WINDOW_SIZES = globals().get(\"WINDOW_SIZES\", [5, 21, 252, 512])\n",
        "\n",
        "summary_results, daily_series, backtester = run_chronos_portfolio_backtest(\n",
        "    start_year=START_YEAR,\n",
        "    end_year=END_YEAR,\n",
        "    window_sizes=WINDOW_SIZES,\n",
        "    npz_path=\"/content/drive/MyDrive/ERP Data/all_window_datasets_unscaled.npz\"\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CHRONOS T5-BASE PORTFOLIO BACKTESTING RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nSummary Results:\")\n",
        "try:\n",
        "    print(summary_results.round(4))\n",
        "except Exception:\n",
        "    print(summary_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c21353c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c21353c7",
        "outputId": "777eea51-9d54-41ec-ce19-7a614a54671d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Saved] 5_factor_analysis_VW_gross.csv \n",
            "[Saved] 5_factor_analysis_VW_net.csv \n",
            "[Saved] 5_factor_analysis_EW_gross.csv \n",
            "[Saved] 5_factor_analysis_EW_net.csv \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-688187439.py:163: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  for _, group in df.groupby([\"scheme\", \"model\", \"window\", \"portfolio_type\"], sort=False):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finish: /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_daily_series_VW_with_rf.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-688187439.py:163: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  for _, group in df.groupby([\"scheme\", \"model\", \"window\", \"portfolio_type\"], sort=False):\n",
            "/tmp/ipython-input-688187439.py:183: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  sp500 = yf.download(\"^GSPC\", start=\"2016-01-01\", end=\"2024-12-31\")\n",
            "[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finish: /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_daily_series_EW_with_rf.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All figures have been generated and saved to: /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_figures/\n",
            "[Update] ΔSharpe has been written to /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_results_daily_rebalance_VW.csv\n",
            "[Update] ΔSharpe has been written to /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_results_daily_rebalance_EW.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ---------- Main function for 5-factor regression ----------\n",
        "def run_factor_regression(port_ret, factors, use_excess=True):\n",
        "    df = pd.concat([port_ret, factors], axis=1, join='inner').dropna()\n",
        "    df.columns = ['ret'] + list(factors.columns)\n",
        "\n",
        "    if use_excess:\n",
        "        y = df['ret'].values\n",
        "    else:\n",
        "        y = df['ret'].values - df['rf'].values\n",
        "\n",
        "    X = df[['mktrf','smb','hml','rmw','cma','umd']].values\n",
        "    X = sm.add_constant(X)\n",
        "\n",
        "    model = sm.OLS(y, X)\n",
        "    res = model.fit()\n",
        "    alpha = res.params[0]          # daily alpha\n",
        "    resid_std = res.resid.std(ddof=1)\n",
        "\n",
        "    ir_daily = alpha / resid_std          # daily IR\n",
        "    ir_annual = ir_daily * np.sqrt(252)   # annualized IR\n",
        "\n",
        "    y_hat = np.asarray(res.fittedvalues)\n",
        "\n",
        "    out = {\n",
        "        'N_obs'            : len(y),\n",
        "        'alpha_daily'      : alpha,\n",
        "        'alpha_annual'     : alpha*252,\n",
        "        't_alpha'          : res.tvalues[0],\n",
        "        'IR_daily'         : ir_daily,\n",
        "        'IR_annual'        : ir_annual,\n",
        "        'R2_zero'          : r2_zero(y, y_hat),\n",
        "    }\n",
        "\n",
        "    factor_names = ['MKT','SMB','HML','RMW','CMA','UMD']\n",
        "    for i, fac in enumerate(factor_names, start=1):\n",
        "        out[f'beta_{fac}'] = res.params[i]\n",
        "        out[f't_{fac}']    = res.tvalues[i]\n",
        "\n",
        "    return out\n",
        "\n",
        "# ---------- 3. Batch run (EW/VW, three portfolio types) ----------\n",
        "def batch_factor_analysis(\n",
        "    daily_df: pd.DataFrame,\n",
        "    factors_path: str,\n",
        "    scheme: str,\n",
        "    tc_levels=(0, 5, 10, 20, 40),\n",
        "    portfolio_types=('long_only','short_only','long_short'),\n",
        "    model_filter=None,\n",
        "    window_filter=None,\n",
        "    gross_only=False,            # If True, only calculate tc=0\n",
        "    out_dir='/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/factor_IR_results',\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate a CSV containing IR results.\n",
        "    gross_only=True  → only tc=0; False → all tc_levels.\n",
        "    \"\"\"\n",
        "    import os\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    fac = (pd.read_csv(factors_path, parse_dates=['date'])\n",
        "             .set_index('date')\n",
        "             .sort_index())\n",
        "\n",
        "    sub = daily_df[daily_df['scheme'] == scheme].copy()\n",
        "    if model_filter is not None:\n",
        "        sub = sub[sub['model'].isin(model_filter)]\n",
        "    if window_filter is not None:\n",
        "        sub = sub[sub['window'].isin(window_filter)]\n",
        "\n",
        "    tc_iter = (0,) if gross_only else tc_levels\n",
        "    results = []\n",
        "\n",
        "    for (model, win, ptype), g in sub.groupby(['model','window','portfolio_type']):\n",
        "        g = g.sort_values('date').set_index(pd.to_datetime(g['date']))\n",
        "\n",
        "        for tc in tc_iter:\n",
        "            col = 'return' if tc == 0 else f'tc{tc}_return'\n",
        "            if col not in g.columns:\n",
        "                continue\n",
        "            port_ret = g[col]\n",
        "            stats = run_factor_regression(port_ret, fac, use_excess=True)\n",
        "            stats.update({\n",
        "                'scheme'        : scheme,\n",
        "                'model'         : model,\n",
        "                'window'        : win,\n",
        "                'portfolio_type': ptype,\n",
        "                'tc_bps'        : tc,\n",
        "            })\n",
        "            results.append(stats)\n",
        "\n",
        "    df_out = pd.DataFrame(results)[[\n",
        "        'scheme','model','window','portfolio_type','tc_bps','N_obs',\n",
        "        'alpha_daily','alpha_annual','t_alpha',\n",
        "        'IR_daily','IR_annual','R2_zero',\n",
        "        'beta_MKT','t_MKT','beta_SMB','t_SMB',\n",
        "        'beta_HML','t_HML','beta_RMW','t_RMW',\n",
        "        'beta_CMA','t_CMA','beta_UMD','t_UMD'\n",
        "    ]]\n",
        "\n",
        "    tag = 'gross' if gross_only else 'net'\n",
        "    fname = f'5_factor_analysis_{scheme}_{tag}.csv'\n",
        "    df_out.to_csv(os.path.join(out_dir, fname), index=False)\n",
        "    return df_out\n",
        "\n",
        "\n",
        "\n",
        "def run_all_factor_tests(vw_csv=\"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_daily_series_VW.csv\",\n",
        "                         ew_csv=\"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_daily_series_EW.csv\",\n",
        "                         factor_csv=\"/content/drive/MyDrive/ERP Data/5_Factors_Plus_Momentum.csv\",\n",
        "                         save_dir=\"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results\",\n",
        "                         y_is_excess=True,\n",
        "                         hac_lags=5,\n",
        "                         save_txt=True):\n",
        "    vw_df = pd.read_csv(vw_csv)\n",
        "    ew_df = pd.read_csv(ew_csv)\n",
        "\n",
        "    vw_gross = batch_factor_analysis(\n",
        "        vw_df, factor_csv, scheme='VW', gross_only=True)\n",
        "    vw_net   = batch_factor_analysis(\n",
        "        vw_df, factor_csv, scheme='VW', gross_only=False)\n",
        "\n",
        "    ew_gross = batch_factor_analysis(\n",
        "        ew_df, factor_csv, scheme='EW', gross_only=True)\n",
        "    ew_net   = batch_factor_analysis(\n",
        "        ew_df, factor_csv, scheme='EW', gross_only=False)\n",
        "\n",
        "    return vw_gross, vw_net, ew_gross, ew_net\n",
        "\n",
        "\n",
        "vw_gross, vw_net, ew_gross, ew_net = run_all_factor_tests()\n",
        "rf_file = \"/content/drive/MyDrive/ERP Data/CRSP_2016_2024_top50_with_exret.csv\"\n",
        "vw_file = \"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_daily_series_VW.csv\"\n",
        "ew_file = \"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_daily_series_EW.csv\"\n",
        "\n",
        "rf_df = pd.read_csv(rf_file, usecols=[\"date\", \"rf\"])\n",
        "rf_df[\"date\"] = pd.to_datetime(rf_df[\"date\"])\n",
        "rf_dict = dict(zip(rf_df[\"date\"], rf_df[\"rf\"]))\n",
        "\n",
        "\n",
        "def adjust_returns_with_rf_grouped(file_path, output_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"], format='mixed', dayfirst=True)\n",
        "\n",
        "    return_cols = [col for col in df.columns if \"return\" in col and \"cumul\" not in col]\n",
        "\n",
        "    order = [\"long_only\", \"short_only\", \"long_short\"]\n",
        "    df[\"portfolio_type\"] = pd.Categorical(df[\"portfolio_type\"], categories=order, ordered=True)\n",
        "\n",
        "    df_list = []\n",
        "    for _, group in df.groupby([\"scheme\", \"model\", \"window\", \"portfolio_type\"], sort=False):\n",
        "        group = group.sort_values(\"date\").copy()\n",
        "        for col in return_cols:\n",
        "            group[col] = group.apply(lambda row: row[col] + rf_dict.get(row[\"date\"], 0), axis=1)\n",
        "\n",
        "            cum_col = col.replace(\"return\", \"cumulative\")\n",
        "            group[cum_col] = np.log1p(group[col]).cumsum()\n",
        "        df_list.append(group)\n",
        "\n",
        "    df_new = pd.concat(df_list).sort_values([\"scheme\", \"model\", \"window\", \"portfolio_type\", \"date\"])\n",
        "    df_new.to_csv(output_path, index=False)\n",
        "\n",
        "adjust_returns_with_rf_grouped(vw_file, \"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_daily_series_VW_with_rf.csv\")\n",
        "adjust_returns_with_rf_grouped(ew_file, \"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_daily_series_EW_with_rf.csv\")\n",
        "\n",
        "sp500 = yf.download(\"^GSPC\", start=\"2016-01-01\", end=\"2024-12-31\")\n",
        "price_col = \"Adj Close\" if \"Adj Close\" in sp500.columns else \"Close\"\n",
        "sp500[\"daily_return\"] = sp500[price_col].pct_change().fillna(0)\n",
        "sp500[\"cum_return\"] = np.cumsum(np.log1p(sp500[\"daily_return\"]))\n",
        "sp500 = sp500[[\"cum_return\"]]\n",
        "sp500.index = pd.to_datetime(sp500.index)\n",
        "\n",
        "files = [\n",
        "    (\"VW\", \"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_daily_series_VW_with_rf.csv\"),\n",
        "    (\"EW\", \"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_daily_series_EW_with_rf.csv\")\n",
        "]\n",
        "tc_levels = [0, 5, 10, 20, 40]\n",
        "windows = [5, 21, 252, 512]\n",
        "strategies = [\"long_only\", \"short_only\", \"long_short\"]\n",
        "\n",
        "output_dir = \"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_figures\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "crisis_periods = [\n",
        "    (datetime(2018, 6, 1), datetime(2019, 1, 1), \"US-China Trade War\"),\n",
        "    (datetime(2020, 2, 1), datetime(2020, 7, 1), \"COVID-19\"),\n",
        "    (datetime(2022, 2, 1), datetime(2022, 6, 1), \"Russia-Ukraine War\"),\n",
        "    (datetime(2023, 1, 1), datetime(2023, 4, 1), \"US Bank Crisis\"),\n",
        "]\n",
        "\n",
        "def plot_comparison_styled(df, scheme, tc, window):\n",
        "    plt.figure(figsize=(15, 12))\n",
        "    model_names = df[\"model\"].unique()\n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, len(model_names)))\n",
        "\n",
        "    offset_step = 0.02\n",
        "\n",
        "    for i, strat in enumerate(strategies, 1):\n",
        "        ax = plt.subplot(3, 1, i)\n",
        "\n",
        "        plt.plot(sp500.index, sp500[\"cum_return\"],\n",
        "                 color=\"black\", lw=2.5, label=\"S&P500 (Total Return)\", zorder=10)\n",
        "\n",
        "        for idx, model_name in enumerate(model_names):\n",
        "            sub = df[(df[\"window\"] == window) &\n",
        "                     (df[\"portfolio_type\"] == strat) &\n",
        "                     (df[\"model\"] == model_name)].sort_values(\"date\")\n",
        "            if sub.empty:\n",
        "                continue\n",
        "\n",
        "            if tc == 0:\n",
        "                ret_col = \"return\"\n",
        "            else:\n",
        "                ret_col = f\"tc{tc}_return\"\n",
        "\n",
        "            if ret_col not in sub.columns:\n",
        "                continue\n",
        "\n",
        "            log_cum = np.cumsum(np.log1p(sub[ret_col].values))\n",
        "\n",
        "            y_shift = idx * offset_step\n",
        "            plt.plot(sub[\"date\"], log_cum + y_shift,\n",
        "                     label=f\"{model_name} ({strat.replace('_',' ').title()})\",\n",
        "                     lw=2, color=colors[idx], alpha=0.9)\n",
        "\n",
        "        for start, end, label in crisis_periods:\n",
        "            ax.axvspan(start, end, color='grey', alpha=0.3)\n",
        "            ax.text(start + pd.Timedelta(days=10),\n",
        "                    ax.get_ylim()[1]*0.92, label, fontsize=8, color='grey')\n",
        "        ax.xaxis.set_major_locator(mdates.YearLocator())\n",
        "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "        ax.set_ylabel(\"Cumulative log return (start = 0)\")\n",
        "        ax.set_title(f\"{scheme} | Window={window} | Strategy={strat} | TC={tc} bps\")\n",
        "        ax.grid(alpha=0.3)\n",
        "        plt.xticks(rotation=30)\n",
        "        plt.legend(bbox_to_anchor=(1.04, 1), loc='upper left', fontsize=8)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    fname = f\"{scheme}_window{window}_TC{tc}.png\"\n",
        "    plt.savefig(os.path.join(output_dir, fname), dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "for scheme, file_path in files:\n",
        "    df = pd.read_csv(file_path)\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "    for tc in tc_levels:\n",
        "        for window in windows:\n",
        "            plot_comparison_styled(df, scheme, tc, window)\n",
        "\n",
        "print(f\"All figures have been generated and saved to: {output_dir}/\")\n",
        "\n",
        "metrics_df = pd.read_csv(\"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_metrics.csv\")[[\"Model\", \"Window\", \"R²_zero\"]]\n",
        "metrics_df.rename(columns={\"Model\": \"model\", \"Window\": \"window\"}, inplace=True)\n",
        "\n",
        "for fname in [\"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_results_daily_rebalance_VW.csv\", \"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_results_daily_rebalance_EW.csv\"]:\n",
        "    df = pd.read_csv(fname)\n",
        "\n",
        "    df = df.merge(metrics_df, on=[\"model\", \"window\"], how=\"left\")\n",
        "\n",
        "    rows = []\n",
        "    for _, row in df.iterrows():\n",
        "        r2 = float(row[\"R²_zero\"]) if not pd.isna(row[\"R²_zero\"]) else 0.0\n",
        "        if row[\"portfolio_type\"] == \"long_only\":\n",
        "            d_sr, sr_star = delta_sharpe(r2, SR_MKT_EX)\n",
        "            row[\"ΔSharpe\"]  = d_sr\n",
        "            row[\"Sharpe*\"]  = sr_star\n",
        "            row[\"baseline\"] = f\"SPX_excess ({SR_MKT_EX:.2f})\"\n",
        "        else:\n",
        "            d_sr, sr_star = delta_sharpe(r2, 0)\n",
        "            row[\"ΔSharpe\"]  = d_sr\n",
        "            row[\"Sharpe*\"]  = sr_star\n",
        "            row[\"baseline\"] = \"cash (0)\"\n",
        "        rows.append(row)\n",
        "\n",
        "    pd.DataFrame(rows).to_csv(fname, index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (Chronos)",
      "language": "python",
      "name": "chronos"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1175f52a956a45cb9f429c373d2c9c6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aec085822f50466e80e1fbac61f749a5",
            "placeholder": "​",
            "style": "IPY_MODEL_b70874d232d74693bf036281edb7c97f",
            "value": " 142/142 [00:00&lt;00:00, 12.0kB/s]"
          }
        },
        "1b1e8ad2e8914b91b36a563f11c7bfc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ca64de2015b45499886841236e81da8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "259df72b69744eba9ae9223ede1e7ddf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26f72fa87d274f42a783ac8675394b16": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d8b2d281bec4e848c61c9e3fb2bb011": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8aac5f4987ae470fbb0739f50fa82d00",
            "max": 2835915592,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1ca64de2015b45499886841236e81da8",
            "value": 2835915592
          }
        },
        "302731da7c72494897d640436c66ad8c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "337e949e516b411f93168d9d7390cb97": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42386fd45f874130b3970ad0cbbde08c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_660edc6a9e1a459d873dc44ae1d33bf2",
            "max": 142,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c1043f7033704ad9b607c2fb9ba9b599",
            "value": 142
          }
        },
        "4903ec51bcbe493fb4a379e6cba0a3b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4ec05d8c11ba42d6b432b6be5f7bea11": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fbc070393744c63819eb1190b197442": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64539b8b90a64dda85e330f13ccd5398": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_302731da7c72494897d640436c66ad8c",
            "placeholder": "​",
            "style": "IPY_MODEL_de51f545826f4d76a5e14418b6d2831a",
            "value": "config.json: "
          }
        },
        "659e95fd144f415683ad9f4eaf25b730": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "660edc6a9e1a459d873dc44ae1d33bf2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "705f095a8bd64cb1ad054005d2242428": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2bba446142c4bf489641957f85c2b5b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4903ec51bcbe493fb4a379e6cba0a3b6",
            "value": 1
          }
        },
        "77db9577636045b3af64ad20c4d4e708": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_64539b8b90a64dda85e330f13ccd5398",
              "IPY_MODEL_705f095a8bd64cb1ad054005d2242428",
              "IPY_MODEL_9f63936d8d6c4bd6be2b8fc026de3d62"
            ],
            "layout": "IPY_MODEL_92a2a444e4fe41e18c66933772dd8386"
          }
        },
        "8088dea0c3cb49e984aae7102ce1a5e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_337e949e516b411f93168d9d7390cb97",
            "placeholder": "​",
            "style": "IPY_MODEL_f1634d486b07408c93f6d037f3a7fa27",
            "value": "generation_config.json: 100%"
          }
        },
        "8aac5f4987ae470fbb0739f50fa82d00": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b86556d2e84408c8c8a0fa614dcc73a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26f72fa87d274f42a783ac8675394b16",
            "placeholder": "​",
            "style": "IPY_MODEL_9888f124f5fd490aa5e3ffe525fa06b7",
            "value": "model.safetensors: 100%"
          }
        },
        "92a2a444e4fe41e18c66933772dd8386": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "979e19bd39424ae38224ccb66faa9c26": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8b86556d2e84408c8c8a0fa614dcc73a",
              "IPY_MODEL_2d8b2d281bec4e848c61c9e3fb2bb011",
              "IPY_MODEL_a810085f6e7e490c93d8b1ea15afc117"
            ],
            "layout": "IPY_MODEL_259df72b69744eba9ae9223ede1e7ddf"
          }
        },
        "9888f124f5fd490aa5e3ffe525fa06b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f63936d8d6c4bd6be2b8fc026de3d62": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d90ffe4b8d1c411ba6e769207d753f35",
            "placeholder": "​",
            "style": "IPY_MODEL_659e95fd144f415683ad9f4eaf25b730",
            "value": " 1.12k/? [00:00&lt;00:00, 103kB/s]"
          }
        },
        "a810085f6e7e490c93d8b1ea15afc117": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fbc070393744c63819eb1190b197442",
            "placeholder": "​",
            "style": "IPY_MODEL_1b1e8ad2e8914b91b36a563f11c7bfc8",
            "value": " 2.84G/2.84G [00:13&lt;00:00, 482MB/s]"
          }
        },
        "aec085822f50466e80e1fbac61f749a5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2bba446142c4bf489641957f85c2b5b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "b70874d232d74693bf036281edb7c97f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1043f7033704ad9b607c2fb9ba9b599": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d1a69df425184c67a3d3db5af21e9be7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8088dea0c3cb49e984aae7102ce1a5e5",
              "IPY_MODEL_42386fd45f874130b3970ad0cbbde08c",
              "IPY_MODEL_1175f52a956a45cb9f429c373d2c9c6f"
            ],
            "layout": "IPY_MODEL_4ec05d8c11ba42d6b432b6be5f7bea11"
          }
        },
        "d90ffe4b8d1c411ba6e769207d753f35": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de51f545826f4d76a5e14418b6d2831a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f1634d486b07408c93f6d037f3a7fa27": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
