{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bd36a91b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd36a91b",
        "outputId": "00bea0ed-4e3e-4129-b836-264eb2ed239b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)\n",
            "fatal: destination path 'chronos-forecasting' already exists and is not an empty directory.\n",
            "/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos-forecasting\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m128.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.65)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.12)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.8)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.18.2)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.13.4)\n",
            "Requirement already satisfied: curl_cffi>=0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.13.0)\n",
            "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (5.29.5)\n",
            "Requirement already satisfied: websockets>=13.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (15.0.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.14.1)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from curl_cffi>=0.7->yfinance) (1.17.1)\n",
            "Requirement already satisfied: certifi>=2024.2.2 in /usr/local/lib/python3.11/dist-packages (from curl_cffi>=0.7->yfinance) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2.5.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# 挂载 Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 创建项目目录并切换\n",
        "!mkdir -p '/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)'\n",
        "%cd '/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)'\n",
        "\n",
        "# 克隆源码库（如果不存在）\n",
        "!git clone https://github.com/amazon-science/chronos-forecasting\n",
        "%cd chronos-forecasting\n",
        "\n",
        "# 安装依赖包\n",
        "%pip install torch transformers datasets accelerate scikit-learn tqdm joblib\n",
        "\n",
        "\n",
        "# print in English; comments in Chinese\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "import warnings\n",
        "from typing import Dict, Any, Optional, Tuple, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "! pip install yfinance\n",
        "import yfinance as yf\n",
        "import statsmodels.api as sm\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "from transformers.trainer_utils import IntervalStrategy\n",
        "from transformers.data.data_collator import default_data_collator\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, PeftModel, TaskType\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from scipy.stats import f as f_dist\n",
        "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
        "\n",
        "\n",
        "# ========================= 基本设置 =========================\n",
        "# 确保本地 chronos 源码可导入（若未 pip 安装）\n",
        "def ensure_chronos_import() -> None:\n",
        "    \"\"\"尝试导入 chronos；失败则添加本地/Colab仓库 src 到 sys.path。\"\"\"\n",
        "    try:\n",
        "        import chronos  # noqa: F401\n",
        "        return\n",
        "    except Exception:\n",
        "        pass\n",
        "    candidates = [\n",
        "        \"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/3_Benchmark/5_Foundation_Models/Chronos/chronos-forecasting/src\",\n",
        "        \"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos-forecasting/src\",\n",
        "        \"/content/chronos-forecasting/src\",\n",
        "    ]\n",
        "    for repo_src in candidates:\n",
        "        if os.path.isdir(repo_src) and repo_src not in sys.path:\n",
        "            sys.path.append(repo_src)\n",
        "            try:\n",
        "                import chronos  # noqa: F401\n",
        "                return\n",
        "            except Exception:\n",
        "                continue\n",
        "    raise ImportError(\"chronos import failed. Please clone chronos-forecasting and ensure its 'src' is on sys.path.\")\n",
        "\n",
        "# 设置随机种子（确保可复现）\n",
        "def set_seed(seed: int = 42) -> None:\n",
        "    \"\"\"设置随机种子（CPU、CUDA、MPS等）\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    if torch.backends.mps.is_available():\n",
        "        try:\n",
        "            torch.mps.manual_seed(seed)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "# 设备检测（优先 MPS）\n",
        "def get_device() -> torch.device:\n",
        "    \"\"\"优先使用 MPS（Apple Silicon），否则使用 CUDA，再否则 CPU\"\"\"\n",
        "    if torch.backends.mps.is_available():\n",
        "        return torch.device(\"mps\")\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")\n",
        "    return torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "28313cbb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28313cbb",
        "outputId": "8c24dece-a4f9-4211-d250-9e2f8465de6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2075427930.py:16: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  px = yf.download(\"^GSPC\", start=\"2016-01-01\", end=\"2024-12-31\")[\"Close\"]\n",
            "[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] S&P500 Excess Sharpe (2016–24) = 0.652\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/tmp/ipython-input-2075427930.py:18: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  rf_align = rf_series.reindex(sp_ret.index).fillna(method=\"ffill\")\n"
          ]
        }
      ],
      "source": [
        "def annual_sharpe(rets, freq=252):\n",
        "    mu = float(np.mean(rets)) * freq\n",
        "    sd = float(np.std(rets, ddof=1)) * np.sqrt(freq)\n",
        "    return mu / sd if sd > 0 else 0\n",
        "\n",
        "# ===  加载无风险利率 & 计算 S&P500 Excess Sharpe ===\n",
        "\n",
        "# Colab 路径\n",
        "rf_file = \"/content/drive/MyDrive/ERP Data/CRSP_2016_2024_top50_with_exret.csv\"\n",
        "try:\n",
        "    rf_df = pd.read_csv(rf_file, usecols=[\"date\", \"rf\"])\n",
        "    rf_df[\"date\"] = pd.to_datetime(rf_df[\"date\"])\n",
        "    rf_df = rf_df.drop_duplicates(\"date\").set_index(\"date\").sort_index()\n",
        "    rf_series = rf_df[\"rf\"].astype(float)\n",
        "\n",
        "    px = yf.download(\"^GSPC\", start=\"2016-01-01\", end=\"2024-12-31\")[\"Close\"]\n",
        "    sp_ret = px.pct_change().dropna()\n",
        "    rf_align = rf_series.reindex(sp_ret.index).fillna(method=\"ffill\")\n",
        "    sp_excess = sp_ret.values - rf_align.values\n",
        "\n",
        "    SR_MKT_EX = annual_sharpe(sp_excess)\n",
        "    print(f\"[INFO] S&P500 Excess Sharpe (2016–24) = {SR_MKT_EX:.3f}\")\n",
        "except Exception as e:\n",
        "    print(f\"Warning: Could not load risk-free rate data: {e}\")\n",
        "    SR_MKT_EX = 0.5  # 使用默认值\n",
        "# === 2. ΔSharpe 计算函数 ===\n",
        "def delta_sharpe(r2_zero: float, sr_base: float):\n",
        "    \"\"\"\n",
        "    若 r2_zero <= 0   → ΔSharpe = 0，Sharpe* = sr_base\n",
        "    若 r2_zero >= 1   → ΔSharpe = 0，Sharpe* = sr_base（极端情况兜底）\n",
        "    其余区间按原公式计算\n",
        "    \"\"\"\n",
        "    if (r2_zero <= 0) or (r2_zero >= 1):\n",
        "        return 0.0, sr_base\n",
        "    sr_star = np.sqrt(sr_base ** 2 + r2_zero) / np.sqrt(1 - r2_zero)\n",
        "    return sr_star - sr_base, sr_star\n",
        "\n",
        "# === Zero-based R² ===\n",
        "def r2_zero(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    计算老师要求的 R² (zero-based, 基准为0)\n",
        "    y_true: 实际值数组 (N,)\n",
        "    y_pred: 预测值数组 (N,)\n",
        "    \"\"\"\n",
        "    rss = np.sum((y_true - y_pred)**2)\n",
        "    tss = np.sum(y_true**2)\n",
        "    return 1 - rss / tss\n",
        "\n",
        "def calc_ic_daily(df, method='spearman'):\n",
        "    \"\"\"\n",
        "    计算每日横截面 RankIC\n",
        "    df: 至少包含 ['signal_date','y_true','y_pred']\n",
        "    \"\"\"\n",
        "    ics = (df.groupby('signal_date')\n",
        "             .apply(lambda g: g['y_pred'].corr(g['y_true'], method=method))\n",
        "             .dropna())\n",
        "    mean_ic = ics.mean()\n",
        "    std_ic  = ics.std(ddof=1)\n",
        "    t_ic    = mean_ic / (std_ic / np.sqrt(len(ics))) if std_ic > 0 else np.nan\n",
        "    pos_ratio = (ics > 0).mean()  # 正相关天数比例\n",
        "    return mean_ic, t_ic, pos_ratio, ics\n",
        "\n",
        "def calc_directional_metrics(y_true, y_pred, permnos=None):\n",
        "    \"\"\"\n",
        "    改进版：\n",
        "    - 样本级符号预测\n",
        "    - 分股票则每支股票分别算 Overall、Up、Down，再平均\n",
        "    \"\"\"\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "\n",
        "    if permnos is None:\n",
        "        # 不分组\n",
        "        s_true = np.sign(y_true)\n",
        "        s_pred = np.sign(y_pred)\n",
        "        mask = s_true != 0  # 避免零值干扰\n",
        "        s_true = s_true[mask]\n",
        "        s_pred = s_pred[mask]\n",
        "\n",
        "        overall_acc = np.mean(s_true == s_pred)\n",
        "\n",
        "        up_mask = s_true > 0\n",
        "        down_mask = s_true < 0\n",
        "        up_acc = np.mean(s_true[up_mask] == s_pred[up_mask]) if np.any(up_mask) else 0\n",
        "        down_acc = np.mean(s_true[down_mask] == s_pred[down_mask]) if np.any(down_mask) else 0\n",
        "\n",
        "    else:\n",
        "        # 分 PERMNO\n",
        "        df = pd.DataFrame({\"permno\": permnos, \"yt\": y_true, \"yp\": y_pred})\n",
        "        overall_accs = []\n",
        "        up_accs = []\n",
        "        down_accs = []\n",
        "\n",
        "        for _, g in df.groupby(\"permno\"):\n",
        "            s_true = np.sign(g[\"yt\"].values)\n",
        "            s_pred = np.sign(g[\"yp\"].values)\n",
        "            mask = s_true != 0\n",
        "            s_true = s_true[mask]\n",
        "            s_pred = s_pred[mask]\n",
        "            if len(s_true) == 0:\n",
        "                continue\n",
        "            overall_accs.append(np.mean(s_true == s_pred))\n",
        "\n",
        "            up_mask = s_true > 0\n",
        "            down_mask = s_true < 0\n",
        "            up_accs.append(np.mean(s_true[up_mask] == s_pred[up_mask]) if np.any(up_mask) else np.nan)\n",
        "            down_accs.append(np.mean(s_true[down_mask] == s_pred[down_mask]) if np.any(down_mask) else np.nan)\n",
        "\n",
        "        overall_acc = np.nanmean(overall_accs)\n",
        "        up_acc = np.nanmean(up_accs)\n",
        "        down_acc = np.nanmean(down_accs)\n",
        "\n",
        "    return overall_acc, up_acc, down_acc\n",
        "\n",
        "\n",
        "# === 组合版回归指标 ===\n",
        "def regression_metrics(y_true, y_pred, k, meta=None, permnos=None):\n",
        "    \"\"\"\n",
        "    包含：\n",
        "    - 回归指标\n",
        "    - 单点方向准确率\n",
        "    - 市值分组指标\n",
        "    \"\"\"\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    n = len(y_true)\n",
        "\n",
        "    r2 = r2_zero(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "\n",
        "    # 新版 Directional Metrics\n",
        "    dir_acc, up_acc, down_acc = calc_directional_metrics(y_true, y_pred, permnos)\n",
        "\n",
        "    metrics = {\n",
        "        \"R²_zero\": r2,\n",
        "        \"RMSE\": rmse,\n",
        "        \"MAE\": mae,\n",
        "        \"MSE\": mse,\n",
        "        \"Directional Accuracy\": dir_acc,\n",
        "        \"Up_Directional_Acc\": up_acc,\n",
        "        \"Down_Directional_Acc\": down_acc\n",
        "    }\n",
        "\n",
        "    # === 市值组 ===\n",
        "    if meta is not None and \"MKTCAP_PERCENTILE\" in meta:\n",
        "        top_mask = meta[\"MKTCAP_PERCENTILE\"] >= 0.75\n",
        "        bottom_mask = meta[\"MKTCAP_PERCENTILE\"] <= 0.25\n",
        "\n",
        "        if np.any(top_mask):\n",
        "            yt_top = y_true[top_mask]\n",
        "            yp_top = y_pred[top_mask]\n",
        "            perm_top = permnos[top_mask] if permnos is not None else None\n",
        "            r2_top = r2_zero(yt_top, yp_top)  # 用r2_zero替换\n",
        "            rmse_top = np.sqrt(mean_squared_error(yt_top, yp_top))\n",
        "            mae_top = mean_absolute_error(yt_top, yp_top)\n",
        "            mse_top = mean_squared_error(yt_top, yp_top)\n",
        "            dir_top, up_top, down_top = calc_directional_metrics(yt_top, yp_top, perm_top)\n",
        "            metrics.update({\n",
        "                \"Top25_R2_zero\": r2_top,\n",
        "                \"Top25_RMSE\": rmse_top,\n",
        "                \"Top25_MAE\": mae_top,\n",
        "                \"Top25_MSE\": mse_top,\n",
        "                \"Top25_Dir_Acc\": dir_top,\n",
        "                \"Top25_Up_Acc\": up_top,\n",
        "                \"Top25_Down_Acc\": down_top\n",
        "            })\n",
        "\n",
        "        if np.any(bottom_mask):\n",
        "            yt_bot = y_true[bottom_mask]\n",
        "            yp_bot = y_pred[bottom_mask]\n",
        "            perm_bot = permnos[bottom_mask] if permnos is not None else None\n",
        "            r2_bot = r2_zero(yt_bot, yp_bot)  # 用r2_zero替换\n",
        "            rmse_bot = np.sqrt(mean_squared_error(yt_bot, yp_bot))\n",
        "            mae_bot = mean_absolute_error(yt_bot, yp_bot)\n",
        "            mse_bot = mean_squared_error(yt_bot, yp_bot)\n",
        "            dir_bot, up_bot, down_bot = calc_directional_metrics(yt_bot, yp_bot, perm_bot)\n",
        "            metrics.update({\n",
        "                \"Bottom25_R2_zero\": r2_bot,\n",
        "                \"Bottom25_RMSE\": rmse_bot,\n",
        "                \"Bottom25_MAE\": mae_bot,\n",
        "                \"Bottom25_MSE\": mse_bot,\n",
        "                \"Bottom25_Dir_Acc\": dir_bot,\n",
        "                \"Bottom25_Up_Acc\": up_bot,\n",
        "                \"Bottom25_Down_Acc\": down_bot\n",
        "            })\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def f_statistic(y_true, y_pred, k):\n",
        "    \"\"\"返回 F 统计量和对应的 p 值\"\"\"\n",
        "    n   = len(y_true)\n",
        "    rss = np.sum((y_true - y_pred) ** 2)\n",
        "    tss = np.sum(y_true ** 2)\n",
        "    r2  = 1 - rss / tss\n",
        "    if (r2 <= 0) or (n <= k):          # 无法解释或自由度不足\n",
        "        return 0.0, 1.0\n",
        "    F = (r2 / k) / ((1 - r2) / (n - k))\n",
        "    p = f_dist.sf(F, k, n - k)         # 上尾概率\n",
        "    return F, p\n",
        "\n",
        "def overall_interval_metrics_method1(y_all, yhat_all, k, permnos_all=None, meta_all=None):\n",
        "    \"\"\"\n",
        "    方式1：整体区间一次性计算指标（2016-2024 全部样本拼接）\n",
        "    返回：一个 dict，可直接喂给 save_metrics()\n",
        "    \"\"\"\n",
        "    # 基本回归 + 方向指标（已写好的组合函数）\n",
        "    base = regression_metrics(\n",
        "        y_true=y_all,\n",
        "        y_pred=yhat_all,\n",
        "        k=k,\n",
        "        meta=meta_all,\n",
        "        permnos=permnos_all\n",
        "    )\n",
        "    F, p = f_statistic(y_all, yhat_all, k)\n",
        "    base[\"F_stat\"]     = F\n",
        "    base[\"F_pvalue\"]   = p\n",
        "    # 也可以顺手补样本数\n",
        "    base[\"N_obs\"] = len(y_all)\n",
        "    # === ΔSharpe：两种基准 ================================================\n",
        "\n",
        "    delta_cash, sr_star_cash = delta_sharpe(base[\"R²_zero\"], sr_base=0)\n",
        "    base[\"ΔSharpe_cash\"]      = delta_cash      # 给 long_short / short_only\n",
        "    base[\"Sharpe*_cash\"]      = sr_star_cash\n",
        "\n",
        "    delta_mkt , sr_star_mkt  = delta_sharpe(base[\"R²_zero\"], sr_base=SR_MKT_EX)\n",
        "    base[\"ΔSharpe_mkt\"]       = delta_mkt       # 给 long_only\n",
        "    base[\"Sharpe*_mkt\"]       = sr_star_mkt\n",
        "\n",
        "    return base\n",
        "\n",
        "def sortino_ratio(rets, freq=252):\n",
        "    \"\"\"计算Sortino Ratio\"\"\"\n",
        "    downside = rets[rets < 0]\n",
        "    if len(downside) == 0:\n",
        "        return np.inf\n",
        "    mu = rets.mean() * freq\n",
        "    sigma = np.sqrt((downside ** 2).mean()) * np.sqrt(freq)\n",
        "    return mu / sigma\n",
        "\n",
        "def cvar(rets, alpha=0.95):\n",
        "    \"\"\"计算CVaR\"\"\"\n",
        "    q = np.quantile(rets, 1 - alpha)\n",
        "    return rets[rets <= q].mean()\n",
        "\n",
        "# 添加缺失的save_metrics函数\n",
        "def save_metrics(metrics_dict, name, window, path=\"portfolio_metrics.csv\"):\n",
        "    \"\"\"保存指标到CSV文件\"\"\"\n",
        "    row = {'Model': name, 'Window': window}\n",
        "    row.update(metrics_dict)\n",
        "\n",
        "    # 检查文件是否存在\n",
        "    if os.path.exists(path):\n",
        "        df = pd.read_csv(path)\n",
        "        df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
        "    else:\n",
        "        df = pd.DataFrame([row])\n",
        "\n",
        "    df.to_csv(path, index=False)\n",
        "    print(f\"Metrics saved for {name}_w{window} to {path}\")\n",
        "    ## ========== Portfolio 构建核心类 ==========\n",
        "# ========== 交易成本设置 ==========\n",
        "TC_GRID = [0.0005, 0.001, 0.002, 0.003, 0.004]  # 5, 10, 20, 30, 40 bps\n",
        "TC_TAG  = {\n",
        "    0.0005: \"tc5\",\n",
        "    0.001:  \"tc10\",\n",
        "    0.002:  \"tc20\",\n",
        "    0.003:  \"tc30\",\n",
        "    0.004:  \"tc40\"\n",
        "}\n",
        "\n",
        "class PortfolioBacktester:\n",
        "    def __init__(self):\n",
        "        self.results = {}\n",
        "\n",
        "    def calc_turnover(self, w_t, r_t, w_tp1):\n",
        "        \"\"\"计算换手率 - 使用用户提供的标准公式\"\"\"\n",
        "        if w_t is None:\n",
        "            return np.sum(np.abs(w_tp1))\n",
        "\n",
        "        gross_ret = np.sum(w_t * r_t)\n",
        "        if abs(1 + gross_ret) < 1e-8:  # 避免除零\n",
        "            return np.sum(np.abs(w_tp1))\n",
        "\n",
        "        passive_weight = w_t * (1 + r_t) / (1 + gross_ret)\n",
        "        turnover = np.sum(np.abs(w_tp1 - passive_weight))\n",
        "        return turnover\n",
        "\n",
        "    def create_portfolios_with_permno_tracking(self, signals, market_caps, permnos, top_pct=0.1, bottom_pct=0.1, weight_scheme=\"VW\"):\n",
        "        \"\"\"\n",
        "        根据信号创建投资组合权重，并严格跟踪permno对齐\n",
        "        weight_scheme: 'VW' 市值加权, 'EW' 等权重\n",
        "        \"\"\"\n",
        "        n_stocks = len(signals)\n",
        "        # ---① 至少留 1 支 ---\n",
        "        top_n    = max(1, int(round(n_stocks * top_pct)))\n",
        "        bottom_n = max(1, int(round(n_stocks * bottom_pct)))\n",
        "\n",
        "        # 按预测值排序，记录原始索引\n",
        "        sorted_idx = np.argsort(signals)[::-1]  # 降序\n",
        "\n",
        "        # 选择top和bottom股票的索引\n",
        "        top_idx = sorted_idx[:top_n]\n",
        "        bottom_idx = sorted_idx[-bottom_n:]  # bottom_n ≥ 1 保证非空\n",
        "\n",
        "        # 为每个组合类型创建单独的权重和permno跟踪\n",
        "        portfolio_data = {}\n",
        "\n",
        "        # Long-only portfolio (Top 10%)\n",
        "        long_weights = np.zeros(n_stocks)\n",
        "        if len(top_idx) > 0:\n",
        "            if weight_scheme == \"VW\":\n",
        "                top_market_caps = market_caps[top_idx]\n",
        "                if np.sum(top_market_caps) > 0:\n",
        "                    long_weights[top_idx] = top_market_caps / np.sum(top_market_caps)\n",
        "            else:  # 'EW'\n",
        "                # ---② EW 分配用实际长度，避免再除 0 ---\n",
        "                long_weights[top_idx] = 1.0 / len(top_idx)\n",
        "\n",
        "        portfolio_data['long_only'] = {\n",
        "            'weights': long_weights,\n",
        "            'permnos': permnos.copy(),\n",
        "            'selected_permnos': permnos[top_idx] if len(top_idx) > 0 else np.array([])\n",
        "        }\n",
        "\n",
        "        # Short-only portfolio (Bottom 10%)\n",
        "        short_weights = np.zeros(n_stocks)\n",
        "        if len(bottom_idx) > 0:\n",
        "            if weight_scheme == \"VW\":\n",
        "                bottom_market_caps = market_caps[bottom_idx]\n",
        "                if np.sum(bottom_market_caps) > 0:\n",
        "                    short_weights[bottom_idx] = -bottom_market_caps / np.sum(bottom_market_caps)\n",
        "            else:  # 'EW'\n",
        "                # ---② EW 分配用实际长度，避免再除 0 ---\n",
        "                short_weights[bottom_idx] = -1.0 / len(bottom_idx)\n",
        "\n",
        "        portfolio_data['short_only'] = {\n",
        "            'weights': short_weights,\n",
        "            'permnos': permnos.copy(),\n",
        "            'selected_permnos': permnos[bottom_idx] if len(bottom_idx) > 0 else np.array([])\n",
        "        }\n",
        "\n",
        "        # Long-Short portfolio (Top 做多 + Bottom 做空)\n",
        "        # -------------------------------------------------\n",
        "        ls_raw = long_weights + short_weights\n",
        "\n",
        "        gross_target = 2.0        #Gross ≈ 2，Net ≈ 0\n",
        "        current_gross = np.sum(np.abs(long_weights)) + np.sum(np.abs(short_weights))\n",
        "        scale = gross_target / current_gross if current_gross > 1e-8 else 0.0\n",
        "        ls_weights = scale * ls_raw\n",
        "        # -------------------------------------------------\n",
        "\n",
        "        ls_selected_permnos = np.concatenate([\n",
        "            permnos[top_idx] if len(top_idx) > 0 else np.array([]),\n",
        "            permnos[bottom_idx] if len(bottom_idx) > 0 else np.array([])\n",
        "        ])\n",
        "\n",
        "        portfolio_data['long_short'] = {\n",
        "            'weights': ls_weights,\n",
        "            'permnos': permnos.copy(),\n",
        "            'selected_permnos': ls_selected_permnos\n",
        "        }\n",
        "\n",
        "        return portfolio_data\n",
        "\n",
        "    def calculate_aligned_portfolio_return(self, portfolio_weights, portfolio_permnos, actual_returns, actual_permnos):\n",
        "        \"\"\"计算严格按permno对齐的组合收益\"\"\"\n",
        "        aligned_returns = np.zeros(len(portfolio_permnos))\n",
        "\n",
        "        # 构建permno到收益的映射\n",
        "        return_dict = dict(zip(actual_permnos, actual_returns))\n",
        "\n",
        "        # 按permno对齐收益\n",
        "        for i, permno in enumerate(portfolio_permnos):\n",
        "            if permno in return_dict:\n",
        "                aligned_returns[i] = return_dict[permno]\n",
        "            # 如果找不到对应收益，保持为0\n",
        "\n",
        "        # 计算组合收益\n",
        "        portfolio_return = np.sum(portfolio_weights * aligned_returns)\n",
        "        return portfolio_return, aligned_returns\n",
        "\n",
        "\n",
        "\n",
        "    def calculate_metrics(self, returns, turnover_series=None):\n",
        "        \"\"\"计算投资组合指标 - 仅返回summary指标，不包含长序列\"\"\"\n",
        "        returns = np.array(returns)\n",
        "\n",
        "        # 基础指标 - 改为日度年化\n",
        "        annual_return = np.mean(returns) * 252  # 日度年化\n",
        "        annual_vol = np.std(returns, ddof=1) * np.sqrt(252)  # 日度年化波动率\n",
        "        sharpe = annual_return / annual_vol if annual_vol > 0 else 0\n",
        "\n",
        "        # 最大回撤\n",
        "        log_cum = np.cumsum(np.log1p(returns))\n",
        "        peak_log = np.maximum.accumulate(log_cum)\n",
        "        dd_log = peak_log - log_cum\n",
        "        max_drawdown = 1 - np.exp(-dd_log.max())\n",
        "        #单日最大回撤\n",
        "        max_1d_loss = np.min(returns)\n",
        "\n",
        "        # 换手率\n",
        "        avg_turnover = np.mean(turnover_series) if turnover_series is not None else 0\n",
        "        # 新增Sortino Ratio和CVaR的计算\n",
        "\n",
        "        sortino = sortino_ratio(returns)\n",
        "        cvar95  = cvar(returns, alpha=0.95)\n",
        "\n",
        "\n",
        "        result = {\n",
        "            'annual_return': annual_return,\n",
        "            'annual_vol': annual_vol,\n",
        "            'sharpe': sharpe,\n",
        "            'max_drawdown': max_drawdown,\n",
        "            'max_1d_loss': max_1d_loss,\n",
        "            'avg_turnover': avg_turnover,\n",
        "            'sortino': sortino,\n",
        "            'cvar95': cvar95\n",
        "        }\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f45fe1ca",
      "metadata": {
        "id": "f45fe1ca"
      },
      "outputs": [],
      "source": [
        "# ===== 附加：回测所需工具函数（本地保存版） =====\n",
        "\n",
        "# 兼容回测：直接返回npz句柄\n",
        "\n",
        "def load_datasets(npz_path: str):\n",
        "    return np.load(npz_path, allow_pickle=True)\n",
        "\n",
        "# 动态批量大小（与训练侧一致）\n",
        "\n",
        "def get_batch_size(window: int) -> int:\n",
        "    return get_dynamic_batch_size(window, base=512)\n",
        "\n",
        "# Chronos滚动批量预测\n",
        "\n",
        "@torch.no_grad()\n",
        "def chronos_rolling_prediction(\n",
        "    pipeline,\n",
        "    X_data: np.ndarray,\n",
        "    batch_size: int = 256,\n",
        "    prediction_length: int = 1,\n",
        "    num_samples: int = 10\n",
        ") -> np.ndarray:\n",
        "    preds: List[float] = []\n",
        "    for i in range(0, len(X_data), batch_size):\n",
        "        ctx_list = [torch.from_numpy(seq.astype(np.float32)) for seq in X_data[i:i + batch_size]]\n",
        "        fr = pipeline.predict(\n",
        "            context=ctx_list,\n",
        "            prediction_length=prediction_length,\n",
        "            num_samples=num_samples\n",
        "        )\n",
        "        if isinstance(fr, torch.Tensor):\n",
        "            # 只取第一步（t=0）在 samples 维上的均值\n",
        "            means = fr[:, 0, :].mean(dim=1).cpu().numpy()\n",
        "        else:\n",
        "            means = np.array([np.asarray(f)[0].mean() for f in fr])\n",
        "        preds.extend(means.tolist())\n",
        "    return np.array(preds, dtype=np.float32)\n",
        "\n",
        "# ========================= 数据集与时间切分 =========================\n",
        "class ChronosWindowDataset(Dataset):\n",
        "    \"\"\"将窗口序列 (X, y) 转为 Chronos 所需 token（不打乱顺序）。\"\"\"\n",
        "    def __init__(self, X: np.ndarray, y: np.ndarray, chronos_tokenizer, prediction_length: int = 1) -> None:\n",
        "        self.X = X.astype(np.float32)\n",
        "        self.y = y.astype(np.float32).reshape(-1)\n",
        "        self.tokenizer = chronos_tokenizer\n",
        "        self.pred_len = int(prediction_length)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        context = torch.from_numpy(self.X[idx])  # (L,)\n",
        "        input_ids, attention_mask, scale = self.tokenizer.context_input_transform(context.unsqueeze(0))\n",
        "        future_target = torch.tensor(self.y[idx: idx + 1]).unsqueeze(0)\n",
        "        labels, labels_mask = self.tokenizer.label_input_transform(future_target, scale)\n",
        "        labels[labels_mask == 0] = -100\n",
        "        return {\n",
        "            \"input_ids\": input_ids.squeeze(0),\n",
        "            \"attention_mask\": attention_mask.squeeze(0),\n",
        "            \"labels\": labels.squeeze(0),\n",
        "        }\n",
        "\n",
        "\n",
        "def load_npz_dataset(npz_path: str) -> Dict[str, Any]:\n",
        "    \"\"\"加载 .npz 数据（包含不同窗口 train/test 的 X, y, meta）。\"\"\"\n",
        "    data = np.load(npz_path, allow_pickle=True)\n",
        "    return {k: data[k] for k in data.files}\n",
        "\n",
        "\n",
        "def extract_split(data: Dict[str, Any], window: int, split: str) -> Tuple[np.ndarray, np.ndarray, pd.DataFrame]:\n",
        "    \"\"\"抽取指定窗口+数据划分（train/test），并返回 X, y, meta DataFrame\"\"\"\n",
        "    X = data[f\"X_{split}_{window}\"]\n",
        "    y = data[f\"y_{split}_{window}\"]\n",
        "    meta_raw = data.get(f\"meta_{split}_{window}\")\n",
        "    if meta_raw is None:\n",
        "        meta = pd.DataFrame({\"PERMNO\": np.arange(len(X))})\n",
        "    else:\n",
        "        if hasattr(meta_raw, \"item\"):\n",
        "            meta = pd.DataFrame(meta_raw.item())\n",
        "        else:\n",
        "            meta = pd.DataFrame(meta_raw)\n",
        "    return X, y, meta\n",
        "\n",
        "\n",
        "def time_based_val_split(X: np.ndarray, y: np.ndarray, meta: pd.DataFrame, val_ratio: float = 0.2) -> Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray], Tuple[pd.DataFrame, pd.DataFrame]]:\n",
        "    \"\"\"按时间顺序切分训练集与验证集（不打乱）。\"\"\"\n",
        "    n = len(X)\n",
        "    val_start = int(math.floor(n * (1.0 - val_ratio)))\n",
        "    X_tr, y_tr = X[:val_start], y[:val_start]\n",
        "    X_va, y_va = X[val_start:], y[val_start:]\n",
        "    meta_tr, meta_va = meta.iloc[:val_start].reset_index(drop=True), meta.iloc[val_start:].reset_index(drop=True)\n",
        "    return (X_tr, y_tr), (X_va, y_va), (meta_tr, meta_va)\n",
        "\n",
        "\n",
        "def get_dynamic_batch_size(window: int, base: int = 512) -> int:\n",
        "    \"\"\"根据窗口大小动态设置 batch size（越长序列越小）。\"\"\"\n",
        "    if window <= 5:\n",
        "        return base\n",
        "    elif window <= 21:\n",
        "        return 128\n",
        "    elif window <= 252:\n",
        "        return 64\n",
        "    elif window <= 512:\n",
        "        return 32\n",
        "    return max(base // 16, 16)\n",
        "# ========================= 禁止打乱的 Trainer + LoRA 构建 =========================\n",
        "class OrderedTrainer(Trainer):\n",
        "    \"\"\"禁止训练/评估时打乱顺序，严格保持时间顺序。\"\"\"\n",
        "    def get_train_dataloader(self) -> DataLoader:  # type: ignore[override]\n",
        "        num_workers = getattr(self.args, \"dataloader_num_workers\", 0)\n",
        "        dl_kwargs = dict(\n",
        "            dataset=self.train_dataset,\n",
        "            batch_size=self.args.per_device_train_batch_size,\n",
        "            shuffle=False,\n",
        "            collate_fn=self.data_collator,\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=getattr(self.args, \"dataloader_pin_memory\", False),\n",
        "            drop_last=getattr(self.args, \"dataloader_drop_last\", False),\n",
        "        )\n",
        "        # 仅当 num_workers>0 时启用，以避免 PyTorch 的限制报错\n",
        "        if num_workers and num_workers > 0:\n",
        "            dl_kwargs[\"persistent_workers\"] = True\n",
        "            dl_kwargs[\"prefetch_factor\"] = 4\n",
        "        return DataLoader(**dl_kwargs)\n",
        "\n",
        "    def get_eval_dataloader(self, eval_dataset=None) -> DataLoader:  # type: ignore[override]\n",
        "        dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
        "        num_workers = getattr(self.args, \"dataloader_num_workers\", 0)\n",
        "        dl_kwargs = dict(\n",
        "            dataset=dataset,\n",
        "            batch_size=self.args.per_device_eval_batch_size,\n",
        "            shuffle=False,\n",
        "            collate_fn=self.data_collator,\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=getattr(self.args, \"dataloader_pin_memory\", False),\n",
        "            drop_last=False,\n",
        "        )\n",
        "        if num_workers and num_workers > 0:\n",
        "            dl_kwargs[\"persistent_workers\"] = True\n",
        "            dl_kwargs[\"prefetch_factor\"] = 4\n",
        "        return DataLoader(**dl_kwargs)\n",
        "\n",
        "\n",
        "def build_lora_config(r: int = 8, alpha: int = 16, dropout: float = 0.05, target_modules: Optional[List[str]] = None) -> LoraConfig:\n",
        "    \"\"\"构建 LoRA 配置；默认作用于 T5 注意力/FFN 关键投影。\"\"\"\n",
        "    if target_modules is None:\n",
        "        target_modules = [\"q\", \"k\", \"v\", \"o\", \"wi\", \"wo\"]\n",
        "    return LoraConfig(\n",
        "        r=r,\n",
        "        lora_alpha=alpha,\n",
        "        lora_dropout=dropout,\n",
        "        bias=\"none\",\n",
        "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "        target_modules=target_modules,\n",
        "    )\n",
        "\n",
        "\n",
        "def bind_strip_num_items(model):\n",
        "    \"\"\"绑定 forward 包装避免不兼容参数：去除 chronos 可能注入的额外kwargs，并仅保留模型forward可接受的参数。\"\"\"\n",
        "    import inspect\n",
        "    original_forward = model.forward\n",
        "    sig = inspect.signature(original_forward)\n",
        "    accepted = set(sig.parameters.keys())\n",
        "    def wrapped_forward(*args, **kwargs):\n",
        "        # 移除常见的额外参数\n",
        "        for bad in [\n",
        "            'num_items',\n",
        "            'num_items_in_batch',\n",
        "            'num_samples',\n",
        "            'prediction_length',\n",
        "            'context',\n",
        "            'scales',\n",
        "        ]:\n",
        "            kwargs.pop(bad, None)\n",
        "        # 仅保留forward签名中存在的参数\n",
        "        kwargs = {k: v for k, v in kwargs.items() if k in accepted}\n",
        "        return original_forward(*args, **kwargs)\n",
        "    model.forward = wrapped_forward\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "# === Colab 训练加速：混合精度、TF32、fused Adam、梯度检查点与更合理的 DataLoader ===\n",
        "# print in English; comments in Chinese\n",
        "import torch\n",
        "\n",
        "# —— 自动选择精度与优化器（Colab GPU 上：优先 bf16, 否则 fp16；Ampere+ 开 TF32；可用则用 fused Adam）\n",
        "\n",
        "def get_precision_and_optim() -> tuple[bool, bool, bool, str]:\n",
        "    use_bf16 = False\n",
        "    use_fp16 = False\n",
        "    allow_tf32 = False\n",
        "    optim_name = \"adamw_torch\"\n",
        "    if torch.cuda.is_available():\n",
        "        try:\n",
        "            major, _ = torch.cuda.get_device_capability(0)\n",
        "        except Exception:\n",
        "            major = 0\n",
        "        # 开启 TF32（对 Ampere 及以上有效，速度更快）\n",
        "        allow_tf32 = major >= 8\n",
        "        try:\n",
        "            use_bf16 = torch.cuda.is_bf16_supported()\n",
        "        except Exception:\n",
        "            use_bf16 = major >= 8\n",
        "        use_fp16 = not use_bf16\n",
        "        try:\n",
        "            # 新版 transformers/torch 优化器名称\n",
        "            optim_name = \"adamw_torch_fused\"\n",
        "        except Exception:\n",
        "            optim_name = \"adamw_torch\"\n",
        "    return use_bf16, use_fp16, allow_tf32, optim_name\n",
        "\n",
        "\n",
        "def configure_torch_backends(allow_tf32: bool) -> None:\n",
        "    # cuDNN/TF32 后端配置（仅对 CUDA 有效）\n",
        "    if torch.cuda.is_available():\n",
        "        try:\n",
        "            torch.backends.cuda.matmul.allow_tf32 = bool(allow_tf32)\n",
        "        except Exception:\n",
        "            pass\n",
        "        try:\n",
        "            torch.backends.cudnn.benchmark = True\n",
        "        except Exception:\n",
        "            pass\n",
        "        # 尝试启用 PyTorch SDPA 的 Flash/Mem-efficient 路径（A100 友好，安全降级）\n",
        "        try:\n",
        "            from torch.backends.cuda import sdp_kernel\n",
        "            sdp_kernel.enable_flash(True)\n",
        "            sdp_kernel.enable_mem_efficient(True)\n",
        "            sdp_kernel.enable_math(False)\n",
        "        except Exception:\n",
        "            pass\n",
        "    # pytorch 2.0+ 的 float32 matmul 精度提示\n",
        "    try:\n",
        "        torch.set_float32_matmul_precision(\"high\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "\n",
        "# —— 覆盖 create_trainer：在 Colab 上自动启用上述设置\n",
        "\n",
        "def create_trainer(\n",
        "    model,\n",
        "    train_ds: Dataset,\n",
        "    val_ds: Dataset,\n",
        "    output_dir: str,\n",
        "    per_device_train_batch_size: int,\n",
        "    per_device_eval_batch_size: int,\n",
        "    learning_rate: float,\n",
        "    weight_decay: float,\n",
        "    num_train_epochs: int,\n",
        "    patience: int,\n",
        "    logging_steps: int = 200,\n",
        "    eval_strategy: str = \"epoch\",\n",
        "    save_strategy: str = \"epoch\",\n",
        "    dataloader_num_workers: int = 0,\n",
        "    data_collator=None,\n",
        "    warmup_ratio: float = 0.10,\n",
        ") -> Trainer:\n",
        "    \"\"\"构建 Trainer（含早停），不打乱顺序，并自动开启混合精度与高性能设置。\"\"\"\n",
        "    use_bf16, use_fp16, allow_tf32, optim_name = get_precision_and_optim()\n",
        "    configure_torch_backends(allow_tf32)\n",
        "\n",
        "    # DataLoader 设置：Colab 上用更多 worker 与 pin_memory 提升吞吐\n",
        "    num_workers = dataloader_num_workers if dataloader_num_workers > 0 else (4 if torch.cuda.is_available() else 0)\n",
        "    pin_memory = True if torch.cuda.is_available() else False\n",
        "\n",
        "    # A100 上适度提高评估聚合步数，降低显存峰值/提升吞吐\n",
        "    try:\n",
        "        eval_accum = 64 if (torch.cuda.is_available() and \"a100\" in torch.cuda.get_device_name(0).lower()) else 32\n",
        "    except Exception:\n",
        "        eval_accum = 32\n",
        "\n",
        "    try:\n",
        "        args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            per_device_train_batch_size=per_device_train_batch_size,\n",
        "            per_device_eval_batch_size=per_device_eval_batch_size,\n",
        "            learning_rate=learning_rate,\n",
        "            weight_decay=weight_decay,\n",
        "            num_train_epochs=num_train_epochs,\n",
        "            warmup_ratio=warmup_ratio,          # <-- 新增\n",
        "            max_grad_norm=1.0,\n",
        "            logging_dir=os.path.join(output_dir, \"logs\"),\n",
        "            logging_steps=logging_steps,\n",
        "            eval_strategy=IntervalStrategy(eval_strategy),\n",
        "            save_strategy=IntervalStrategy(save_strategy),\n",
        "            save_total_limit=1,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"eval_loss\",\n",
        "            greater_is_better=False,\n",
        "            report_to=[],\n",
        "            remove_unused_columns=False,\n",
        "            dataloader_num_workers=num_workers,\n",
        "            dataloader_pin_memory=pin_memory,\n",
        "            dataloader_drop_last=False,\n",
        "            fp16=use_fp16,\n",
        "            bf16=use_bf16,\n",
        "            tf32=allow_tf32,\n",
        "            gradient_accumulation_steps=1,\n",
        "            gradient_checkpointing=True,\n",
        "            group_by_length=False,\n",
        "            optim=optim_name,\n",
        "            eval_accumulation_steps=eval_accum,  # 评估显存聚合，降低峰值\n",
        "        )\n",
        "    except TypeError:\n",
        "        # 兼容旧版 transformers 的字段名\n",
        "        args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            per_device_train_batch_size=per_device_train_batch_size,\n",
        "            per_device_eval_batch_size=per_device_eval_batch_size,\n",
        "            learning_rate=learning_rate,\n",
        "            weight_decay=weight_decay,\n",
        "            num_train_epochs=num_train_epochs,\n",
        "            warmup_ratio=warmup_ratio,          # <-- 同样加上\n",
        "            max_grad_norm=1.0,\n",
        "            logging_dir=os.path.join(output_dir, \"logs\"),\n",
        "            logging_steps=logging_steps,\n",
        "            evaluation_strategy=IntervalStrategy(eval_strategy),\n",
        "            save_strategy=IntervalStrategy(save_strategy),\n",
        "            save_total_limit=1,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"eval_loss\",\n",
        "            greater_is_better=False,\n",
        "            report_to=[],\n",
        "            remove_unused_columns=False,\n",
        "            dataloader_num_workers=num_workers,\n",
        "            dataloader_pin_memory=pin_memory,\n",
        "            dataloader_drop_last=False,\n",
        "            fp16=use_fp16,\n",
        "            bf16=use_bf16,\n",
        "            gradient_accumulation_steps=1,\n",
        "            gradient_checkpointing=True,\n",
        "            group_by_length=False,\n",
        "            optim=optim_name,\n",
        "            eval_accumulation_steps=eval_accum,\n",
        "        )\n",
        "\n",
        "    trainer = OrderedTrainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=val_ds,\n",
        "        data_collator=(data_collator or default_data_collator),\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=patience)],\n",
        "    )\n",
        "\n",
        "    # label_names 兼容处理\n",
        "    try:\n",
        "        trainer.label_names = [\"labels\"]\n",
        "    except Exception:\n",
        "        try:\n",
        "            trainer.label_names = []\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # 梯度检查点（双保险）\n",
        "    try:\n",
        "        model.gradient_checkpointing_enable()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return trainer\n",
        "\n",
        "\n",
        "# —— A100 识别与 torch.compile（安全降级） ——\n",
        "def is_a100() -> bool:\n",
        "    if not torch.cuda.is_available():\n",
        "        return False\n",
        "    try:\n",
        "        name = torch.cuda.get_device_name(0).lower()\n",
        "        if \"a100\" in name:\n",
        "            return True\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        major, _ = torch.cuda.get_device_capability(0)\n",
        "        total_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "        return (major >= 8) and (total_gb >= 35)\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "\n",
        "def maybe_torch_compile(model):\n",
        "    \"\"\"仅在显式开启时尝试编译；默认关闭以避免 Inductor/Quantization 相关报错。\"\"\"\n",
        "    USE_TORCH_COMPILE = os.environ.get(\"USE_TORCH_COMPILE\", \"0\") == \"1\"\n",
        "    if not (USE_TORCH_COMPILE and is_a100()):\n",
        "        return model\n",
        "    try:\n",
        "        # 遇到编译错误时自动回退到 eager，避免训练中断\n",
        "        import torch._dynamo\n",
        "        torch._dynamo.config.suppress_errors = True\n",
        "        compiled = torch.compile(model, mode=\"reduce-overhead\", fullgraph=False, backend=\"inductor\")\n",
        "        return compiled\n",
        "    except Exception:\n",
        "        return model\n",
        "\n",
        "# === 批量大小自适应（Colab 版）：按可用显存动态放大 train/eval batch size ===\n",
        "import math\n",
        "\n",
        "def get_colab_memory_hint(default_bs: int) -> int:\n",
        "    \"\"\"基于 GPU 是否可用与显存大小给出倍率，保守放大。\n",
        "    - 无 GPU：返回默认\n",
        "    - T4/V100(≈16GB)：×1.0~1.5\n",
        "    - A100(40GB)：×2\n",
        "    \"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        return default_bs\n",
        "    try:\n",
        "        total = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB\n",
        "    except Exception:\n",
        "        total = 16\n",
        "    if total >= 35:\n",
        "        mult = 2.0\n",
        "    elif total >= 22:\n",
        "        mult = 1.5\n",
        "    else:\n",
        "        mult = 1.0\n",
        "    return int(max(1, round(default_bs * mult)))\n",
        "\n",
        "def adapt_per_window_cfg_for_colab(cfg_dict: dict) -> dict:\n",
        "    \"\"\"在 GPU/Colab 下放大 batch size（A100≈40GB 翻倍），CPU 保持不变。\"\"\"\n",
        "    try:\n",
        "        if not torch.cuda.is_available():\n",
        "            return cfg_dict\n",
        "        new_cfg = {}\n",
        "        for w, cfg in cfg_dict.items():\n",
        "            base_train = int(cfg.get('train_bs', 256))\n",
        "            base_eval  = int(cfg.get('eval_bs', max(256, base_train)))\n",
        "            train_bs = get_colab_memory_hint(base_train)\n",
        "            eval_bs  = min(1024, max(base_eval, train_bs * 2))\n",
        "            ncfg = dict(cfg)\n",
        "            ncfg['train_bs'] = train_bs\n",
        "            ncfg['eval_bs']  = eval_bs\n",
        "            new_cfg[w] = ncfg\n",
        "        print('Using Colab-adaptive batch sizes:')\n",
        "        for w in sorted(new_cfg):\n",
        "            c = new_cfg[w]\n",
        "            print(f\"window={w}: train_bs={c['train_bs']}, eval_bs={c['eval_bs']}\")\n",
        "        return new_cfg\n",
        "    except Exception:\n",
        "        return cfg_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a13d92e8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272,
          "referenced_widgets": [
            "77db9577636045b3af64ad20c4d4e708",
            "64539b8b90a64dda85e330f13ccd5398",
            "705f095a8bd64cb1ad054005d2242428",
            "9f63936d8d6c4bd6be2b8fc026de3d62",
            "92a2a444e4fe41e18c66933772dd8386",
            "302731da7c72494897d640436c66ad8c",
            "de51f545826f4d76a5e14418b6d2831a",
            "b2bba446142c4bf489641957f85c2b5b",
            "4903ec51bcbe493fb4a379e6cba0a3b6",
            "d90ffe4b8d1c411ba6e769207d753f35",
            "659e95fd144f415683ad9f4eaf25b730",
            "979e19bd39424ae38224ccb66faa9c26",
            "8b86556d2e84408c8c8a0fa614dcc73a",
            "2d8b2d281bec4e848c61c9e3fb2bb011",
            "a810085f6e7e490c93d8b1ea15afc117",
            "259df72b69744eba9ae9223ede1e7ddf",
            "26f72fa87d274f42a783ac8675394b16",
            "9888f124f5fd490aa5e3ffe525fa06b7",
            "8aac5f4987ae470fbb0739f50fa82d00",
            "1ca64de2015b45499886841236e81da8",
            "5fbc070393744c63819eb1190b197442",
            "1b1e8ad2e8914b91b36a563f11c7bfc8",
            "d1a69df425184c67a3d3db5af21e9be7",
            "8088dea0c3cb49e984aae7102ce1a5e5",
            "42386fd45f874130b3970ad0cbbde08c",
            "1175f52a956a45cb9f429c373d2c9c6f",
            "4ec05d8c11ba42d6b432b6be5f7bea11",
            "337e949e516b411f93168d9d7390cb97",
            "f1634d486b07408c93f6d037f3a7fa27",
            "660edc6a9e1a459d873dc44ae1d33bf2",
            "c1043f7033704ad9b607c2fb9ba9b599",
            "aec085822f50466e80e1fbac61f749a5",
            "b70874d232d74693bf036281edb7c97f"
          ]
        },
        "id": "a13d92e8",
        "outputId": "c687bea6-b600-4157-ac2a-8f176f1c8c9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77db9577636045b3af64ad20c4d4e708"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.84G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "979e19bd39424ae38224ccb66faa9c26"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1a69df425184c67a3d3db5af21e9be7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset: /content/drive/MyDrive/ERP Data/all_window_datasets_unscaled.npz\n"
          ]
        }
      ],
      "source": [
        "# ========================= 全局配置与 Tokenizer/Collator =========================\n",
        "set_seed(42)\n",
        "ensure_chronos_import()\n",
        "\n",
        "from chronos import BaseChronosPipeline, ChronosConfig\n",
        "\n",
        "# 路径与基础模型\n",
        "base_model_id = \"amazon/chronos-t5-large\"\n",
        "windows = [5, 21, 252, 512]\n",
        "\n",
        "# 数据路径（不打乱时间顺序）\n",
        "# Google Colab 路径\n",
        "default_data_path = \"/content/drive/MyDrive/ERP Data/all_window_datasets_unscaled.npz\"\n",
        "assert os.path.exists(default_data_path), \"Dataset not found. Please ensure all_window_datasets_unscaled.npz exists on Drive.\"\n",
        "data_path = os.environ.get(\"ERP_DATA_PATH\", default_data_path)\n",
        "\n",
        "# 保存目录（Colab 统一路径）\n",
        "root_dir = \"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results\"\n",
        "save_models_dir = os.path.join(root_dir, \"models\")\n",
        "save_preds_dir = os.path.join(root_dir, \"predictions\")\n",
        "save_results_dir = root_dir\n",
        "for d in [save_models_dir, save_preds_dir, save_results_dir]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "# 设备与基础 pipeline（用于推理 tokenizer 配置）\n",
        "device = get_device()\n",
        "print(f\"Device: {device}\")\n",
        "pipeline = BaseChronosPipeline.from_pretrained(\n",
        "    base_model_id,\n",
        "    device_map=\"mps\" if (device.type == \"mps\") else (\"auto\" if torch.cuda.is_available() else None),\n",
        "    torch_dtype=torch.float32,\n",
        ")\n",
        "\n",
        "# 从基础配置构建训练用 config（prediction_length=1）\n",
        "base_cfg: ChronosConfig = pipeline.model.config  # type: ignore\n",
        "train_cfg = ChronosConfig(\n",
        "    tokenizer_class=base_cfg.tokenizer_class,\n",
        "    tokenizer_kwargs=base_cfg.tokenizer_kwargs,\n",
        "    context_length=base_cfg.context_length,\n",
        "    prediction_length=1,\n",
        "    n_tokens=base_cfg.n_tokens,\n",
        "    n_special_tokens=base_cfg.n_special_tokens,\n",
        "    pad_token_id=base_cfg.pad_token_id,\n",
        "    eos_token_id=base_cfg.eos_token_id,\n",
        "    use_eos_token=base_cfg.use_eos_token,\n",
        "    model_type=base_cfg.model_type,\n",
        "    num_samples=base_cfg.num_samples,\n",
        "    temperature=base_cfg.temperature,\n",
        "    top_k=base_cfg.top_k,\n",
        "    top_p=base_cfg.top_p,\n",
        ")\n",
        "train_tokenizer = train_cfg.create_tokenizer()\n",
        "\n",
        "# Collator: 按批右侧填充 input_ids，同时修整 labels 长度\n",
        "class ChronosPadCollator:\n",
        "    def __init__(self, pad_token_id: int, label_len_expected: int):\n",
        "        self.pad_token_id = pad_token_id\n",
        "        self.label_len_expected = int(label_len_expected)\n",
        "    def __call__(self, features: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
        "        max_inp = max(f[\"input_ids\"].shape[0] for f in features)\n",
        "        input_ids, attention_masks, labels = [], [], []\n",
        "        for f in features:\n",
        "            Li = f[\"input_ids\"].shape[0]\n",
        "            pad_inp = max_inp - Li\n",
        "            input_ids.append(torch.nn.functional.pad(f[\"input_ids\"], (0, pad_inp), value=self.pad_token_id))\n",
        "            attention_masks.append(torch.nn.functional.pad(f[\"attention_mask\"].to(torch.long), (0, pad_inp), value=0))\n",
        "            lbl = f[\"labels\"]\n",
        "            Lexp = self.label_len_expected\n",
        "            if lbl.shape[0] > Lexp:\n",
        "                lbl = lbl[-Lexp:]\n",
        "            elif lbl.shape[0] < Lexp:\n",
        "                lbl = torch.nn.functional.pad(lbl, (0, Lexp - lbl.shape[0]), value=-100)\n",
        "            labels.append(lbl)\n",
        "        return {\n",
        "            \"input_ids\": torch.stack(input_ids, dim=0),\n",
        "            \"attention_mask\": torch.stack(attention_masks, dim=0),\n",
        "            \"labels\": torch.stack(labels, dim=0),\n",
        "        }\n",
        "\n",
        "label_len_expected = train_cfg.prediction_length + (1 if (train_cfg.use_eos_token and train_cfg.model_type==\"seq2seq\") else 0)\n",
        "pad_collator = ChronosPadCollator(pad_token_id=train_tokenizer.config.pad_token_id, label_len_expected=label_len_expected)\n",
        "\n",
        "# 加载数据\n",
        "print(f\"Loading dataset: {data_path}\")\n",
        "data = load_npz_dataset(data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8GhEFa4R5q9K",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8GhEFa4R5q9K",
        "outputId": "528401f8-7895-45e7-87ea-a7ff151ff60d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using per-window empirical hyperparameters.\n",
            "Using Colab-adaptive batch sizes:\n",
            "window=5: train_bs=1024, eval_bs=1024\n",
            "window=21: train_bs=1024, eval_bs=1024\n",
            "window=252: train_bs=256, eval_bs=512\n",
            "window=512: train_bs=128, eval_bs=256\n",
            "\n",
            "===== Training LoRA for window=5 =====\n",
            "trainable params: 8,650,752 || all params: 717,614,080 || trainable%: 1.2055\n",
            "Start fine-tuning (single window)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='770' max='770' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [770/770 10:16, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.391183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>4.659300</td>\n",
              "      <td>3.386115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.380100</td>\n",
              "      <td>3.385463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.375300</td>\n",
              "      <td>3.385317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.375300</td>\n",
              "      <td>3.386373</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Saved] LoRA adapter: /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/models/chronos_t5_large_lora_w5/chronos_t5_lora_adapter\n",
            "\n",
            "===== Training LoRA for window=21 =====\n",
            "trainable params: 8,650,752 || all params: 717,614,080 || trainable%: 1.2055\n",
            "Start fine-tuning (single window)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='770' max='770' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [770/770 19:08, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.326613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>4.721500</td>\n",
              "      <td>3.321280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.315400</td>\n",
              "      <td>3.320620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.312900</td>\n",
              "      <td>3.320474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.312900</td>\n",
              "      <td>3.321683</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Saved] LoRA adapter: /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/models/chronos_t5_large_lora_w21/chronos_t5_lora_adapter\n",
            "\n",
            "===== Training LoRA for window=252 =====\n",
            "trainable params: 17,301,504 || all params: 726,264,832 || trainable%: 2.3823\n",
            "Start fine-tuning (single window)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2885' max='2885' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2885/2885 3:27:50, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.345300</td>\n",
              "      <td>3.309152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.312400</td>\n",
              "      <td>3.306160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.293500</td>\n",
              "      <td>3.305461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.289900</td>\n",
              "      <td>3.304948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.283600</td>\n",
              "      <td>3.304564</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Saved] LoRA adapter: /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/models/chronos_t5_large_lora_w252/chronos_t5_lora_adapter\n",
            "\n",
            "===== Training LoRA for window=512 =====\n",
            "trainable params: 17,301,504 || all params: 726,264,832 || trainable%: 2.3823\n",
            "Start fine-tuning (single window)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5365' max='5365' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5365/5365 8:04:07, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.254800</td>\n",
              "      <td>3.293710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.185800</td>\n",
              "      <td>3.289315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.262900</td>\n",
              "      <td>3.287551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.233000</td>\n",
              "      <td>3.286320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.139300</td>\n",
              "      <td>3.286705</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Saved] LoRA adapter: /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/models/chronos_t5_large_lora_w512/chronos_t5_lora_adapter\n",
            "[Save] Metrics saved to: /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/chronos_t5_large_lora_per_window_metrics.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ========================= 经验超参数（每窗口） =========================\n",
        "per_window_cfg = {\n",
        "    5:   dict(epochs=5, patience=2, lr=5e-4, wd=0.005, r=8,  alpha=16,\n",
        "              dropout=0.05, warmup_ratio=0.10, train_bs=512, eval_bs=512),\n",
        "    21:  dict(epochs=5, patience=2, lr=6e-4, wd=0.005, r=8,  alpha=16,\n",
        "              dropout=0.05, warmup_ratio=0.10, train_bs=512, eval_bs=512),\n",
        "    252: dict(epochs=5, patience=2, lr=3e-4, wd=0.010, r=16, alpha=32,\n",
        "              dropout=0.07, warmup_ratio=0.10, train_bs=128, eval_bs=256),\n",
        "    512: dict(epochs=5, patience=2, lr=2e-4, wd=0.010, r=16, alpha=32,\n",
        "              dropout=0.08, warmup_ratio=0.10, train_bs=64,  eval_bs=128),\n",
        "}\n",
        "print(\"Using per-window empirical hyperparameters.\")\n",
        "per_window_cfg = adapt_per_window_cfg_for_colab(per_window_cfg)\n",
        "# ========================= 按窗口训练 LoRA、评估并保存 =========================\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "# 控制是否在训练后做测试集预测评估（默认关闭）\n",
        "DO_TEST_EVAL = False\n",
        "\n",
        "results_summary: List[Dict[str, Any]] = []\n",
        "\n",
        "for ws in windows:\n",
        "    cfg = per_window_cfg[ws]\n",
        "    print(f\"\\n===== Training LoRA for window={ws} =====\")\n",
        "\n",
        "    # 获取该窗口数据（不打乱时间顺序）\n",
        "    X_train, y_train, meta_train = extract_split(data, ws, split=\"train\")\n",
        "    X_test, y_test, meta_test = extract_split(data, ws, split=\"test\")\n",
        "    (X_tr, y_tr), (X_va, y_va), _ = time_based_val_split(X_train, y_train, meta_train, val_ratio=0.2)\n",
        "\n",
        "    # 构造该窗口的 tokenizer 和 dataset\n",
        "    base_cfg: ChronosConfig = pipeline.model.config  # type: ignore\n",
        "    ws_cfg = ChronosConfig(\n",
        "        tokenizer_class=base_cfg.tokenizer_class,\n",
        "        tokenizer_kwargs=base_cfg.tokenizer_kwargs,\n",
        "        context_length=base_cfg.context_length,\n",
        "        prediction_length=1,\n",
        "        n_tokens=base_cfg.n_tokens,\n",
        "        n_special_tokens=base_cfg.n_special_tokens,\n",
        "        pad_token_id=base_cfg.pad_token_id,\n",
        "        eos_token_id=base_cfg.eos_token_id,\n",
        "        use_eos_token=base_cfg.use_eos_token,\n",
        "        model_type=base_cfg.model_type,\n",
        "        num_samples=base_cfg.num_samples,\n",
        "        temperature=base_cfg.temperature,\n",
        "        top_k=base_cfg.top_k,\n",
        "        top_p=base_cfg.top_p,\n",
        "    )\n",
        "    ws_tokenizer = ws_cfg.create_tokenizer()\n",
        "\n",
        "    ds_tr = ChronosWindowDataset(X_tr, y_tr, ws_tokenizer, prediction_length=1)\n",
        "    ds_va = ChronosWindowDataset(X_va, y_va, ws_tokenizer, prediction_length=1)\n",
        "\n",
        "    # 基础模型 + LoRA（绑定 forward 包装避免不兼容参数）\n",
        "    base = AutoModelForSeq2SeqLM.from_pretrained(base_model_id)\n",
        "    base = bind_strip_num_items(base)\n",
        "    lora_cfg = build_lora_config(r=cfg[\"r\"], alpha=cfg[\"alpha\"], dropout=cfg[\"dropout\"], target_modules=[\"q\",\"k\",\"v\",\"o\",\"wi\",\"wo\"])\n",
        "    peft_model = get_peft_model(base, lora_cfg)\n",
        "    # A100 上尝试编译 LoRA 模型以提高吞吐\n",
        "    try:\n",
        "        peft_model.print_trainable_parameters()\n",
        "    except Exception:\n",
        "        pass\n",
        "    num_trainable = sum(p.requires_grad for p in peft_model.parameters())\n",
        "    if num_trainable == 0:\n",
        "        # 回退到更宽的 T5 命名覆盖（含 wi_0/wi_1）\n",
        "        fallback_targets = [\"q\", \"k\", \"v\", \"o\", \"wi_0\", \"wi_1\", \"wo\"]\n",
        "        lora_cfg_fb = build_lora_config(r=cfg[\"r\"], alpha=cfg[\"alpha\"], dropout=cfg[\"dropout\"], target_modules=fallback_targets)\n",
        "        peft_model = get_peft_model(base, lora_cfg_fb)\n",
        "        try:\n",
        "            peft_model.print_trainable_parameters()\n",
        "        except Exception:\n",
        "            pass\n",
        "    # 梯度检查点需要输入可求导\n",
        "    try:\n",
        "        peft_model.enable_input_require_grads()\n",
        "    except Exception:\n",
        "        pass\n",
        "    # A100 上可选编译（默认关闭，需设置 USE_TORCH_COMPILE=1 才启用）\n",
        "    peft_model = maybe_torch_compile(peft_model)\n",
        "    try:\n",
        "        peft_model.config.use_cache = False  # 训练阶段禁用KV缓存，去除past_key_values弃用警告\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    out_dir_ws = os.path.join(save_models_dir, f\"chronos_t5_large_lora_w{ws}\")\n",
        "    os.makedirs(out_dir_ws, exist_ok=True)\n",
        "\n",
        "    trainer_ws = create_trainer(\n",
        "        model=peft_model,\n",
        "        train_ds=ds_tr,\n",
        "        val_ds=ds_va,\n",
        "        output_dir=out_dir_ws,\n",
        "        per_device_train_batch_size=cfg[\"train_bs\"],\n",
        "        per_device_eval_batch_size=cfg[\"eval_bs\"],\n",
        "        learning_rate=cfg[\"lr\"],\n",
        "        weight_decay=cfg[\"wd\"],\n",
        "        num_train_epochs=cfg[\"epochs\"],\n",
        "        patience=cfg[\"patience\"],\n",
        "        logging_steps=200,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        dataloader_num_workers=0,\n",
        "        data_collator=pad_collator,\n",
        "    )\n",
        "\n",
        "    print(\"Start fine-tuning (single window)...\")\n",
        "    train_output = trainer_ws.train()\n",
        "\n",
        "    # 保存 LoRA 适配器\n",
        "    adapter_dir = os.path.join(out_dir_ws, \"chronos_t5_lora_adapter\"); os.makedirs(adapter_dir, exist_ok=True)\n",
        "    peft_model.save_pretrained(adapter_dir)\n",
        "    print(f\"[Saved] LoRA adapter: {adapter_dir}\")\n",
        "\n",
        "    if DO_TEST_EVAL:\n",
        "        # 推理：用“干净”基础模型 + 该窗口适配器，走 chronos pipeline 的 predict（批量）\n",
        "        base_infer = AutoModelForSeq2SeqLM.from_pretrained(base_model_id)\n",
        "        peft_infer = PeftModel.from_pretrained(base_infer, adapter_dir).to(device)\n",
        "        peft_infer = maybe_torch_compile(peft_infer)\n",
        "        peft_infer.eval()\n",
        "        try:\n",
        "            peft_infer.config.use_cache = True\n",
        "        except Exception:\n",
        "            pass\n",
        "        pipeline.model.model = peft_infer\n",
        "\n",
        "        # 批量推理工具\n",
        "        @torch.no_grad()\n",
        "        def batch_predict_with_pipeline(X: np.ndarray, pipeline, batch_size: int = 256, num_samples: int = 10, prediction_length: int = 1) -> np.ndarray:\n",
        "            preds: List[float] = []\n",
        "            for i in tqdm(range(0, len(X), batch_size), desc=\"Batch Inference\"):\n",
        "                ctx_list = [torch.from_numpy(seq.astype(np.float32)) for seq in X[i:i + batch_size]]\n",
        "                fr = pipeline.predict(context=ctx_list, prediction_length=prediction_length, num_samples=num_samples)\n",
        "                means = fr.mean(dim=(1, 2)).cpu().numpy() if isinstance(fr, torch.Tensor) else np.array([np.array(f).mean() for f in fr])\n",
        "                preds.extend(means.tolist())\n",
        "            return np.array(preds, dtype=np.float32)\n",
        "\n",
        "        # 测试集评估并保存（若开启）\n",
        "        y_pred = batch_predict_with_pipeline(X_test, pipeline, batch_size=cfg[\"eval_bs\"], num_samples=10, prediction_length=1).reshape(-1)\n",
        "        permnos = meta_test[\"PERMNO\"].values if \"PERMNO\" in meta_test.columns else None\n",
        "        k_features = X_test.shape[1]\n",
        "        metrics = regression_metrics(y_true=y_test.reshape(-1), y_pred=y_pred.reshape(-1), k=k_features, meta=meta_test, permnos=permnos)\n",
        "\n",
        "        pred_df = pd.DataFrame({\n",
        "            \"PERMNO\": meta_test.get(\"PERMNO\", pd.Series([np.nan] * len(y_pred))),\n",
        "            \"DATE\": meta_test.get(\"DATE\", meta_test.get(\"date\", pd.Series([np.nan] * len(y_pred)))),\n",
        "            \"y_true\": y_test.reshape(-1),\n",
        "            \"y_pred\": y_pred,\n",
        "        })\n",
        "        csv_path = os.path.join(save_preds_dir, f\"chronos_t5_large_lora_w{ws}.csv\")\n",
        "        pred_df.to_csv(csv_path, index=False)\n",
        "\n",
        "        # 记录摘要\n",
        "        results_summary.append({\"Window\": ws, **metrics, \"pred_path\": csv_path, \"adapter_dir\": adapter_dir})\n",
        "\n",
        "# 汇总保存\n",
        "metrics_csv = os.path.join(save_results_dir, \"chronos_t5_large_lora_per_window_metrics.csv\")\n",
        "pd.DataFrame(results_summary).to_csv(metrics_csv, index=False)\n",
        "print(f\"[Save] Metrics saved to: {metrics_csv}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "550d2a27",
      "metadata": {
        "id": "550d2a27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f10045be-c724-4e13-d8c1-0b42f9b55641"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Chronos T5-Large Portfolio Backtesting...\n",
            "Starting Daily Rebalance Portfolio Backtesting Simulation\n",
            "Processing window size: 5\n",
            "[INFO] Loaded LoRA adapter for window=5 from: /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/models/chronos_t5_large_lora_w5/chronos_t5_lora_adapter\n",
            "[INFO] Backtest inference model: LoRA (window=5)\n",
            "  Model: chronos large, Scheme: VW\n",
            "  Processing year: 2016\n",
            "  Processing year: 2017\n",
            "  Processing year: 2018\n",
            "  Processing year: 2019\n",
            "  Processing year: 2020\n",
            "  Processing year: 2021\n",
            "  Processing year: 2022\n",
            "  Processing year: 2023\n",
            "  Processing year: 2024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2075427930.py:55: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda g: g['y_pred'].corr(g['y_true'], method=method))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics saved for chronos large_w5 to /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_metrics.csv\n",
            "  Model: chronos large, Scheme: EW\n",
            "  Processing year: 2016\n",
            "  Processing year: 2017\n",
            "  Processing year: 2018\n",
            "  Processing year: 2019\n",
            "  Processing year: 2020\n",
            "  Processing year: 2021\n",
            "  Processing year: 2022\n",
            "  Processing year: 2023\n",
            "  Processing year: 2024\n",
            "Processing window size: 21\n",
            "[INFO] Loaded LoRA adapter for window=21 from: /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/models/chronos_t5_large_lora_w21/chronos_t5_lora_adapter\n",
            "[INFO] Backtest inference model: LoRA (window=21)\n",
            "  Model: chronos large, Scheme: VW\n",
            "  Processing year: 2016\n",
            "  Processing year: 2017\n",
            "  Processing year: 2018\n",
            "  Processing year: 2019\n",
            "  Processing year: 2020\n",
            "  Processing year: 2021\n",
            "  Processing year: 2022\n",
            "  Processing year: 2023\n",
            "  Processing year: 2024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2075427930.py:55: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda g: g['y_pred'].corr(g['y_true'], method=method))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics saved for chronos large_w21 to /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_metrics.csv\n",
            "  Model: chronos large, Scheme: EW\n",
            "  Processing year: 2016\n",
            "  Processing year: 2017\n",
            "  Processing year: 2018\n",
            "  Processing year: 2019\n",
            "  Processing year: 2020\n",
            "  Processing year: 2021\n",
            "  Processing year: 2022\n",
            "  Processing year: 2023\n",
            "  Processing year: 2024\n",
            "Processing window size: 252\n",
            "[INFO] Loaded LoRA adapter for window=252 from: /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/models/chronos_t5_large_lora_w252/chronos_t5_lora_adapter\n",
            "[INFO] Backtest inference model: LoRA (window=252)\n",
            "  Model: chronos large, Scheme: VW\n",
            "  Processing year: 2016\n",
            "  Processing year: 2017\n",
            "  Processing year: 2018\n",
            "  Processing year: 2019\n",
            "  Processing year: 2020\n",
            "  Processing year: 2021\n",
            "  Processing year: 2022\n",
            "  Processing year: 2023\n",
            "  Processing year: 2024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2075427930.py:55: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda g: g['y_pred'].corr(g['y_true'], method=method))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics saved for chronos large_w252 to /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_metrics.csv\n",
            "  Model: chronos large, Scheme: EW\n",
            "  Processing year: 2016\n",
            "  Processing year: 2017\n",
            "  Processing year: 2018\n",
            "  Processing year: 2019\n",
            "  Processing year: 2020\n",
            "  Processing year: 2021\n",
            "  Processing year: 2022\n",
            "  Processing year: 2023\n",
            "  Processing year: 2024\n",
            "Processing window size: 512\n",
            "[INFO] Loaded LoRA adapter for window=512 from: /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/models/chronos_t5_large_lora_w512/chronos_t5_lora_adapter\n",
            "[INFO] Backtest inference model: LoRA (window=512)\n",
            "  Model: chronos large, Scheme: VW\n",
            "  Processing year: 2016\n",
            "  Processing year: 2017\n",
            "  Processing year: 2018\n",
            "  Processing year: 2019\n",
            "  Processing year: 2020\n",
            "  Processing year: 2021\n",
            "  Processing year: 2022\n",
            "  Processing year: 2023\n",
            "  Processing year: 2024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2075427930.py:55: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda g: g['y_pred'].corr(g['y_true'], method=method))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics saved for chronos large_w512 to /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_metrics.csv\n",
            "  Model: chronos large, Scheme: EW\n",
            "  Processing year: 2016\n",
            "  Processing year: 2017\n",
            "  Processing year: 2018\n",
            "  Processing year: 2019\n",
            "  Processing year: 2020\n",
            "  Processing year: 2021\n",
            "  Processing year: 2022\n",
            "  Processing year: 2023\n",
            "  Processing year: 2024\n",
            "VW results saved to /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_results_daily_rebalance_VW.csv\n",
            "EW results saved to /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_results_daily_rebalance_EW.csv\n",
            "VW results saved to /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_daily_series_VW.csv\n",
            "EW results saved to /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_daily_series_EW.csv\n",
            "Saved 443400 prediction rows to predictions_daily.csv\n",
            "Generated 24 portfolio summary records\n",
            "Generated 52272 daily series records\n",
            "\\n============================================================\n",
            "CHRONOS T5-BASE PORTFOLIO BACKTESTING RESULTS\n",
            "============================================================\n",
            "\\nSummary Results:\n",
            "   scheme          model  window portfolio_type  annual_return  annual_vol  \\\n",
            "0      VW  chronos large       5      long_only         0.1056      0.2037   \n",
            "1      VW  chronos large       5     short_only        -0.1431      0.1997   \n",
            "2      VW  chronos large       5     long_short        -0.0375      0.1887   \n",
            "3      EW  chronos large       5      long_only         0.0827      0.2134   \n",
            "4      EW  chronos large       5     short_only        -0.1874      0.1979   \n",
            "5      EW  chronos large       5     long_short        -0.1047      0.1642   \n",
            "6      VW  chronos large      21      long_only         0.1367      0.2074   \n",
            "7      VW  chronos large      21     short_only        -0.1460      0.1980   \n",
            "8      VW  chronos large      21     long_short        -0.0093      0.1903   \n",
            "9      EW  chronos large      21      long_only         0.1048      0.2198   \n",
            "10     EW  chronos large      21     short_only        -0.1505      0.1994   \n",
            "11     EW  chronos large      21     long_short        -0.0457      0.1663   \n",
            "12     VW  chronos large     252      long_only         0.1443      0.2047   \n",
            "13     VW  chronos large     252     short_only        -0.1053      0.1981   \n",
            "14     VW  chronos large     252     long_short         0.0390      0.2013   \n",
            "15     EW  chronos large     252      long_only         0.1765      0.2087   \n",
            "16     EW  chronos large     252     short_only        -0.1351      0.2082   \n",
            "17     EW  chronos large     252     long_short         0.0414      0.1792   \n",
            "18     VW  chronos large     512      long_only         0.1253      0.2085   \n",
            "19     VW  chronos large     512     short_only        -0.1073      0.1885   \n",
            "20     VW  chronos large     512     long_short         0.0179      0.1967   \n",
            "21     EW  chronos large     512      long_only         0.0923      0.2124   \n",
            "22     EW  chronos large     512     short_only        -0.1754      0.1914   \n",
            "23     EW  chronos large     512     long_short        -0.0831      0.1727   \n",
            "\n",
            "    sharpe  max_drawdown  max_1d_loss  avg_turnover  ...  tc20_sharpe  \\\n",
            "0   0.5183        0.2420      -0.0528        1.7573  ...      -3.8207   \n",
            "1  -0.7167        0.7807      -0.0734        1.7773  ...      -5.1987   \n",
            "2  -0.1989        0.5131      -0.0473        3.5348  ...      -9.5948   \n",
            "3   0.3877        0.3861      -0.0629        1.5827  ...      -3.3515   \n",
            "4  -0.9473        0.8508      -0.0473        1.7016  ...      -5.2790   \n",
            "5  -0.6377        0.6926      -0.0459        3.2848  ...     -10.7094   \n",
            "6   0.6591        0.3315      -0.0588        1.7471  ...      -3.5792   \n",
            "7  -0.7373        0.7757      -0.0487        1.7718  ...      -5.2261   \n",
            "8  -0.0490        0.4541      -0.0547        3.5188  ...      -9.3133   \n",
            "9   0.4768        0.3260      -0.0617        1.6045  ...      -3.1968   \n",
            "10 -0.7546        0.8120      -0.0488        1.6575  ...      -4.9377   \n",
            "11 -0.2746        0.5872      -0.0416        3.2620  ...     -10.0713   \n",
            "12  0.7048        0.2810      -0.0492        1.7327  ...      -3.5489   \n",
            "13 -0.5315        0.7513      -0.0734        1.7704  ...      -5.0202   \n",
            "14  0.1937        0.4955      -0.0816        3.5029  ...      -8.5328   \n",
            "15  0.8457        0.4122      -0.0611        1.6117  ...      -3.0389   \n",
            "16 -0.6489        0.7840      -0.0543        1.6338  ...      -4.5946   \n",
            "17  0.2312        0.5525      -0.0633        3.2455  ...      -8.8807   \n",
            "18  0.6009        0.3597      -0.0562        1.7643  ...      -3.6548   \n",
            "19 -0.5696        0.7144      -0.0449        1.7809  ...      -5.3254   \n",
            "20  0.0912        0.3273      -0.0596        3.5450  ...      -8.9511   \n",
            "21  0.4345        0.3841      -0.0568        1.6192  ...      -3.3996   \n",
            "22 -0.9161        0.8362      -0.0431        1.6214  ...      -5.1792   \n",
            "23 -0.4811        0.6768      -0.0475        3.2407  ...      -9.9117   \n",
            "\n",
            "    tc20_max_drawdown  tc30_annual_return  tc30_annual_vol  tc30_sharpe  \\\n",
            "0             -0.9990             -1.2229           0.2047      -5.9750   \n",
            "1             -0.9999             -1.4867           0.2002      -7.4264   \n",
            "2             -1.0000             -2.7099           0.1907     -14.2111   \n",
            "3             -0.9983             -1.1138           0.2135      -5.2169   \n",
            "4             -0.9999             -1.4739           0.1982      -7.4364   \n",
            "5             -1.0000             -2.5880           0.1650     -15.6892   \n",
            "6             -0.9987             -1.1841           0.2083      -5.6839   \n",
            "7             -0.9999             -1.4855           0.1995      -7.4466   \n",
            "8             -1.0000             -2.6695           0.1926     -13.8595   \n",
            "9             -0.9982             -1.1082           0.2205      -5.0249   \n",
            "10            -0.9998             -1.4035           0.2000      -7.0184   \n",
            "11            -1.0000             -2.5117           0.1690     -14.8617   \n",
            "12            -0.9985             -1.1656           0.2061      -5.6568   \n",
            "13            -0.9999             -1.4437           0.1993      -7.2443   \n",
            "14            -1.0000             -2.6092           0.2035     -12.8219   \n",
            "15            -0.9967             -1.0419           0.2097      -4.9698   \n",
            "16            -0.9998             -1.3702           0.2090      -6.5563   \n",
            "17            -1.0000             -2.4122           0.1802     -13.3893   \n",
            "18            -0.9989             -1.2085           0.2096      -5.7666   \n",
            "19            -0.9999             -1.4537           0.1891      -7.6875   \n",
            "20            -1.0000             -2.6621           0.1986     -13.4025   \n",
            "21            -0.9985             -1.1318           0.2133      -5.3056   \n",
            "22            -0.9999             -1.4011           0.1920      -7.2988   \n",
            "23            -1.0000             -2.5330           0.1739     -14.5651   \n",
            "\n",
            "    tc30_max_drawdown  tc40_annual_return  tc40_annual_vol  tc40_sharpe  \\\n",
            "0             -1.0000             -1.6658           0.2054      -8.1119   \n",
            "1             -1.0000             -1.9346           0.2008      -9.6367   \n",
            "2             -1.0000             -3.6006           0.1922     -18.7339   \n",
            "3             -0.9999             -1.5127           0.2138      -7.0748   \n",
            "4             -1.0000             -1.9027           0.1985      -9.5831   \n",
            "5             -1.0000             -3.4158           0.1659     -20.5918   \n",
            "6             -1.0000             -1.6244           0.2090      -7.7717   \n",
            "7             -1.0000             -1.9319           0.2003      -9.6432   \n",
            "8             -1.0000             -3.5563           0.1942     -18.3112   \n",
            "9             -0.9999             -1.5125           0.2210      -6.8431   \n",
            "10            -1.0000             -1.8212           0.2004      -9.0867   \n",
            "11            -1.0000             -3.3337           0.1705     -19.5476   \n",
            "12            -1.0000             -1.6023           0.2069      -7.7439   \n",
            "13            -1.0000             -1.8899           0.2001      -9.4468   \n",
            "14            -1.0000             -3.4919           0.2051     -17.0284   \n",
            "15            -0.9999             -1.4481           0.2102      -6.8883   \n",
            "16            -1.0000             -1.7820           0.2095      -8.5062   \n",
            "17            -1.0000             -3.2300           0.1811     -17.8374   \n",
            "18            -1.0000             -1.6531           0.2103      -7.8604   \n",
            "19            -1.0000             -1.9025           0.1897     -10.0302   \n",
            "20            -1.0000             -3.5555           0.2000     -17.7754   \n",
            "21            -1.0000             -1.5399           0.2139      -7.1999   \n",
            "22            -1.0000             -1.8097           0.1924      -9.4041   \n",
            "23            -1.0000             -3.3497           0.1750     -19.1438   \n",
            "\n",
            "    tc40_max_drawdown  \n",
            "0                -1.0  \n",
            "1                -1.0  \n",
            "2                -1.0  \n",
            "3                -1.0  \n",
            "4                -1.0  \n",
            "5                -1.0  \n",
            "6                -1.0  \n",
            "7                -1.0  \n",
            "8                -1.0  \n",
            "9                -1.0  \n",
            "10               -1.0  \n",
            "11               -1.0  \n",
            "12               -1.0  \n",
            "13               -1.0  \n",
            "14               -1.0  \n",
            "15               -1.0  \n",
            "16               -1.0  \n",
            "17               -1.0  \n",
            "18               -1.0  \n",
            "19               -1.0  \n",
            "20               -1.0  \n",
            "21               -1.0  \n",
            "22               -1.0  \n",
            "23               -1.0  \n",
            "\n",
            "[24 rows x 32 columns]\n"
          ]
        }
      ],
      "source": [
        "# ========== 日频预测隔日调仓Portfolio模拟主函数 ==========\n",
        "\n",
        "def run_chronos_portfolio_backtest(start_year=2016, end_year=2024, window_sizes=None, model_names=None,\n",
        "                                           npz_path=\"/content/drive/MyDrive/ERP Data/all_window_datasets_unscaled.npz\"):\n",
        "    \"\"\"\n",
        "Portfolio模拟（日频预测隔日调仓）：\n",
        "    1. 使用Chronos T5 Large模型进行零样本预测\n",
        "    2. 日频预测→日频信号\n",
        "    3. 日频组合构建（T+1建仓，严格permno对齐）\n",
        "    4. 分离summary指标和时间序列数据\n",
        "    \"\"\"\n",
        "    if window_sizes is None:\n",
        "        window_sizes = [5, 21, 252, 512]\n",
        "    if model_names is None:\n",
        "        model_names = [\"chronos large\"]\n",
        "\n",
        "    print(\"Starting Daily Rebalance Portfolio Backtesting Simulation\")\n",
        "\n",
        "    # 初始化回测器\n",
        "    backtester = PortfolioBacktester()\n",
        "\n",
        "    # 加载数据\n",
        "    datasets = load_datasets(npz_path)\n",
        "\n",
        "    # 存储结果\n",
        "    summary_results = []  # 仅存储summary指标\n",
        "    daily_series_data = []  # 存储日度时间序列\n",
        "    pred_rows = []          # 存所有预测\n",
        "\n",
        "    # 权重方案\n",
        "    WEIGHT_SCHEMES = [\"VW\", \"EW\"]\n",
        "\n",
        "    for window in window_sizes:\n",
        "        print(f\"Processing window size: {window}\")\n",
        "\n",
        "        # ==== 加载对应窗口的LoRA适配器用于推理 ====\n",
        "        try:\n",
        "            from transformers import AutoModelForSeq2SeqLM\n",
        "            adapter_dir = f\"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/models/chronos_t5_large_lora_w{window}/chronos_t5_lora_adapter\"\n",
        "            base_infer = AutoModelForSeq2SeqLM.from_pretrained(\"amazon/chronos-t5-large\")\n",
        "            peft_infer = PeftModel.from_pretrained(base_infer, adapter_dir).to(device).eval()\n",
        "            try:\n",
        "                peft_infer.config.use_cache = True\n",
        "            except Exception:\n",
        "                pass\n",
        "            pipeline.model.model = peft_infer\n",
        "            print(f\"[INFO] Loaded LoRA adapter for window={window} from: {adapter_dir}\")\n",
        "            print(f\"[INFO] Backtest inference model: LoRA (window={window})\")\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Could not load LoRA adapter for window={window}: {e}. Using base model.\")\n",
        "            print(f\"[INFO] Backtest inference model: BASE (window={window})\")\n",
        "\n",
        "        # 加载测试数据\n",
        "        X_test = datasets[f\"X_test_{window}\"]\n",
        "        y_test = datasets[f\"y_test_{window}\"]\n",
        "        meta_test_dict = datasets[f\"meta_test_{window}\"].item()\n",
        "        meta_test = pd.DataFrame.from_dict(meta_test_dict)\n",
        "\n",
        "        # 提取所需信息\n",
        "        permnos_test = meta_test[\"PERMNO\"].values\n",
        "        meta_test[\"signal_date\"]  = pd.to_datetime(meta_test[\"date\"])\n",
        "        meta_test[\"ret_date\"]     = pd.to_datetime(meta_test[\"ret_date\"])\n",
        "        market_caps = meta_test.get(\"MKTCAP\", np.ones(len(permnos_test)))\n",
        "\n",
        "        # 确保日期列转换为datetime类型\n",
        "        meta_test['date'] = pd.to_datetime(meta_test[\"date\"])\n",
        "        dates_test = meta_test['signal_date']\n",
        "\n",
        "        for model_name in model_names:\n",
        "            for scheme in WEIGHT_SCHEMES:\n",
        "                all_y_true   = []\n",
        "                all_y_pred   = []\n",
        "                all_permnos  = []\n",
        "                all_meta     = []\n",
        "                print(f\"  Model: {model_name}, Scheme: {scheme}\")\n",
        "\n",
        "                            # 为每个组合类型单独存储数据\n",
        "                portfolio_daily_data = {\n",
        "                'long_only': {'returns': [], 'turnovers': [], 'dates': []},\n",
        "                'short_only': {'returns': [], 'turnovers': [], 'dates': []},\n",
        "                'long_short': {'returns': [], 'turnovers': [], 'dates': []}\n",
        "            }\n",
        "\n",
        "                # 存储上一期权重（为每个组合单独维护）\n",
        "                prev_portfolio_data = {'long_only': None, 'short_only': None, 'long_short': None}\n",
        "\n",
        "                # === 日频信号缓冲 ===\n",
        "                signals_buf = {}   # {datetime: DataFrame}\n",
        "\n",
        "                # 按年份循环进行Chronos预测\n",
        "                for year in range(start_year, min(end_year + 1, 2025)):\n",
        "                    print(f\"  Processing year: {year}\")\n",
        "\n",
        "                    # 筛选当年数据\n",
        "                    year_mask = (dates_test.dt.year == year)\n",
        "                    if not np.any(year_mask):\n",
        "                        continue\n",
        "\n",
        "                    X_year = X_test[year_mask]\n",
        "                    y_year = y_test[year_mask]\n",
        "                    permnos_year = permnos_test[year_mask]\n",
        "                    market_caps_year = market_caps[year_mask]\n",
        "                    dates_year = dates_test[year_mask]\n",
        "                    ret_dates_year = meta_test.loc[year_mask, 'ret_date'].values\n",
        "\n",
        "                    # 使用Chronos进行预测\n",
        "                    batch_size = get_batch_size(window)\n",
        "                    predictions_year = chronos_rolling_prediction(\n",
        "                        pipeline=pipeline,\n",
        "                        X_data=X_year,\n",
        "                        batch_size=batch_size,\n",
        "                        prediction_length=1\n",
        "                    )\n",
        "\n",
        "                    # 按日期分组处理（日频预测）\n",
        "                    df_quarter = pd.DataFrame({\n",
        "                        'signal_date': dates_year,\n",
        "                        'ret_date': ret_dates_year,\n",
        "                        'permno': permnos_year,\n",
        "                        'market_cap': market_caps_year,\n",
        "                        'actual_return': y_year,\n",
        "                        'prediction': predictions_year\n",
        "                    })\n",
        "\n",
        "                    # ====== 新增：只在第一个scheme里收集（不重复） ======\n",
        "                    if scheme == 'VW':\n",
        "                        df_q_save = df_quarter[['signal_date','ret_date','permno',\n",
        "                                                'actual_return','prediction','market_cap']].copy()\n",
        "                        df_q_save.rename(columns={'actual_return':'y_true',\n",
        "                                                  'prediction':'y_pred'}, inplace=True)\n",
        "                        df_q_save['model']  = model_name\n",
        "                        df_q_save['window'] = window\n",
        "                        pred_rows.append(df_q_save)\n",
        "                    # =====================================================\n",
        "\n",
        "                    # —— 用于方式1整体区间计算\n",
        "                    all_y_true.append(df_quarter['actual_return'].values)\n",
        "                    all_y_pred.append(df_quarter['prediction'].values)\n",
        "                    all_permnos.append(df_quarter['permno'].values)\n",
        "                    all_meta.append(meta_test.loc[year_mask, :])\n",
        "\n",
        "\n",
        "                    # 按日期循环（T+1建仓逻辑）\n",
        "                    for signal_date, sig_grp in df_quarter.groupby('signal_date'):\n",
        "\n",
        "                        # (1) 本日信号：只算出来，先放缓存，不建仓\n",
        "                        daily_signals = (\n",
        "                            sig_grp.groupby('permno')['prediction'].mean()        # 本日信号\n",
        "                                  .to_frame('prediction')\n",
        "                                  .join(sig_grp.groupby('permno')['market_cap'].mean())\n",
        "                        )\n",
        "                        signals_buf[signal_date] = daily_signals\n",
        "\n",
        "                        # (2) 必须等到 **下一日** 才用\"昨日信号\"建仓\n",
        "                        prev_date = signal_date - pd.tseries.offsets.BDay(1)\n",
        "                        if prev_date not in signals_buf:\n",
        "                            continue                             # 第一日无昨日信号 → 不交易\n",
        "\n",
        "                        sigs = signals_buf.pop(prev_date)       # ← 取出昨日的信号 DataFrame\n",
        "\n",
        "                        # (3) 用今日真实收益结算（ret_date == signal_date）\n",
        "                        ret_grp = df_quarter[df_quarter['ret_date'] == signal_date]\n",
        "                        if len(ret_grp) == 0:\n",
        "                            continue\n",
        "\n",
        "                        daily_actual_returns = (\n",
        "                            ret_grp.groupby('permno')['actual_return']\n",
        "                                   .mean()                                      # 日度收益直接取均值\n",
        "                                   .reindex(sigs.index, fill_value=0)          # 与信号股票全集对齐\n",
        "                                   .values\n",
        "                        )\n",
        "                        daily_permnos = sigs.index.values\n",
        "\n",
        "\n",
        "                        # (4) 生成 3 组权重\n",
        "                        portfolios_data = backtester.create_portfolios_with_permno_tracking(\n",
        "                            signals      = sigs['prediction'].values,\n",
        "                            market_caps  = sigs['market_cap'].values,\n",
        "                            permnos      = daily_permnos,\n",
        "                            weight_scheme= scheme\n",
        "                        )\n",
        "\n",
        "                        # 为每个组合类型单独处理\n",
        "                        for portfolio_type in ['long_only', 'short_only', 'long_short']:\n",
        "                            portfolio_info = portfolios_data[portfolio_type]\n",
        "\n",
        "                            # 计算严格对齐的组合收益\n",
        "                            portfolio_return, aligned_returns = backtester.calculate_aligned_portfolio_return(\n",
        "                                portfolio_weights=portfolio_info['weights'],\n",
        "                                portfolio_permnos=portfolio_info['permnos'],\n",
        "                                actual_returns=daily_actual_returns,\n",
        "                                actual_permnos=daily_permnos\n",
        "                            )\n",
        "\n",
        "                            if prev_portfolio_data[portfolio_type] is not None:\n",
        "                                # 1) 把昨日 & 今日权重都变成 Series，index=PERMNO\n",
        "                                prev_w_ser = pd.Series(\n",
        "                                    prev_portfolio_data[portfolio_type]['weights'],\n",
        "                                    index=prev_portfolio_data[portfolio_type]['permnos']\n",
        "                                )\n",
        "                                cur_w_ser = pd.Series(\n",
        "                                    portfolio_info['weights'],\n",
        "                                    index=portfolio_info['permnos']\n",
        "                                )\n",
        "\n",
        "                                # 2) 昨日真实收益向量也先做成 Series\n",
        "                                prev_r_ser = pd.Series(\n",
        "                                    prev_portfolio_data[portfolio_type]['aligned_returns'],\n",
        "                                    index=prev_portfolio_data[portfolio_type]['permnos']\n",
        "                                )\n",
        "\n",
        "                                # 3) 用 .reindex() 把\"昨日权重/收益\"补到与今日股票全集一致，缺位=0\n",
        "                                aligned_prev_w = prev_w_ser.reindex(cur_w_ser.index, fill_value=0).values\n",
        "                                aligned_prev_r = prev_r_ser.reindex(cur_w_ser.index, fill_value=0).values\n",
        "\n",
        "                                # 4) 今日目标权重 & 今日真实收益已天然对应 cur_w_ser.index\n",
        "                                aligned_cur_w = cur_w_ser.values\n",
        "\n",
        "                                # 5) 现在三条向量维度完全一致，可以安全算换手率\n",
        "                                turnover = backtester.calc_turnover(\n",
        "                                    w_t  = aligned_prev_w,\n",
        "                                    r_t  = aligned_prev_r,\n",
        "                                    w_tp1= aligned_cur_w\n",
        "                                )\n",
        "                            else:\n",
        "                                # 首日 / 上一期空仓 —— 全仓买入\n",
        "                                turnover = np.sum(np.abs(portfolio_info['weights']))\n",
        "\n",
        "\n",
        "                            # 存储数据\n",
        "                            portfolio_daily_data[portfolio_type]['returns'].append(portfolio_return)\n",
        "                            portfolio_daily_data[portfolio_type]['turnovers'].append(turnover)\n",
        "                            portfolio_daily_data[portfolio_type]['dates'].append(signal_date)\n",
        "\n",
        "                            # 更新上一期组合数据\n",
        "                            prev_portfolio_data[portfolio_type] = {\n",
        "                                'weights'        : portfolio_info['weights'],\n",
        "                                'permnos'        : portfolio_info['permnos'],\n",
        "                                'aligned_returns': aligned_returns\n",
        "                            }\n",
        "\n",
        "                # 计算最终指标并存储结果\n",
        "                for portfolio_type in ['long_only', 'short_only', 'long_short']:\n",
        "                    portfolio_data = portfolio_daily_data[portfolio_type]\n",
        "\n",
        "                    if len(portfolio_data['returns']) > 0:\n",
        "                        # 计算summary指标（日度年化）\n",
        "                        metrics = backtester.calculate_metrics(\n",
        "                            returns=portfolio_data['returns'],\n",
        "                            turnover_series=portfolio_data['turnovers']\n",
        "                        )\n",
        "\n",
        "                        # ---- 5/10/20/30/40 bps 逐档计算 ----\n",
        "                        rets = np.array(portfolio_data['returns'])\n",
        "                        tovs = np.array(portfolio_data['turnovers'])\n",
        "\n",
        "                        for tc in TC_GRID:\n",
        "                            tag = TC_TAG[tc]\n",
        "                            adj = rets - tovs * tc\n",
        "\n",
        "                            ann_ret = adj.mean() * 252\n",
        "                            ann_vol = adj.std(ddof=1) * np.sqrt(252)\n",
        "                            sharpe  = ann_ret / ann_vol if ann_vol > 0 else 0\n",
        "\n",
        "                            cum_adj = np.cumprod(1 + adj)\n",
        "                            mdd = ((cum_adj - np.maximum.accumulate(cum_adj)) /\n",
        "                                   np.maximum.accumulate(cum_adj)).min()\n",
        "\n",
        "                            metrics[f'{tag}_annual_return'] = ann_ret\n",
        "                            metrics[f'{tag}_annual_vol']    = ann_vol\n",
        "                            metrics[f'{tag}_sharpe']        = sharpe\n",
        "                            metrics[f'{tag}_max_drawdown']  = mdd\n",
        "\n",
        "                        # 存储summary结果 - 修复问题2：使用小写统一列名\n",
        "                        summary_results.append({\n",
        "                            'scheme': scheme,\n",
        "                            'model': model_name,\n",
        "                            'window': window,\n",
        "                            'portfolio_type': portfolio_type,\n",
        "                            **metrics\n",
        "                        })\n",
        "\n",
        "                        # ====== 先一次性算好序列 ======\n",
        "                        rets_arr = np.array(portfolio_data['returns'])\n",
        "                        tovs_arr = np.array(portfolio_data['turnovers'])\n",
        "                        cum_no_tc = np.log1p(rets_arr).cumsum()\n",
        "\n",
        "                        tc_ret_dict = {}\n",
        "                        tc_cum_dict = {}\n",
        "                        for tc in TC_GRID:\n",
        "                            tag = TC_TAG[tc]\n",
        "                            r = rets_arr - tovs_arr * tc\n",
        "                            tc_ret_dict[tag] = r\n",
        "                            tc_cum_dict[tag] = np.log1p(r).cumsum()\n",
        "\n",
        "                        # ====== 再逐日写入 ======\n",
        "                        for i, date in enumerate(portfolio_data['dates']):\n",
        "                            row = {\n",
        "                                'scheme'        : scheme,\n",
        "                                'model'         : model_name,\n",
        "                                'window'        : window,\n",
        "                                'portfolio_type': portfolio_type,\n",
        "                                'date'          : str(date),\n",
        "                                'return'        : rets_arr[i],\n",
        "                                'turnover'      : tovs_arr[i],\n",
        "                                'cumulative'    : cum_no_tc[i],\n",
        "                            }\n",
        "                            for tag in TC_TAG.values():\n",
        "                                row[f'{tag}_return']     = tc_ret_dict[tag][i]\n",
        "                                row[f'{tag}_cumulative'] = tc_cum_dict[tag][i]\n",
        "\n",
        "                            daily_series_data.append(row)\n",
        "\n",
        "                # 在完成当前scheme的所有portfolio_type处理后计算整体回归指标\n",
        "                if scheme == \"VW\" and len(all_y_true) > 0:\n",
        "                    y_all    = np.concatenate(all_y_true)\n",
        "                    yhat_all = np.concatenate(all_y_pred)\n",
        "                    perm_all = np.concatenate(all_permnos)\n",
        "                    meta_all = pd.concat(all_meta, ignore_index=True)\n",
        "\n",
        "                    k = X_test.shape[1]  # 自变量个数（同一 window 下固定）\n",
        "\n",
        "                    m1_metrics = overall_interval_metrics_method1(\n",
        "                        y_all, yhat_all, k,\n",
        "                        permnos_all=perm_all,\n",
        "                        meta_all=meta_all\n",
        "                    )\n",
        "\n",
        "                    # ==== 计算横截面 RankIC ====\n",
        "                    full_pred_df = pd.concat(pred_rows, ignore_index=True)\n",
        "\n",
        "                    # 关键：保证日期是 datetime\n",
        "                    full_pred_df['signal_date'] = pd.to_datetime(full_pred_df['signal_date'], errors='coerce')\n",
        "\n",
        "                    # 只用“当前 window + 当前 model”的样本\n",
        "                    cur = full_pred_df.loc[\n",
        "                        (full_pred_df['window'] == window) &\n",
        "                        (full_pred_df['model'] == model_name),\n",
        "                        ['signal_date', 'y_true', 'y_pred']\n",
        "                    ].dropna()\n",
        "\n",
        "                    # 可选：样本太少时防守\n",
        "                    if len(cur) >= 30:\n",
        "                        mean_ic, t_ic, pos_ic, _ = calc_ic_daily(cur, method='spearman')\n",
        "                    else:\n",
        "                        mean_ic, t_ic, pos_ic = np.nan, np.nan, np.nan\n",
        "\n",
        "                    m1_metrics['RankIC_mean']  = mean_ic\n",
        "                    m1_metrics['RankIC_t']     = t_ic\n",
        "                    m1_metrics['RankIC_pos%']  = pos_ic\n",
        "\n",
        "                    save_metrics(m1_metrics, name=model_name, window=window,\n",
        "                        path=\"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_metrics.csv\")\n",
        "\n",
        "    # 创建DataFrame并确保所有TC指标都存在\n",
        "    summary_df = pd.DataFrame(summary_results)\n",
        "    daily_df = pd.DataFrame(daily_series_data) if daily_series_data else pd.DataFrame()\n",
        "\n",
        "    # 填充可能的NaN值\n",
        "    tc_columns = [c for c in summary_df.columns if c.startswith('tc')]\n",
        "    summary_df[tc_columns] = summary_df[tc_columns].fillna(0.0)\n",
        "\n",
        "    # 按 scheme 拆分保存到四个文件\n",
        "    def save_split_by_scheme(df, base_filename):\n",
        "        \"\"\"按scheme拆分保存文件的辅助函数\"\"\"\n",
        "        if df.empty:\n",
        "            print(f\"Warning: DataFrame is empty, skipping save for {base_filename}\")\n",
        "            return None, None\n",
        "\n",
        "        vw_df = df[df['scheme'] == 'VW']\n",
        "        ew_df = df[df['scheme'] == 'EW']\n",
        "\n",
        "        out_dir = \"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results\"\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "        vw_filename = os.path.join(out_dir, f\"{base_filename}_VW.csv\")\n",
        "        ew_filename = os.path.join(out_dir, f\"{base_filename}_EW.csv\")\n",
        "\n",
        "        vw_df.to_csv(vw_filename, index=False)\n",
        "        ew_df.to_csv(ew_filename, index=False)\n",
        "\n",
        "        print(f\"VW results saved to {vw_filename}\")\n",
        "        print(f\"EW results saved to {ew_filename}\")\n",
        "\n",
        "        return vw_filename, ew_filename\n",
        "\n",
        "    # 保存拆分后的文件\n",
        "    save_split_by_scheme(summary_df, \"portfolio_results_daily_rebalance\")\n",
        "\n",
        "    if not daily_df.empty:\n",
        "        save_split_by_scheme(daily_df, \"portfolio_daily_series\")\n",
        "\n",
        "    # ===== 保存预测表 =====\n",
        "    if pred_rows:\n",
        "        out_dir = \"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results\"\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "        pred_df = pd.concat(pred_rows, ignore_index=True)\n",
        "        pred_df.to_csv(os.path.join(out_dir, \"predictions_daily.csv\"), index=False)\n",
        "        print(f\"Saved {len(pred_df)} prediction rows to predictions_daily.csv\")\n",
        "\n",
        "    print(f\"Generated {len(summary_results)} portfolio summary records\")\n",
        "    print(f\"Generated {len(daily_series_data)} daily series records\")\n",
        "\n",
        "    # 确保返回正确的DataFrame格式\n",
        "    return summary_df, daily_df, backtester\n",
        "# ========== 运行回测 ==========\n",
        "\n",
        "print(\"Starting Chronos T5-Large Portfolio Backtesting...\")\n",
        "\n",
        "# 本地运行设置（若上方未定义）\n",
        "START_YEAR = globals().get(\"START_YEAR\", 2016)\n",
        "END_YEAR   = globals().get(\"END_YEAR\", 2024)\n",
        "WINDOW_SIZES = globals().get(\"WINDOW_SIZES\", [5, 21, 252, 512])\n",
        "\n",
        "# 运行回测，注意END_YEAR应该是2024\n",
        "summary_results, daily_series, backtester = run_chronos_portfolio_backtest(\n",
        "    start_year=START_YEAR,\n",
        "    end_year=END_YEAR,\n",
        "    window_sizes=WINDOW_SIZES,\n",
        "    npz_path=\"/content/drive/MyDrive/ERP Data/all_window_datasets_unscaled.npz\"\n",
        ")\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*60)\n",
        "print(\"CHRONOS T5-BASE PORTFOLIO BACKTESTING RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 显示汇总结果\n",
        "print(\"\\\\nSummary Results:\")\n",
        "try:\n",
        "    print(summary_results.round(4))\n",
        "except Exception:\n",
        "    print(summary_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c21353c7",
      "metadata": {
        "id": "c21353c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "777eea51-9d54-41ec-ce19-7a614a54671d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Saved] 5_factor_analysis_VW_gross.csv \n",
            "[Saved] 5_factor_analysis_VW_net.csv \n",
            "[Saved] 5_factor_analysis_EW_gross.csv \n",
            "[Saved] 5_factor_analysis_EW_net.csv \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-688187439.py:163: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  for _, group in df.groupby([\"scheme\", \"model\", \"window\", \"portfolio_type\"], sort=False):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finish: /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_daily_series_VW_with_rf.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-688187439.py:163: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  for _, group in df.groupby([\"scheme\", \"model\", \"window\", \"portfolio_type\"], sort=False):\n",
            "/tmp/ipython-input-688187439.py:183: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  sp500 = yf.download(\"^GSPC\", start=\"2016-01-01\", end=\"2024-12-31\")\n",
            "[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finish: /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_daily_series_EW_with_rf.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All figures have been generated and saved to: /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_figures/\n",
            "[Update] ΔSharpe has been written to /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_results_daily_rebalance_VW.csv\n",
            "[Update] ΔSharpe has been written to /content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_results_daily_rebalance_EW.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "  # ---------- 5因子回归主函数 -----------\n",
        "def run_factor_regression(port_ret, factors, use_excess=True):\n",
        "    df = pd.concat([port_ret, factors], axis=1, join='inner').dropna()\n",
        "    df.columns = ['ret'] + list(factors.columns)\n",
        "\n",
        "    if use_excess:\n",
        "        y = df['ret'].values\n",
        "    else:\n",
        "        y = df['ret'].values - df['rf'].values\n",
        "\n",
        "    X = df[['mktrf','smb','hml','rmw','cma','umd']].values\n",
        "    X = sm.add_constant(X)\n",
        "\n",
        "    model = sm.OLS(y, X)\n",
        "    res = model.fit()\n",
        "    alpha = res.params[0]          # 日度 α\n",
        "    resid_std = res.resid.std(ddof=1)\n",
        "\n",
        "    ir_daily = alpha / resid_std          # 日度 IR\n",
        "    ir_annual = ir_daily * np.sqrt(252)   # 年化 IR\n",
        "\n",
        "\n",
        "    y_hat = np.asarray(res.fittedvalues)  # 改这里\n",
        "\n",
        "    out = {\n",
        "        'N_obs'            : len(y),\n",
        "        'alpha_daily'      : alpha,\n",
        "        'alpha_annual'     : alpha*252,\n",
        "        't_alpha'          : res.tvalues[0],\n",
        "        'IR_daily'         : ir_daily,\n",
        "        'IR_annual'        : ir_annual,\n",
        "        'R2_zero'          : r2_zero(y, y_hat),\n",
        "    }\n",
        "\n",
        "    factor_names = ['MKT','SMB','HML','RMW','CMA','UMD']\n",
        "    for i, fac in enumerate(factor_names, start=1):\n",
        "        out[f'beta_{fac}'] = res.params[i]\n",
        "        out[f't_{fac}']    = res.tvalues[i]\n",
        "\n",
        "    return out\n",
        "\n",
        "# ---------- 3. 批量跑（EW/VW、三种组合） ----------\n",
        "def batch_factor_analysis(\n",
        "    daily_df: pd.DataFrame,\n",
        "    factors_path: str,\n",
        "    scheme: str,\n",
        "    tc_levels=(0, 5, 10, 20, 40),\n",
        "    portfolio_types=('long_only','short_only','long_short'),\n",
        "    model_filter=None,\n",
        "    window_filter=None,\n",
        "    gross_only=False,            # ← True 时只算 tc=0\n",
        "    out_dir='/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/factor_IR_results',\n",
        "):\n",
        "    \"\"\"\n",
        "    生成一张含 IR 的 CSV。\n",
        "    gross_only=True  → 仅 tc=0；False → tc_levels 全部。\n",
        "    \"\"\"\n",
        "    import os\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    # 读取因子\n",
        "    fac = (pd.read_csv(factors_path, parse_dates=['date'])\n",
        "             .set_index('date')\n",
        "             .sort_index())\n",
        "\n",
        "    # 过滤组合表\n",
        "    sub = daily_df[daily_df['scheme'] == scheme].copy()\n",
        "    if model_filter is not None:\n",
        "        sub = sub[sub['model'].isin(model_filter)]\n",
        "    if window_filter is not None:\n",
        "        sub = sub[sub['window'].isin(window_filter)]\n",
        "\n",
        "    tc_iter = (0,) if gross_only else tc_levels\n",
        "    results = []\n",
        "\n",
        "    for (model, win, ptype), g in sub.groupby(['model','window','portfolio_type']):\n",
        "        g = g.sort_values('date').set_index(pd.to_datetime(g['date']))\n",
        "\n",
        "        for tc in tc_iter:\n",
        "            col = 'return' if tc == 0 else f'tc{tc}_return'\n",
        "            if col not in g.columns:\n",
        "                continue  # 防御\n",
        "            port_ret = g[col]\n",
        "            stats = run_factor_regression(port_ret, fac, use_excess=True)\n",
        "            stats.update({\n",
        "                'scheme'        : scheme,\n",
        "                'model'         : model,\n",
        "                'window'        : win,\n",
        "                'portfolio_type': ptype,\n",
        "                'tc_bps'        : tc,\n",
        "            })\n",
        "            results.append(stats)\n",
        "\n",
        "    df_out = pd.DataFrame(results)[[\n",
        "        'scheme','model','window','portfolio_type','tc_bps','N_obs',\n",
        "        'alpha_daily','alpha_annual','t_alpha',\n",
        "        'IR_daily','IR_annual','R2_zero',\n",
        "        'beta_MKT','t_MKT','beta_SMB','t_SMB',\n",
        "        'beta_HML','t_HML','beta_RMW','t_RMW',\n",
        "        'beta_CMA','t_CMA','beta_UMD','t_UMD'\n",
        "    ]]\n",
        "\n",
        "    tag = 'gross' if gross_only else 'net'\n",
        "    fname = f'5_factor_analysis_{scheme}_{tag}.csv'\n",
        "    df_out.to_csv(os.path.join(out_dir, fname), index=False)\n",
        "    print(f'[Saved] {fname} ')\n",
        "    return df_out\n",
        "\n",
        "\n",
        "\n",
        "def run_all_factor_tests(vw_csv=\"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_daily_series_VW.csv\",\n",
        "                         ew_csv=\"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_daily_series_EW.csv\",\n",
        "                         factor_csv=\"/content/drive/MyDrive/ERP Data/5_Factors_Plus_Momentum.csv\",\n",
        "                         save_dir=\"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results\",\n",
        "                         y_is_excess=True,\n",
        "                         hac_lags=5,\n",
        "                         save_txt=True):\n",
        "    vw_df = pd.read_csv(vw_csv)\n",
        "    ew_df = pd.read_csv(ew_csv)\n",
        "\n",
        "    # ----- VW -----\n",
        "    vw_gross = batch_factor_analysis(\n",
        "        vw_df, factor_csv, scheme='VW', gross_only=True)    # 只 tc=0\n",
        "    vw_net   = batch_factor_analysis(\n",
        "        vw_df, factor_csv, scheme='VW', gross_only=False)   # 含 0/5/10/20/40\n",
        "\n",
        "    # ----- EW -----\n",
        "    ew_gross = batch_factor_analysis(\n",
        "        ew_df, factor_csv, scheme='EW', gross_only=True)\n",
        "    ew_net   = batch_factor_analysis(\n",
        "        ew_df, factor_csv, scheme='EW', gross_only=False)\n",
        "\n",
        "    # 想要什么就 return 什么；这里示例全部返回\n",
        "    return vw_gross, vw_net, ew_gross, ew_net\n",
        "\n",
        "\n",
        "vw_gross, vw_net, ew_gross, ew_net = run_all_factor_tests()# === 文件路径 ===\n",
        "rf_file = \"/content/drive/MyDrive/ERP Data/CRSP_2016_2024_top50_with_exret.csv\"\n",
        "vw_file = \"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_daily_series_VW.csv\"  # 本地路径\n",
        "ew_file = \"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_daily_series_EW.csv\"  # 本地路径\n",
        "\n",
        "# === 加载 rf (无风险利率) 数据 ===\n",
        "\n",
        "rf_df = pd.read_csv(rf_file, usecols=[\"date\", \"rf\"])\n",
        "rf_df[\"date\"] = pd.to_datetime(rf_df[\"date\"])\n",
        "rf_dict = dict(zip(rf_df[\"date\"], rf_df[\"rf\"]))  # 转成字典方便查找\n",
        "\n",
        "\n",
        "def adjust_returns_with_rf_grouped(file_path, output_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    # 使用 format='mixed' 或 dayfirst=True 来处理不同的日期格式\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"], format='mixed', dayfirst=True)\n",
        "\n",
        "    # 找出所有 return 列（包括 tc5_return, tc10_return 等）\n",
        "    return_cols = [col for col in df.columns if \"return\" in col and \"cumul\" not in col]\n",
        "\n",
        "    # 强制 portfolio_type 顺序，避免 groupby 自动排序错乱\n",
        "    order = [\"long_only\", \"short_only\", \"long_short\"]\n",
        "    df[\"portfolio_type\"] = pd.Categorical(df[\"portfolio_type\"], categories=order, ordered=True)\n",
        "\n",
        "    df_list = []\n",
        "    # 按策略/模型/窗口分组，分别加 rf 和重新算 cumulative\n",
        "    for _, group in df.groupby([\"scheme\", \"model\", \"window\", \"portfolio_type\"], sort=False):\n",
        "        group = group.sort_values(\"date\").copy()\n",
        "        for col in return_cols:\n",
        "            # 每天的 return 加回 rf\n",
        "            group[col] = group.apply(lambda row: row[col] + rf_dict.get(row[\"date\"], 0), axis=1)\n",
        "\n",
        "            # 找到对应的 cumulative 列（和原表一致命名）\n",
        "            cum_col = col.replace(\"return\", \"cumulative\")\n",
        "            group[cum_col] = np.log1p(group[col]).cumsum()\n",
        "        df_list.append(group)\n",
        "\n",
        "    # 合并结果并按顺序输出\n",
        "    df_new = pd.concat(df_list).sort_values([\"scheme\", \"model\", \"window\", \"portfolio_type\", \"date\"])\n",
        "    df_new.to_csv(output_path, index=False)\n",
        "    print(f\"Finish: {output_path}\")\n",
        "\n",
        "# === 分别处理 VW 和 EW 文件 ===\n",
        "adjust_returns_with_rf_grouped(vw_file, \"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_daily_series_VW_with_rf.csv\")\n",
        "adjust_returns_with_rf_grouped(ew_file, \"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_daily_series_EW_with_rf.csv\")\n",
        "# ========== 下载 S&P500 (2016-2024) ==========\n",
        "sp500 = yf.download(\"^GSPC\", start=\"2016-01-01\", end=\"2024-12-31\")\n",
        "price_col = \"Adj Close\" if \"Adj Close\" in sp500.columns else \"Close\"\n",
        "sp500[\"daily_return\"] = sp500[price_col].pct_change().fillna(0)\n",
        "# 论文公式：累计 log return\n",
        "sp500[\"cum_return\"] = np.cumsum(np.log1p(sp500[\"daily_return\"]))\n",
        "sp500 = sp500[[\"cum_return\"]]\n",
        "sp500.index = pd.to_datetime(sp500.index)\n",
        "\n",
        "# ========== 配置 ==========\n",
        "files = [\n",
        "    (\"VW\", \"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_daily_series_VW_with_rf.csv\"),\n",
        "    (\"EW\", \"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_daily_series_EW_with_rf.csv\")\n",
        "]\n",
        "tc_levels = [0, 5, 10, 20, 40]      # 交易成本 (bps)\n",
        "windows = [5, 21, 252, 512]         # 窗口\n",
        "strategies = [\"long_only\", \"short_only\", \"long_short\"]\n",
        "\n",
        "output_dir = \"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_figures\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# 灰色经济事件区间\n",
        "crisis_periods = [\n",
        "    (datetime(2018, 6, 1), datetime(2019, 1, 1), \"US-China Trade War\"),\n",
        "    (datetime(2020, 2, 1), datetime(2020, 7, 1), \"COVID-19\"),\n",
        "    (datetime(2022, 2, 1), datetime(2022, 6, 1), \"Russia-Ukraine War\"),\n",
        "    (datetime(2023, 1, 1), datetime(2023, 4, 1), \"US Bank Crisis\"),\n",
        "]\n",
        "\n",
        "def plot_comparison_styled(df, scheme, tc, window):\n",
        "    plt.figure(figsize=(15, 12))\n",
        "    model_names = df[\"model\"].unique()\n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, len(model_names)))\n",
        "\n",
        "    offset_step = 0.02\n",
        "\n",
        "    for i, strat in enumerate(strategies, 1):\n",
        "        ax = plt.subplot(3, 1, i)\n",
        "\n",
        "        # baseline ----------------------------------------------------------------\n",
        "        plt.plot(sp500.index, sp500[\"cum_return\"],\n",
        "                 color=\"black\", lw=2.5, label=\"S&P500 (Total Return)\", zorder=10)\n",
        "\n",
        "        # -------------------------------------------------------------------------\n",
        "        for idx, model_name in enumerate(model_names):\n",
        "            sub = df[(df[\"window\"] == window) &\n",
        "                     (df[\"portfolio_type\"] == strat) &\n",
        "                     (df[\"model\"] == model_name)].sort_values(\"date\")\n",
        "            if sub.empty:\n",
        "                continue\n",
        "\n",
        "            # --------- ① 关键：按 tc 取列名 -------------\n",
        "            if tc == 0:\n",
        "                ret_col = \"return\"          # 原始超额收益\n",
        "            else:\n",
        "                ret_col = f\"tc{tc}_return\"  # 含交易成本的列\n",
        "            # -------------------------------------------\n",
        "\n",
        "            if ret_col not in sub.columns:\n",
        "                # 防呆：列不存在就跳过\n",
        "                continue\n",
        "\n",
        "            # --------- ② 仍然用论文公式做累计 ----------\n",
        "            log_cum = np.cumsum(np.log1p(sub[ret_col].values))\n",
        "            # -------------------------------------------\n",
        "\n",
        "            y_shift = idx * offset_step\n",
        "            plt.plot(sub[\"date\"], log_cum + y_shift,\n",
        "                     label=f\"{model_name} ({strat.replace('_',' ').title()})\",\n",
        "                     lw=2, color=colors[idx], alpha=0.9)\n",
        "\n",
        "        # 灰色阴影、省略号 …… (保持和原来一样)\n",
        "        # ------------------------------------------------\n",
        "        for start, end, label in crisis_periods:\n",
        "            ax.axvspan(start, end, color='grey', alpha=0.3)\n",
        "            ax.text(start + pd.Timedelta(days=10),\n",
        "                    ax.get_ylim()[1]*0.92, label, fontsize=8, color='grey')\n",
        "        ax.xaxis.set_major_locator(mdates.YearLocator())\n",
        "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "        ax.set_ylabel(\"Cumulative log return (start = 0)\")\n",
        "        ax.set_title(f\"{scheme} | Window={window} | Strategy={strat} | TC={tc} bps\")\n",
        "        ax.grid(alpha=0.3)\n",
        "        plt.xticks(rotation=30)\n",
        "        plt.legend(bbox_to_anchor=(1.04, 1), loc='upper left', fontsize=8)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    fname = f\"{scheme}_window{window}_TC{tc}.png\"\n",
        "    plt.savefig(os.path.join(output_dir, fname), dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# ========== 主循环，生成所有图 ==========\n",
        "for scheme, file_path in files:\n",
        "    df = pd.read_csv(file_path)\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "    for tc in tc_levels:\n",
        "        for window in windows:\n",
        "            plot_comparison_styled(df, scheme, tc, window)\n",
        "\n",
        "print(f\"All figures have been generated and saved to: {output_dir}/\")\n",
        "\n",
        "# === 3. 加载 portfolio_metrics.csv 里的 R²_zero ===\n",
        "metrics_df = pd.read_csv(\"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_metrics.csv\")[[\"Model\", \"Window\", \"R²_zero\"]]\n",
        "metrics_df.rename(columns={\"Model\": \"model\", \"Window\": \"window\"}, inplace=True)\n",
        "\n",
        "# === 4. 处理 VW/EW 两个文件 ===\n",
        "for fname in [\"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_results_daily_rebalance_VW.csv\", \"/content/drive/MyDrive/chronos_t5_large_project_portfolio(FineTuning)/chronos_results/portfolio_results_daily_rebalance_EW.csv\"]:\n",
        "    df = pd.read_csv(fname)\n",
        "\n",
        "    # 按 model+window 合并 R²_zero\n",
        "    df = df.merge(metrics_df, on=[\"model\", \"window\"], how=\"left\")\n",
        "\n",
        "    # 循环计算 ΔSharpe 和 Sharpe*\n",
        "    rows = []\n",
        "    for _, row in df.iterrows():\n",
        "        r2 = float(row[\"R²_zero\"]) if not pd.isna(row[\"R²_zero\"]) else 0.0\n",
        "        if row[\"portfolio_type\"] == \"long_only\":\n",
        "            d_sr, sr_star = delta_sharpe(r2, SR_MKT_EX)  # 基准=SPX Excess\n",
        "            row[\"ΔSharpe\"]  = d_sr\n",
        "            row[\"Sharpe*\"]  = sr_star\n",
        "            row[\"baseline\"] = f\"SPX_excess ({SR_MKT_EX:.2f})\"\n",
        "        else:\n",
        "            d_sr, sr_star = delta_sharpe(r2, 0)  # 基准=现金\n",
        "            row[\"ΔSharpe\"]  = d_sr\n",
        "            row[\"Sharpe*\"]  = sr_star\n",
        "            row[\"baseline\"] = \"cash (0)\"\n",
        "        rows.append(row)\n",
        "\n",
        "    pd.DataFrame(rows).to_csv(fname, index=False)\n",
        "    print(f\"[Update] ΔSharpe has been written to {fname}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (Chronos)",
      "language": "python",
      "name": "chronos"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "77db9577636045b3af64ad20c4d4e708": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_64539b8b90a64dda85e330f13ccd5398",
              "IPY_MODEL_705f095a8bd64cb1ad054005d2242428",
              "IPY_MODEL_9f63936d8d6c4bd6be2b8fc026de3d62"
            ],
            "layout": "IPY_MODEL_92a2a444e4fe41e18c66933772dd8386"
          }
        },
        "64539b8b90a64dda85e330f13ccd5398": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_302731da7c72494897d640436c66ad8c",
            "placeholder": "​",
            "style": "IPY_MODEL_de51f545826f4d76a5e14418b6d2831a",
            "value": "config.json: "
          }
        },
        "705f095a8bd64cb1ad054005d2242428": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2bba446142c4bf489641957f85c2b5b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4903ec51bcbe493fb4a379e6cba0a3b6",
            "value": 1
          }
        },
        "9f63936d8d6c4bd6be2b8fc026de3d62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d90ffe4b8d1c411ba6e769207d753f35",
            "placeholder": "​",
            "style": "IPY_MODEL_659e95fd144f415683ad9f4eaf25b730",
            "value": " 1.12k/? [00:00&lt;00:00, 103kB/s]"
          }
        },
        "92a2a444e4fe41e18c66933772dd8386": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "302731da7c72494897d640436c66ad8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de51f545826f4d76a5e14418b6d2831a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2bba446142c4bf489641957f85c2b5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "4903ec51bcbe493fb4a379e6cba0a3b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d90ffe4b8d1c411ba6e769207d753f35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "659e95fd144f415683ad9f4eaf25b730": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "979e19bd39424ae38224ccb66faa9c26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8b86556d2e84408c8c8a0fa614dcc73a",
              "IPY_MODEL_2d8b2d281bec4e848c61c9e3fb2bb011",
              "IPY_MODEL_a810085f6e7e490c93d8b1ea15afc117"
            ],
            "layout": "IPY_MODEL_259df72b69744eba9ae9223ede1e7ddf"
          }
        },
        "8b86556d2e84408c8c8a0fa614dcc73a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26f72fa87d274f42a783ac8675394b16",
            "placeholder": "​",
            "style": "IPY_MODEL_9888f124f5fd490aa5e3ffe525fa06b7",
            "value": "model.safetensors: 100%"
          }
        },
        "2d8b2d281bec4e848c61c9e3fb2bb011": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8aac5f4987ae470fbb0739f50fa82d00",
            "max": 2835915592,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1ca64de2015b45499886841236e81da8",
            "value": 2835915592
          }
        },
        "a810085f6e7e490c93d8b1ea15afc117": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fbc070393744c63819eb1190b197442",
            "placeholder": "​",
            "style": "IPY_MODEL_1b1e8ad2e8914b91b36a563f11c7bfc8",
            "value": " 2.84G/2.84G [00:13&lt;00:00, 482MB/s]"
          }
        },
        "259df72b69744eba9ae9223ede1e7ddf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26f72fa87d274f42a783ac8675394b16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9888f124f5fd490aa5e3ffe525fa06b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8aac5f4987ae470fbb0739f50fa82d00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ca64de2015b45499886841236e81da8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5fbc070393744c63819eb1190b197442": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b1e8ad2e8914b91b36a563f11c7bfc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1a69df425184c67a3d3db5af21e9be7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8088dea0c3cb49e984aae7102ce1a5e5",
              "IPY_MODEL_42386fd45f874130b3970ad0cbbde08c",
              "IPY_MODEL_1175f52a956a45cb9f429c373d2c9c6f"
            ],
            "layout": "IPY_MODEL_4ec05d8c11ba42d6b432b6be5f7bea11"
          }
        },
        "8088dea0c3cb49e984aae7102ce1a5e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_337e949e516b411f93168d9d7390cb97",
            "placeholder": "​",
            "style": "IPY_MODEL_f1634d486b07408c93f6d037f3a7fa27",
            "value": "generation_config.json: 100%"
          }
        },
        "42386fd45f874130b3970ad0cbbde08c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_660edc6a9e1a459d873dc44ae1d33bf2",
            "max": 142,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c1043f7033704ad9b607c2fb9ba9b599",
            "value": 142
          }
        },
        "1175f52a956a45cb9f429c373d2c9c6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aec085822f50466e80e1fbac61f749a5",
            "placeholder": "​",
            "style": "IPY_MODEL_b70874d232d74693bf036281edb7c97f",
            "value": " 142/142 [00:00&lt;00:00, 12.0kB/s]"
          }
        },
        "4ec05d8c11ba42d6b432b6be5f7bea11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "337e949e516b411f93168d9d7390cb97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1634d486b07408c93f6d037f3a7fa27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "660edc6a9e1a459d873dc44ae1d33bf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1043f7033704ad9b607c2fb9ba9b599": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aec085822f50466e80e1fbac61f749a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b70874d232d74693bf036281edb7c97f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}