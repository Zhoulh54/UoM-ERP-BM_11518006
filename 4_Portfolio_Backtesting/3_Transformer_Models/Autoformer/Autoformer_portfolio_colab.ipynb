{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iO5Ub-EQfFBw",
        "outputId": "8b48f917-11ae-432b-949f-0b74a4715e75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing device: cuda\n",
            "GPU: Tesla T4\n",
            "CUDA Version: 12.4\n",
            "Available GPU memory: 14.7 GB\n",
            "CUDA cache cleared\n"
          ]
        }
      ],
      "source": [
        "# Google Colab environment setup\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install required packages\n",
        "%pip install optuna yfinance -q\n",
        "\n",
        "import copy\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "import gc\n",
        "import torch.cuda.amp as amp\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "import optuna\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from scipy.stats import f as f_dist\n",
        "import yfinance as yf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from typing import Optional\n",
        "\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Colab T4 GPU optimization\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"CUDA cache cleared\")\n",
        "\n",
        "def get_device_info():\n",
        "    \"\"\"Get device information\"\"\"\n",
        "    if device.type == 'cuda':\n",
        "        return f\"NVIDIA GPU: {torch.cuda.get_device_name()}\"\n",
        "    else:\n",
        "        return \"CPU\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSCrv1DkfFBx",
        "outputId": "8c8ba951-4371-4f4f-eeda-c361087add27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Load] Y scaler loaded for window 5: /content/drive/MyDrive/ERP Data/scaler_y_window_5.pkl\n",
            "[Load] Y scaler loaded for window 21: /content/drive/MyDrive/ERP Data/scaler_y_window_21.pkl\n",
            "[Load] Y scaler loaded for window 252: /content/drive/MyDrive/ERP Data/scaler_y_window_252.pkl\n",
            "[Load] Y scaler loaded for window 512: /content/drive/MyDrive/ERP Data/scaler_y_window_512.pkl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] S&P500 Excess Sharpe (2016–24) = 0.652\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Google Drive path settings\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/ERP Data\"\n",
        "DATA_PATH = f\"{DRIVE_PATH}\"\n",
        "MODELS_PATH = f\"{DRIVE_PATH}/models\"\n",
        "\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "os.makedirs(\"predictions\", exist_ok=True)\n",
        "\n",
        "def r2_zero(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute zero-based R² (baseline is 0)\n",
        "    y_true: true values (N,)\n",
        "    y_pred: predicted values (N,)\n",
        "    \"\"\"\n",
        "    rss = np.sum((y_true - y_pred)**2)\n",
        "    tss = np.sum(y_true**2)\n",
        "    return 1 - rss / tss\n",
        "\n",
        "import joblib\n",
        "\n",
        "def load_y_scaler(window_size, scaler_dir=DATA_PATH):\n",
        "    \"\"\"\n",
        "    Load the y scaler for a given window size.\n",
        "\n",
        "    Args:\n",
        "        window_size: window size (5, 21, 252, 512)\n",
        "        scaler_dir: directory path for scaler files\n",
        "\n",
        "    Returns:\n",
        "        scaler object or None\n",
        "    \"\"\"\n",
        "    scaler_path = f\"{scaler_dir}/scaler_y_window_{window_size}.pkl\"\n",
        "    try:\n",
        "        scaler = joblib.load(scaler_path)\n",
        "        print(f\"[Load] Y scaler loaded for window {window_size}: {scaler_path}\")\n",
        "        return scaler\n",
        "    except Exception as e:\n",
        "        print(f\"[Error] Failed to load Y scaler for window {window_size}: {e}\")\n",
        "        return None\n",
        "\n",
        "def inverse_transform_y(y_data, scaler):\n",
        "    \"\"\"\n",
        "    Inverse transform y data using the provided scaler.\n",
        "\n",
        "    Args:\n",
        "        y_data: standardized data (numpy array)\n",
        "        scaler: scaler object for inverse transformation\n",
        "\n",
        "    Returns:\n",
        "        Inverse transformed data\n",
        "    \"\"\"\n",
        "    if scaler is None:\n",
        "        print(\"[Warning] No scaler provided, returning original data\")\n",
        "        return y_data\n",
        "\n",
        "    try:\n",
        "        if y_data.ndim == 1:\n",
        "            y_2d = y_data.reshape(-1, 1)\n",
        "        else:\n",
        "            y_2d = y_data\n",
        "\n",
        "        inversed = scaler.inverse_transform(y_2d).flatten()\n",
        "        print(f\"[Info] Inverse transformed {len(y_data)} samples\")\n",
        "        return inversed\n",
        "    except Exception as e:\n",
        "        print(f\"[Error] Failed to inverse transform: {e}\")\n",
        "        return y_data\n",
        "\n",
        "Y_SCALERS = {}\n",
        "for window in [5, 21, 252, 512]:\n",
        "    Y_SCALERS[window] = load_y_scaler(window)\n",
        "\n",
        "def calc_ic_daily(df, method='spearman'):\n",
        "    \"\"\"\n",
        "    Calculate daily cross-sectional RankIC.\n",
        "    df: must contain ['signal_date','y_true','y_pred']\n",
        "    \"\"\"\n",
        "    ics = (df.groupby('signal_date')\n",
        "             .apply(lambda g: g['y_pred'].corr(g['y_true'], method=method))\n",
        "             .dropna())\n",
        "    mean_ic = ics.mean()\n",
        "    std_ic  = ics.std(ddof=1)\n",
        "    t_ic    = mean_ic / (std_ic / np.sqrt(len(ics))) if std_ic > 0 else np.nan\n",
        "    pos_ratio = (ics > 0).mean()\n",
        "    return mean_ic, t_ic, pos_ratio, ics\n",
        "\n",
        "def annual_sharpe(rets, freq=252):\n",
        "    mu = float(np.mean(rets)) * freq\n",
        "    sd = float(np.std(rets, ddof=1)) * np.sqrt(freq)\n",
        "    return mu / sd if sd > 0 else 0\n",
        "\n",
        "def delta_sharpe(r2_zero: float, sr_base: float):\n",
        "    \"\"\"\n",
        "    If r2_zero <= 0   → ΔSharpe = 0, Sharpe* = sr_base\n",
        "    If r2_zero >= 1   → ΔSharpe = 0, Sharpe* = sr_base\n",
        "    Otherwise, use the original formula\n",
        "    \"\"\"\n",
        "    if (r2_zero <= 0) or (r2_zero >= 1):\n",
        "        return 0.0, sr_base\n",
        "    sr_star = np.sqrt(sr_base ** 2 + r2_zero) / np.sqrt(1 - r2_zero)\n",
        "    return sr_star - sr_base, sr_star\n",
        "\n",
        "rf_file = f\"{DATA_PATH}/CRSP_2016_2024_top50_with_exret.csv\"\n",
        "rf_df = pd.read_csv(rf_file, usecols=[\"date\", \"rf\"])\n",
        "rf_df[\"date\"] = pd.to_datetime(rf_df[\"date\"])\n",
        "rf_df = rf_df.drop_duplicates(\"date\").set_index(\"date\").sort_index()\n",
        "rf_series = rf_df[\"rf\"].astype(float)\n",
        "\n",
        "px = yf.download(\"^GSPC\", start=\"2016-01-01\", end=\"2024-12-31\")[\"Close\"]\n",
        "sp_ret = px.pct_change().dropna()\n",
        "rf_align = rf_series.reindex(sp_ret.index).fillna(method=\"ffill\")\n",
        "sp_excess = sp_ret.values - rf_align.values\n",
        "\n",
        "SR_MKT_EX = annual_sharpe(sp_excess)\n",
        "print(f\"[INFO] S&P500 Excess Sharpe (2016–24) = {SR_MKT_EX:.3f}\")\n",
        "\n",
        "def calc_directional_metrics(y_true, y_pred, permnos=None):\n",
        "    \"\"\"\n",
        "    Improved version:\n",
        "    - Sample-level sign prediction\n",
        "    - If grouped by stock, calculate Overall, Up, Down for each stock and then average\n",
        "    \"\"\"\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "\n",
        "    if permnos is None:\n",
        "        s_true = np.sign(y_true)\n",
        "        s_pred = np.sign(y_pred)\n",
        "        mask = s_true != 0\n",
        "        s_true = s_true[mask]\n",
        "        s_pred = s_pred[mask]\n",
        "\n",
        "        overall_acc = np.mean(s_true == s_pred)\n",
        "\n",
        "        up_mask = s_true > 0\n",
        "        down_mask = s_true < 0\n",
        "        up_acc = np.mean(s_true[up_mask] == s_pred[up_mask]) if np.any(up_mask) else 0\n",
        "        down_acc = np.mean(s_true[down_mask] == s_pred[down_mask]) if np.any(down_mask) else 0\n",
        "\n",
        "    else:\n",
        "        df = pd.DataFrame({\"permno\": permnos, \"yt\": y_true, \"yp\": y_pred})\n",
        "        overall_accs = []\n",
        "        up_accs = []\n",
        "        down_accs = []\n",
        "\n",
        "        for _, g in df.groupby(\"permno\"):\n",
        "            s_true = np.sign(g[\"yt\"].values)\n",
        "            s_pred = np.sign(g[\"yp\"].values)\n",
        "            mask = s_true != 0\n",
        "            s_true = s_true[mask]\n",
        "            s_pred = s_pred[mask]\n",
        "            if len(s_true) == 0:\n",
        "                continue\n",
        "            overall_accs.append(np.mean(s_true == s_pred))\n",
        "\n",
        "            up_mask = s_true > 0\n",
        "            down_mask = s_true < 0\n",
        "            up_accs.append(np.mean(s_true[up_mask] == s_pred[up_mask]) if np.any(up_mask) else np.nan)\n",
        "            down_accs.append(np.mean(s_true[down_mask] == s_pred[down_mask]) if np.any(down_mask) else np.nan)\n",
        "\n",
        "        overall_acc = np.nanmean(overall_accs)\n",
        "        up_acc = np.nanmean(up_accs)\n",
        "        down_acc = np.nanmean(down_accs)\n",
        "\n",
        "    return overall_acc, up_acc, down_acc\n",
        "\n",
        "def regression_metrics(y_true, y_pred, k, meta=None, permnos=None):\n",
        "    \"\"\"\n",
        "    Includes:\n",
        "    - Regression metrics\n",
        "    - Pointwise directional accuracy\n",
        "    - Market cap group metrics\n",
        "    \"\"\"\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    n = len(y_true)\n",
        "\n",
        "    r2 = r2_zero(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "\n",
        "    dir_acc, up_acc, down_acc = calc_directional_metrics(y_true, y_pred, permnos)\n",
        "\n",
        "    metrics = {\n",
        "        \"R²_zero\": r2,\n",
        "        \"RMSE\": rmse,\n",
        "        \"MAE\": mae,\n",
        "        \"MSE\": mse,\n",
        "        \"Directional Accuracy\": dir_acc,\n",
        "        \"Up_Directional_Acc\": up_acc,\n",
        "        \"Down_Directional_Acc\": down_acc\n",
        "    }\n",
        "\n",
        "    if meta is not None and \"MKTCAP_PERCENTILE\" in meta:\n",
        "        top_mask = meta[\"MKTCAP_PERCENTILE\"] >= 0.75\n",
        "        bottom_mask = meta[\"MKTCAP_PERCENTILE\"] <= 0.25\n",
        "\n",
        "        if np.any(top_mask):\n",
        "            yt_top = y_true[top_mask]\n",
        "            yp_top = y_pred[top_mask]\n",
        "            perm_top = permnos[top_mask] if permnos is not None else None\n",
        "            r2_top = r2_zero(yt_top, yp_top)\n",
        "            rmse_top = np.sqrt(mean_squared_error(yt_top, yp_top))\n",
        "            mae_top = mean_absolute_error(yt_top, yp_top)\n",
        "            mse_top = mean_squared_error(yt_top, yp_top)\n",
        "            dir_top, up_top, down_top = calc_directional_metrics(yt_top, yp_top, perm_top)\n",
        "            metrics.update({\n",
        "                \"Top25_R2_zero\": r2_top,\n",
        "                \"Top25_RMSE\": rmse_top,\n",
        "                \"Top25_MAE\": mae_top,\n",
        "                \"Top25_MSE\": mse_top,\n",
        "                \"Top25_Dir_Acc\": dir_top,\n",
        "                \"Top25_Up_Acc\": up_top,\n",
        "                \"Top25_Down_Acc\": down_top\n",
        "            })\n",
        "\n",
        "        if np.any(bottom_mask):\n",
        "            yt_bot = y_true[bottom_mask]\n",
        "            yp_bot = y_pred[bottom_mask]\n",
        "            perm_bot = permnos[bottom_mask] if permnos is not None else None\n",
        "            r2_bot = r2_zero(yt_bot, yp_bot)\n",
        "            rmse_bot = np.sqrt(mean_squared_error(yt_bot, yp_bot))\n",
        "            mae_bot = mean_absolute_error(yt_bot, yp_bot)\n",
        "            mse_bot = mean_squared_error(yt_bot, yp_bot)\n",
        "            dir_bot, up_bot, down_bot = calc_directional_metrics(yt_bot, yp_bot, perm_bot)\n",
        "            metrics.update({\n",
        "                \"Bottom25_R2_zero\": r2_bot,\n",
        "                \"Bottom25_RMSE\": rmse_bot,\n",
        "                \"Bottom25_MAE\": mae_bot,\n",
        "                \"Bottom25_MSE\": mse_bot,\n",
        "                \"Bottom25_Dir_Acc\": dir_bot,\n",
        "                \"Bottom25_Up_Acc\": up_bot,\n",
        "                \"Bottom25_Down_Acc\": down_bot\n",
        "            })\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def f_statistic(y_true, y_pred, k):\n",
        "    \"\"\"Return F statistic and corresponding p-value\"\"\"\n",
        "    n   = len(y_true)\n",
        "    rss = np.sum((y_true - y_pred) ** 2)\n",
        "    tss = np.sum(y_true ** 2)\n",
        "    r2  = 1 - rss / tss\n",
        "    if (r2 <= 0) or (n <= k):\n",
        "        return 0.0, 1.0\n",
        "    F = (r2 / k) / ((1 - r2) / (n - k))\n",
        "    p = f_dist.sf(F, k, n - k)\n",
        "    return F, p\n",
        "\n",
        "def overall_interval_metrics_method1(y_all, yhat_all, k, permnos_all=None, meta_all=None):\n",
        "    \"\"\"\n",
        "    Method 1: Calculate metrics for the entire interval at once (2016-2024, all samples concatenated)\n",
        "    Returns: a dict, can be directly used for save_metrics()\n",
        "    \"\"\"\n",
        "    base = regression_metrics(\n",
        "        y_true=y_all,\n",
        "        y_pred=yhat_all,\n",
        "        k=k,\n",
        "        meta=meta_all,\n",
        "        permnos=permnos_all\n",
        "    )\n",
        "    F, p = f_statistic(y_all, yhat_all, k)\n",
        "    base[\"F_stat\"]     = F\n",
        "    base[\"F_pvalue\"]   = p\n",
        "    base[\"N_obs\"] = len(y_all)\n",
        "\n",
        "    delta_cash, sr_star_cash = delta_sharpe(base[\"R²_zero\"], sr_base=0)\n",
        "    base[\"ΔSharpe_cash\"]      = delta_cash\n",
        "    base[\"Sharpe*_cash\"]      = sr_star_cash\n",
        "\n",
        "    delta_mkt , sr_star_mkt  = delta_sharpe(base[\"R²_zero\"], sr_base=SR_MKT_EX)\n",
        "    base[\"ΔSharpe_mkt\"]       = delta_mkt\n",
        "    base[\"Sharpe*_mkt\"]       = sr_star_mkt\n",
        "\n",
        "    return base\n",
        "\n",
        "def sortino_ratio(rets, freq=252):\n",
        "    \"\"\"Compute Sortino Ratio\"\"\"\n",
        "    downside = rets[rets < 0]\n",
        "    if len(downside) == 0:\n",
        "        return np.inf\n",
        "    mu = rets.mean() * freq\n",
        "    sigma = np.sqrt((downside ** 2).mean()) * np.sqrt(freq)\n",
        "    return mu / sigma\n",
        "\n",
        "def cvar(rets, alpha=0.95):\n",
        "    \"\"\"Compute CVaR\"\"\"\n",
        "    q = np.quantile(rets, 1 - alpha)\n",
        "    return rets[rets <= q].mean()\n",
        "\n",
        "def save_predictions(model_name, window_size, y_true, y_pred, permnos, path=\"predictions/\"):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        \"PERMNO\": permnos,\n",
        "        \"y_true\": y_true,\n",
        "        \"y_pred\": y_pred\n",
        "    })\n",
        "\n",
        "    filename = f\"{model_name}_w{window_size}.csv\"\n",
        "    df.to_csv(os.path.join(path, filename), index=False)\n",
        "    print(f\"[Save] {filename}\")\n",
        "\n",
        "def save_metrics(metrics_dict, name, window, path=\"results.csv\"):\n",
        "    \"\"\"Save evaluation metrics\"\"\"\n",
        "    row = pd.DataFrame([metrics_dict])\n",
        "    row.insert(0, \"Model\", name)\n",
        "    row.insert(1, \"Window\", window)\n",
        "\n",
        "    if os.path.exists(path):\n",
        "        df = pd.read_csv(path)\n",
        "        df = df[~((df[\"Model\"] == name) & (df[\"Window\"] == window))]\n",
        "        df = pd.concat([df, row], ignore_index=True)\n",
        "        df.to_csv(path, index=False)\n",
        "        print(f\"[Update] Metrics updated for {name} w={window}\")\n",
        "    else:\n",
        "        row.to_csv(path, index=False)\n",
        "        print(f\"[Create] New metrics file created with {name} w={window}\")\n",
        "\n",
        "def get_quarter_periods(start_year=2015, end_year=2024):\n",
        "    \"\"\"Generate quarter sequence\"\"\"\n",
        "    quarters = []\n",
        "    for year in range(start_year, end_year + 1):\n",
        "        for q in range(1, 5):\n",
        "            quarters.append((year, q))\n",
        "    return quarters\n",
        "\n",
        "def load_datasets(npz_path):\n",
        "    \"\"\"Load dataset\"\"\"\n",
        "    data = np.load(npz_path, allow_pickle=True)\n",
        "    datasets = {}\n",
        "    for key in data.files:\n",
        "        datasets[key] = data[key]\n",
        "    return datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7uKNUNvfFBy"
      },
      "outputs": [],
      "source": [
        "# ===== Autoformer Model Architecture =====\n",
        "\n",
        "def pad_to_pow2(x):\n",
        "    \"\"\"Pad the time dimension of the input sequence to the nearest power of 2\"\"\"\n",
        "    L = x.size(-2)\n",
        "    pow2 = 1 << (L - 1).bit_length()\n",
        "    if pow2 == L:\n",
        "        return x, 0\n",
        "    pad_len = pow2 - L\n",
        "    return F.pad(x, (0, 0, 0, 0, 0, pad_len)), pad_len\n",
        "\n",
        "class AutoCorrelation(nn.Module):\n",
        "    \"\"\"Auto-Correlation mechanism\"\"\"\n",
        "    def __init__(self, factor=1, scale=None, attention_dropout=0.1):\n",
        "        super(AutoCorrelation, self).__init__()\n",
        "        self.factor = factor\n",
        "        self.scale = scale\n",
        "        self.dropout = nn.Dropout(attention_dropout)\n",
        "\n",
        "    def time_delay_agg_training(self, values, corr):\n",
        "        head = values.shape[1]\n",
        "        channel = values.shape[2]\n",
        "        length = values.shape[3]\n",
        "        top_k = max(1, min(int(self.factor * length), length))\n",
        "        mean_value = torch.mean(torch.mean(corr, dim=1), dim=1)\n",
        "\n",
        "        available_length = mean_value.shape[-1]\n",
        "        top_k = min(top_k, available_length)\n",
        "\n",
        "        if top_k <= 0 or available_length <= 0:\n",
        "            return values\n",
        "\n",
        "        index = torch.topk(torch.mean(mean_value, dim=0), top_k, dim=-1)[1]\n",
        "        weights = torch.stack([mean_value[:, index[i]] for i in range(top_k)], dim=-1)\n",
        "        tmp_corr = torch.softmax(weights, dim=-1)\n",
        "        tmp_values = values\n",
        "        delays_agg = torch.zeros_like(values).float()\n",
        "        for i in range(top_k):\n",
        "            pattern = torch.roll(tmp_values, -int(index[i]), -1)\n",
        "            scale = tmp_corr[:, i][:, None, None, None]\n",
        "            delays_agg = delays_agg + pattern * scale\n",
        "        return delays_agg\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        B, L, H, E = queries.shape\n",
        "        _, S, _, D = values.shape\n",
        "\n",
        "        if L < 2:\n",
        "            return values\n",
        "\n",
        "        if L > S:\n",
        "            zeros = torch.zeros_like(queries[:, :(L - S), :]).float()\n",
        "            values = torch.cat([values, zeros], dim=1)\n",
        "            keys = torch.cat([keys, zeros], dim=1)\n",
        "        else:\n",
        "            values = values[:, :L, :, :]\n",
        "            keys = keys[:, :L, :, :]\n",
        "\n",
        "        original_L = L\n",
        "        queries, pad_len = pad_to_pow2(queries)\n",
        "        keys, _ = pad_to_pow2(keys)\n",
        "        values, _ = pad_to_pow2(values)\n",
        "\n",
        "        if pad_len > 0:\n",
        "            L = queries.size(-2)\n",
        "\n",
        "        use_fp32 = (L & (L-1)) != 0 or L == 512\n",
        "\n",
        "        with amp.autocast(enabled=False):\n",
        "            q_fft = torch.fft.rfft(\n",
        "                queries.float().permute(0, 2, 3, 1).contiguous(),\n",
        "                dim=-1\n",
        "            )\n",
        "            k_fft = torch.fft.rfft(\n",
        "                keys.float().permute(0, 2, 3, 1).contiguous(),\n",
        "                dim=-1\n",
        "            )\n",
        "        res = q_fft * torch.conj(k_fft)\n",
        "\n",
        "        corr = torch.fft.irfft(res, dim=-1)\n",
        "\n",
        "        if pad_len > 0:\n",
        "            corr = corr[..., :original_L]\n",
        "\n",
        "        V = self.time_delay_agg_training(values.permute(0, 2, 3, 1).contiguous(), corr).permute(0, 3, 1, 2)\n",
        "        return V.contiguous()\n",
        "\n",
        "class AutoformerLayer(nn.Module):\n",
        "    \"\"\"Autoformer layer\"\"\"\n",
        "    def __init__(self, d_model, n_heads, d_ff=None, dropout=0.1, activation=\"relu\"):\n",
        "        super(AutoformerLayer, self).__init__()\n",
        "        d_ff = d_ff or 4 * d_model\n",
        "        self.attention = AutoCorrelation(attention_dropout=dropout)\n",
        "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
        "\n",
        "        head_dim = max(1, d_model // n_heads)\n",
        "        self.projection = nn.Linear(d_model, n_heads * head_dim)\n",
        "        self.n_heads = n_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L, _ = x.shape\n",
        "        H = self.n_heads\n",
        "\n",
        "        projected = self.projection(x)\n",
        "        head_dim = projected.shape[-1] // H\n",
        "        queries = keys = values = projected.view(B, L, H, head_dim)\n",
        "\n",
        "        new_x = self.attention(queries, keys, values)\n",
        "        if new_x.dim() == 4:\n",
        "            new_x = new_x.view(B, L, -1)\n",
        "        x = x + self.dropout(new_x)\n",
        "        y = x = self.norm1(x)\n",
        "\n",
        "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
        "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
        "\n",
        "        return self.norm2(x + y)\n",
        "\n",
        "class Autoformer(nn.Module):\n",
        "    \"\"\"Autoformer model - sequence input, automatic sequence length selection\"\"\"\n",
        "    def __init__(self, input_size, d_model=64, n_heads=4, e_layers=2, d_ff=256, dropout=0.1, seq_len=None):\n",
        "        super(Autoformer, self).__init__()\n",
        "\n",
        "        if seq_len is None:\n",
        "            for sl in range(8, 1, -1):\n",
        "                if input_size % sl == 0:\n",
        "                    seq_len = sl\n",
        "                    break\n",
        "            if seq_len is None:\n",
        "                seq_len = 1\n",
        "\n",
        "        self.seq_len = seq_len\n",
        "        self.feature_dim = input_size // seq_len\n",
        "\n",
        "        print(f\"[Autoformer] Using sequence length seq_len={seq_len}, feature dimension feature_dim={self.feature_dim}\")\n",
        "\n",
        "        self.input_projection = nn.Linear(self.feature_dim, d_model)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            AutoformerLayer(d_model, n_heads, d_ff, dropout) for _ in range(e_layers)\n",
        "        ])\n",
        "\n",
        "        self.output_projection = nn.Linear(d_model, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        if self.seq_len > 1:\n",
        "            x = x.view(batch_size, self.seq_len, -1)\n",
        "        else:\n",
        "            x = x.unsqueeze(1)\n",
        "\n",
        "        x = self.input_projection(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        x = x[:, -1, :]\n",
        "\n",
        "        x = self.output_projection(x)\n",
        "        return x.squeeze(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJiZgqHZfFBy"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 0) Utilities (keep the same interface as your original script)\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "def get_default_device() -> torch.device:\n",
        "    \"\"\"Automatically select GPU / MPS / CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")\n",
        "    if torch.backends.mps.is_available():\n",
        "        return torch.device(\"mps\")\n",
        "    return torch.device(\"cpu\")\n",
        "\n",
        "DEVICE_TUNE  = get_default_device()\n",
        "DEVICE_TRAIN = get_default_device()\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"Release GPU memory\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stopping mechanism\"\"\"\n",
        "    def __init__(self, patience=20, min_delta=0, restore_best_weights=True):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.restore_best_weights = restore_best_weights\n",
        "        self.best_loss = None\n",
        "        self.counter = 0\n",
        "        self.best_weights = None\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "            self.save_checkpoint(model)\n",
        "        elif val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "            self.save_checkpoint(model)\n",
        "        else:\n",
        "            self.counter += 1\n",
        "\n",
        "        if self.counter >= self.patience:\n",
        "            if self.restore_best_weights:\n",
        "                model.load_state_dict(self.best_weights)\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def save_checkpoint(self, model):\n",
        "        \"\"\"Save best weights\"\"\"\n",
        "        self.best_weights = copy.deepcopy(model.state_dict())\n",
        "\n",
        "class AutoformerWrapper:\n",
        "    \"\"\"\n",
        "    * fit / predict / partial_fit / clear_loss_history / set_params\n",
        "    * Supports warm‑start (continue training from previous quarter)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size: int,\n",
        "        seq_len: Optional[int] = None,\n",
        "        d_model: int = 64,\n",
        "        n_heads: int = 4,\n",
        "        e_layers: int = 2,\n",
        "        d_ff: int = 256,\n",
        "        dropout: float = 0.1,\n",
        "        learning_rate: float = 1e-3,\n",
        "        batch_size: int = 1024,\n",
        "        max_epochs: int = 30,\n",
        "        warm_start_epochs: int = 5,\n",
        "        training_device: Optional[torch.device] = None,\n",
        "    ):\n",
        "        self.input_size = input_size\n",
        "        self.seq_len = seq_len\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.e_layers = e_layers\n",
        "        self.d_ff = d_ff\n",
        "        self.dropout = dropout\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.max_epochs = max_epochs\n",
        "        self.warm_start_epochs = warm_start_epochs\n",
        "        self.training_device = training_device or get_default_device()\n",
        "\n",
        "        self._build_model()\n",
        "        self.is_fitted = False\n",
        "        self.loss_history: dict[str, list[float]] = {}\n",
        "\n",
        "    def _build_model(self):\n",
        "        self.model = Autoformer(\n",
        "            input_size=self.input_size,\n",
        "            seq_len=self.seq_len,\n",
        "            d_model=self.d_model,\n",
        "            n_heads=self.n_heads,\n",
        "            e_layers=self.e_layers,\n",
        "            d_ff=self.d_ff,\n",
        "            dropout=self.dropout\n",
        "        ).to(self.training_device)\n",
        "\n",
        "    def _init_training_components(self):\n",
        "        self.optimizer = optim.AdamW(self.model.parameters(), lr=self.learning_rate, weight_decay=1e-4)\n",
        "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode=\"min\", factor=0.5, patience=5, verbose=False)\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.early_stopping = EarlyStopping(patience=5)\n",
        "        self.scaler = GradScaler()\n",
        "\n",
        "    def _make_loaders(self, X, y, validation_split):\n",
        "        val_size = int(len(X) * validation_split) if validation_split > 0 else max(1, int(len(X) * 0.1))\n",
        "        X_train, X_val = X[:-val_size], X[-val_size:]\n",
        "        y_train, y_val = y[:-val_size], y[-val_size:]\n",
        "\n",
        "        train_ds = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
        "        val_ds   = TensorDataset(torch.FloatTensor(X_val),   torch.FloatTensor(y_val))\n",
        "        pin_mem  = self.training_device.type == \"cuda\"\n",
        "        num_workers = 4 if self.training_device.type == \"cuda\" else 0\n",
        "        train_loader = DataLoader(train_ds, batch_size=self.batch_size, shuffle=False,\n",
        "                                  drop_last=True, pin_memory=pin_mem, num_workers=num_workers,\n",
        "                                  persistent_workers=True if num_workers > 0 else False,\n",
        "                                  prefetch_factor=4 if num_workers > 0 else 2)\n",
        "        val_loader   = DataLoader(val_ds, batch_size=self.batch_size, shuffle=False,\n",
        "                                  pin_memory=pin_mem, num_workers=num_workers,\n",
        "                                  persistent_workers=True if num_workers > 0 else False,\n",
        "                                  prefetch_factor=4 if num_workers > 0 else 2)\n",
        "        return train_loader, val_loader\n",
        "\n",
        "    def fit(self, X, y, validation_split: float = 0.1, warm_start: bool = False, verbose: bool = True):\n",
        "        if self.training_device.type == \"cuda\":\n",
        "            torch.backends.cudnn.benchmark = True\n",
        "            torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "        if not warm_start:\n",
        "            self._build_model()\n",
        "            self._init_training_components()\n",
        "        else:\n",
        "            if verbose:\n",
        "                print(\"    [Warm Start] continue training existing Autoformer weights ...\")\n",
        "            self.early_stopping = EarlyStopping(patience=10)\n",
        "\n",
        "        if self.training_device.type == \"cuda\":\n",
        "            free_mem = torch.cuda.mem_get_info()[0] / 1024**3\n",
        "            if free_mem < 4:\n",
        "                self.batch_size = max(32, self.batch_size // 2)\n",
        "                if verbose:\n",
        "                    print(f\"    Dynamically adjust batch_size to {self.batch_size} (free memory: {free_mem:.1f} GiB)\")\n",
        "\n",
        "        train_loader, val_loader = self._make_loaders(X, y, validation_split)\n",
        "        epochs = self.warm_start_epochs if warm_start else self.max_epochs\n",
        "        if verbose:\n",
        "            print(f\"    Training Autoformer for {epochs} epochs on {self.training_device}\")\n",
        "\n",
        "        losses = {\"train\": [], \"val\": []}\n",
        "        for ep in range(epochs):\n",
        "            self.model.train()\n",
        "            train_loss = 0.0\n",
        "            for bx, by in train_loader:\n",
        "                bx, by = bx.to(self.training_device), by.to(self.training_device)\n",
        "                self.optimizer.zero_grad()\n",
        "                with autocast():\n",
        "                    loss = self.criterion(self.model(bx), by)\n",
        "                self.scaler.scale(loss).backward()\n",
        "                self.scaler.unscale_(self.optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "                train_loss += loss.item()\n",
        "            train_loss /= len(train_loader)\n",
        "\n",
        "            self.model.eval()\n",
        "            val_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for bx, by in val_loader:\n",
        "                    bx, by = bx.to(self.training_device), by.to(self.training_device)\n",
        "                    val_loss += self.criterion(self.model(bx), by).item()\n",
        "            val_loss /= len(val_loader)\n",
        "\n",
        "            losses[\"train\"].append(train_loss)\n",
        "            losses[\"val\"].append(val_loss)\n",
        "\n",
        "            if verbose and ((ep + 1) % 5 == 0 or ep == 0):\n",
        "                print(f\"    Epoch {ep+1:3d}/{epochs}: Train={train_loss:.6f}, Val={val_loss:.6f}\")\n",
        "\n",
        "            self.scheduler.step(val_loss)\n",
        "            if self.early_stopping(val_loss, self.model):\n",
        "                if verbose:\n",
        "                    print(f\"    Early-stopped at epoch {ep+1}\")\n",
        "                break\n",
        "\n",
        "            if self.training_device.type == \"cuda\" and (ep + 1) % 5 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "                torch.cuda.ipc_collect()\n",
        "\n",
        "        self.loss_history = losses\n",
        "        clear_memory()\n",
        "        self.is_fitted = True\n",
        "\n",
        "    def predict(self, X):\n",
        "        if not self.is_fitted:\n",
        "            raise RuntimeError(\"Call fit() before predict()\")\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            X_tensor = torch.FloatTensor(X).to(self.training_device)\n",
        "            preds = self.model(X_tensor).cpu().numpy()\n",
        "        return preds\n",
        "\n",
        "    def partial_fit(self, X_new, y_new, validation_split: float = 0.1, extra_epochs: int = 5, verbose: bool = True):\n",
        "        if not self.is_fitted:\n",
        "            raise RuntimeError(\"Must fit() once before partial_fit()\")\n",
        "        self.early_stopping = EarlyStopping(patience=10)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"    Partial-fit on {len(X_new)} samples for {extra_epochs} epochs ...\")\n",
        "        train_loader, val_loader = self._make_loaders(X_new, y_new, validation_split)\n",
        "\n",
        "        for ep in range(extra_epochs):\n",
        "            self.model.train()\n",
        "            train_loss = 0.0\n",
        "            for bx, by in train_loader:\n",
        "                bx, by = bx.to(self.training_device), by.to(self.training_device)\n",
        "                self.optimizer.zero_grad()\n",
        "                with autocast():\n",
        "                    loss = self.criterion(self.model(bx), by)\n",
        "                self.scaler.scale(loss).backward()\n",
        "                self.scaler.unscale_(self.optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "                train_loss += loss.item()\n",
        "            train_loss /= len(train_loader)\n",
        "\n",
        "            self.model.eval()\n",
        "            val_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for bx, by in val_loader:\n",
        "                    bx, by = bx.to(self.training_device), by.to(self.training_device)\n",
        "                    val_loss += self.criterion(self.model(bx), by).item()\n",
        "            val_loss /= len(val_loader)\n",
        "\n",
        "            if verbose and (ep % 2 == 0 or ep == extra_epochs - 1):\n",
        "                print(f\"        Epoch {ep+1:2d}/{extra_epochs}: Train={train_loss:.6f}, Val={val_loss:.6f}\")\n",
        "\n",
        "            self.scheduler.step(val_loss)\n",
        "            if self.early_stopping(val_loss, self.model):\n",
        "                if verbose:\n",
        "                    print(f\"        Early-stopped at epoch {ep+1}\")\n",
        "                break\n",
        "\n",
        "            if self.training_device.type == \"cuda\" and (ep + 1) % 5 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "                torch.cuda.ipc_collect()\n",
        "        clear_memory()\n",
        "\n",
        "    def clear_loss_history(self):\n",
        "        if hasattr(self, \"loss_history\"):\n",
        "            del self.loss_history\n",
        "\n",
        "    def set_params(self, **params):\n",
        "        \"\"\"Dynamically modify learning rate / batch_size / dropout etc. and rebuild optimizer if needed\"\"\"\n",
        "        rebuild = False\n",
        "        for k, v in params.items():\n",
        "            if hasattr(self, k):\n",
        "                if getattr(self, k) != v:\n",
        "                    setattr(self, k, v)\n",
        "                    rebuild = rebuild or (k in {\"d_model\", \"n_heads\", \"e_layers\", \"d_ff\", \"dropout\", \"seq_len\"})\n",
        "        if rebuild:\n",
        "            self._build_model()\n",
        "        else:\n",
        "            self._init_training_components()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZftKF9jGfFBy"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------\n",
        "# 0) Constants\n",
        "# ------------------------------------------------------------------\n",
        "TUNED_WINDOW_AF = 5        # Only perform Optuna when window=5\n",
        "DEVICE_TUNE_AF  = DEVICE_TUNE\n",
        "DEVICE_TRAIN_AF = DEVICE_TRAIN\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 1) Save / Load Autoformer model (only save state_dict)\n",
        "def _af_path(name: str, window: int, year: int, quarter: Optional[int], model_dir: str = \"models\"):\n",
        "    return f\"{model_dir}/{name}_w{window}_{year}{'' if quarter is None else f'Q{quarter}'}.pth\"\n",
        "\n",
        "def save_autoformer_model(wrapper, name, window, year, quarter=None, model_dir=\"models\"):\n",
        "    \"\"\"\n",
        "    Save state_dict and architecture hyperparameters; fully compatible with old format (old files will not be overwritten).\n",
        "    \"\"\"\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    path = _af_path(name, window, year, quarter, model_dir)\n",
        "    ckpt = {\n",
        "        \"state_dict\": wrapper.model.state_dict(),\n",
        "        \"hyper_params\": dict(\n",
        "            input_size = wrapper.input_size,\n",
        "            seq_len    = wrapper.seq_len,\n",
        "            d_model    = wrapper.d_model,\n",
        "            n_heads    = wrapper.n_heads,\n",
        "            e_layers   = wrapper.e_layers,\n",
        "            d_ff       = wrapper.d_ff,\n",
        "            dropout    = wrapper.dropout,\n",
        "            learning_rate = wrapper.learning_rate,\n",
        "            batch_size    = wrapper.batch_size,\n",
        "            max_epochs    = wrapper.max_epochs,\n",
        "            warm_start_epochs = wrapper.warm_start_epochs\n",
        "        )\n",
        "    }\n",
        "    torch.save(ckpt, path)\n",
        "    print(f\"[Saved] {path}\")\n",
        "\n",
        "def load_autoformer_model(name: str, window: int, year: int, quarter: Optional[int],\n",
        "                          training_device, fallback_hp: dict | None = None, model_dir: str = \"models\"):\n",
        "    \"\"\"\n",
        "    If file or hyperparameters are missing, return None (outer layer will automatically cold-start).\n",
        "    \"\"\"\n",
        "    path = _af_path(name, window, year, quarter, model_dir)\n",
        "    if not os.path.exists(path):\n",
        "        return None\n",
        "    ckpt = torch.load(path, map_location=\"cpu\")\n",
        "    hp = ckpt.get(\"hyper_params\", None)\n",
        "    if hp is None:\n",
        "        print(f\"[Warn] {path} missing hyper_params, cold-start\")\n",
        "        return None\n",
        "    # Rebuild network structure\n",
        "    wrapper = AutoformerWrapper(\n",
        "        input_size = hp[\"input_size\"],\n",
        "        seq_len    = hp[\"seq_len\"],\n",
        "        d_model    = hp[\"d_model\"],\n",
        "        n_heads    = hp[\"n_heads\"],\n",
        "        e_layers   = hp[\"e_layers\"],\n",
        "        d_ff       = hp[\"d_ff\"],\n",
        "        dropout    = hp[\"dropout\"],\n",
        "        learning_rate = hp[\"learning_rate\"],\n",
        "        batch_size    = hp[\"batch_size\"],\n",
        "        max_epochs    = hp[\"max_epochs\"],\n",
        "        warm_start_epochs = hp[\"warm_start_epochs\"],\n",
        "        training_device   = training_device,\n",
        "    )\n",
        "    try:\n",
        "        wrapper.model.load_state_dict(ckpt[\"state_dict\"], strict=True)\n",
        "    except RuntimeError as e:\n",
        "        print(f\"[Warn] state_dict mismatch ({e}), cold-start\")\n",
        "        return None\n",
        "    wrapper.is_fitted = True\n",
        "    wrapper._init_training_components()\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 2) Optuna hyperparameter tuning (only called for window=5)\n",
        "# ------------------------------------------------------------------\n",
        "def tune_autoformer_with_optuna(X, y, seq_len, n_trials: int = 5):\n",
        "    \"\"\"\n",
        "    Use AutoformerWrapper for cross-validation hyperparameter tuning, optimized for T4 GPU.\n",
        "    \"\"\"\n",
        "    print(\"Start hyperparameter tuning for T4 GPU...\")\n",
        "    tscv = TimeSeriesSplit(n_splits=3)\n",
        "\n",
        "    def objective(trial):\n",
        "        # Use smaller parameters for long sequences (252, 256)\n",
        "        if seq_len >= 252:\n",
        "            hp = dict(\n",
        "                d_model       = trial.suggest_categorical(\"d_model\", [32, 64]),\n",
        "                n_heads       = trial.suggest_categorical(\"n_heads\", [2, 4]),\n",
        "                e_layers      = 1,\n",
        "                d_ff          = trial.suggest_categorical(\"d_ff\", [128, 256]),\n",
        "                dropout       = trial.suggest_float(\"dropout\", 0.1, 0.3),\n",
        "                learning_rate = trial.suggest_float(\"lr\", 1e-4, 3e-3, log=True),\n",
        "                batch_size    = trial.suggest_categorical(\"batch_size\", [256, 512, 1024]),\n",
        "                max_epochs    = 6,\n",
        "                warm_start_epochs = 5,\n",
        "            )\n",
        "        else:\n",
        "            hp = dict(\n",
        "                d_model       = trial.suggest_categorical(\"d_model\", [64, 128]),\n",
        "                n_heads       = 4,\n",
        "                e_layers      = trial.suggest_int(\"e_layers\", 1, 3),\n",
        "                d_ff          = trial.suggest_categorical(\"d_ff\", [256, 512]),\n",
        "                dropout       = trial.suggest_float(\"dropout\", 0.1, 0.4),\n",
        "                learning_rate = trial.suggest_float(\"lr\", 5e-5, 5e-3, log=True),\n",
        "                batch_size    = trial.suggest_categorical(\"batch_size\", [512, 1024, 2048]),\n",
        "                max_epochs    = 8,\n",
        "                warm_start_epochs = 5,\n",
        "            )\n",
        "\n",
        "        cv_mse = []\n",
        "        for tr_idx, val_idx in tscv.split(X):\n",
        "            X_tr, X_val = X[tr_idx], X[val_idx]\n",
        "            y_tr, y_val = y[tr_idx], y[val_idx]\n",
        "\n",
        "            model = AutoformerWrapper(\n",
        "                input_size   = X.shape[1],\n",
        "                seq_len      = seq_len,\n",
        "                d_model      = hp[\"d_model\"],\n",
        "                n_heads      = hp[\"n_heads\"],\n",
        "                e_layers     = hp[\"e_layers\"],\n",
        "                d_ff         = hp[\"d_ff\"],\n",
        "                dropout      = hp[\"dropout\"],\n",
        "                learning_rate= hp[\"learning_rate\"],\n",
        "                batch_size   = hp[\"batch_size\"],\n",
        "                max_epochs   = hp[\"max_epochs\"],\n",
        "                warm_start_epochs = hp.get(\"warm_start_epochs\", 5),\n",
        "                training_device = DEVICE_TUNE_AF,\n",
        "            )\n",
        "            model.fit(X_tr, y_tr, validation_split=0.0, warm_start=False, verbose=False)\n",
        "            preds = model.predict(X_val)\n",
        "            cv_mse.append(mean_squared_error(y_val, preds))\n",
        "            del model\n",
        "            torch.cuda.empty_cache()\n",
        "            torch.cuda.ipc_collect()\n",
        "            gc.collect()\n",
        "            clear_memory()\n",
        "        return float(np.mean(cv_mse))\n",
        "\n",
        "    study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
        "    study.optimize(objective, n_trials=n_trials, n_jobs=1)\n",
        "\n",
        "    best = study.best_params or {}\n",
        "    best_hp = dict(\n",
        "        d_model   = best.get(\"d_model\", 64),\n",
        "        n_heads   = 4,\n",
        "        e_layers  = best.get(\"e_layers\", 2),\n",
        "        d_ff      = best.get(\"d_ff\", 256),\n",
        "        dropout   = best.get(\"dropout\", 0.1),\n",
        "        learning_rate = best.get(\"lr\", 1e-3),\n",
        "        batch_size    = best.get(\"batch_size\", 1024),\n",
        "        max_epochs    = 30,\n",
        "        warm_start_epochs = 5,\n",
        "    )\n",
        "    print(f\"[Optuna-AF] best_MSE={study.best_value:.6f}, params={best_hp}\")\n",
        "    return best_hp\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "def train_autoformer_models_expanding_quarterly(\n",
        "    start_year: int = 2015,\n",
        "    end_year: int = 2024,\n",
        "    window_sizes: list[int] | None = None,\n",
        "    npz_path: str | None = None,\n",
        "    n_trials_optuna: int = 15,\n",
        "    drive_models_path: str = f\"{DRIVE_PATH}/models_backup\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Same as the original process, only replaced with AutoformerWrapper, optimized for Google Colab and T4 GPU.\n",
        "    Before training, scan local models and skip if already present; if model exists, directly read hyperparameters.\n",
        "    \"\"\"\n",
        "    if window_sizes is None:\n",
        "        window_sizes = [5, 21, 252, 512]\n",
        "    if npz_path is None:\n",
        "        npz_path = f\"{DATA_PATH}/all_window_datasets_scaled.npz\"\n",
        "\n",
        "    print(f\"Starting **Autoformer** quarterly expanding training {start_year}-{end_year}\")\n",
        "    print(f\"Using data from: {npz_path}\")\n",
        "    data = load_datasets(npz_path)\n",
        "\n",
        "    best_params_cache: dict[str, dict] = {}\n",
        "    quarters_to_tune = {(2020, 4)}\n",
        "\n",
        "    for window in window_sizes:\n",
        "        print(f\"\\n=== Window = {window} ===\")\n",
        "        X_train_init = data[f\"X_train_{window}\"]\n",
        "        y_train_init = data[f\"y_train_{window}\"]\n",
        "        X_test_full  = data[f\"X_test_{window}\"]\n",
        "        y_test_full  = data[f\"y_test_{window}\"]\n",
        "        meta_test    = pd.DataFrame.from_dict(data[f\"meta_test_{window}\"].item())\n",
        "        meta_test[\"ret_date\"] = pd.to_datetime(meta_test[\"ret_date\"])\n",
        "\n",
        "        total_feat = X_train_init.shape[1]\n",
        "\n",
        "        # Initial hyperparameter tuning (only for window == TUNED_WINDOW_AF)\n",
        "        cache_key = f\"AF_w{window}\"\n",
        "        if window == TUNED_WINDOW_AF:\n",
        "            # Check if model for start_year Q4 already exists\n",
        "            tuned_model = load_autoformer_model(\n",
        "                \"Autoformer\", window,\n",
        "                start_year, 4,\n",
        "                training_device=DEVICE_TRAIN_AF,\n",
        "                model_dir=drive_models_path\n",
        "            )\n",
        "            if tuned_model is not None:\n",
        "                # Load hyperparameters from existing model\n",
        "                hp = {\n",
        "                    \"d_model\": tuned_model.d_model,\n",
        "                    \"n_heads\": tuned_model.n_heads,\n",
        "                    \"e_layers\": tuned_model.e_layers,\n",
        "                    \"d_ff\": tuned_model.d_ff,\n",
        "                    \"dropout\": tuned_model.dropout,\n",
        "                    \"learning_rate\": tuned_model.learning_rate,\n",
        "                    \"batch_size\": tuned_model.batch_size,\n",
        "                    \"max_epochs\": tuned_model.max_epochs,\n",
        "                    \"warm_start_epochs\": tuned_model.warm_start_epochs,\n",
        "                }\n",
        "                print(\"[Skip-Optuna] hyper-params loaded from existing model at {}Q4\".format(start_year))\n",
        "            else:\n",
        "                print(\"  - Optuna tuning on initial window...\")\n",
        "                hp = tune_autoformer_with_optuna(\n",
        "                    X_train_init,\n",
        "                    y_train_init,\n",
        "                    seq_len=window,\n",
        "                    n_trials=n_trials_optuna\n",
        "                )\n",
        "        else:\n",
        "            hp = None\n",
        "        best_params_cache[cache_key] = hp\n",
        "\n",
        "        # Quarterly loop\n",
        "        for year, quarter in get_quarter_periods(start_year, end_year):\n",
        "            if (year == start_year and quarter < 4) or (year == end_year and quarter > 3):\n",
        "                continue\n",
        "\n",
        "            # Check if model already exists locally, skip if so\n",
        "            existing = load_autoformer_model(\n",
        "                \"Autoformer\", window,\n",
        "                year, quarter,\n",
        "                training_device=DEVICE_TRAIN_AF,\n",
        "                model_dir=drive_models_path\n",
        "            )\n",
        "            if existing is not None:\n",
        "                print(f\"[Skip] Model already trained for window={window}, {year}Q{quarter}\")\n",
        "                continue\n",
        "\n",
        "            print(f\"\\n[Window {window}] {year}Q{quarter}\")\n",
        "\n",
        "            # Expand training set\n",
        "            if not (year == start_year and quarter == 4):\n",
        "                py, pq = (year, quarter - 1) if quarter > 1 else (year - 1, 4)\n",
        "                mask_prev = (\n",
        "                    (meta_test[\"ret_date\"].dt.year == py) &\n",
        "                    (meta_test[\"ret_date\"].dt.quarter == pq)\n",
        "                )\n",
        "                if mask_prev.any():\n",
        "                    X_prev, y_prev = X_test_full[mask_prev], y_test_full[mask_prev]\n",
        "                    X_train_init = np.vstack([X_train_init, X_prev])\n",
        "                    y_train_init = np.hstack([y_train_init, y_prev])\n",
        "                    print(f\"    +{mask_prev.sum()} obs from {py}Q{pq} -> expanding size {len(y_train_init)}\")\n",
        "\n",
        "            # Get/copy hyperparameters\n",
        "            hp = best_params_cache.get(cache_key)\n",
        "            if hp is None:\n",
        "                hp = best_params_cache[f\"AF_w{TUNED_WINDOW_AF}\"].copy()\n",
        "                # For long sequences (252, 256), set optimized small parameters\n",
        "                if window >= 252:\n",
        "                    hp.update({\n",
        "                        \"d_model\": 32,\n",
        "                        \"n_heads\": 2,\n",
        "                        \"e_layers\": 1,\n",
        "                        \"d_ff\": 128,\n",
        "                        \"batch_size\": 512,\n",
        "                        \"max_epochs\": 25,\n",
        "                        \"warm_start_epochs\": 5\n",
        "                    })\n",
        "                    print(f\"    [Long Sequence] Applied optimized parameters for window {window}\")\n",
        "                best_params_cache[cache_key] = hp\n",
        "\n",
        "            if (year, quarter) in quarters_to_tune and window == TUNED_WINDOW_AF:\n",
        "                print(\"    Re-tuning via Optuna...\")\n",
        "                hp = tune_autoformer_with_optuna(\n",
        "                    X_train_init,\n",
        "                    y_train_init,\n",
        "                    seq_len=window,\n",
        "                    n_trials=n_trials_optuna\n",
        "                )\n",
        "                best_params_cache[cache_key] = hp\n",
        "\n",
        "            # Load previous quarter's model for warm-start\n",
        "            model_prev = None\n",
        "            if not (year == start_year and quarter == 4):\n",
        "                if quarter > 1:\n",
        "                    py, pq = year, quarter - 1\n",
        "                else:\n",
        "                    py, pq = year - 1, 4\n",
        "                print(f\"    Attempting warm-start from previous model: {py}Q{pq}\")\n",
        "                model_prev = load_autoformer_model(\n",
        "                    \"Autoformer\",\n",
        "                    window,\n",
        "                    py, pq,\n",
        "                    training_device=DEVICE_TRAIN_AF,\n",
        "                    model_dir=drive_models_path\n",
        "                )\n",
        "\n",
        "            # Train or fine-tune\n",
        "            if model_prev is not None:\n",
        "                print(\"    Warm-start from last quarter...\")\n",
        "                model_prev.partial_fit(\n",
        "                    X_train_init,\n",
        "                    y_train_init,\n",
        "                    validation_split=0.1,\n",
        "                    extra_epochs=hp.get(\"warm_start_epochs\", 5)\n",
        "                )\n",
        "                af_wrap = model_prev\n",
        "            else:\n",
        "                print(\"    Cold-start training...\")\n",
        "                af_wrap = AutoformerWrapper(\n",
        "                    input_size          = total_feat,\n",
        "                    seq_len             = window,\n",
        "                    d_model             = hp[\"d_model\"],\n",
        "                    n_heads             = hp[\"n_heads\"],\n",
        "                    e_layers            = hp[\"e_layers\"],\n",
        "                    d_ff                = hp[\"d_ff\"],\n",
        "                    dropout             = hp[\"dropout\"],\n",
        "                    learning_rate       = hp[\"learning_rate\"],\n",
        "                    batch_size          = hp[\"batch_size\"],\n",
        "                    max_epochs          = hp[\"max_epochs\"],\n",
        "                    warm_start_epochs   = hp.get(\"warm_start_epochs\", 5),\n",
        "                    training_device     = DEVICE_TRAIN_AF,\n",
        "                )\n",
        "                af_wrap.fit(\n",
        "                    X_train_init,\n",
        "                    y_train_init,\n",
        "                    validation_split=0.1,\n",
        "                    warm_start=False\n",
        "                )\n",
        "\n",
        "            # Save to Drive backup directory\n",
        "            save_autoformer_model(\n",
        "                af_wrap,\n",
        "                \"Autoformer\",\n",
        "                window, year, quarter,\n",
        "                model_dir=drive_models_path\n",
        "            )\n",
        "            del af_wrap\n",
        "            for _name in ('train_loader', 'val_loader'):\n",
        "                if _name in locals():\n",
        "                    del locals()[_name]\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "            torch.cuda.ipc_collect()\n",
        "            gc.collect()\n",
        "            clear_memory()\n",
        "\n",
        "    print(\"All Autoformer quarterly expanding models trained and saved to\", drive_models_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krYC_6oZfFBz"
      },
      "outputs": [],
      "source": [
        "# ===== Portfolio Core Class =====\n",
        "# ===== Transaction Cost Settings =====\n",
        "TC_GRID = [0.0005, 0.001, 0.002, 0.003, 0.004]  # 5, 10, 20, 30, 40 bps\n",
        "TC_TAG  = {\n",
        "    0.0005: \"tc5\",\n",
        "    0.001:  \"tc10\",\n",
        "    0.002:  \"tc20\",\n",
        "    0.003:  \"tc30\",\n",
        "    0.004:  \"tc40\"\n",
        "}\n",
        "\n",
        "class PortfolioBacktester:\n",
        "    def __init__(self):\n",
        "        self.results = {}\n",
        "\n",
        "    def calc_turnover(self, w_t, r_t, w_tp1):\n",
        "    \n",
        "        if w_t is None:\n",
        "            return np.sum(np.abs(w_tp1))\n",
        "\n",
        "        gross_ret = np.sum(w_t * r_t)\n",
        "        if abs(1 + gross_ret) < 1e-8:  # Avoid division by zero\n",
        "            return np.sum(np.abs(w_tp1))\n",
        "\n",
        "        passive_weight = w_t * (1 + r_t) / (1 + gross_ret)\n",
        "        turnover = np.sum(np.abs(w_tp1 - passive_weight))\n",
        "        return turnover\n",
        "\n",
        "    def create_portfolios_with_permno_tracking(self, signals, market_caps, permnos, top_pct=0.1, bottom_pct=0.1, weight_scheme=\"VW\"):\n",
        "        \"\"\"\n",
        "        Create portfolio weights based on signals, strictly tracking permno alignment.\n",
        "        weight_scheme: 'VW' for value-weighted, 'EW' for equal-weighted\n",
        "        \"\"\"\n",
        "        n_stocks = len(signals)\n",
        "        top_n    = max(1, int(round(n_stocks * top_pct)))\n",
        "        bottom_n = max(1, int(round(n_stocks * bottom_pct)))\n",
        "\n",
        "        sorted_idx = np.argsort(signals)[::-1]  # Descending order\n",
        "\n",
        "        top_idx = sorted_idx[:top_n]\n",
        "        bottom_idx = sorted_idx[-bottom_n:]  # bottom_n >= 1 ensures non-empty\n",
        "\n",
        "        portfolio_data = {}\n",
        "\n",
        "        # Long-only portfolio (Top 10%)\n",
        "        long_weights = np.zeros(n_stocks)\n",
        "        if len(top_idx) > 0:\n",
        "            if weight_scheme == \"VW\":\n",
        "                top_market_caps = market_caps[top_idx]\n",
        "                if np.sum(top_market_caps) > 0:\n",
        "                    long_weights[top_idx] = top_market_caps / np.sum(top_market_caps)\n",
        "            else:  # 'EW'\n",
        "                long_weights[top_idx] = 1.0 / len(top_idx)\n",
        "\n",
        "        portfolio_data['long_only'] = {\n",
        "            'weights': long_weights,\n",
        "            'permnos': permnos.copy(),\n",
        "            'selected_permnos': permnos[top_idx] if len(top_idx) > 0 else np.array([])\n",
        "        }\n",
        "\n",
        "        # Short-only portfolio (Bottom 10%)\n",
        "        short_weights = np.zeros(n_stocks)\n",
        "        if len(bottom_idx) > 0:\n",
        "            if weight_scheme == \"VW\":\n",
        "                bottom_market_caps = market_caps[bottom_idx]\n",
        "                if np.sum(bottom_market_caps) > 0:\n",
        "                    short_weights[bottom_idx] = -bottom_market_caps / np.sum(bottom_market_caps)\n",
        "            else:  # 'EW'\n",
        "                short_weights[bottom_idx] = -1.0 / len(bottom_idx)\n",
        "\n",
        "        portfolio_data['short_only'] = {\n",
        "            'weights': short_weights,\n",
        "            'permnos': permnos.copy(),\n",
        "            'selected_permnos': permnos[bottom_idx] if len(bottom_idx) > 0 else np.array([])\n",
        "        }\n",
        "\n",
        "        # Long-Short portfolio (Top long + Bottom short)\n",
        "        ls_raw = long_weights + short_weights\n",
        "\n",
        "        gross_target = 2.0        # Gross ≈ 2, Net ≈ 0\n",
        "        current_gross = np.sum(np.abs(long_weights)) + np.sum(np.abs(short_weights))\n",
        "        scale = gross_target / current_gross if current_gross > 1e-8 else 0.0\n",
        "        ls_weights = scale * ls_raw\n",
        "\n",
        "        ls_selected_permnos = np.concatenate([\n",
        "            permnos[top_idx] if len(top_idx) > 0 else np.array([]),\n",
        "            permnos[bottom_idx] if len(bottom_idx) > 0 else np.array([])\n",
        "        ])\n",
        "\n",
        "        portfolio_data['long_short'] = {\n",
        "            'weights': ls_weights,\n",
        "            'permnos': permnos.copy(),\n",
        "            'selected_permnos': ls_selected_permnos\n",
        "        }\n",
        "\n",
        "        return portfolio_data\n",
        "\n",
        "    def calculate_aligned_portfolio_return(self, portfolio_weights, portfolio_permnos, actual_returns, actual_permnos):\n",
        "        \"\"\"Calculate portfolio return strictly aligned by permno\"\"\"\n",
        "        aligned_returns = np.zeros(len(portfolio_permnos))\n",
        "\n",
        "        return_dict = dict(zip(actual_permnos, actual_returns))\n",
        "\n",
        "        for i, permno in enumerate(portfolio_permnos):\n",
        "            if permno in return_dict:\n",
        "                aligned_returns[i] = return_dict[permno]\n",
        "            # If not found, keep as 0\n",
        "\n",
        "        portfolio_return = np.sum(portfolio_weights * aligned_returns)\n",
        "        return portfolio_return, aligned_returns\n",
        "\n",
        "    def calculate_metrics(self, returns, turnover_series=None):\n",
        "        \"\"\"Calculate portfolio metrics - only returns summary metrics, not long series\"\"\"\n",
        "        returns = np.array(returns)\n",
        "\n",
        "        annual_return = np.mean(returns) * 252  # Annualized daily return\n",
        "        annual_vol = np.std(returns, ddof=1) * np.sqrt(252)  # Annualized daily volatility\n",
        "        sharpe = annual_return / annual_vol if annual_vol > 0 else 0\n",
        "\n",
        "        log_cum = np.cumsum(np.log1p(returns))\n",
        "        peak_log = np.maximum.accumulate(log_cum)\n",
        "        dd_log = peak_log - log_cum\n",
        "        max_drawdown = 1 - np.exp(-dd_log.max())\n",
        "        max_1d_loss = np.min(returns)\n",
        "\n",
        "        avg_turnover = np.mean(turnover_series) if turnover_series is not None else 0\n",
        "\n",
        "        sortino = sortino_ratio(returns)\n",
        "        cvar95  = cvar(returns, alpha=0.95)\n",
        "\n",
        "        result = {\n",
        "            'annual_return': annual_return,\n",
        "            'annual_vol': annual_vol,\n",
        "            'sharpe': sharpe,\n",
        "            'max_drawdown': max_drawdown,\n",
        "            'max_1d_loss': max_1d_loss,\n",
        "            'avg_turnover': avg_turnover,\n",
        "            'sortino': sortino,\n",
        "            'cvar95': cvar95\n",
        "        }\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKplIgg5fFBz"
      },
      "outputs": [],
      "source": [
        "# Main function for daily prediction and next-day rebalancing portfolio simulation\n",
        "\n",
        "def run_portfolio_simulation_daily_rebalance(start_year=2016, end_year=2024, window_sizes=None, model_names=None,\n",
        "                                           npz_path=None):\n",
        "    \"\"\"\n",
        "Portfolio simulation (daily prediction and next-day rebalancing):\n",
        "    1. Quarterly model loading (using models trained with quarterly expanding window)\n",
        "    2. Daily prediction to daily signals\n",
        "    3. Daily portfolio construction (T+1 rebalancing, strict permno alignment)\n",
        "    4. Separate summary metrics and time series data\n",
        "    \"\"\"\n",
        "    if window_sizes is None:\n",
        "        window_sizes = [5, 21, 252, 512]\n",
        "    if model_names is None:\n",
        "        model_names = [\"Autoformer\"]\n",
        "    if npz_path is None:\n",
        "        npz_path = f\"{DATA_PATH}/all_window_datasets_scaled.npz\"\n",
        "\n",
        "    print(\"Starting Daily Rebalance Portfolio Backtesting Simulation\")\n",
        "\n",
        "    backtester = PortfolioBacktester()\n",
        "\n",
        "    datasets = load_datasets(npz_path)\n",
        "\n",
        "    summary_results = []\n",
        "    daily_series_data = []\n",
        "    pred_rows = []\n",
        "\n",
        "    WEIGHT_SCHEMES = [\"VW\", \"EW\"]\n",
        "\n",
        "    for window in window_sizes:\n",
        "        print(f\"Processing window size: {window}\")\n",
        "\n",
        "        X_test = datasets[f\"X_test_{window}\"]\n",
        "        y_test = datasets[f\"y_test_{window}\"]\n",
        "        input_size = X_test.shape[1]\n",
        "        meta_test_dict = datasets[f\"meta_test_{window}\"].item()\n",
        "        meta_test = pd.DataFrame.from_dict(meta_test_dict)\n",
        "\n",
        "        permnos_test = meta_test[\"PERMNO\"].values\n",
        "        meta_test[\"signal_date\"]  = pd.to_datetime(meta_test[\"date\"])\n",
        "        meta_test[\"ret_date\"]     = pd.to_datetime(meta_test[\"ret_date\"])\n",
        "        market_caps = meta_test.get(\"MKTCAP\", np.ones(len(permnos_test)))\n",
        "\n",
        "        meta_test['date'] = pd.to_datetime(meta_test[\"date\"])\n",
        "        dates_test = meta_test['signal_date']\n",
        "\n",
        "        for model_name in model_names:\n",
        "            for scheme in WEIGHT_SCHEMES:\n",
        "                all_y_true   = []\n",
        "                all_y_pred   = []\n",
        "                all_permnos  = []\n",
        "                all_meta     = []\n",
        "                print(f\"  Model: {model_name}, Scheme: {scheme}\")\n",
        "\n",
        "                portfolio_daily_data = {\n",
        "                    'long_only': {'returns': [], 'turnovers': [], 'dates': []},\n",
        "                    'short_only': {'returns': [], 'turnovers': [], 'dates': []},\n",
        "                    'long_short': {'returns': [], 'turnovers': [], 'dates': []}\n",
        "                }\n",
        "\n",
        "                prev_portfolio_data = {'long_only': None, 'short_only': None, 'long_short': None}\n",
        "\n",
        "                signals_buf = {}\n",
        "\n",
        "                for year in range(start_year, min(end_year + 1, 2025)):\n",
        "                    for quarter in range(1, 5):\n",
        "                        # Determine model file year and quarter (T+1 logic: use previous quarter's model to predict current quarter)\n",
        "                        if quarter == 1:\n",
        "                            model_file_year, model_file_quarter = year - 1, 4\n",
        "                        else:\n",
        "                            model_file_year, model_file_quarter = year, quarter - 1\n",
        "                        MODEL_BACKUP_DIR = \"/content/drive/MyDrive/ERP Data/models_backup\"\n",
        "                        pth_path = os.path.join(\n",
        "                            MODEL_BACKUP_DIR,\n",
        "                            f\"{model_name}_w{window}_{model_file_year}Q{model_file_quarter}.pth\"\n",
        "                        )\n",
        "\n",
        "                        if not os.path.exists(pth_path):\n",
        "                            print(f\"      Skip: Model file not found {pth_path}\")\n",
        "                            continue\n",
        "\n",
        "                        autoformer = load_autoformer_model(\n",
        "                            name=\"Autoformer\",\n",
        "                            window=window,\n",
        "                            year=model_file_year,\n",
        "                            quarter=model_file_quarter,\n",
        "                            training_device=DEVICE_TRAIN,\n",
        "                            model_dir=MODEL_BACKUP_DIR\n",
        "                        )\n",
        "                        if autoformer is None:\n",
        "                            print(f\"      Skip: Failed to load model {pth_path} (hyperparameters mismatch or missing)\")\n",
        "                            continue\n",
        "                        model = autoformer\n",
        "\n",
        "                        quarter_mask = (\n",
        "                            (dates_test.dt.year == year) &\n",
        "                            (dates_test.dt.quarter == quarter)\n",
        "                        )\n",
        "                        if not np.any(quarter_mask):\n",
        "                            continue\n",
        "\n",
        "                        X_quarter = X_test[quarter_mask]\n",
        "                        y_quarter = y_test[quarter_mask]\n",
        "                        permnos_quarter = permnos_test[quarter_mask]\n",
        "                        market_caps_quarter = market_caps[quarter_mask]\n",
        "                        dates_quarter = dates_test[quarter_mask]\n",
        "                        ret_dates_quarter = meta_test.loc[quarter_mask, 'ret_date'].values\n",
        "\n",
        "                        preds = model.predict(X_quarter)\n",
        "\n",
        "                        y_scaler = Y_SCALERS.get(window)\n",
        "                        if y_scaler is not None:\n",
        "                            preds = inverse_transform_y(preds, y_scaler)\n",
        "                            y_quarter = inverse_transform_y(y_quarter, y_scaler)\n",
        "                            print(f\"      Applied inverse scaling for window {window} - {len(preds)} predictions\")\n",
        "                        else:\n",
        "                            print(f\"      [Warning] No Y scaler found for window {window}, using original scale\")\n",
        "\n",
        "                        if len(preds) < len(y_quarter):\n",
        "                            gap = len(y_quarter) - len(preds)\n",
        "                            dates_quarter       = dates_quarter[gap:]\n",
        "                            ret_dates_quarter   = ret_dates_quarter[gap:]\n",
        "                            permnos_quarter     = permnos_quarter[gap:]\n",
        "                            market_caps_quarter = market_caps_quarter[gap:]\n",
        "                            y_quarter           = y_quarter[gap:]\n",
        "                            meta_quarter        = meta_test.loc[quarter_mask].iloc[gap:]\n",
        "                        else:\n",
        "                            meta_quarter        = meta_test.loc[quarter_mask]\n",
        "\n",
        "                        assert len(preds) == len(y_quarter)\n",
        "\n",
        "                        df_quarter = pd.DataFrame({\n",
        "                            \"signal_date\"   : dates_quarter,\n",
        "                            \"ret_date\"      : ret_dates_quarter,\n",
        "                            \"permno\"        : permnos_quarter,\n",
        "                            \"market_cap\"    : market_caps_quarter,\n",
        "                            \"actual_return\" : y_quarter,\n",
        "                            \"prediction\"    : preds\n",
        "                        })\n",
        "\n",
        "                        if scheme == 'VW':\n",
        "                            df_q_save = df_quarter[['signal_date','ret_date','permno',\n",
        "                                                    'actual_return','prediction','market_cap']].copy()\n",
        "                            df_q_save.rename(columns={'actual_return':'y_true',\n",
        "                                                      'prediction':'y_pred'}, inplace=True)\n",
        "                            df_q_save['model']  = model_name\n",
        "                            df_q_save['window'] = window\n",
        "                            pred_rows.append(df_q_save)\n",
        "\n",
        "                        all_y_true.append(df_quarter['actual_return'].values)\n",
        "                        all_y_pred.append(df_quarter['prediction'].values)\n",
        "                        all_permnos.append(df_quarter['permno'].values)\n",
        "                        meta_quarter = meta_test.loc[quarter_mask].copy()\n",
        "                        if len(preds) < len(meta_quarter):\n",
        "                            gap = len(meta_quarter) - len(preds)\n",
        "                            meta_quarter = meta_quarter.iloc[gap:]\n",
        "\n",
        "                        all_meta.append(meta_quarter)\n",
        "\n",
        "                        for signal_date, sig_grp in df_quarter.groupby('signal_date'):\n",
        "\n",
        "                            daily_signals = (\n",
        "                                sig_grp.groupby('permno')['prediction'].mean()\n",
        "                                      .to_frame('prediction')\n",
        "                                      .join(sig_grp.groupby('permno')['market_cap'].mean())\n",
        "                            )\n",
        "                            signals_buf[signal_date] = daily_signals\n",
        "\n",
        "                            prev_date = signal_date - pd.tseries.offsets.BDay(1)\n",
        "                            if prev_date not in signals_buf:\n",
        "                                continue\n",
        "\n",
        "                            sigs = signals_buf.pop(prev_date)\n",
        "                            if prev_date in signals_buf:\n",
        "                                del signals_buf[prev_date]\n",
        "\n",
        "                            ret_grp = df_quarter[df_quarter['ret_date'] == signal_date]\n",
        "                            if len(ret_grp) == 0:\n",
        "                                continue\n",
        "\n",
        "                            daily_actual_returns = (\n",
        "                                ret_grp.groupby('permno')['actual_return']\n",
        "                                       .mean()\n",
        "                                       .reindex(sigs.index, fill_value=0)\n",
        "                                       .values\n",
        "                            )\n",
        "                            daily_permnos = sigs.index.values\n",
        "\n",
        "                            portfolios_data = backtester.create_portfolios_with_permno_tracking(\n",
        "                                signals      = sigs['prediction'].values,\n",
        "                                market_caps  = sigs['market_cap'].values,\n",
        "                                permnos      = daily_permnos,\n",
        "                                weight_scheme= scheme\n",
        "                            )\n",
        "\n",
        "                            for portfolio_type in ['long_only', 'short_only', 'long_short']:\n",
        "                                portfolio_info = portfolios_data[portfolio_type]\n",
        "\n",
        "                                portfolio_return, aligned_returns = backtester.calculate_aligned_portfolio_return(\n",
        "                                    portfolio_weights=portfolio_info['weights'],\n",
        "                                    portfolio_permnos=portfolio_info['permnos'],\n",
        "                                    actual_returns=daily_actual_returns,\n",
        "                                    actual_permnos=daily_permnos\n",
        "                                )\n",
        "\n",
        "                                if prev_portfolio_data[portfolio_type] is not None:\n",
        "                                    prev_w_ser = pd.Series(\n",
        "                                        prev_portfolio_data[portfolio_type]['weights'],\n",
        "                                        index=prev_portfolio_data[portfolio_type]['permnos']\n",
        "                                    )\n",
        "                                    cur_w_ser = pd.Series(\n",
        "                                        portfolio_info['weights'],\n",
        "                                        index=portfolio_info['permnos']\n",
        "                                    )\n",
        "\n",
        "                                    prev_r_ser = pd.Series(\n",
        "                                        prev_portfolio_data[portfolio_type]['aligned_returns'],\n",
        "                                        index=prev_portfolio_data[portfolio_type]['permnos']\n",
        "                                    )\n",
        "\n",
        "                                    aligned_prev_w = prev_w_ser.reindex(cur_w_ser.index, fill_value=0).values\n",
        "                                    aligned_prev_r = prev_r_ser.reindex(cur_w_ser.index, fill_value=0).values\n",
        "\n",
        "                                    aligned_cur_w = cur_w_ser.values\n",
        "\n",
        "                                    turnover = backtester.calc_turnover(\n",
        "                                        w_t  = aligned_prev_w,\n",
        "                                        r_t  = aligned_prev_r,\n",
        "                                        w_tp1= aligned_cur_w\n",
        "                                    )\n",
        "                                else:\n",
        "                                    turnover = np.sum(np.abs(portfolio_info['weights']))\n",
        "\n",
        "                                portfolio_daily_data[portfolio_type]['returns'].append(portfolio_return)\n",
        "                                portfolio_daily_data[portfolio_type]['turnovers'].append(turnover)\n",
        "                                portfolio_daily_data[portfolio_type]['dates'].append(signal_date)\n",
        "\n",
        "                                prev_portfolio_data[portfolio_type] = {\n",
        "                                    'weights'        : portfolio_info['weights'],\n",
        "                                    'permnos'        : portfolio_info['permnos'],\n",
        "                                    'aligned_returns': aligned_returns\n",
        "                                }\n",
        "\n",
        "                for portfolio_type in ['long_only', 'short_only', 'long_short']:\n",
        "                    portfolio_data = portfolio_daily_data[portfolio_type]\n",
        "\n",
        "                    if len(portfolio_data['returns']) > 0:\n",
        "                        metrics = backtester.calculate_metrics(\n",
        "                            returns=portfolio_data['returns'],\n",
        "                            turnover_series=portfolio_data['turnovers']\n",
        "                        )\n",
        "\n",
        "                        rets = np.array(portfolio_data['returns'])\n",
        "                        tovs = np.array(portfolio_data['turnovers'])\n",
        "\n",
        "                        for tc in TC_GRID:\n",
        "                            tag = TC_TAG[tc]\n",
        "                            adj = rets - tovs * tc\n",
        "\n",
        "                            ann_ret = adj.mean() * 252\n",
        "                            ann_vol = adj.std(ddof=1) * np.sqrt(252)\n",
        "                            sharpe  = ann_ret / ann_vol if ann_vol > 0 else 0\n",
        "\n",
        "                            cum_adj = np.cumprod(1 + adj)\n",
        "                            mdd = ((cum_adj - np.maximum.accumulate(cum_adj)) /\n",
        "                                   np.maximum.accumulate(cum_adj)).min()\n",
        "\n",
        "                            metrics[f'{tag}_annual_return'] = ann_ret\n",
        "                            metrics[f'{tag}_annual_vol']    = ann_vol\n",
        "                            metrics[f'{tag}_sharpe']        = sharpe\n",
        "                            metrics[f'{tag}_max_drawdown']  = mdd\n",
        "\n",
        "                        summary_results.append({\n",
        "                            'scheme': scheme,\n",
        "                            'model': model_name,\n",
        "                            'window': window,\n",
        "                            'portfolio_type': portfolio_type,\n",
        "                            **metrics\n",
        "                        })\n",
        "\n",
        "                        rets_arr = np.array(portfolio_data['returns'])\n",
        "                        tovs_arr = np.array(portfolio_data['turnovers'])\n",
        "                        cum_no_tc = np.log1p(rets_arr).cumsum()\n",
        "\n",
        "                        tc_ret_dict = {}\n",
        "                        tc_cum_dict = {}\n",
        "                        for tc in TC_GRID:\n",
        "                            tag = TC_TAG[tc]\n",
        "                            r = rets_arr - tovs_arr * tc\n",
        "                            tc_ret_dict[tag] = r\n",
        "                            tc_cum_dict[tag] = np.log1p(r).cumsum()\n",
        "\n",
        "                        for i, date in enumerate(portfolio_data['dates']):\n",
        "                            row = {\n",
        "                                'scheme'        : scheme,\n",
        "                                'model'         : model_name,\n",
        "                                'window'        : window,\n",
        "                                'portfolio_type': portfolio_type,\n",
        "                                'date'          : str(date),\n",
        "                                'return'        : rets_arr[i],\n",
        "                                'turnover'      : tovs_arr[i],\n",
        "                                'cumulative'    : cum_no_tc[i],\n",
        "                            }\n",
        "                            for tag in TC_TAG.values():\n",
        "                                row[f'{tag}_return']     = tc_ret_dict[tag][i]\n",
        "                                row[f'{tag}_cumulative'] = tc_cum_dict[tag][i]\n",
        "\n",
        "                            daily_series_data.append(row)\n",
        "\n",
        "                if scheme == \"VW\" and len(all_y_true) > 0:\n",
        "                    y_all    = np.concatenate(all_y_true)\n",
        "                    yhat_all = np.concatenate(all_y_pred)\n",
        "                    perm_all = np.concatenate(all_permnos)\n",
        "                    meta_all = pd.concat(all_meta, ignore_index=True)\n",
        "\n",
        "                    k = X_test.shape[1]\n",
        "\n",
        "                    m1_metrics = overall_interval_metrics_method1(\n",
        "                        y_all, yhat_all, k,\n",
        "                        permnos_all=perm_all,\n",
        "                        meta_all=meta_all\n",
        "                    )\n",
        "\n",
        "                    full_pred_df = pd.concat(pred_rows, ignore_index=True)\n",
        "                    mean_ic, t_ic, pos_ic, _ = calc_ic_daily(full_pred_df, method='spearman')\n",
        "                    m1_metrics['RankIC_mean']  = mean_ic\n",
        "                    m1_metrics['RankIC_t']     = t_ic\n",
        "                    m1_metrics['RankIC_pos%']  = pos_ic\n",
        "\n",
        "                    save_metrics(m1_metrics, name=model_name, window=window,\n",
        "                        path=\"portfolio_metrics.csv\")\n",
        "\n",
        "    summary_df = pd.DataFrame(summary_results)\n",
        "    daily_df = pd.DataFrame(daily_series_data) if daily_series_data else pd.DataFrame()\n",
        "\n",
        "    tc_columns = [c for c in summary_df.columns if c.startswith('tc')]\n",
        "    summary_df[tc_columns] = summary_df[tc_columns].fillna(0.0)\n",
        "\n",
        "    def save_split_by_scheme(df, base_filename):\n",
        "        \"\"\"Helper function to save files split by scheme\"\"\"\n",
        "        if df.empty:\n",
        "            print(f\"Warning: DataFrame is empty, skipping save for {base_filename}\")\n",
        "            return None, None\n",
        "\n",
        "        vw_df = df[df['scheme'] == 'VW']\n",
        "        ew_df = df[df['scheme'] == 'EW']\n",
        "\n",
        "        vw_filename = f\"{base_filename}_VW.csv\"\n",
        "        ew_filename = f\"{base_filename}_EW.csv\"\n",
        "\n",
        "        vw_df.to_csv(vw_filename, index=False)\n",
        "        ew_df.to_csv(ew_filename, index=False)\n",
        "\n",
        "        print(f\"VW results saved to {vw_filename}\")\n",
        "        print(f\"EW results saved to {ew_filename}\")\n",
        "\n",
        "        return vw_filename, ew_filename\n",
        "\n",
        "    save_split_by_scheme(summary_df, \"portfolio_results_daily_rebalance\")\n",
        "\n",
        "    if not daily_df.empty:\n",
        "        save_split_by_scheme(daily_df, \"portfolio_daily_series\")\n",
        "\n",
        "    if pred_rows:\n",
        "        pred_df = pd.concat(pred_rows, ignore_index=True)\n",
        "        pred_df.to_csv(\"predictions_daily.csv\", index=False)\n",
        "        print(f\"Saved {len(pred_df)} prediction rows to predictions_daily.csv\")\n",
        "\n",
        "    print(f\"Generated {len(summary_results)} portfolio summary records\")\n",
        "    print(f\"Generated {len(daily_series_data)} daily series records\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()\n",
        "    gc.collect()\n",
        "\n",
        "    return summary_df, daily_df, backtester\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1NKPXF4fFBz"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtakHz77fFB0",
        "outputId": "97e73ed2-72e4-4644-c3bb-24193dc3ac9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting **Autoformer** quarterly expanding training 2015‑2024\n",
            "Using data from: /content/drive/MyDrive/ERP Data/all_window_datasets_scaled.npz\n",
            "\n",
            "=== Window = 5 ===\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip-Optuna] hyper-params loaded from existing model at 2015Q4\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=5, 2015Q4\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=5, 2016Q1\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=5, 2016Q2\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=5, 2016Q3\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=5, 2016Q4\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=5, 2017Q1\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=5, 2017Q2\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=5, 2017Q3\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=5, 2017Q4\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=5, 2018Q1\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=5, 2018Q2\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=5, 2018Q3\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=5, 2018Q4\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=5, 2019Q1\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=5, 2019Q2\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=5, 2019Q3\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=5, 2019Q4\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=5, 2020Q1\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=5, 2020Q2\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=5, 2020Q3\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=5, 2020Q4\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=5, 2021Q1\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=5, 2021Q2\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=5, 2021Q3\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=5, 2021Q4\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=5, 2022Q1\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=5, 2022Q2\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=5, 2022Q3\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=5, 2022Q4\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=5, 2023Q1\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=5, 2023Q2\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=5, 2023Q3\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=5, 2023Q4\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=5, 2024Q1\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=5, 2024Q2\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=5, 2024Q3\n",
            "\n",
            "=== Window = 21 ===\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=21, 2015Q4\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=21, 2016Q1\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=21, 2016Q2\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=21, 2016Q3\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=21, 2016Q4\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=21, 2017Q1\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=21, 2017Q2\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=21, 2017Q3\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=21, 2017Q4\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=21, 2018Q1\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=21, 2018Q2\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=21, 2018Q3\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=21, 2018Q4\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=21, 2019Q1\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=21, 2019Q2\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=21, 2019Q3\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=21, 2019Q4\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=21, 2020Q1\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=21, 2020Q2\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=21, 2020Q3\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=21, 2020Q4\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=21, 2021Q1\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=21, 2021Q2\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=21, 2021Q3\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=21, 2021Q4\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=21, 2022Q1\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=21, 2022Q2\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=21, 2022Q3\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=21, 2022Q4\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=21, 2023Q1\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=21, 2023Q2\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=21, 2023Q3\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=21, 2023Q4\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=21, 2024Q1\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=21, 2024Q2\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=21, 2024Q3\n",
            "\n",
            "=== Window = 252 ===\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=252, 2015Q4\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=252, 2016Q1\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=252, 2016Q2\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=252, 2016Q3\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=252, 2016Q4\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=252, 2017Q1\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=252, 2017Q2\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=252, 2017Q3\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=252, 2017Q4\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=252, 2018Q1\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=252, 2018Q2\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=252, 2018Q3\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=252, 2018Q4\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=252, 2019Q1\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=252, 2019Q2\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=252, 2019Q3\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=252, 2019Q4\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=252, 2020Q1\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=252, 2020Q2\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=252, 2020Q3\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=252, 2020Q4\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=252, 2021Q1\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=252, 2021Q2\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=252, 2021Q3\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=252, 2021Q4\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=252, 2022Q1\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=252, 2022Q2\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=252, 2022Q3\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=252, 2022Q4\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=252, 2023Q1\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=252, 2023Q2\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=252, 2023Q3\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=252, 2023Q4\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=252, 2024Q1\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=252, 2024Q2\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=252, 2024Q3\n",
            "\n",
            "=== Window = 512 ===\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=512, 2015Q4\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=512, 2016Q1\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=512, 2016Q2\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=512, 2016Q3\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=512, 2016Q4\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=512, 2017Q1\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=512, 2017Q2\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=512, 2017Q3\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=512, 2017Q4\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=512, 2018Q1\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=512, 2018Q2\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=512, 2018Q3\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=512, 2018Q4\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=512, 2019Q1\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=512, 2019Q2\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=512, 2019Q3\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=512, 2019Q4\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=512, 2020Q1\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=512, 2020Q2\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=512, 2020Q3\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=512, 2020Q4\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=512, 2021Q1\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=512, 2021Q2\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=512, 2021Q3\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=512, 2021Q4\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=512, 2022Q1\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=512, 2022Q2\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=512, 2022Q3\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Skip] Model already trained for window=512, 2022Q4\n",
            "\n",
            "[Window 512] 2023Q1\n",
            "    +3070 obs from 2022Q4 -> expanding size 174640\n",
            "    [Long Sequence] Applied optimized parameters for window 512\n",
            "    Attempting warm-start from previous model: 2022Q4\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "    Warm‑start from last quarter…\n",
            "    Partial‑fit on 174640 samples for 5 epochs …\n",
            "        Epoch  1/5: Train=1.040167, Val=0.679513\n",
            "        Epoch  3/5: Train=1.040236, Val=0.679517\n",
            "        Epoch  5/5: Train=1.040263, Val=0.679500\n",
            "[Saved] /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2023Q1.pth\n",
            "\n",
            "[Window 512] 2023Q2\n",
            "    +3064 obs from 2023Q1 -> expanding size 177704\n",
            "    Attempting warm-start from previous model: 2023Q1\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "    Warm‑start from last quarter…\n",
            "    Partial‑fit on 177704 samples for 5 epochs …\n",
            "        Epoch  1/5: Train=1.029514, Val=0.752803\n",
            "        Epoch  3/5: Train=1.029697, Val=0.752793\n",
            "        Epoch  5/5: Train=1.029661, Val=0.752639\n",
            "[Saved] /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2023Q2.pth\n",
            "\n",
            "[Window 512] 2023Q3\n",
            "    +3069 obs from 2023Q2 -> expanding size 180773\n",
            "    Attempting warm-start from previous model: 2023Q2\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "    Warm‑start from last quarter…\n",
            "    Partial‑fit on 180773 samples for 5 epochs …\n",
            "        Epoch  1/5: Train=1.020600, Val=0.805295\n",
            "        Epoch  3/5: Train=1.020689, Val=0.805494\n",
            "        Epoch  5/5: Train=1.020672, Val=0.805416\n",
            "[Saved] /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2023Q3.pth\n",
            "\n",
            "[Window 512] 2023Q4\n",
            "    +3121 obs from 2023Q3 -> expanding size 183894\n",
            "    Attempting warm-start from previous model: 2023Q3\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "    Warm‑start from last quarter…\n",
            "    Partial‑fit on 183894 samples for 5 epochs …\n",
            "        Epoch  1/5: Train=1.009452, Val=0.837607\n",
            "        Epoch  3/5: Train=1.009508, Val=0.837684\n",
            "        Epoch  5/5: Train=1.009442, Val=0.837728\n",
            "[Saved] /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2023Q4.pth\n",
            "\n",
            "[Window 512] 2024Q1\n",
            "    +3113 obs from 2023Q4 -> expanding size 187007\n",
            "    Attempting warm-start from previous model: 2023Q4\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "    Warm‑start from last quarter…\n",
            "    Partial‑fit on 187007 samples for 5 epochs …\n",
            "        Epoch  1/5: Train=1.006430, Val=0.838637\n",
            "        Epoch  3/5: Train=1.006285, Val=0.838605\n",
            "        Epoch  5/5: Train=1.006408, Val=0.838730\n",
            "[Saved] /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2024Q1.pth\n",
            "\n",
            "[Window 512] 2024Q2\n",
            "    +3026 obs from 2024Q1 -> expanding size 190033\n",
            "    Attempting warm-start from previous model: 2024Q1\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "    Warm‑start from last quarter…\n",
            "    Partial‑fit on 190033 samples for 5 epochs …\n",
            "        Epoch  1/5: Train=1.001067, Val=0.834697\n",
            "        Epoch  3/5: Train=1.001115, Val=0.834601\n",
            "        Epoch  5/5: Train=1.001040, Val=0.834602\n",
            "[Saved] /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2024Q2.pth\n",
            "\n",
            "[Window 512] 2024Q3\n",
            "    +3112 obs from 2024Q2 -> expanding size 193145\n",
            "    Attempting warm-start from previous model: 2024Q2\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "    Warm‑start from last quarter…\n",
            "    Partial‑fit on 193145 samples for 5 epochs …\n",
            "        Epoch  1/5: Train=1.004566, Val=0.763746\n",
            "        Epoch  3/5: Train=1.004354, Val=0.763880\n",
            "        Epoch  5/5: Train=1.004426, Val=0.763607\n",
            "[Saved] /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2024Q3.pth\n",
            "All Autoformer quarterly expanding models trained and saved to /content/drive/MyDrive/ERP Data/models_backup\n"
          ]
        }
      ],
      "source": [
        "train_autoformer_models_expanding_quarterly()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMnEnytF2Vn_",
        "outputId": "afa1e1fa-7f63-4ead-d85d-0fb3556a6b27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2020Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w21_2015Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 21\n",
            "  seq_len: 21\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.2934383608021032\n",
            "  learning_rate: 0.00011858650433000944\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w21_2016Q1.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 21\n",
            "  seq_len: 21\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.2934383608021032\n",
            "  learning_rate: 0.00011858650433000944\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w21_2016Q2.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 21\n",
            "  seq_len: 21\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.2934383608021032\n",
            "  learning_rate: 0.00011858650433000944\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w21_2016Q3.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 21\n",
            "  seq_len: 21\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.2934383608021032\n",
            "  learning_rate: 0.00011858650433000944\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w21_2016Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 21\n",
            "  seq_len: 21\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.2934383608021032\n",
            "  learning_rate: 0.00011858650433000944\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w21_2017Q1.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 21\n",
            "  seq_len: 21\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.2934383608021032\n",
            "  learning_rate: 0.00011858650433000944\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w21_2017Q2.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 21\n",
            "  seq_len: 21\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.2934383608021032\n",
            "  learning_rate: 0.00011858650433000944\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w21_2017Q3.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 21\n",
            "  seq_len: 21\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.2934383608021032\n",
            "  learning_rate: 0.00011858650433000944\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w21_2017Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 21\n",
            "  seq_len: 21\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.2934383608021032\n",
            "  learning_rate: 0.00011858650433000944\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w21_2018Q1.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 21\n",
            "  seq_len: 21\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.2934383608021032\n",
            "  learning_rate: 0.00011858650433000944\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w21_2018Q2.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 21\n",
            "  seq_len: 21\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.2934383608021032\n",
            "  learning_rate: 0.00011858650433000944\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w21_2018Q3.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 21\n",
            "  seq_len: 21\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.2934383608021032\n",
            "  learning_rate: 0.00011858650433000944\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w21_2018Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 21\n",
            "  seq_len: 21\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.2934383608021032\n",
            "  learning_rate: 0.00011858650433000944\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w21_2019Q1.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 21\n",
            "  seq_len: 21\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.2934383608021032\n",
            "  learning_rate: 0.00011858650433000944\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w21_2019Q2.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 21\n",
            "  seq_len: 21\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.2934383608021032\n",
            "  learning_rate: 0.00011858650433000944\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w21_2019Q3.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 21\n",
            "  seq_len: 21\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.2934383608021032\n",
            "  learning_rate: 0.00011858650433000944\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w21_2019Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 21\n",
            "  seq_len: 21\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.2934383608021032\n",
            "  learning_rate: 0.00011858650433000944\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w21_2020Q1.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 21\n",
            "  seq_len: 21\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.2934383608021032\n",
            "  learning_rate: 0.00011858650433000944\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w21_2020Q2.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 21\n",
            "  seq_len: 21\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.2934383608021032\n",
            "  learning_rate: 0.00011858650433000944\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w21_2020Q3.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 21\n",
            "  seq_len: 21\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.2934383608021032\n",
            "  learning_rate: 0.00011858650433000944\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w21_2020Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 21\n",
            "  seq_len: 21\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.2934383608021032\n",
            "  learning_rate: 0.00011858650433000944\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w21_2021Q1.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 21\n",
            "  seq_len: 21\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.2934383608021032\n",
            "  learning_rate: 0.00011858650433000944\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w21_2021Q2.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 21\n",
            "  seq_len: 21\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.2934383608021032\n",
            "  learning_rate: 0.00011858650433000944\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w21_2021Q3.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 21\n",
            "  seq_len: 21\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.2934383608021032\n",
            "  learning_rate: 0.00011858650433000944\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w21_2021Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 21\n",
            "  seq_len: 21\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.2934383608021032\n",
            "  learning_rate: 0.00011858650433000944\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w21_2022Q1.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 21\n",
            "  seq_len: 21\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.2934383608021032\n",
            "  learning_rate: 0.00011858650433000944\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w21_2022Q2.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 21\n",
            "  seq_len: 21\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.2934383608021032\n",
            "  learning_rate: 0.00011858650433000944\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w21_2022Q3.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 21\n",
            "  seq_len: 21\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.2934383608021032\n",
            "  learning_rate: 0.00011858650433000944\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w21_2022Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 21\n",
            "  seq_len: 21\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.2934383608021032\n",
            "  learning_rate: 0.00011858650433000944\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w21_2023Q1.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 21\n",
            "  seq_len: 21\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.2934383608021032\n",
            "  learning_rate: 0.00011858650433000944\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w21_2023Q2.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 21\n",
            "  seq_len: 21\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.2934383608021032\n",
            "  learning_rate: 0.00011858650433000944\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w21_2023Q3.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 21\n",
            "  seq_len: 21\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.2934383608021032\n",
            "  learning_rate: 0.00011858650433000944\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w21_2023Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 21\n",
            "  seq_len: 21\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.2934383608021032\n",
            "  learning_rate: 0.00011858650433000944\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w21_2024Q1.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 21\n",
            "  seq_len: 21\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.2934383608021032\n",
            "  learning_rate: 0.00011858650433000944\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w21_2024Q2.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 21\n",
            "  seq_len: 21\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.2934383608021032\n",
            "  learning_rate: 0.00011858650433000944\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w21_2024Q3.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 21\n",
            "  seq_len: 21\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.2934383608021032\n",
            "  learning_rate: 0.00011858650433000944\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w252_2015Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 252\n",
            "  seq_len: 252\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w252_2016Q1.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 252\n",
            "  seq_len: 252\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w252_2016Q2.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 252\n",
            "  seq_len: 252\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w252_2016Q3.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 252\n",
            "  seq_len: 252\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w252_2016Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 252\n",
            "  seq_len: 252\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w252_2017Q1.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 252\n",
            "  seq_len: 252\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w252_2017Q2.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 252\n",
            "  seq_len: 252\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w252_2017Q3.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 252\n",
            "  seq_len: 252\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w252_2017Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 252\n",
            "  seq_len: 252\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w252_2018Q1.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 252\n",
            "  seq_len: 252\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w252_2018Q2.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 252\n",
            "  seq_len: 252\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w252_2018Q3.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 252\n",
            "  seq_len: 252\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w252_2018Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 252\n",
            "  seq_len: 252\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w252_2019Q1.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 252\n",
            "  seq_len: 252\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w252_2019Q2.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 252\n",
            "  seq_len: 252\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w252_2019Q3.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 252\n",
            "  seq_len: 252\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w252_2019Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 252\n",
            "  seq_len: 252\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w252_2020Q1.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 252\n",
            "  seq_len: 252\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w252_2020Q2.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 252\n",
            "  seq_len: 252\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w252_2020Q3.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 252\n",
            "  seq_len: 252\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w252_2020Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 252\n",
            "  seq_len: 252\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w252_2021Q1.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 252\n",
            "  seq_len: 252\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w252_2021Q2.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 252\n",
            "  seq_len: 252\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w252_2021Q3.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 252\n",
            "  seq_len: 252\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w252_2021Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 252\n",
            "  seq_len: 252\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w252_2022Q1.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 252\n",
            "  seq_len: 252\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w252_2022Q2.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 252\n",
            "  seq_len: 252\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w252_2022Q3.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 252\n",
            "  seq_len: 252\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w252_2022Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 252\n",
            "  seq_len: 252\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w252_2023Q1.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 252\n",
            "  seq_len: 252\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w252_2023Q2.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 252\n",
            "  seq_len: 252\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w252_2023Q3.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 252\n",
            "  seq_len: 252\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w252_2023Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 252\n",
            "  seq_len: 252\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w252_2024Q1.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 252\n",
            "  seq_len: 252\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w252_2024Q2.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 252\n",
            "  seq_len: 252\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w252_2024Q3.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 252\n",
            "  seq_len: 252\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2015Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 512\n",
            "  seq_len: 512\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2016Q1.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 512\n",
            "  seq_len: 512\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2016Q2.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 512\n",
            "  seq_len: 512\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2016Q3.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 512\n",
            "  seq_len: 512\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2016Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 512\n",
            "  seq_len: 512\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2017Q1.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 512\n",
            "  seq_len: 512\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2017Q2.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 512\n",
            "  seq_len: 512\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2017Q3.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 512\n",
            "  seq_len: 512\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2017Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 512\n",
            "  seq_len: 512\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2018Q1.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 512\n",
            "  seq_len: 512\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2018Q2.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 512\n",
            "  seq_len: 512\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2018Q3.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 512\n",
            "  seq_len: 512\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2018Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 512\n",
            "  seq_len: 512\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2019Q1.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 512\n",
            "  seq_len: 512\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2019Q2.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 512\n",
            "  seq_len: 512\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2019Q3.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 512\n",
            "  seq_len: 512\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2019Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 512\n",
            "  seq_len: 512\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2020Q1.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 512\n",
            "  seq_len: 512\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2020Q2.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 512\n",
            "  seq_len: 512\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2020Q3.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 512\n",
            "  seq_len: 512\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2020Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 512\n",
            "  seq_len: 512\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2021Q1.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 512\n",
            "  seq_len: 512\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2021Q2.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 512\n",
            "  seq_len: 512\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2021Q3.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 512\n",
            "  seq_len: 512\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2021Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 512\n",
            "  seq_len: 512\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2022Q1.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 512\n",
            "  seq_len: 512\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2022Q2.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 512\n",
            "  seq_len: 512\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2022Q3.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 512\n",
            "  seq_len: 512\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2022Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 512\n",
            "  seq_len: 512\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2023Q1.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 512\n",
            "  seq_len: 512\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2023Q2.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 512\n",
            "  seq_len: 512\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2023Q3.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 512\n",
            "  seq_len: 512\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2023Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 512\n",
            "  seq_len: 512\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2024Q1.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 512\n",
            "  seq_len: 512\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2024Q2.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 512\n",
            "  seq_len: 512\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w512_2024Q3.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 512\n",
            "  seq_len: 512\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  e_layers: 1\n",
            "  d_ff: 128\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 25\n",
            "  warm_start_epochs: 5\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2015Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2016Q1.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2016Q2.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2016Q3.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2016Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2017Q1.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2017Q2.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2017Q3.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2017Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2018Q1.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2018Q2.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2018Q3.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2018Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2019Q1.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2019Q2.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2019Q3.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2019Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2020Q1.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2020Q2.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2020Q3.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2020Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2021Q1.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2021Q2.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2021Q3.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2021Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2022Q1.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2022Q2.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2022Q3.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2022Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2023Q1.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2023Q2.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2023Q3.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2023Q4.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2024Q1.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2024Q2.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n",
            "\n",
            "=== Inspecting: /content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2024Q3.pth ===\n",
            "Keys in checkpoint: ['state_dict', 'hyper_params']\n",
            "Found hyper_params:\n",
            "  input_size: 5\n",
            "  seq_len: 5\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  e_layers: 3\n",
            "  d_ff: 256\n",
            "  dropout: 0.37656227050693514\n",
            "  learning_rate: 7.515450322528411e-05\n",
            "  batch_size: 512\n",
            "  max_epochs: 30\n",
            "  warm_start_epochs: 8\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "def inspect_pth_hyperparams(pth_path: str):\n",
        "    \"\"\"\n",
        "    Load a .pth file and print its hyperparameters if available.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        ckpt = torch.load(pth_path, map_location='cpu')\n",
        "    except Exception as e:\n",
        "        print(f\"[Error] Failed to load {pth_path}: {e}\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n=== Inspecting: {pth_path} ===\")\n",
        "    print(\"Keys in checkpoint:\", list(ckpt.keys()))\n",
        "    if 'hyper_params' in ckpt:\n",
        "        print(\"Found hyper_params:\")\n",
        "        for k, v in ckpt['hyper_params'].items():\n",
        "            print(f\"  {k}: {v}\")\n",
        "    else:\n",
        "        print(\"No 'hyper_params' field found. This file may be an old format or only contains state_dict.\")\n",
        "\n",
        "def batch_inspect_dir(models_dir: str):\n",
        "    \"\"\"\n",
        "    Iterate all .pth files in the directory and call inspect_pth_hyperparams for each.\n",
        "    \"\"\"\n",
        "    pth_files = list(Path(models_dir).rglob(\"*.pth\"))\n",
        "    if not pth_files:\n",
        "        print(f\"No .pth files found in {models_dir}\")\n",
        "        return\n",
        "    for pth in sorted(pth_files):\n",
        "        inspect_pth_hyperparams(str(pth))\n",
        "\n",
        "inspect_pth_hyperparams(\"/content/drive/MyDrive/ERP Data/models_backup/Autoformer_w5_2020Q4.pth\")\n",
        "batch_inspect_dir(\"/content/drive/MyDrive/ERP Data/models_backup\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unJ71EaOfFB0",
        "outputId": "7a21763d-2ff4-4950-8bd4-858a765f9516"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Daily Rebalance Portfolio Backtesting Simulation\n",
            "Processing window size: 5\n",
            "  Model: Autoformer, Scheme: VW\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3006 samples\n",
            "[Info] Inverse transformed 3006 samples\n",
            "      Applied inverse scaling for window 5 - 3006 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3170 samples\n",
            "[Info] Inverse transformed 3170 samples\n",
            "      Applied inverse scaling for window 5 - 3170 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3176 samples\n",
            "[Info] Inverse transformed 3176 samples\n",
            "      Applied inverse scaling for window 5 - 3176 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3123 samples\n",
            "[Info] Inverse transformed 3123 samples\n",
            "      Applied inverse scaling for window 5 - 3123 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3083 samples\n",
            "[Info] Inverse transformed 3083 samples\n",
            "      Applied inverse scaling for window 5 - 3083 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3122 samples\n",
            "[Info] Inverse transformed 3122 samples\n",
            "      Applied inverse scaling for window 5 - 3122 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3114 samples\n",
            "[Info] Inverse transformed 3114 samples\n",
            "      Applied inverse scaling for window 5 - 3114 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3115 samples\n",
            "[Info] Inverse transformed 3115 samples\n",
            "      Applied inverse scaling for window 5 - 3115 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2996 samples\n",
            "[Info] Inverse transformed 2996 samples\n",
            "      Applied inverse scaling for window 5 - 2996 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3160 samples\n",
            "[Info] Inverse transformed 3160 samples\n",
            "      Applied inverse scaling for window 5 - 3160 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3125 samples\n",
            "[Info] Inverse transformed 3125 samples\n",
            "      Applied inverse scaling for window 5 - 3125 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3045 samples\n",
            "[Info] Inverse transformed 3045 samples\n",
            "      Applied inverse scaling for window 5 - 3045 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3022 samples\n",
            "[Info] Inverse transformed 3022 samples\n",
            "      Applied inverse scaling for window 5 - 3022 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3120 samples\n",
            "[Info] Inverse transformed 3120 samples\n",
            "      Applied inverse scaling for window 5 - 3120 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3167 samples\n",
            "[Info] Inverse transformed 3167 samples\n",
            "      Applied inverse scaling for window 5 - 3167 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3179 samples\n",
            "[Info] Inverse transformed 3179 samples\n",
            "      Applied inverse scaling for window 5 - 3179 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2595 samples\n",
            "[Info] Inverse transformed 2595 samples\n",
            "      Applied inverse scaling for window 5 - 2595 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2839 samples\n",
            "[Info] Inverse transformed 2839 samples\n",
            "      Applied inverse scaling for window 5 - 2839 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3151 samples\n",
            "[Info] Inverse transformed 3151 samples\n",
            "      Applied inverse scaling for window 5 - 3151 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3114 samples\n",
            "[Info] Inverse transformed 3114 samples\n",
            "      Applied inverse scaling for window 5 - 3114 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2989 samples\n",
            "[Info] Inverse transformed 2989 samples\n",
            "      Applied inverse scaling for window 5 - 2989 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3132 samples\n",
            "[Info] Inverse transformed 3132 samples\n",
            "      Applied inverse scaling for window 5 - 3132 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3173 samples\n",
            "[Info] Inverse transformed 3173 samples\n",
            "      Applied inverse scaling for window 5 - 3173 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3153 samples\n",
            "[Info] Inverse transformed 3153 samples\n",
            "      Applied inverse scaling for window 5 - 3153 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3029 samples\n",
            "[Info] Inverse transformed 3029 samples\n",
            "      Applied inverse scaling for window 5 - 3029 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2969 samples\n",
            "[Info] Inverse transformed 2969 samples\n",
            "      Applied inverse scaling for window 5 - 2969 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3152 samples\n",
            "[Info] Inverse transformed 3152 samples\n",
            "      Applied inverse scaling for window 5 - 3152 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3070 samples\n",
            "[Info] Inverse transformed 3070 samples\n",
            "      Applied inverse scaling for window 5 - 3070 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3064 samples\n",
            "[Info] Inverse transformed 3064 samples\n",
            "      Applied inverse scaling for window 5 - 3064 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3069 samples\n",
            "[Info] Inverse transformed 3069 samples\n",
            "      Applied inverse scaling for window 5 - 3069 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3121 samples\n",
            "[Info] Inverse transformed 3121 samples\n",
            "      Applied inverse scaling for window 5 - 3121 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3113 samples\n",
            "[Info] Inverse transformed 3113 samples\n",
            "      Applied inverse scaling for window 5 - 3113 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3026 samples\n",
            "[Info] Inverse transformed 3026 samples\n",
            "      Applied inverse scaling for window 5 - 3026 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3112 samples\n",
            "[Info] Inverse transformed 3112 samples\n",
            "      Applied inverse scaling for window 5 - 3112 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3162 samples\n",
            "[Info] Inverse transformed 3162 samples\n",
            "      Applied inverse scaling for window 5 - 3162 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3094 samples\n",
            "[Info] Inverse transformed 3094 samples\n",
            "      Applied inverse scaling for window 5 - 3094 predictions\n",
            "[Create] New metrics file created with Autoformer w=5\n",
            "  Model: Autoformer, Scheme: EW\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3006 samples\n",
            "[Info] Inverse transformed 3006 samples\n",
            "      Applied inverse scaling for window 5 - 3006 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3170 samples\n",
            "[Info] Inverse transformed 3170 samples\n",
            "      Applied inverse scaling for window 5 - 3170 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3176 samples\n",
            "[Info] Inverse transformed 3176 samples\n",
            "      Applied inverse scaling for window 5 - 3176 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3123 samples\n",
            "[Info] Inverse transformed 3123 samples\n",
            "      Applied inverse scaling for window 5 - 3123 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3083 samples\n",
            "[Info] Inverse transformed 3083 samples\n",
            "      Applied inverse scaling for window 5 - 3083 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3122 samples\n",
            "[Info] Inverse transformed 3122 samples\n",
            "      Applied inverse scaling for window 5 - 3122 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3114 samples\n",
            "[Info] Inverse transformed 3114 samples\n",
            "      Applied inverse scaling for window 5 - 3114 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3115 samples\n",
            "[Info] Inverse transformed 3115 samples\n",
            "      Applied inverse scaling for window 5 - 3115 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2996 samples\n",
            "[Info] Inverse transformed 2996 samples\n",
            "      Applied inverse scaling for window 5 - 2996 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3160 samples\n",
            "[Info] Inverse transformed 3160 samples\n",
            "      Applied inverse scaling for window 5 - 3160 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3125 samples\n",
            "[Info] Inverse transformed 3125 samples\n",
            "      Applied inverse scaling for window 5 - 3125 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3045 samples\n",
            "[Info] Inverse transformed 3045 samples\n",
            "      Applied inverse scaling for window 5 - 3045 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3022 samples\n",
            "[Info] Inverse transformed 3022 samples\n",
            "      Applied inverse scaling for window 5 - 3022 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3120 samples\n",
            "[Info] Inverse transformed 3120 samples\n",
            "      Applied inverse scaling for window 5 - 3120 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3167 samples\n",
            "[Info] Inverse transformed 3167 samples\n",
            "      Applied inverse scaling for window 5 - 3167 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3179 samples\n",
            "[Info] Inverse transformed 3179 samples\n",
            "      Applied inverse scaling for window 5 - 3179 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2595 samples\n",
            "[Info] Inverse transformed 2595 samples\n",
            "      Applied inverse scaling for window 5 - 2595 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2839 samples\n",
            "[Info] Inverse transformed 2839 samples\n",
            "      Applied inverse scaling for window 5 - 2839 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3151 samples\n",
            "[Info] Inverse transformed 3151 samples\n",
            "      Applied inverse scaling for window 5 - 3151 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3114 samples\n",
            "[Info] Inverse transformed 3114 samples\n",
            "      Applied inverse scaling for window 5 - 3114 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2989 samples\n",
            "[Info] Inverse transformed 2989 samples\n",
            "      Applied inverse scaling for window 5 - 2989 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3132 samples\n",
            "[Info] Inverse transformed 3132 samples\n",
            "      Applied inverse scaling for window 5 - 3132 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3173 samples\n",
            "[Info] Inverse transformed 3173 samples\n",
            "      Applied inverse scaling for window 5 - 3173 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3153 samples\n",
            "[Info] Inverse transformed 3153 samples\n",
            "      Applied inverse scaling for window 5 - 3153 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3029 samples\n",
            "[Info] Inverse transformed 3029 samples\n",
            "      Applied inverse scaling for window 5 - 3029 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2969 samples\n",
            "[Info] Inverse transformed 2969 samples\n",
            "      Applied inverse scaling for window 5 - 2969 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3152 samples\n",
            "[Info] Inverse transformed 3152 samples\n",
            "      Applied inverse scaling for window 5 - 3152 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3070 samples\n",
            "[Info] Inverse transformed 3070 samples\n",
            "      Applied inverse scaling for window 5 - 3070 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3064 samples\n",
            "[Info] Inverse transformed 3064 samples\n",
            "      Applied inverse scaling for window 5 - 3064 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3069 samples\n",
            "[Info] Inverse transformed 3069 samples\n",
            "      Applied inverse scaling for window 5 - 3069 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3121 samples\n",
            "[Info] Inverse transformed 3121 samples\n",
            "      Applied inverse scaling for window 5 - 3121 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3113 samples\n",
            "[Info] Inverse transformed 3113 samples\n",
            "      Applied inverse scaling for window 5 - 3113 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3026 samples\n",
            "[Info] Inverse transformed 3026 samples\n",
            "      Applied inverse scaling for window 5 - 3026 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3112 samples\n",
            "[Info] Inverse transformed 3112 samples\n",
            "      Applied inverse scaling for window 5 - 3112 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3162 samples\n",
            "[Info] Inverse transformed 3162 samples\n",
            "      Applied inverse scaling for window 5 - 3162 predictions\n",
            "[Autoformer] Using sequence length seq_len=5, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3094 samples\n",
            "[Info] Inverse transformed 3094 samples\n",
            "      Applied inverse scaling for window 5 - 3094 predictions\n",
            "Processing window size: 21\n",
            "  Model: Autoformer, Scheme: VW\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3006 samples\n",
            "[Info] Inverse transformed 3006 samples\n",
            "      Applied inverse scaling for window 21 - 3006 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3170 samples\n",
            "[Info] Inverse transformed 3170 samples\n",
            "      Applied inverse scaling for window 21 - 3170 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3176 samples\n",
            "[Info] Inverse transformed 3176 samples\n",
            "      Applied inverse scaling for window 21 - 3176 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3123 samples\n",
            "[Info] Inverse transformed 3123 samples\n",
            "      Applied inverse scaling for window 21 - 3123 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3083 samples\n",
            "[Info] Inverse transformed 3083 samples\n",
            "      Applied inverse scaling for window 21 - 3083 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3122 samples\n",
            "[Info] Inverse transformed 3122 samples\n",
            "      Applied inverse scaling for window 21 - 3122 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3114 samples\n",
            "[Info] Inverse transformed 3114 samples\n",
            "      Applied inverse scaling for window 21 - 3114 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3115 samples\n",
            "[Info] Inverse transformed 3115 samples\n",
            "      Applied inverse scaling for window 21 - 3115 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2996 samples\n",
            "[Info] Inverse transformed 2996 samples\n",
            "      Applied inverse scaling for window 21 - 2996 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3160 samples\n",
            "[Info] Inverse transformed 3160 samples\n",
            "      Applied inverse scaling for window 21 - 3160 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3125 samples\n",
            "[Info] Inverse transformed 3125 samples\n",
            "      Applied inverse scaling for window 21 - 3125 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3045 samples\n",
            "[Info] Inverse transformed 3045 samples\n",
            "      Applied inverse scaling for window 21 - 3045 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3022 samples\n",
            "[Info] Inverse transformed 3022 samples\n",
            "      Applied inverse scaling for window 21 - 3022 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3120 samples\n",
            "[Info] Inverse transformed 3120 samples\n",
            "      Applied inverse scaling for window 21 - 3120 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3167 samples\n",
            "[Info] Inverse transformed 3167 samples\n",
            "      Applied inverse scaling for window 21 - 3167 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3179 samples\n",
            "[Info] Inverse transformed 3179 samples\n",
            "      Applied inverse scaling for window 21 - 3179 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2595 samples\n",
            "[Info] Inverse transformed 2595 samples\n",
            "      Applied inverse scaling for window 21 - 2595 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2839 samples\n",
            "[Info] Inverse transformed 2839 samples\n",
            "      Applied inverse scaling for window 21 - 2839 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3151 samples\n",
            "[Info] Inverse transformed 3151 samples\n",
            "      Applied inverse scaling for window 21 - 3151 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3114 samples\n",
            "[Info] Inverse transformed 3114 samples\n",
            "      Applied inverse scaling for window 21 - 3114 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2989 samples\n",
            "[Info] Inverse transformed 2989 samples\n",
            "      Applied inverse scaling for window 21 - 2989 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3132 samples\n",
            "[Info] Inverse transformed 3132 samples\n",
            "      Applied inverse scaling for window 21 - 3132 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3173 samples\n",
            "[Info] Inverse transformed 3173 samples\n",
            "      Applied inverse scaling for window 21 - 3173 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3153 samples\n",
            "[Info] Inverse transformed 3153 samples\n",
            "      Applied inverse scaling for window 21 - 3153 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3029 samples\n",
            "[Info] Inverse transformed 3029 samples\n",
            "      Applied inverse scaling for window 21 - 3029 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2969 samples\n",
            "[Info] Inverse transformed 2969 samples\n",
            "      Applied inverse scaling for window 21 - 2969 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3152 samples\n",
            "[Info] Inverse transformed 3152 samples\n",
            "      Applied inverse scaling for window 21 - 3152 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3070 samples\n",
            "[Info] Inverse transformed 3070 samples\n",
            "      Applied inverse scaling for window 21 - 3070 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3064 samples\n",
            "[Info] Inverse transformed 3064 samples\n",
            "      Applied inverse scaling for window 21 - 3064 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3069 samples\n",
            "[Info] Inverse transformed 3069 samples\n",
            "      Applied inverse scaling for window 21 - 3069 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3121 samples\n",
            "[Info] Inverse transformed 3121 samples\n",
            "      Applied inverse scaling for window 21 - 3121 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3113 samples\n",
            "[Info] Inverse transformed 3113 samples\n",
            "      Applied inverse scaling for window 21 - 3113 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3026 samples\n",
            "[Info] Inverse transformed 3026 samples\n",
            "      Applied inverse scaling for window 21 - 3026 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3112 samples\n",
            "[Info] Inverse transformed 3112 samples\n",
            "      Applied inverse scaling for window 21 - 3112 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3162 samples\n",
            "[Info] Inverse transformed 3162 samples\n",
            "      Applied inverse scaling for window 21 - 3162 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3094 samples\n",
            "[Info] Inverse transformed 3094 samples\n",
            "      Applied inverse scaling for window 21 - 3094 predictions\n",
            "[Update] Metrics updated for Autoformer w=21\n",
            "  Model: Autoformer, Scheme: EW\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3006 samples\n",
            "[Info] Inverse transformed 3006 samples\n",
            "      Applied inverse scaling for window 21 - 3006 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3170 samples\n",
            "[Info] Inverse transformed 3170 samples\n",
            "      Applied inverse scaling for window 21 - 3170 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3176 samples\n",
            "[Info] Inverse transformed 3176 samples\n",
            "      Applied inverse scaling for window 21 - 3176 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3123 samples\n",
            "[Info] Inverse transformed 3123 samples\n",
            "      Applied inverse scaling for window 21 - 3123 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3083 samples\n",
            "[Info] Inverse transformed 3083 samples\n",
            "      Applied inverse scaling for window 21 - 3083 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3122 samples\n",
            "[Info] Inverse transformed 3122 samples\n",
            "      Applied inverse scaling for window 21 - 3122 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3114 samples\n",
            "[Info] Inverse transformed 3114 samples\n",
            "      Applied inverse scaling for window 21 - 3114 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3115 samples\n",
            "[Info] Inverse transformed 3115 samples\n",
            "      Applied inverse scaling for window 21 - 3115 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2996 samples\n",
            "[Info] Inverse transformed 2996 samples\n",
            "      Applied inverse scaling for window 21 - 2996 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3160 samples\n",
            "[Info] Inverse transformed 3160 samples\n",
            "      Applied inverse scaling for window 21 - 3160 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3125 samples\n",
            "[Info] Inverse transformed 3125 samples\n",
            "      Applied inverse scaling for window 21 - 3125 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3045 samples\n",
            "[Info] Inverse transformed 3045 samples\n",
            "      Applied inverse scaling for window 21 - 3045 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3022 samples\n",
            "[Info] Inverse transformed 3022 samples\n",
            "      Applied inverse scaling for window 21 - 3022 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3120 samples\n",
            "[Info] Inverse transformed 3120 samples\n",
            "      Applied inverse scaling for window 21 - 3120 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3167 samples\n",
            "[Info] Inverse transformed 3167 samples\n",
            "      Applied inverse scaling for window 21 - 3167 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3179 samples\n",
            "[Info] Inverse transformed 3179 samples\n",
            "      Applied inverse scaling for window 21 - 3179 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2595 samples\n",
            "[Info] Inverse transformed 2595 samples\n",
            "      Applied inverse scaling for window 21 - 2595 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2839 samples\n",
            "[Info] Inverse transformed 2839 samples\n",
            "      Applied inverse scaling for window 21 - 2839 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3151 samples\n",
            "[Info] Inverse transformed 3151 samples\n",
            "      Applied inverse scaling for window 21 - 3151 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3114 samples\n",
            "[Info] Inverse transformed 3114 samples\n",
            "      Applied inverse scaling for window 21 - 3114 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2989 samples\n",
            "[Info] Inverse transformed 2989 samples\n",
            "      Applied inverse scaling for window 21 - 2989 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3132 samples\n",
            "[Info] Inverse transformed 3132 samples\n",
            "      Applied inverse scaling for window 21 - 3132 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3173 samples\n",
            "[Info] Inverse transformed 3173 samples\n",
            "      Applied inverse scaling for window 21 - 3173 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3153 samples\n",
            "[Info] Inverse transformed 3153 samples\n",
            "      Applied inverse scaling for window 21 - 3153 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3029 samples\n",
            "[Info] Inverse transformed 3029 samples\n",
            "      Applied inverse scaling for window 21 - 3029 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2969 samples\n",
            "[Info] Inverse transformed 2969 samples\n",
            "      Applied inverse scaling for window 21 - 2969 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3152 samples\n",
            "[Info] Inverse transformed 3152 samples\n",
            "      Applied inverse scaling for window 21 - 3152 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3070 samples\n",
            "[Info] Inverse transformed 3070 samples\n",
            "      Applied inverse scaling for window 21 - 3070 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3064 samples\n",
            "[Info] Inverse transformed 3064 samples\n",
            "      Applied inverse scaling for window 21 - 3064 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3069 samples\n",
            "[Info] Inverse transformed 3069 samples\n",
            "      Applied inverse scaling for window 21 - 3069 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3121 samples\n",
            "[Info] Inverse transformed 3121 samples\n",
            "      Applied inverse scaling for window 21 - 3121 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3113 samples\n",
            "[Info] Inverse transformed 3113 samples\n",
            "      Applied inverse scaling for window 21 - 3113 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3026 samples\n",
            "[Info] Inverse transformed 3026 samples\n",
            "      Applied inverse scaling for window 21 - 3026 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3112 samples\n",
            "[Info] Inverse transformed 3112 samples\n",
            "      Applied inverse scaling for window 21 - 3112 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3162 samples\n",
            "[Info] Inverse transformed 3162 samples\n",
            "      Applied inverse scaling for window 21 - 3162 predictions\n",
            "[Autoformer] Using sequence length seq_len=21, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3094 samples\n",
            "[Info] Inverse transformed 3094 samples\n",
            "      Applied inverse scaling for window 21 - 3094 predictions\n",
            "Processing window size: 252\n",
            "  Model: Autoformer, Scheme: VW\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3006 samples\n",
            "[Info] Inverse transformed 3006 samples\n",
            "      Applied inverse scaling for window 252 - 3006 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3170 samples\n",
            "[Info] Inverse transformed 3170 samples\n",
            "      Applied inverse scaling for window 252 - 3170 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3176 samples\n",
            "[Info] Inverse transformed 3176 samples\n",
            "      Applied inverse scaling for window 252 - 3176 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3123 samples\n",
            "[Info] Inverse transformed 3123 samples\n",
            "      Applied inverse scaling for window 252 - 3123 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3083 samples\n",
            "[Info] Inverse transformed 3083 samples\n",
            "      Applied inverse scaling for window 252 - 3083 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3122 samples\n",
            "[Info] Inverse transformed 3122 samples\n",
            "      Applied inverse scaling for window 252 - 3122 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3114 samples\n",
            "[Info] Inverse transformed 3114 samples\n",
            "      Applied inverse scaling for window 252 - 3114 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3115 samples\n",
            "[Info] Inverse transformed 3115 samples\n",
            "      Applied inverse scaling for window 252 - 3115 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2996 samples\n",
            "[Info] Inverse transformed 2996 samples\n",
            "      Applied inverse scaling for window 252 - 2996 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3160 samples\n",
            "[Info] Inverse transformed 3160 samples\n",
            "      Applied inverse scaling for window 252 - 3160 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3125 samples\n",
            "[Info] Inverse transformed 3125 samples\n",
            "      Applied inverse scaling for window 252 - 3125 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3045 samples\n",
            "[Info] Inverse transformed 3045 samples\n",
            "      Applied inverse scaling for window 252 - 3045 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3022 samples\n",
            "[Info] Inverse transformed 3022 samples\n",
            "      Applied inverse scaling for window 252 - 3022 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3120 samples\n",
            "[Info] Inverse transformed 3120 samples\n",
            "      Applied inverse scaling for window 252 - 3120 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3167 samples\n",
            "[Info] Inverse transformed 3167 samples\n",
            "      Applied inverse scaling for window 252 - 3167 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3179 samples\n",
            "[Info] Inverse transformed 3179 samples\n",
            "      Applied inverse scaling for window 252 - 3179 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2595 samples\n",
            "[Info] Inverse transformed 2595 samples\n",
            "      Applied inverse scaling for window 252 - 2595 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2839 samples\n",
            "[Info] Inverse transformed 2839 samples\n",
            "      Applied inverse scaling for window 252 - 2839 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3151 samples\n",
            "[Info] Inverse transformed 3151 samples\n",
            "      Applied inverse scaling for window 252 - 3151 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3114 samples\n",
            "[Info] Inverse transformed 3114 samples\n",
            "      Applied inverse scaling for window 252 - 3114 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2989 samples\n",
            "[Info] Inverse transformed 2989 samples\n",
            "      Applied inverse scaling for window 252 - 2989 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3132 samples\n",
            "[Info] Inverse transformed 3132 samples\n",
            "      Applied inverse scaling for window 252 - 3132 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3173 samples\n",
            "[Info] Inverse transformed 3173 samples\n",
            "      Applied inverse scaling for window 252 - 3173 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3153 samples\n",
            "[Info] Inverse transformed 3153 samples\n",
            "      Applied inverse scaling for window 252 - 3153 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3029 samples\n",
            "[Info] Inverse transformed 3029 samples\n",
            "      Applied inverse scaling for window 252 - 3029 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2969 samples\n",
            "[Info] Inverse transformed 2969 samples\n",
            "      Applied inverse scaling for window 252 - 2969 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3152 samples\n",
            "[Info] Inverse transformed 3152 samples\n",
            "      Applied inverse scaling for window 252 - 3152 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3070 samples\n",
            "[Info] Inverse transformed 3070 samples\n",
            "      Applied inverse scaling for window 252 - 3070 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3064 samples\n",
            "[Info] Inverse transformed 3064 samples\n",
            "      Applied inverse scaling for window 252 - 3064 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3069 samples\n",
            "[Info] Inverse transformed 3069 samples\n",
            "      Applied inverse scaling for window 252 - 3069 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3121 samples\n",
            "[Info] Inverse transformed 3121 samples\n",
            "      Applied inverse scaling for window 252 - 3121 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3113 samples\n",
            "[Info] Inverse transformed 3113 samples\n",
            "      Applied inverse scaling for window 252 - 3113 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3026 samples\n",
            "[Info] Inverse transformed 3026 samples\n",
            "      Applied inverse scaling for window 252 - 3026 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3112 samples\n",
            "[Info] Inverse transformed 3112 samples\n",
            "      Applied inverse scaling for window 252 - 3112 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3162 samples\n",
            "[Info] Inverse transformed 3162 samples\n",
            "      Applied inverse scaling for window 252 - 3162 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3094 samples\n",
            "[Info] Inverse transformed 3094 samples\n",
            "      Applied inverse scaling for window 252 - 3094 predictions\n",
            "[Update] Metrics updated for Autoformer w=252\n",
            "  Model: Autoformer, Scheme: EW\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3006 samples\n",
            "[Info] Inverse transformed 3006 samples\n",
            "      Applied inverse scaling for window 252 - 3006 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3170 samples\n",
            "[Info] Inverse transformed 3170 samples\n",
            "      Applied inverse scaling for window 252 - 3170 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3176 samples\n",
            "[Info] Inverse transformed 3176 samples\n",
            "      Applied inverse scaling for window 252 - 3176 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3123 samples\n",
            "[Info] Inverse transformed 3123 samples\n",
            "      Applied inverse scaling for window 252 - 3123 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3083 samples\n",
            "[Info] Inverse transformed 3083 samples\n",
            "      Applied inverse scaling for window 252 - 3083 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3122 samples\n",
            "[Info] Inverse transformed 3122 samples\n",
            "      Applied inverse scaling for window 252 - 3122 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3114 samples\n",
            "[Info] Inverse transformed 3114 samples\n",
            "      Applied inverse scaling for window 252 - 3114 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3115 samples\n",
            "[Info] Inverse transformed 3115 samples\n",
            "      Applied inverse scaling for window 252 - 3115 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2996 samples\n",
            "[Info] Inverse transformed 2996 samples\n",
            "      Applied inverse scaling for window 252 - 2996 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3160 samples\n",
            "[Info] Inverse transformed 3160 samples\n",
            "      Applied inverse scaling for window 252 - 3160 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3125 samples\n",
            "[Info] Inverse transformed 3125 samples\n",
            "      Applied inverse scaling for window 252 - 3125 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3045 samples\n",
            "[Info] Inverse transformed 3045 samples\n",
            "      Applied inverse scaling for window 252 - 3045 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3022 samples\n",
            "[Info] Inverse transformed 3022 samples\n",
            "      Applied inverse scaling for window 252 - 3022 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3120 samples\n",
            "[Info] Inverse transformed 3120 samples\n",
            "      Applied inverse scaling for window 252 - 3120 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3167 samples\n",
            "[Info] Inverse transformed 3167 samples\n",
            "      Applied inverse scaling for window 252 - 3167 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3179 samples\n",
            "[Info] Inverse transformed 3179 samples\n",
            "      Applied inverse scaling for window 252 - 3179 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2595 samples\n",
            "[Info] Inverse transformed 2595 samples\n",
            "      Applied inverse scaling for window 252 - 2595 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2839 samples\n",
            "[Info] Inverse transformed 2839 samples\n",
            "      Applied inverse scaling for window 252 - 2839 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3151 samples\n",
            "[Info] Inverse transformed 3151 samples\n",
            "      Applied inverse scaling for window 252 - 3151 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3114 samples\n",
            "[Info] Inverse transformed 3114 samples\n",
            "      Applied inverse scaling for window 252 - 3114 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2989 samples\n",
            "[Info] Inverse transformed 2989 samples\n",
            "      Applied inverse scaling for window 252 - 2989 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3132 samples\n",
            "[Info] Inverse transformed 3132 samples\n",
            "      Applied inverse scaling for window 252 - 3132 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3173 samples\n",
            "[Info] Inverse transformed 3173 samples\n",
            "      Applied inverse scaling for window 252 - 3173 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3153 samples\n",
            "[Info] Inverse transformed 3153 samples\n",
            "      Applied inverse scaling for window 252 - 3153 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3029 samples\n",
            "[Info] Inverse transformed 3029 samples\n",
            "      Applied inverse scaling for window 252 - 3029 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2969 samples\n",
            "[Info] Inverse transformed 2969 samples\n",
            "      Applied inverse scaling for window 252 - 2969 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3152 samples\n",
            "[Info] Inverse transformed 3152 samples\n",
            "      Applied inverse scaling for window 252 - 3152 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3070 samples\n",
            "[Info] Inverse transformed 3070 samples\n",
            "      Applied inverse scaling for window 252 - 3070 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3064 samples\n",
            "[Info] Inverse transformed 3064 samples\n",
            "      Applied inverse scaling for window 252 - 3064 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3069 samples\n",
            "[Info] Inverse transformed 3069 samples\n",
            "      Applied inverse scaling for window 252 - 3069 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3121 samples\n",
            "[Info] Inverse transformed 3121 samples\n",
            "      Applied inverse scaling for window 252 - 3121 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3113 samples\n",
            "[Info] Inverse transformed 3113 samples\n",
            "      Applied inverse scaling for window 252 - 3113 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3026 samples\n",
            "[Info] Inverse transformed 3026 samples\n",
            "      Applied inverse scaling for window 252 - 3026 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3112 samples\n",
            "[Info] Inverse transformed 3112 samples\n",
            "      Applied inverse scaling for window 252 - 3112 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3162 samples\n",
            "[Info] Inverse transformed 3162 samples\n",
            "      Applied inverse scaling for window 252 - 3162 predictions\n",
            "[Autoformer] Using sequence length seq_len=252, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3094 samples\n",
            "[Info] Inverse transformed 3094 samples\n",
            "      Applied inverse scaling for window 252 - 3094 predictions\n",
            "Processing window size: 512\n",
            "  Model: Autoformer, Scheme: VW\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3006 samples\n",
            "[Info] Inverse transformed 3006 samples\n",
            "      Applied inverse scaling for window 512 - 3006 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3170 samples\n",
            "[Info] Inverse transformed 3170 samples\n",
            "      Applied inverse scaling for window 512 - 3170 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3176 samples\n",
            "[Info] Inverse transformed 3176 samples\n",
            "      Applied inverse scaling for window 512 - 3176 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3123 samples\n",
            "[Info] Inverse transformed 3123 samples\n",
            "      Applied inverse scaling for window 512 - 3123 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3083 samples\n",
            "[Info] Inverse transformed 3083 samples\n",
            "      Applied inverse scaling for window 512 - 3083 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3122 samples\n",
            "[Info] Inverse transformed 3122 samples\n",
            "      Applied inverse scaling for window 512 - 3122 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3114 samples\n",
            "[Info] Inverse transformed 3114 samples\n",
            "      Applied inverse scaling for window 512 - 3114 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3115 samples\n",
            "[Info] Inverse transformed 3115 samples\n",
            "      Applied inverse scaling for window 512 - 3115 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2996 samples\n",
            "[Info] Inverse transformed 2996 samples\n",
            "      Applied inverse scaling for window 512 - 2996 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3160 samples\n",
            "[Info] Inverse transformed 3160 samples\n",
            "      Applied inverse scaling for window 512 - 3160 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3125 samples\n",
            "[Info] Inverse transformed 3125 samples\n",
            "      Applied inverse scaling for window 512 - 3125 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3045 samples\n",
            "[Info] Inverse transformed 3045 samples\n",
            "      Applied inverse scaling for window 512 - 3045 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3022 samples\n",
            "[Info] Inverse transformed 3022 samples\n",
            "      Applied inverse scaling for window 512 - 3022 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3120 samples\n",
            "[Info] Inverse transformed 3120 samples\n",
            "      Applied inverse scaling for window 512 - 3120 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3167 samples\n",
            "[Info] Inverse transformed 3167 samples\n",
            "      Applied inverse scaling for window 512 - 3167 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3179 samples\n",
            "[Info] Inverse transformed 3179 samples\n",
            "      Applied inverse scaling for window 512 - 3179 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2595 samples\n",
            "[Info] Inverse transformed 2595 samples\n",
            "      Applied inverse scaling for window 512 - 2595 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2839 samples\n",
            "[Info] Inverse transformed 2839 samples\n",
            "      Applied inverse scaling for window 512 - 2839 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3151 samples\n",
            "[Info] Inverse transformed 3151 samples\n",
            "      Applied inverse scaling for window 512 - 3151 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3114 samples\n",
            "[Info] Inverse transformed 3114 samples\n",
            "      Applied inverse scaling for window 512 - 3114 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2989 samples\n",
            "[Info] Inverse transformed 2989 samples\n",
            "      Applied inverse scaling for window 512 - 2989 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3132 samples\n",
            "[Info] Inverse transformed 3132 samples\n",
            "      Applied inverse scaling for window 512 - 3132 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3173 samples\n",
            "[Info] Inverse transformed 3173 samples\n",
            "      Applied inverse scaling for window 512 - 3173 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3153 samples\n",
            "[Info] Inverse transformed 3153 samples\n",
            "      Applied inverse scaling for window 512 - 3153 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3029 samples\n",
            "[Info] Inverse transformed 3029 samples\n",
            "      Applied inverse scaling for window 512 - 3029 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2969 samples\n",
            "[Info] Inverse transformed 2969 samples\n",
            "      Applied inverse scaling for window 512 - 2969 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3152 samples\n",
            "[Info] Inverse transformed 3152 samples\n",
            "      Applied inverse scaling for window 512 - 3152 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3070 samples\n",
            "[Info] Inverse transformed 3070 samples\n",
            "      Applied inverse scaling for window 512 - 3070 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3064 samples\n",
            "[Info] Inverse transformed 3064 samples\n",
            "      Applied inverse scaling for window 512 - 3064 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3069 samples\n",
            "[Info] Inverse transformed 3069 samples\n",
            "      Applied inverse scaling for window 512 - 3069 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3121 samples\n",
            "[Info] Inverse transformed 3121 samples\n",
            "      Applied inverse scaling for window 512 - 3121 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3113 samples\n",
            "[Info] Inverse transformed 3113 samples\n",
            "      Applied inverse scaling for window 512 - 3113 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3026 samples\n",
            "[Info] Inverse transformed 3026 samples\n",
            "      Applied inverse scaling for window 512 - 3026 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3112 samples\n",
            "[Info] Inverse transformed 3112 samples\n",
            "      Applied inverse scaling for window 512 - 3112 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3162 samples\n",
            "[Info] Inverse transformed 3162 samples\n",
            "      Applied inverse scaling for window 512 - 3162 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3094 samples\n",
            "[Info] Inverse transformed 3094 samples\n",
            "      Applied inverse scaling for window 512 - 3094 predictions\n",
            "[Update] Metrics updated for Autoformer w=512\n",
            "  Model: Autoformer, Scheme: EW\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3006 samples\n",
            "[Info] Inverse transformed 3006 samples\n",
            "      Applied inverse scaling for window 512 - 3006 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3170 samples\n",
            "[Info] Inverse transformed 3170 samples\n",
            "      Applied inverse scaling for window 512 - 3170 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3176 samples\n",
            "[Info] Inverse transformed 3176 samples\n",
            "      Applied inverse scaling for window 512 - 3176 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3123 samples\n",
            "[Info] Inverse transformed 3123 samples\n",
            "      Applied inverse scaling for window 512 - 3123 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3083 samples\n",
            "[Info] Inverse transformed 3083 samples\n",
            "      Applied inverse scaling for window 512 - 3083 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3122 samples\n",
            "[Info] Inverse transformed 3122 samples\n",
            "      Applied inverse scaling for window 512 - 3122 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3114 samples\n",
            "[Info] Inverse transformed 3114 samples\n",
            "      Applied inverse scaling for window 512 - 3114 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3115 samples\n",
            "[Info] Inverse transformed 3115 samples\n",
            "      Applied inverse scaling for window 512 - 3115 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2996 samples\n",
            "[Info] Inverse transformed 2996 samples\n",
            "      Applied inverse scaling for window 512 - 2996 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3160 samples\n",
            "[Info] Inverse transformed 3160 samples\n",
            "      Applied inverse scaling for window 512 - 3160 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3125 samples\n",
            "[Info] Inverse transformed 3125 samples\n",
            "      Applied inverse scaling for window 512 - 3125 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3045 samples\n",
            "[Info] Inverse transformed 3045 samples\n",
            "      Applied inverse scaling for window 512 - 3045 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3022 samples\n",
            "[Info] Inverse transformed 3022 samples\n",
            "      Applied inverse scaling for window 512 - 3022 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3120 samples\n",
            "[Info] Inverse transformed 3120 samples\n",
            "      Applied inverse scaling for window 512 - 3120 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3167 samples\n",
            "[Info] Inverse transformed 3167 samples\n",
            "      Applied inverse scaling for window 512 - 3167 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3179 samples\n",
            "[Info] Inverse transformed 3179 samples\n",
            "      Applied inverse scaling for window 512 - 3179 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2595 samples\n",
            "[Info] Inverse transformed 2595 samples\n",
            "      Applied inverse scaling for window 512 - 2595 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2839 samples\n",
            "[Info] Inverse transformed 2839 samples\n",
            "      Applied inverse scaling for window 512 - 2839 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3151 samples\n",
            "[Info] Inverse transformed 3151 samples\n",
            "      Applied inverse scaling for window 512 - 3151 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3114 samples\n",
            "[Info] Inverse transformed 3114 samples\n",
            "      Applied inverse scaling for window 512 - 3114 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2989 samples\n",
            "[Info] Inverse transformed 2989 samples\n",
            "      Applied inverse scaling for window 512 - 2989 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3132 samples\n",
            "[Info] Inverse transformed 3132 samples\n",
            "      Applied inverse scaling for window 512 - 3132 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3173 samples\n",
            "[Info] Inverse transformed 3173 samples\n",
            "      Applied inverse scaling for window 512 - 3173 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3153 samples\n",
            "[Info] Inverse transformed 3153 samples\n",
            "      Applied inverse scaling for window 512 - 3153 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3029 samples\n",
            "[Info] Inverse transformed 3029 samples\n",
            "      Applied inverse scaling for window 512 - 3029 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 2969 samples\n",
            "[Info] Inverse transformed 2969 samples\n",
            "      Applied inverse scaling for window 512 - 2969 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3152 samples\n",
            "[Info] Inverse transformed 3152 samples\n",
            "      Applied inverse scaling for window 512 - 3152 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3070 samples\n",
            "[Info] Inverse transformed 3070 samples\n",
            "      Applied inverse scaling for window 512 - 3070 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3064 samples\n",
            "[Info] Inverse transformed 3064 samples\n",
            "      Applied inverse scaling for window 512 - 3064 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3069 samples\n",
            "[Info] Inverse transformed 3069 samples\n",
            "      Applied inverse scaling for window 512 - 3069 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3121 samples\n",
            "[Info] Inverse transformed 3121 samples\n",
            "      Applied inverse scaling for window 512 - 3121 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3113 samples\n",
            "[Info] Inverse transformed 3113 samples\n",
            "      Applied inverse scaling for window 512 - 3113 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3026 samples\n",
            "[Info] Inverse transformed 3026 samples\n",
            "      Applied inverse scaling for window 512 - 3026 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3112 samples\n",
            "[Info] Inverse transformed 3112 samples\n",
            "      Applied inverse scaling for window 512 - 3112 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3162 samples\n",
            "[Info] Inverse transformed 3162 samples\n",
            "      Applied inverse scaling for window 512 - 3162 predictions\n",
            "[Autoformer] Using sequence length seq_len=512, feature dimension feature_dim=1\n",
            "[Info] Inverse transformed 3094 samples\n",
            "[Info] Inverse transformed 3094 samples\n",
            "      Applied inverse scaling for window 512 - 3094 predictions\n",
            "VW results saved to portfolio_results_daily_rebalance_VW.csv\n",
            "EW results saved to portfolio_results_daily_rebalance_EW.csv\n",
            "VW results saved to portfolio_daily_series_VW.csv\n",
            "EW results saved to portfolio_daily_series_EW.csv\n",
            "Saved 443400 prediction rows to predictions_daily.csv\n",
            "Generated 24 portfolio summary records\n",
            "Generated 51672 daily series records\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(   scheme       model  window portfolio_type  annual_return  annual_vol  \\\n",
              " 0      VW  Autoformer       5      long_only       0.229915    0.219225   \n",
              " 1      VW  Autoformer       5     short_only      -0.072934    0.200977   \n",
              " 2      VW  Autoformer       5     long_short       0.156980    0.230570   \n",
              " 3      EW  Autoformer       5      long_only       0.218786    0.228469   \n",
              " 4      EW  Autoformer       5     short_only      -0.087921    0.208340   \n",
              " 5      EW  Autoformer       5     long_short       0.130865    0.221968   \n",
              " 6      VW  Autoformer      21      long_only       0.139522    0.225618   \n",
              " 7      VW  Autoformer      21     short_only      -0.093289    0.185859   \n",
              " 8      VW  Autoformer      21     long_short       0.046233    0.228513   \n",
              " 9      EW  Autoformer      21      long_only       0.141756    0.239952   \n",
              " 10     EW  Autoformer      21     short_only      -0.092815    0.190245   \n",
              " 11     EW  Autoformer      21     long_short       0.048941    0.222173   \n",
              " 12     VW  Autoformer     252      long_only       0.140225    0.207322   \n",
              " 13     VW  Autoformer     252     short_only      -0.160286    0.194740   \n",
              " 14     VW  Autoformer     252     long_short      -0.020061    0.218681   \n",
              " 15     EW  Autoformer     252      long_only       0.166735    0.209493   \n",
              " 16     EW  Autoformer     252     short_only      -0.152011    0.198535   \n",
              " 17     EW  Autoformer     252     long_short       0.014724    0.211451   \n",
              " 18     VW  Autoformer     512      long_only       0.117648    0.212762   \n",
              " 19     VW  Autoformer     512     short_only      -0.179908    0.186875   \n",
              " 20     VW  Autoformer     512     long_short      -0.062260    0.216813   \n",
              " 21     EW  Autoformer     512      long_only       0.130621    0.218161   \n",
              " 22     EW  Autoformer     512     short_only      -0.181169    0.184312   \n",
              " 23     EW  Autoformer     512     long_short      -0.050548    0.209112   \n",
              " \n",
              "       sharpe  max_drawdown  max_1d_loss  avg_turnover  ...  tc20_sharpe  \\\n",
              " 0   1.048758      0.239559    -0.077703      1.269442  ...    -1.862821   \n",
              " 1  -0.362898      0.715505    -0.073426      1.216709  ...    -3.404153   \n",
              " 2   0.680837      0.232829    -0.075020      2.486292  ...    -4.706655   \n",
              " 3   0.957617      0.250712    -0.079536      1.026447  ...    -1.304437   \n",
              " 4  -0.422007      0.742209    -0.073426      1.003453  ...    -2.845950   \n",
              " 5   0.589567      0.377523    -0.062899      2.030548  ...    -3.997939   \n",
              " 6   0.618399      0.490164    -0.059278      1.332161  ...    -2.342802   \n",
              " 7  -0.501931      0.721353    -0.076739      1.514160  ...    -4.574063   \n",
              " 8   0.202322      0.469060    -0.073236      2.846750  ...    -6.001269   \n",
              " 9   0.590770      0.418169    -0.057260      1.082988  ...    -1.680287   \n",
              " 10 -0.487872      0.674011    -0.077158      1.331492  ...    -4.002161   \n",
              " 11  0.220284      0.447087    -0.054901      2.415268  ...    -5.209473   \n",
              " 12  0.676364      0.379013    -0.068088      1.817624  ...    -3.733144   \n",
              " 13 -0.823077      0.838786    -0.073426      1.808987  ...    -5.490847   \n",
              " 14 -0.091738      0.730017    -0.070493      3.626482  ...    -8.413373   \n",
              " 15  0.795897      0.290073    -0.061060      1.720467  ...    -3.338831   \n",
              " 16 -0.765664      0.805057    -0.073426      1.706764  ...    -5.082485   \n",
              " 17  0.069634      0.363287    -0.062478      3.426954  ...    -8.063270   \n",
              " 18  0.552954      0.366811    -0.068088      1.816631  ...    -3.744580   \n",
              " 19 -0.962714      0.857899    -0.054420      1.804795  ...    -5.814912   \n",
              " 20 -0.287159      0.741939    -0.070493      3.621587  ...    -8.677151   \n",
              " 21  0.598738      0.318077    -0.061060      1.714804  ...    -3.359979   \n",
              " 22 -0.982950      0.838960    -0.054890      1.716177  ...    -5.661814   \n",
              " 23 -0.241728      0.590851    -0.070441      3.430953  ...    -8.484412   \n",
              " \n",
              "     tc20_max_drawdown  tc30_annual_return  tc30_annual_vol  tc30_sharpe  \\\n",
              " 0           -0.975570           -0.729784         0.221047    -3.301494   \n",
              " 1           -0.997746           -0.992766         0.202559    -4.901112   \n",
              " 2           -0.999932           -1.722656         0.235240    -7.322978   \n",
              " 3           -0.938131           -0.557208         0.229360    -2.429401   \n",
              " 4           -0.995195           -0.846531         0.209046    -4.049495   \n",
              " 5           -0.999613           -1.404230         0.224532    -6.254040   \n",
              " 6           -0.991482           -0.867592         0.228336    -3.799620   \n",
              " 7           -0.999459           -1.237993         0.188469    -6.568680   \n",
              " 8           -0.999995           -2.105910         0.233885    -9.004028   \n",
              " 9           -0.975221           -0.676982         0.241104    -2.807838   \n",
              " 10          -0.998837           -1.099423         0.191515    -5.740669   \n",
              " 11          -0.999964           -1.777002         0.226002    -7.862777   \n",
              " 12          -0.998966           -1.233899         0.208291    -5.923922   \n",
              " 13          -0.999919           -1.527880         0.195723    -7.806345   \n",
              " 14          -1.000000           -2.761682         0.220530   -12.522929   \n",
              " 15          -0.997937           -1.133938         0.210088    -5.397433   \n",
              " 16          -0.999865           -1.442324         0.199665    -7.223716   \n",
              " 17          -1.000000           -2.576053         0.213227   -12.081242   \n",
              " 18          -0.999102           -1.255725         0.213466    -5.882543   \n",
              " 19          -0.999927           -1.544333         0.187851    -8.221052   \n",
              " 20          -1.000000           -2.800180         0.218301   -12.827128   \n",
              " 21          -0.998449           -1.165771         0.218625    -5.332279   \n",
              " 22          -0.999894           -1.478599         0.185190    -7.984241   \n",
              " 23          -1.000000           -2.644349         0.210473   -12.563816   \n",
              " \n",
              "     tc30_max_drawdown  tc40_annual_return  tc40_annual_vol  tc40_sharpe  \\\n",
              " 0           -0.998424           -1.049683         0.222457    -4.718580   \n",
              " 1           -0.999831           -1.299377         0.204014    -6.369056   \n",
              " 2           -1.000000           -2.349202         0.238363    -9.855550   \n",
              " 3           -0.993192           -0.815873         0.230047    -3.546552   \n",
              " 4           -0.999431           -1.099402         0.209704    -5.242635   \n",
              " 5           -0.999995           -1.915928         0.226235    -8.468734   \n",
              " 6           -0.999521           -1.203296         0.230034    -5.230954   \n",
              " 7           -0.999979           -1.619562         0.190058    -8.521387   \n",
              " 8           -1.000000           -2.823291         0.237092   -11.908014   \n",
              " 9           -0.997607           -0.949895         0.241974    -3.925605   \n",
              " 10          -0.999933           -1.434959         0.192381    -7.458941   \n",
              " 11          -1.000000           -2.385649         0.228167   -10.455738   \n",
              " 12          -0.999979           -1.691940         0.208889    -8.099694   \n",
              " 13          -0.999998           -1.983745         0.196366   -10.102284   \n",
              " 14          -1.000000           -3.675555         0.221714   -16.577886   \n",
              " 15          -0.999950           -1.567496         0.210530    -7.445484   \n",
              " 16          -0.999997           -1.872429         0.200300    -9.348099   \n",
              " 17          -1.000000           -3.439645         0.214331   -16.048320   \n",
              " 18          -0.999982           -1.713517         0.213983    -8.007733   \n",
              " 19          -0.999998           -1.999141         0.188492   -10.605990   \n",
              " 20          -1.000000           -3.712820         0.219345   -16.926870   \n",
              " 21          -0.999962           -1.597902         0.219027    -7.295453   \n",
              " 22          -0.999997           -1.911076         0.185739   -10.289028   \n",
              " 23          -1.000000           -3.508949         0.211430   -16.596300   \n",
              " \n",
              "     tc40_max_drawdown  \n",
              " 0           -0.999899  \n",
              " 1           -0.999988  \n",
              " 2           -1.000000  \n",
              " 3           -0.999258  \n",
              " 4           -0.999935  \n",
              " 5           -1.000000  \n",
              " 6           -0.999973  \n",
              " 7           -0.999999  \n",
              " 8           -1.000000  \n",
              " 9           -0.999770  \n",
              " 10          -0.999996  \n",
              " 11          -1.000000  \n",
              " 12          -1.000000  \n",
              " 13          -1.000000  \n",
              " 14          -1.000000  \n",
              " 15          -0.999999  \n",
              " 16          -1.000000  \n",
              " 17          -1.000000  \n",
              " 18          -1.000000  \n",
              " 19          -1.000000  \n",
              " 20          -1.000000  \n",
              " 21          -0.999999  \n",
              " 22          -1.000000  \n",
              " 23          -1.000000  \n",
              " \n",
              " [24 rows x 32 columns],\n",
              "       scheme       model  window portfolio_type                 date  \\\n",
              " 0         VW  Autoformer       5      long_only  2016-01-05 00:00:00   \n",
              " 1         VW  Autoformer       5      long_only  2016-01-06 00:00:00   \n",
              " 2         VW  Autoformer       5      long_only  2016-01-07 00:00:00   \n",
              " 3         VW  Autoformer       5      long_only  2016-01-08 00:00:00   \n",
              " 4         VW  Autoformer       5      long_only  2016-01-11 00:00:00   \n",
              " ...      ...         ...     ...            ...                  ...   \n",
              " 51667     EW  Autoformer     512     long_short  2024-12-20 00:00:00   \n",
              " 51668     EW  Autoformer     512     long_short  2024-12-23 00:00:00   \n",
              " 51669     EW  Autoformer     512     long_short  2024-12-24 00:00:00   \n",
              " 51670     EW  Autoformer     512     long_short  2024-12-27 00:00:00   \n",
              " 51671     EW  Autoformer     512     long_short  2024-12-30 00:00:00   \n",
              " \n",
              "          return  turnover  cumulative  tc5_return  tc5_cumulative  \\\n",
              " 0      0.001353  1.000000    0.001352    0.000853        0.000853   \n",
              " 1     -0.009845  2.000000   -0.008542   -0.010845       -0.010052   \n",
              " 2      0.005435  0.593979   -0.003121    0.005138       -0.004927   \n",
              " 3     -0.001066  1.655411   -0.004188   -0.001893       -0.006822   \n",
              " 4     -0.007666  1.091079   -0.011883   -0.008211       -0.015067   \n",
              " ...         ...       ...         ...         ...             ...   \n",
              " 51667  0.007474  3.972321   -0.597527    0.005488       -4.288171   \n",
              " 51668  0.000891  4.009181   -0.596637   -0.001114       -4.289285   \n",
              " 51669 -0.000061  3.190868   -0.596698   -0.001657       -4.290943   \n",
              " 51670 -0.001472  4.009396   -0.598171   -0.003477       -4.294426   \n",
              " 51671 -0.020889  3.590601   -0.619281   -0.022684       -4.317371   \n",
              " \n",
              "        tc10_return  tc10_cumulative  tc20_return  tc20_cumulative  \\\n",
              " 0         0.000353         0.000353    -0.000647        -0.000647   \n",
              " 1        -0.011845        -0.011563    -0.013845        -0.014589   \n",
              " 2         0.004841        -0.006733     0.004247        -0.010351   \n",
              " 3        -0.002721        -0.009458    -0.004376        -0.014737   \n",
              " 4        -0.008757        -0.018253    -0.009848        -0.024633   \n",
              " ...            ...              ...          ...              ...   \n",
              " 51667     0.003502        -7.985279    -0.000471       -15.398984   \n",
              " 51668    -0.003118        -7.988402    -0.007128       -15.406137   \n",
              " 51669    -0.003252        -7.991660    -0.006443       -15.412601   \n",
              " 51670    -0.005482        -7.997157    -0.009491       -15.422137   \n",
              " 51671    -0.024479        -8.021940    -0.028070       -15.450608   \n",
              " \n",
              "        tc30_return  tc30_cumulative  tc40_return  tc40_cumulative  \n",
              " 0        -0.001647        -0.001648    -0.002647        -0.002650  \n",
              " 1        -0.015845        -0.017620    -0.017845        -0.020657  \n",
              " 2         0.003653        -0.013974     0.003059        -0.017602  \n",
              " 3        -0.006032        -0.020024    -0.007687        -0.025319  \n",
              " 4        -0.010939        -0.031023    -0.012030        -0.037422  \n",
              " ...            ...              ...          ...              ...  \n",
              " 51667    -0.004443       -22.838826    -0.008415       -30.304995  \n",
              " 51668    -0.011137       -22.850026    -0.015146       -30.320257  \n",
              " 51669    -0.009634       -22.859706    -0.012825       -30.333165  \n",
              " 51670    -0.013500       -22.873299    -0.017510       -30.350830  \n",
              " 51671    -0.031660       -22.905471    -0.035251       -30.386717  \n",
              " \n",
              " [51672 rows x 18 columns],\n",
              " <__main__.PortfolioBacktester at 0x7d61a18918d0>)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run_portfolio_simulation_daily_rebalance()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HInsGRlNfFB0",
        "outputId": "21a60ee2-1829-4285-b6e5-d349ddda4284"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Saved] 5_factor_analysis_VW_gross.csv \n",
            "[Saved] 5_factor_analysis_VW_net.csv \n",
            "[Saved] 5_factor_analysis_EW_gross.csv \n",
            "[Saved] 5_factor_analysis_EW_net.csv \n"
          ]
        }
      ],
      "source": [
        "def run_factor_regression(port_ret, factors, use_excess=True):\n",
        "    df = pd.concat([port_ret, factors], axis=1, join='inner').dropna()\n",
        "    df.columns = ['ret'] + list(factors.columns)\n",
        "\n",
        "    if use_excess:\n",
        "        y = df['ret'].values\n",
        "    else:\n",
        "        y = df['ret'].values - df['rf'].values\n",
        "\n",
        "    X = df[['mktrf','smb','hml','rmw','cma','umd']].values\n",
        "    X = sm.add_constant(X)\n",
        "\n",
        "    model = sm.OLS(y, X)\n",
        "    res = model.fit()\n",
        "    alpha = res.params[0]\n",
        "    resid_std = res.resid.std(ddof=1)\n",
        "\n",
        "    ir_daily = alpha / resid_std\n",
        "    ir_annual = ir_daily * np.sqrt(252)\n",
        "\n",
        "    y_hat = np.asarray(res.fittedvalues)\n",
        "\n",
        "    out = {\n",
        "        'N_obs'            : len(y),\n",
        "        'alpha_daily'      : alpha,\n",
        "        'alpha_annual'     : alpha*252,\n",
        "        't_alpha'          : res.tvalues[0],\n",
        "        'IR_daily'         : ir_daily,\n",
        "        'IR_annual'        : ir_annual,\n",
        "        'R2_zero'          : r2_zero(y, y_hat),\n",
        "    }\n",
        "\n",
        "    factor_names = ['MKT','SMB','HML','RMW','CMA','UMD']\n",
        "    for i, fac in enumerate(factor_names, start=1):\n",
        "        out[f'beta_{fac}'] = res.params[i]\n",
        "        out[f't_{fac}']    = res.tvalues[i]\n",
        "\n",
        "    return out\n",
        "\n",
        "def batch_factor_analysis(\n",
        "    daily_df: pd.DataFrame,\n",
        "    factors_path: str,\n",
        "    scheme: str,\n",
        "    tc_levels=(0, 5, 10, 20, 40),\n",
        "    portfolio_types=('long_only','short_only','long_short'),\n",
        "    model_filter=None,\n",
        "    window_filter=None,\n",
        "    gross_only=False,\n",
        "    out_dir='factor_IR_results',\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate a CSV file containing IR results.\n",
        "    If gross_only=True, only tc=0 is calculated; if False, all tc_levels are included.\n",
        "    \"\"\"\n",
        "    import os\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    fac = (pd.read_csv(factors_path, parse_dates=['date'])\n",
        "             .set_index('date')\n",
        "             .sort_index())\n",
        "\n",
        "    sub = daily_df[daily_df['scheme'] == scheme].copy()\n",
        "    if model_filter is not None:\n",
        "        sub = sub[sub['model'].isin(model_filter)]\n",
        "    if window_filter is not None:\n",
        "        sub = sub[sub['window'].isin(window_filter)]\n",
        "\n",
        "    tc_iter = (0,) if gross_only else tc_levels\n",
        "    results = []\n",
        "\n",
        "    for (model, win, ptype), g in sub.groupby(['model','window','portfolio_type']):\n",
        "        g = g.sort_values('date').set_index(pd.to_datetime(g['date']))\n",
        "\n",
        "        for tc in tc_iter:\n",
        "            col = 'return' if tc == 0 else f'tc{tc}_return'\n",
        "            if col not in g.columns:\n",
        "                continue\n",
        "            port_ret = g[col]\n",
        "            stats = run_factor_regression(port_ret, fac, use_excess=True)\n",
        "            stats.update({\n",
        "                'scheme'        : scheme,\n",
        "                'model'         : model,\n",
        "                'window'        : win,\n",
        "                'portfolio_type': ptype,\n",
        "                'tc_bps'        : tc,\n",
        "            })\n",
        "            results.append(stats)\n",
        "\n",
        "    df_out = pd.DataFrame(results)[[\n",
        "        'scheme','model','window','portfolio_type','tc_bps','N_obs',\n",
        "        'alpha_daily','alpha_annual','t_alpha',\n",
        "        'IR_daily','IR_annual','R2_zero',\n",
        "        'beta_MKT','t_MKT','beta_SMB','t_SMB',\n",
        "        'beta_HML','t_HML','beta_RMW','t_RMW',\n",
        "        'beta_CMA','t_CMA','beta_UMD','t_UMD'\n",
        "    ]]\n",
        "\n",
        "    tag = 'gross' if gross_only else 'net'\n",
        "    fname = f'5_factor_analysis_{scheme}_{tag}.csv'\n",
        "    df_out.to_csv(os.path.join(out_dir, fname), index=False)\n",
        "    print(f'[Saved] {fname} ')\n",
        "    return df_out\n",
        "\n",
        "\n",
        "\n",
        "def run_all_factor_tests(vw_csv=\"portfolio_daily_series_VW.csv\",\n",
        "                         ew_csv=\"portfolio_daily_series_EW.csv\",\n",
        "                         factor_csv=None,\n",
        "                         save_dir=\"results\",\n",
        "                         y_is_excess=True,\n",
        "                         hac_lags=5,\n",
        "                         save_txt=True):\n",
        "    if factor_csv is None:\n",
        "        factor_csv = f\"{DATA_PATH}/5_Factors_Plus_Momentum.csv\"\n",
        "    vw_df = pd.read_csv(vw_csv)\n",
        "    ew_df = pd.read_csv(ew_csv)\n",
        "\n",
        "    vw_gross = batch_factor_analysis(\n",
        "        vw_df, factor_csv, scheme='VW', gross_only=True)\n",
        "    vw_net   = batch_factor_analysis(\n",
        "        vw_df, factor_csv, scheme='VW', gross_only=False)\n",
        "\n",
        "    ew_gross = batch_factor_analysis(\n",
        "        ew_df, factor_csv, scheme='EW', gross_only=True)\n",
        "    ew_net   = batch_factor_analysis(\n",
        "        ew_df, factor_csv, scheme='EW', gross_only=False)\n",
        "\n",
        "    return vw_gross, vw_net, ew_gross, ew_net\n",
        "\n",
        "\n",
        "vw_gross, vw_net, ew_gross, ew_net = run_all_factor_tests()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQMNsYqmfFB0",
        "outputId": "e06008a9-2346-4e33-9bba-b4f4b4e20cbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finish: portfolio_daily_series_VW_with_rf.csv\n",
            "Finish: portfolio_daily_series_EW_with_rf.csv\n"
          ]
        }
      ],
      "source": [
        "# === File paths ===\n",
        "rf_file = f\"{DATA_PATH}/CRSP_2016_2024_top50_with_exret.csv\"\n",
        "vw_file = \"portfolio_daily_series_VW.csv\"\n",
        "ew_file = \"portfolio_daily_series_EW.csv\"\n",
        "\n",
        "# === Load risk-free rate data ===\n",
        "\n",
        "rf_df = pd.read_csv(rf_file, usecols=[\"date\", \"rf\"])\n",
        "rf_df[\"date\"] = pd.to_datetime(rf_df[\"date\"])\n",
        "rf_dict = dict(zip(rf_df[\"date\"], rf_df[\"rf\"]))\n",
        "\n",
        "\n",
        "def adjust_returns_with_rf_grouped(file_path, output_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"], format='mixed', dayfirst=True)\n",
        "\n",
        "    return_cols = [col for col in df.columns if \"return\" in col and \"cumul\" not in col]\n",
        "\n",
        "    order = [\"long_only\", \"short_only\", \"long_short\"]\n",
        "    df[\"portfolio_type\"] = pd.Categorical(df[\"portfolio_type\"], categories=order, ordered=True)\n",
        "\n",
        "    df_list = []\n",
        "    for _, group in df.groupby([\"scheme\", \"model\", \"window\", \"portfolio_type\"], sort=False):\n",
        "        group = group.sort_values(\"date\").copy()\n",
        "        for col in return_cols:\n",
        "            group[col] = group.apply(lambda row: row[col] + rf_dict.get(row[\"date\"], 0), axis=1)\n",
        "            cum_col = col.replace(\"return\", \"cumulative\")\n",
        "            group[cum_col] = np.log1p(group[col]).cumsum()\n",
        "        df_list.append(group)\n",
        "\n",
        "    df_new = pd.concat(df_list).sort_values([\"scheme\", \"model\", \"window\", \"portfolio_type\", \"date\"])\n",
        "    df_new.to_csv(output_path, index=False)\n",
        "    print(f\"Finished: {output_path}\")\n",
        "\n",
        "adjust_returns_with_rf_grouped(vw_file, \"portfolio_daily_series_VW_with_rf.csv\")\n",
        "adjust_returns_with_rf_grouped(ew_file, \"portfolio_daily_series_EW_with_rf.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUkwd-0EfFB0",
        "outputId": "fc77069d-c065-4e11-dcef-66b104c290fa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All figures have been generated and saved to: Baseline_Portfolio/\n"
          ]
        }
      ],
      "source": [
        "# ======== Download S&P500 (2016-2024) ========\n",
        "sp500 = yf.download(\"^GSPC\", start=\"2016-01-01\", end=\"2024-12-31\")\n",
        "price_col = \"Adj Close\" if \"Adj Close\" in sp500.columns else \"Close\"\n",
        "sp500[\"daily_return\"] = sp500[price_col].pct_change().fillna(0)\n",
        "# Cumulative log return\n",
        "sp500[\"cum_return\"] = np.cumsum(np.log1p(sp500[\"daily_return\"]))\n",
        "sp500 = sp500[[\"cum_return\"]]\n",
        "sp500.index = pd.to_datetime(sp500.index)\n",
        "\n",
        "# ======== Configuration ========\n",
        "files = [\n",
        "    (\"VW\", \"portfolio_daily_series_VW_with_rf.csv\"),\n",
        "    (\"EW\", \"portfolio_daily_series_EW_with_rf.csv\")\n",
        "]\n",
        "tc_levels = [0, 5, 10, 20, 40]      # Transaction cost (bps)\n",
        "windows = [5, 21, 252, 512]         # Window size\n",
        "strategies = [\"long_only\", \"short_only\", \"long_short\"]\n",
        "\n",
        "output_dir = \"Baseline_Portfolio\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Economic event periods (for shading)\n",
        "crisis_periods = [\n",
        "    (datetime(2018, 6, 1), datetime(2019, 1, 1), \"US-China Trade War\"),\n",
        "    (datetime(2020, 2, 1), datetime(2020, 7, 1), \"COVID-19\"),\n",
        "    (datetime(2022, 2, 1), datetime(2022, 6, 1), \"Russia-Ukraine War\"),\n",
        "    (datetime(2023, 1, 1), datetime(2023, 4, 1), \"US Bank Crisis\"),\n",
        "]\n",
        "\n",
        "def plot_comparison_styled(df, scheme, tc, window):\n",
        "    plt.figure(figsize=(15, 12))\n",
        "    model_names = df[\"model\"].unique()\n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, len(model_names)))\n",
        "\n",
        "    offset_step = 0.02\n",
        "\n",
        "    for i, strat in enumerate(strategies, 1):\n",
        "        ax = plt.subplot(3, 1, i)\n",
        "\n",
        "        plt.plot(sp500.index, sp500[\"cum_return\"],\n",
        "                 color=\"black\", lw=2.5, label=\"S&P500 (Total Return)\", zorder=10)\n",
        "\n",
        "        for idx, model_name in enumerate(model_names):\n",
        "            sub = df[(df[\"window\"] == window) &\n",
        "                     (df[\"portfolio_type\"] == strat) &\n",
        "                     (df[\"model\"] == model_name)].sort_values(\"date\")\n",
        "            if sub.empty:\n",
        "                continue\n",
        "\n",
        "            if tc == 0:\n",
        "                ret_col = \"return\"          # Raw excess return\n",
        "            else:\n",
        "                ret_col = f\"tc{tc}_return\"  # Return with transaction cost\n",
        "\n",
        "            if ret_col not in sub.columns:\n",
        "                continue\n",
        "\n",
        "            log_cum = np.cumsum(np.log1p(sub[ret_col].values))\n",
        "\n",
        "            y_shift = idx * offset_step\n",
        "            plt.plot(sub[\"date\"], log_cum + y_shift,\n",
        "                     label=f\"{model_name} ({strat.replace('_',' ').title()})\",\n",
        "                     lw=2, color=colors[idx], alpha=0.9)\n",
        "\n",
        "        for start, end, label in crisis_periods:\n",
        "            ax.axvspan(start, end, color='grey', alpha=0.3)\n",
        "            ax.text(start + pd.Timedelta(days=10),\n",
        "                    ax.get_ylim()[1]*0.92, label, fontsize=8, color='grey')\n",
        "        ax.xaxis.set_major_locator(mdates.YearLocator())\n",
        "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "        ax.set_ylabel(\"Cumulative log return (start = 0)\")\n",
        "        ax.set_title(f\"{scheme} | Window={window} | Strategy={strat} | TC={tc} bps\")\n",
        "        ax.grid(alpha=0.3)\n",
        "        plt.xticks(rotation=30)\n",
        "        plt.legend(bbox_to_anchor=(1.04, 1), loc='upper left', fontsize=8)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    fname = f\"{scheme}_window{window}_TC{tc}_logreturn_offset.png\"\n",
        "    plt.savefig(os.path.join(output_dir, fname), dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "# ======== Main loop to generate all figures ========\n",
        "for scheme, file_path in files:\n",
        "    df = pd.read_csv(file_path)\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "    for tc in tc_levels:\n",
        "        for window in windows:\n",
        "            plot_comparison_styled(df, scheme, tc, window)\n",
        "\n",
        "print(f\"All figures have been generated and saved to: {output_dir}/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJxHhZCPfFB0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load R²_zero from portfolio_metrics.csv\n",
        "metrics_df = pd.read_csv(\"portfolio_metrics.csv\")[[\"Model\", \"Window\", \"R²_zero\"]]\n",
        "metrics_df.rename(columns={\"Model\": \"model\", \"Window\": \"window\"}, inplace=True)\n",
        "\n",
        "# Process VW/EW files\n",
        "for fname in [\"portfolio_results_daily_rebalance_VW.csv\", \"portfolio_results_daily_rebalance_EW.csv\"]:\n",
        "    df = pd.read_csv(fname)\n",
        "\n",
        "    # Merge R²_zero by model and window\n",
        "    df = df.merge(metrics_df, on=[\"model\", \"window\"], how=\"left\")\n",
        "\n",
        "    rows = []\n",
        "    for _, row in df.iterrows():\n",
        "        r2 = float(row[\"R²_zero\"]) if not pd.isna(row[\"R²_zero\"]) else 0.0\n",
        "        if row[\"portfolio_type\"] == \"long_only\":\n",
        "            d_sr, sr_star = delta_sharpe(r2, SR_MKT_EX)\n",
        "            row[\"ΔSharpe\"]  = d_sr\n",
        "            row[\"Sharpe*\"]  = sr_star\n",
        "            row[\"baseline\"] = f\"SPX_excess ({SR_MKT_EX:.2f})\"\n",
        "        else:\n",
        "            d_sr, sr_star = delta_sharpe(r2, 0)\n",
        "            row[\"ΔSharpe\"]  = d_sr\n",
        "            row[\"Sharpe*\"]  = sr_star\n",
        "            row[\"baseline\"] = \"cash (0)\"\n",
        "        rows.append(row)\n",
        "\n",
        "    pd.DataFrame(rows).to_csv(fname, index=False)\n",
        "    print(f\"[Update] ΔSharpe has been written to {fname}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cslo-cT8fFB0",
        "outputId": "d43b3b02-879b-426e-d5c5-20f6b8de4d5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backup functions defined:\n",
            "- backup_to_drive(): Backup models and results to Google Drive\n",
            "- download_results(): Download result files to local\n",
            "Backing up models to Google Drive...\n",
            "Models backed up to /content/drive/MyDrive/ERP Data/models_backup\n",
            "Backing up results to Google Drive...\n",
            "Results backed up to /content/drive/MyDrive/ERP Data/results_backup\n",
            "Backing up Baseline_Portfolio figures to Google Drive...\n",
            "Backed up 40 figures to /content/drive/MyDrive/ERP Data/figures_backup/Baseline_Portfolio\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "def backup_to_drive():\n",
        "    \"\"\"Backup trained models, result files, and Baseline_Portfolio figures to Google Drive.\"\"\"\n",
        "\n",
        "    drive_models_path  = f\"{DRIVE_PATH}/models_backup\"\n",
        "    drive_results_path = f\"{DRIVE_PATH}/results_backup\"\n",
        "    drive_figures_path = f\"{DRIVE_PATH}/figures_backup\"\n",
        "    os.makedirs(drive_models_path,  exist_ok=True)\n",
        "    os.makedirs(drive_results_path, exist_ok=True)\n",
        "    os.makedirs(drive_figures_path, exist_ok=True)\n",
        "\n",
        "    if os.path.exists(\"models\"):\n",
        "        print(\"Backing up models to Google Drive...\")\n",
        "        for file in os.listdir(\"models\"):\n",
        "            if file.endswith(\".pth\"):\n",
        "                shutil.copy(f\"models/{file}\", f\"{drive_models_path}/{file}\")\n",
        "        print(f\"Models backed up to {drive_models_path}\")\n",
        "\n",
        "    result_files = [\n",
        "        \"portfolio_results_daily_rebalance_VW.csv\",\n",
        "        \"portfolio_results_daily_rebalance_EW.csv\",\n",
        "        \"portfolio_daily_series_VW.csv\",\n",
        "        \"portfolio_daily_series_EW.csv\",\n",
        "        \"portfolio_metrics.csv\",\n",
        "        \"predictions_daily.csv\",\n",
        "    ]\n",
        "    print(\"Backing up results to Google Drive...\")\n",
        "    for file in result_files:\n",
        "        if os.path.exists(file):\n",
        "            shutil.copy(file, f\"{drive_results_path}/{file}\")\n",
        "    print(f\"Results backed up to {drive_results_path}\")\n",
        "\n",
        "    if os.path.exists(\"Baseline_Portfolio\"):\n",
        "        print(\"Backing up Baseline_Portfolio figures to Google Drive...\")\n",
        "        bp_drive_path = os.path.join(drive_figures_path, \"Baseline_Portfolio\")\n",
        "        os.makedirs(bp_drive_path, exist_ok=True)\n",
        "\n",
        "        img_cnt = 0\n",
        "        for file in os.listdir(\"Baseline_Portfolio\"):\n",
        "            if file.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".pdf\", \".svg\")):\n",
        "                shutil.copy(\n",
        "                    os.path.join(\"Baseline_Portfolio\", file),\n",
        "                    os.path.join(bp_drive_path, file)\n",
        "                )\n",
        "                img_cnt += 1\n",
        "        print(f\"Backed up {img_cnt} figures to {bp_drive_path}\")\n",
        "    else:\n",
        "        print(\"Baseline_Portfolio directory not found, skipping figure backup\")\n",
        "\n",
        "def download_results():\n",
        "    \"\"\"Download result files to local machine.\"\"\"\n",
        "    from google.colab import files\n",
        "\n",
        "    result_files = [\n",
        "        \"portfolio_results_daily_rebalance_VW.csv\",\n",
        "        \"portfolio_results_daily_rebalance_EW.csv\",\n",
        "        \"portfolio_daily_series_VW.csv\",\n",
        "        \"portfolio_daily_series_EW.csv\",\n",
        "        \"portfolio_metrics.csv\"\n",
        "    ]\n",
        "\n",
        "    for file in result_files:\n",
        "        if os.path.exists(file):\n",
        "            files.download(file)\n",
        "\n",
        "print(\"Backup functions defined:\")\n",
        "print(\"- backup_to_drive(): Backup models and results to Google Drive\")\n",
        "print(\"- download_results(): Download result files to local\")\n",
        "\n",
        "backup_to_drive()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (tf-mac)",
      "language": "python",
      "name": "tf-mac"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
