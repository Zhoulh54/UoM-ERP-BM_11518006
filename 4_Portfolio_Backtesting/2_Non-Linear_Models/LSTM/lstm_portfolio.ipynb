{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps (Apple Metal GPU)\n",
      " MPS cache cleared\n"
     ]
    }
   ],
   "source": [
    "# Numerical computation & data processing\n",
    "import os\n",
    "import copy\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine learning & deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import optuna\n",
    "\n",
    "# Statistics & finance tools\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import f as f_dist\n",
    "import yfinance as yf\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from typing import Optional\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration - prefer MPS (Mac GPU), then CUDA, then CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(f\"Using device: {device} (Apple Metal GPU)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Using device: {device} (NVIDIA GPU)\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(f\"Using device: {device} (CPU only)\")\n",
    "\n",
    "# MPS specific settings\n",
    "if device.type == 'mps':\n",
    "    torch.mps.empty_cache()\n",
    "    print(\"MPS cache cleared\")\n",
    "\n",
    "def get_device_info():\n",
    "    \"\"\"Get device information\"\"\"\n",
    "    if device.type == 'mps':\n",
    "        return f\"Apple Metal GPU (MPS) - Mac M-series chip\"\n",
    "    elif device.type == 'cuda':\n",
    "        return f\"NVIDIA GPU: {torch.cuda.get_device_name()}\"\n",
    "    else:\n",
    "        return \"CPU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loaded] Y scaler for window 5\n",
      "[Loaded] Y scaler for window 21\n",
      "[Loaded] Y scaler for window 252\n",
      "[Loaded] Y scaler for window 512\n",
      "[INFO] S&P500 Excess Sharpe (2016–24) = 0.652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_y_scalers():\n",
    "    \"\"\"Load y scalers for all windows\"\"\"\n",
    "    scalers = {}\n",
    "    windows = [5, 21, 252, 512]\n",
    "    for window in windows:\n",
    "        scaler_path = f\"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/scaler_y_window_{window}.pkl\"\n",
    "        try:\n",
    "            scalers[window] = joblib.load(scaler_path)\n",
    "            print(f\"[Loaded] Y scaler for window {window}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"[Warning] Y scaler not found for window {window}: {scaler_path}\")\n",
    "            scalers[window] = None\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Failed to load Y scaler for window {window}: {e}\")\n",
    "            scalers[window] = None\n",
    "    return scalers\n",
    "\n",
    "def inverse_transform_y(y_scaled, scaler):\n",
    "    \"\"\"Inverse transform standardized y values\"\"\"\n",
    "    if scaler is None:\n",
    "        print(\"[Warning] Scaler is None, returning original values\")\n",
    "        return y_scaled\n",
    "    \n",
    "    y_scaled = np.array(y_scaled)\n",
    "    if y_scaled.ndim == 1:\n",
    "        y_scaled = y_scaled.reshape(-1, 1)\n",
    "    \n",
    "    try:\n",
    "        y_original = scaler.inverse_transform(y_scaled)\n",
    "        if y_original.shape[1] == 1:\n",
    "            return y_original.flatten()\n",
    "        return y_original\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to inverse transform: {e}\")\n",
    "        return y_scaled.flatten() if y_scaled.ndim > 1 else y_scaled\n",
    "\n",
    "Y_SCALERS = load_y_scalers()\n",
    "\n",
    "def r2_zero(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute zero-based R² (baseline is zero)\n",
    "    y_true: array of true values (N,)\n",
    "    y_pred: array of predicted values (N,)\n",
    "    \"\"\"\n",
    "    rss = np.sum((y_true - y_pred)**2)  \n",
    "    tss = np.sum(y_true**2)            \n",
    "    return 1 - rss / tss\n",
    "\n",
    "def calc_ic_daily(df, method='spearman'):\n",
    "    \"\"\"\n",
    "    Calculate daily cross-sectional RankIC.\n",
    "    df: must contain ['signal_date','y_true','y_pred']\n",
    "    \"\"\"\n",
    "    ics = (df.groupby('signal_date')\n",
    "             .apply(lambda g: g['y_pred'].corr(g['y_true'], method=method))\n",
    "             .dropna())\n",
    "    mean_ic = ics.mean()\n",
    "    std_ic  = ics.std(ddof=1)\n",
    "    t_ic    = mean_ic / (std_ic / np.sqrt(len(ics))) if std_ic > 0 else np.nan\n",
    "    pos_ratio = (ics > 0).mean()\n",
    "    return mean_ic, t_ic, pos_ratio, ics\n",
    "\n",
    "def annual_sharpe(rets, freq=252):\n",
    "    mu = float(np.mean(rets)) * freq\n",
    "    sd = float(np.std(rets, ddof=1)) * np.sqrt(freq)\n",
    "    return mu / sd if sd > 0 else 0\n",
    "\n",
    "def delta_sharpe(r2_zero: float, sr_base: float):\n",
    "    \"\"\"\n",
    "    If r2_zero <= 0   → ΔSharpe = 0, Sharpe* = sr_base\n",
    "    If r2_zero >= 1   → ΔSharpe = 0, Sharpe* = sr_base (edge case)\n",
    "    Otherwise, use the original formula\n",
    "    \"\"\"\n",
    "    if (r2_zero <= 0) or (r2_zero >= 1):\n",
    "        return 0.0, sr_base\n",
    "    sr_star = np.sqrt(sr_base ** 2 + r2_zero) / np.sqrt(1 - r2_zero)\n",
    "    return sr_star - sr_base, sr_star\n",
    "\n",
    "rf_file = \"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/CRSP_2016_2024_top50_with_exret.csv\"\n",
    "rf_df = pd.read_csv(rf_file, usecols=[\"date\", \"rf\"])\n",
    "rf_df[\"date\"] = pd.to_datetime(rf_df[\"date\"])\n",
    "rf_df = rf_df.drop_duplicates(\"date\").set_index(\"date\").sort_index()\n",
    "rf_series = rf_df[\"rf\"].astype(float)\n",
    "\n",
    "px = yf.download(\"^GSPC\", start=\"2016-01-01\", end=\"2024-12-31\")[\"Close\"]\n",
    "sp_ret = px.pct_change().dropna()\n",
    "rf_align = rf_series.reindex(sp_ret.index).fillna(method=\"ffill\")\n",
    "sp_excess = sp_ret.values - rf_align.values\n",
    "\n",
    "SR_MKT_EX = annual_sharpe(sp_excess)\n",
    "print(f\"[INFO] S&P500 Excess Sharpe (2016–24) = {SR_MKT_EX:.3f}\")\n",
    "\n",
    "def calc_directional_metrics(y_true, y_pred, permnos=None):\n",
    "\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    if permnos is None:\n",
    "        s_true = np.sign(y_true)\n",
    "        s_pred = np.sign(y_pred)\n",
    "        mask = s_true != 0\n",
    "        s_true = s_true[mask]\n",
    "        s_pred = s_pred[mask]\n",
    "\n",
    "        overall_acc = np.mean(s_true == s_pred)\n",
    "\n",
    "        up_mask = s_true > 0\n",
    "        down_mask = s_true < 0\n",
    "        up_acc = np.mean(s_true[up_mask] == s_pred[up_mask]) if np.any(up_mask) else 0\n",
    "        down_acc = np.mean(s_true[down_mask] == s_pred[down_mask]) if np.any(down_mask) else 0\n",
    "\n",
    "    else:\n",
    "        df = pd.DataFrame({\"permno\": permnos, \"yt\": y_true, \"yp\": y_pred})\n",
    "        overall_accs = []\n",
    "        up_accs = []\n",
    "        down_accs = []\n",
    "\n",
    "        for _, g in df.groupby(\"permno\"):\n",
    "            s_true = np.sign(g[\"yt\"].values)\n",
    "            s_pred = np.sign(g[\"yp\"].values)\n",
    "            mask = s_true != 0\n",
    "            s_true = s_true[mask]\n",
    "            s_pred = s_pred[mask]\n",
    "            if len(s_true) == 0:\n",
    "                continue\n",
    "            overall_accs.append(np.mean(s_true == s_pred))\n",
    "\n",
    "            up_mask = s_true > 0\n",
    "            down_mask = s_true < 0\n",
    "            up_accs.append(np.mean(s_true[up_mask] == s_pred[up_mask]) if np.any(up_mask) else np.nan)\n",
    "            down_accs.append(np.mean(s_true[down_mask] == s_pred[down_mask]) if np.any(down_mask) else np.nan)\n",
    "\n",
    "        overall_acc = np.nanmean(overall_accs)\n",
    "        up_acc = np.nanmean(up_accs)\n",
    "        down_acc = np.nanmean(down_accs)\n",
    "\n",
    "    return overall_acc, up_acc, down_acc\n",
    "\n",
    "def regression_metrics(y_true, y_pred, k, meta=None, permnos=None):\n",
    "    \"\"\"\n",
    "    Includes:\n",
    "    - Regression metrics\n",
    "    - Pointwise directional accuracy\n",
    "    - Market cap group metrics\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    n = len(y_true)\n",
    "\n",
    "    r2 = r2_zero(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "\n",
    "    dir_acc, up_acc, down_acc = calc_directional_metrics(y_true, y_pred, permnos)\n",
    "\n",
    "    metrics = {\n",
    "        \"R²_zero\": r2,\n",
    "        \"RMSE\": rmse,\n",
    "        \"MAE\": mae,\n",
    "        \"MSE\": mse,\n",
    "        \"Directional Accuracy\": dir_acc,\n",
    "        \"Up_Directional_Acc\": up_acc,\n",
    "        \"Down_Directional_Acc\": down_acc\n",
    "    }\n",
    "\n",
    "    if meta is not None and \"MKTCAP_PERCENTILE\" in meta:\n",
    "        top_mask = meta[\"MKTCAP_PERCENTILE\"] >= 0.75\n",
    "        bottom_mask = meta[\"MKTCAP_PERCENTILE\"] <= 0.25\n",
    "\n",
    "        if np.any(top_mask):\n",
    "            yt_top = y_true[top_mask]\n",
    "            yp_top = y_pred[top_mask]\n",
    "            perm_top = permnos[top_mask] if permnos is not None else None\n",
    "            r2_top = r2_zero(yt_top, yp_top)\n",
    "            rmse_top = np.sqrt(mean_squared_error(yt_top, yp_top))\n",
    "            mae_top = mean_absolute_error(yt_top, yp_top)\n",
    "            mse_top = mean_squared_error(yt_top, yp_top)\n",
    "            dir_top, up_top, down_top = calc_directional_metrics(yt_top, yp_top, perm_top)\n",
    "            metrics.update({\n",
    "                \"Top25_R2_zero\": r2_top,\n",
    "                \"Top25_RMSE\": rmse_top,\n",
    "                \"Top25_MAE\": mae_top,\n",
    "                \"Top25_MSE\": mse_top,\n",
    "                \"Top25_Dir_Acc\": dir_top,\n",
    "                \"Top25_Up_Acc\": up_top,\n",
    "                \"Top25_Down_Acc\": down_top\n",
    "            })\n",
    "\n",
    "        if np.any(bottom_mask):\n",
    "            yt_bot = y_true[bottom_mask]\n",
    "            yp_bot = y_pred[bottom_mask]\n",
    "            perm_bot = permnos[bottom_mask] if permnos is not None else None\n",
    "            r2_bot = r2_zero(yt_bot, yp_bot)\n",
    "            rmse_bot = np.sqrt(mean_squared_error(yt_bot, yp_bot))\n",
    "            mae_bot = mean_absolute_error(yt_bot, yp_bot)\n",
    "            mse_bot = mean_squared_error(yt_bot, yp_bot)\n",
    "            dir_bot, up_bot, down_bot = calc_directional_metrics(yt_bot, yp_bot, perm_bot)\n",
    "            metrics.update({\n",
    "                \"Bottom25_R2_zero\": r2_bot,\n",
    "                \"Bottom25_RMSE\": rmse_bot,\n",
    "                \"Bottom25_MAE\": mae_bot,\n",
    "                \"Bottom25_MSE\": mse_bot,\n",
    "                \"Bottom25_Dir_Acc\": dir_bot,\n",
    "                \"Bottom25_Up_Acc\": up_bot,\n",
    "                \"Bottom25_Down_Acc\": down_bot\n",
    "            })\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def f_statistic(y_true, y_pred, k):\n",
    "    \"\"\"Return F statistic and corresponding p-value\"\"\"\n",
    "    n   = len(y_true)\n",
    "    rss = np.sum((y_true - y_pred) ** 2)\n",
    "    tss = np.sum(y_true ** 2)\n",
    "    r2  = 1 - rss / tss\n",
    "    if (r2 <= 0) or (n <= k):\n",
    "        return 0.0, 1.0\n",
    "    F = (r2 / k) / ((1 - r2) / (n - k))\n",
    "    p = f_dist.sf(F, k, n - k)\n",
    "    return F, p\n",
    "\n",
    "def overall_interval_metrics_method1(y_all, yhat_all, k, permnos_all=None, meta_all=None):\n",
    "    \"\"\"\n",
    "    Method 1: Compute metrics for the entire interval (2016-2024, all samples concatenated)\n",
    "    Returns: a dict, can be directly passed to save_metrics()\n",
    "    \"\"\"\n",
    "    base = regression_metrics(\n",
    "        y_true=y_all, \n",
    "        y_pred=yhat_all, \n",
    "        k=k, \n",
    "        meta=meta_all, \n",
    "        permnos=permnos_all\n",
    "    )\n",
    "    F, p = f_statistic(y_all, yhat_all, k)\n",
    "    base[\"F_stat\"]     = F\n",
    "    base[\"F_pvalue\"]   = p\n",
    "    base[\"N_obs\"] = len(y_all)\n",
    "    \n",
    "    delta_cash, sr_star_cash = delta_sharpe(base[\"R²_zero\"], sr_base=0)\n",
    "    base[\"ΔSharpe_cash\"]      = delta_cash\n",
    "    base[\"Sharpe*_cash\"]      = sr_star_cash\n",
    "\n",
    "    delta_mkt , sr_star_mkt  = delta_sharpe(base[\"R²_zero\"], sr_base=SR_MKT_EX)\n",
    "    base[\"ΔSharpe_mkt\"]       = delta_mkt\n",
    "    base[\"Sharpe*_mkt\"]       = sr_star_mkt\n",
    "    \n",
    "    return base\n",
    "\n",
    "def sortino_ratio(rets, freq=252):\n",
    "    \"\"\"Compute Sortino Ratio\"\"\"\n",
    "    downside = rets[rets < 0]\n",
    "    if len(downside) == 0:\n",
    "        return np.inf\n",
    "    mu = rets.mean() * freq\n",
    "    sigma = np.sqrt((downside ** 2).mean()) * np.sqrt(freq)\n",
    "    return mu / sigma\n",
    "\n",
    "def cvar(rets, alpha=0.95):\n",
    "    \"\"\"Compute CVaR\"\"\"\n",
    "    q = np.quantile(rets, 1 - alpha)\n",
    "    return rets[rets <= q].mean()\n",
    "\n",
    "def save_predictions(model_name, window_size, y_true, y_pred, permnos, path=\"predictions/\"):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        \"PERMNO\": permnos,\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred\n",
    "    })\n",
    "\n",
    "    filename = f\"{model_name}_w{window_size}.csv\"\n",
    "    df.to_csv(os.path.join(path, filename), index=False)\n",
    "    print(f\"[Save] {filename}\")\n",
    "\n",
    "def save_metrics(metrics_dict, name, window, path=\"results.csv\"):\n",
    "    \"\"\"Save evaluation metrics\"\"\"\n",
    "    row = pd.DataFrame([metrics_dict])\n",
    "    row.insert(0, \"Model\", name)\n",
    "    row.insert(1, \"Window\", window)\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path)\n",
    "        df = df[~((df[\"Model\"] == name) & (df[\"Window\"] == window))]\n",
    "        df = pd.concat([df, row], ignore_index=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        print(f\"[Update] Metrics updated for {name} w={window}\")\n",
    "    else:\n",
    "        row.to_csv(path, index=False)\n",
    "        print(f\"[Create] New metrics file created with {name} w={window}\")\n",
    "\n",
    "def get_quarter_periods(start_year=2015, end_year=2024):\n",
    "    \"\"\"Generate quarter sequence\"\"\"\n",
    "    quarters = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for q in range(1, 5):\n",
    "            quarters.append((year, q))\n",
    "    return quarters\n",
    "\n",
    "def save_model_with_quarter(lstm_wrapper, name, window, year, quarter,\n",
    "                            path=\"models/\"):\n",
    "    \"\"\"\n",
    "    Safely save LSTM model parameters, handle MPS device compatibility and file size optimization\n",
    "    \"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    lstm_wrapper.clear_loss_history()\n",
    "    \n",
    "    original_device = lstm_wrapper.training_device\n",
    "    lstm_wrapper.model.to('cpu')\n",
    "    \n",
    "    pth_file = f\"{name}_w{window}_{year}Q{quarter}.pth\"\n",
    "    torch.save(lstm_wrapper.model.state_dict(),\n",
    "               os.path.join(path, pth_file))\n",
    "    \n",
    "    lstm_wrapper.model.to(original_device)\n",
    "    \n",
    "    print(f\"[Saved] {pth_file}\")\n",
    "    \n",
    "def load_datasets(npz_path):\n",
    "    \"\"\"Load dataset\"\"\"\n",
    "    data = np.load(npz_path, allow_pickle=True) \n",
    "    datasets = {}\n",
    "    for key in data.files:\n",
    "        datasets[key] = data[key]\n",
    "    return datasets\n",
    "\n",
    "def find_coef_step(model):\n",
    "    \"\"\"\n",
    "    Get model coefficients, handle Pipeline and single estimator.\n",
    "    For nonlinear models (e.g., MLP), return None to indicate no coefficients.\n",
    "    \"\"\"\n",
    "    if hasattr(model, 'named_steps'):\n",
    "        for name, est in model.named_steps.items():\n",
    "            if hasattr(est, 'coef_'):\n",
    "                return name, est\n",
    "            if isinstance(est, Pipeline):\n",
    "                for subname, subest in est.named_steps.items():\n",
    "                    if hasattr(subest, 'coef_'):\n",
    "                        return f\"{name}__{subname}\", subest\n",
    "        return None, None\n",
    "    else:\n",
    "        if hasattr(model, 'coef_'):\n",
    "            return 'model', model\n",
    "    \n",
    "    raise ValueError(\"No estimator with coef_ found in model\")\n",
    "    \n",
    "def train_or_skip(model, train_loader, valid_loader, window_size, year, quarter, **train_kwargs):\n",
    "    save_path = f\"models/NN1_w{window_size}_{year}Q{quarter}.pth\"\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"[Skip Training] Model already exists: {save_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"[Training] Start training model: {save_path}\")\n",
    "    train_model(model, train_loader, valid_loader, **train_kwargs)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"[Done] Model saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 0) Utilities (keep the same interface as the original script)\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def get_default_device() -> torch.device:\n",
    "    \"\"\"Automatically select GPU / MPS / CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "    \n",
    "DEVICE_TUNE  = torch.device(\"cpu\")\n",
    "DEVICE_TRAIN = get_default_device()\n",
    "\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    \"\"\"Release GPU / MPS memory, no-op for CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Early Stopping\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Same as original API, just set default patience to 10\"\"\"\n",
    "\n",
    "    def __init__(self, patience: int = 10, min_delta: float = 0.0, restore_best_weights: bool = True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_loss: Optional[float] = None\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def __call__(self, val_loss: float, model: nn.Module) -> bool:\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self._save(model)\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self._save(model)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "\n",
    "        if self.counter >= self.patience:\n",
    "            if self.restore_best_weights and self.best_weights is not None:\n",
    "                model.load_state_dict(self.best_weights)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _save(self, model: nn.Module):\n",
    "        # Move weights to CPU, replace original copy.deepcopy\n",
    "        self.best_weights = {\n",
    "            k: v.detach().to(\"cpu\")\n",
    "            for k, v in model.state_dict().items()\n",
    "        }\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) LSTM Base Model\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        seq_len: int,\n",
    "        hidden_size: int = 64,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.2,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.features_per_ts = input_size // seq_len\n",
    "        assert (\n",
    "            input_size % seq_len == 0\n",
    "        ), \"input_size must be divisible by seq_len for reshape -> (B, seq_len, feat_per_ts)\"\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.features_per_ts,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_weights(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LSTM):\n",
    "            for name, p in m.named_parameters():\n",
    "                if \"weight\" in name:\n",
    "                    nn.init.xavier_uniform_(p)\n",
    "                else:\n",
    "                    nn.init.constant_(p, 0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, input_size)  OR (B, seq_len, feat_per_ts)\n",
    "        if x.ndim == 2:\n",
    "            bsz = x.size(0)\n",
    "            x = x.view(bsz, self.seq_len, self.features_per_ts)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last = lstm_out[:, -1, :]  # (B, hidden)\n",
    "        last = self.dropout(last)\n",
    "        return self.out(last).squeeze(-1)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) LSTM Wrapper \n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "class LSTMWrapper:\n",
    "   \n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        seq_len: int,\n",
    "        hidden_size: int = 64,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.2,\n",
    "        learning_rate: float = 1e-3,\n",
    "        batch_size: int = 256,\n",
    "        max_epochs: int = 40,\n",
    "        warm_start_epochs: int = 10,\n",
    "        training_device: Optional[torch.device] = None,\n",
    "    ):\n",
    "        self.input_size = input_size\n",
    "        self.seq_len = seq_len\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.max_epochs = max_epochs\n",
    "        self.warm_start_epochs = warm_start_epochs\n",
    "        self.training_device = training_device or get_default_device()\n",
    "\n",
    "        self._build_model()\n",
    "        self.is_fitted = False\n",
    "        self.loss_history: dict[str, list[float]] = {}\n",
    "\n",
    "    def _build_model(self):\n",
    "        self.model = LSTMModel(\n",
    "            input_size=self.input_size,\n",
    "            seq_len=self.seq_len,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=self.dropout,\n",
    "        ).to(self.training_device)\n",
    "        self._init_training_components()\n",
    "\n",
    "    def _init_training_components(self):\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode=\"min\", factor=0.5, patience=5, verbose=False)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.early_stopping = EarlyStopping(patience=5)\n",
    "\n",
    "    def _make_loaders(self, X, y, validation_split):\n",
    "        val_size = int(len(X) * validation_split) if validation_split > 0 else max(1, int(len(X) * 0.1))\n",
    "        X_train, X_val = X[:-val_size], X[-val_size:]\n",
    "        y_train, y_val = y[:-val_size], y[-val_size:]\n",
    "\n",
    "        train_ds = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
    "        val_ds = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))\n",
    "        train_loader = DataLoader(train_ds, batch_size=self.batch_size, shuffle=False, drop_last=True)\n",
    "        val_loader = DataLoader(val_ds, batch_size=self.batch_size, shuffle=False)\n",
    "        return train_loader, val_loader\n",
    "\n",
    "    def fit(self, X, y, validation_split: float = 0.1, warm_start: bool = False, verbose: bool = True):\n",
    "        if not warm_start:\n",
    "            self._build_model()\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"    [Warm Start] continue training existing LSTM weights ...\")\n",
    "            self.early_stopping = EarlyStopping(patience=5)\n",
    "\n",
    "        train_loader, val_loader = self._make_loaders(X, y, validation_split)\n",
    "        epochs = self.warm_start_epochs if warm_start else self.max_epochs\n",
    "        if verbose:\n",
    "            print(f\"    Training LSTM for {epochs} epochs on {self.training_device}\")\n",
    "\n",
    "        losses = {\"train\": [], \"val\": []}\n",
    "        for ep in range(epochs):\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            for bx, by in train_loader:\n",
    "                bx, by = bx.to(self.training_device), by.to(self.training_device)\n",
    "                self.optimizer.zero_grad()\n",
    "                preds = self.model(bx)\n",
    "                loss = self.criterion(preds, by)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            train_loss /= len(train_loader)\n",
    "\n",
    "            self.model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for bx, by in val_loader:\n",
    "                    bx, by = bx.to(self.training_device), by.to(self.training_device)\n",
    "                    loss = self.criterion(self.model(bx), by)\n",
    "                    val_loss += loss.item()\n",
    "            val_loss /= len(val_loader)\n",
    "\n",
    "            losses[\"train\"].append(train_loss)\n",
    "            losses[\"val\"].append(val_loss)\n",
    "\n",
    "            if verbose and ((ep + 1) % 5 == 0 or ep == 0):\n",
    "                print(f\"    Epoch {ep+1:3d}/{epochs}: Train={train_loss:.6f}, Val={val_loss:.6f}\")\n",
    "\n",
    "            self.scheduler.step(val_loss)\n",
    "            if self.early_stopping(val_loss, self.model):\n",
    "                if verbose:\n",
    "                    print(f\"    Early-stopped at epoch {ep+1}\")\n",
    "                break\n",
    "\n",
    "            if self.training_device.type == \"mps\" and (ep + 1) % 5 == 0:\n",
    "                torch.mps.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "        self.loss_history = losses\n",
    "        clear_memory()\n",
    "        self.is_fitted = True\n",
    "\n",
    "    def predict(self, X):\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Call fit() before predict()\")\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.FloatTensor(X).to(self.training_device)\n",
    "            preds = self.model(X_tensor).cpu().numpy()\n",
    "        return preds\n",
    "\n",
    "    def partial_fit(self, X_new, y_new, validation_split: float = 0.1, extra_epochs: int = 5, verbose: bool = True):\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Must fit() once before partial_fit()\")\n",
    "        self.early_stopping = EarlyStopping(patience=5)\n",
    "        if verbose:\n",
    "            print(f\"    Partial-fit on {len(X_new)} samples for {extra_epochs} epochs ...\")\n",
    "\n",
    "        train_loader, val_loader = self._make_loaders(X_new, y_new, validation_split)\n",
    "        for ep in range(extra_epochs):\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            for bx, by in train_loader:\n",
    "                bx, by = bx.to(self.training_device), by.to(self.training_device)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.criterion(self.model(bx), by)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            train_loss /= len(train_loader)\n",
    "\n",
    "            self.model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for bx, by in val_loader:\n",
    "                    bx, by = bx.to(self.training_device), by.to(self.training_device)\n",
    "                    val_loss += self.criterion(self.model(bx), by).item()\n",
    "            val_loss /= len(val_loader)\n",
    "\n",
    "            if verbose and (ep % 2 == 0 or ep == extra_epochs - 1):\n",
    "                print(f\"        Epoch {ep+1:2d}/{extra_epochs}: Train={train_loss:.6f}, Val={val_loss:.6f}\")\n",
    "\n",
    "            self.scheduler.step(val_loss)\n",
    "            if self.training_device.type == \"mps\" and (ep + 1) % 5 == 0:\n",
    "                torch.mps.empty_cache()\n",
    "                gc.collect()\n",
    "            if self.early_stopping(val_loss, self.model):\n",
    "                if verbose:\n",
    "                    print(f\"        Early-stopped at epoch {ep+1}\")\n",
    "                break\n",
    "        clear_memory()\n",
    "\n",
    "    def clear_loss_history(self):\n",
    "        if hasattr(self, \"loss_history\"):\n",
    "            del self.loss_history\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for k, v in params.items():\n",
    "            if hasattr(self, k):\n",
    "                setattr(self, k, v)\n",
    "        self._init_training_components()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 1. Only window=5 uses Optuna tuning ===============================\n",
    "TUNED_WINDOW = 5  # Other windows use the best hyperparameters from window=5\n",
    "\n",
    "def tune_lstm_with_optuna(X, y, seq_len, n_trials: int = 10):\n",
    "    \"\"\"LSTM hyperparameter search with time series CV (only called for window=5)\"\"\"\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    total_feat = X.shape[1]\n",
    "    print(\"Start hyperparameter tuning...\")\n",
    "    def objective(trial):\n",
    "        params = dict(\n",
    "            batch_size   = trial.suggest_categorical(\"batch_size\",  [32, 64, 128]),\n",
    "            learning_rate= trial.suggest_float(\"learning_rate\",   1e-4, 1e-2, log=True),\n",
    "            dropout      = trial.suggest_float(\"dropout\",         0.0,  0.3),\n",
    "            hidden_size  = trial.suggest_categorical(\"hidden_size\", [64, 128]),\n",
    "            num_layers   = trial.suggest_int(\"num_layers\",        1, 3),\n",
    "            max_epochs   = 10,          # Use short epochs for tuning\n",
    "        )\n",
    "\n",
    "        cv_mse = []\n",
    "        for tr_idx, val_idx in tscv.split(X):\n",
    "            X_tr, X_val = X[tr_idx], X[val_idx]\n",
    "            y_tr, y_val = y[tr_idx], y[val_idx]\n",
    "\n",
    "            model = LSTMWrapper(\n",
    "                input_size   = total_feat,\n",
    "                seq_len      = seq_len,\n",
    "                hidden_size  = params[\"hidden_size\"],\n",
    "                num_layers   = params[\"num_layers\"],\n",
    "                dropout      = params[\"dropout\"],\n",
    "                learning_rate= params[\"learning_rate\"],\n",
    "                batch_size   = params[\"batch_size\"],\n",
    "                max_epochs   = params[\"max_epochs\"],\n",
    "                training_device  = DEVICE_TUNE,\n",
    "            )\n",
    "            model.fit(X_tr, y_tr, validation_split=0.0, warm_start=False)\n",
    "            preds = model.predict(X_val)\n",
    "            cv_mse.append(mean_squared_error(y_val, preds))\n",
    "            del model\n",
    "            clear_memory()\n",
    "        return float(np.mean(cv_mse))\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "    study.optimize(objective, n_trials=n_trials, n_jobs=1)\n",
    "\n",
    "    if study.best_trial is None:\n",
    "        # Default fallback\n",
    "        return dict(batch_size=32, learning_rate=1e-3, dropout=0.2,\n",
    "                    hidden_size=64, num_layers=2, max_epochs=25)\n",
    "\n",
    "    best = study.best_params\n",
    "    best[\"max_epochs\"] = 25          # Use more epochs for final training\n",
    "    print(f\"[Optuna-LSTM] best_MSE={study.best_value:.6f}, params={best}\")\n",
    "    return best\n",
    "\n",
    "# ========== 2. Model saving / loading ============================\n",
    "\n",
    "def _lstm_save_path(name: str, window: int, year: int, quarter: int | None):\n",
    "    if quarter is None:\n",
    "        return f\"models/{name}_w{window}_{year}.pth\"\n",
    "    return f\"models/{name}_w{window}_{year}Q{quarter}.pth\"\n",
    "\n",
    "\n",
    "def save_lstm_model(wrapper: LSTMWrapper,\n",
    "                    name: str, window: int, year: int, quarter: int | None = None):\n",
    "    \"\"\"\n",
    "    Save model weights and structure hyperparameters\n",
    "    \"\"\"\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    path = _lstm_save_path(name, window, year, quarter)\n",
    "\n",
    "    ckpt = {\n",
    "        \"state_dict\": wrapper.model.state_dict(),\n",
    "        \"hyper_params\": dict(\n",
    "            input_size   = wrapper.input_size,\n",
    "            seq_len      = wrapper.seq_len,\n",
    "            hidden_size  = wrapper.hidden_size,\n",
    "            num_layers   = wrapper.num_layers,\n",
    "            dropout      = wrapper.dropout,\n",
    "            learning_rate= wrapper.learning_rate,\n",
    "            batch_size   = wrapper.batch_size,\n",
    "            max_epochs   = wrapper.max_epochs,\n",
    "        )\n",
    "    }\n",
    "\n",
    "    torch.save(ckpt, path)\n",
    "    print(f\"[Saved] {path}\")\n",
    "\n",
    "\n",
    "def load_lstm_model(name: str, window: int, year: int,\n",
    "                    quarter: int | None = None,\n",
    "                    training_device: torch.device | None = None):\n",
    "    \"\"\"\n",
    "    Load checkpoint from disk and restore LSTMWrapper (including structure hyperparameters)\n",
    "    \"\"\"\n",
    "    path = _lstm_save_path(name, window, year, quarter)\n",
    "    if not os.path.exists(path):\n",
    "        return None                       # Return None if file does not exist\n",
    "\n",
    "    ckpt = torch.load(path, map_location=\"cpu\")\n",
    "\n",
    "    hp = ckpt[\"hyper_params\"]\n",
    "    wrapper = LSTMWrapper(\n",
    "        input_size     = hp[\"input_size\"],\n",
    "        seq_len        = hp[\"seq_len\"],\n",
    "        hidden_size    = hp[\"hidden_size\"],\n",
    "        num_layers     = hp[\"num_layers\"],\n",
    "        dropout        = hp[\"dropout\"],\n",
    "        learning_rate  = hp.get(\"learning_rate\", 1e-3),\n",
    "        batch_size     = hp.get(\"batch_size\", 32),\n",
    "        max_epochs     = hp.get(\"max_epochs\", 25),\n",
    "        training_device= training_device or DEVICE_TRAIN,\n",
    "    )\n",
    "    wrapper.model.load_state_dict(ckpt[\"state_dict\"])\n",
    "    wrapper.is_fitted = True\n",
    "    return wrapper\n",
    "\n",
    "# ========== 3. Quarterly expanding training ================================\n",
    "\n",
    "def train_lstm_models_expanding_quarterly(start_year: int = 2015, end_year: int = 2024,\n",
    "                                          window_sizes: list[int] | None = None,\n",
    "                                          npz_path: str = \"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/all_window_datasets_scaled.npz\",\n",
    "                                          n_trials_optuna: int = 10):\n",
    "    \n",
    "    if window_sizes is None:\n",
    "        window_sizes = [5, 21, 252, 512]\n",
    "\n",
    "    print(f\"Starting LSTM quarterly expanding training {start_year}-{end_year}\")\n",
    "    data = load_datasets(npz_path)\n",
    "\n",
    "    best_params_cache: dict[str, dict] = {}\n",
    "    quarters_to_tune = {(2020, 4)}\n",
    "\n",
    "    for window in window_sizes:\n",
    "        print(f\"\\n=== Window = {window} ===\")\n",
    "        X_train_init = data[f\"X_train_{window}\"]\n",
    "        y_train_init = data[f\"y_train_{window}\"]\n",
    "        X_test_full  = data[f\"X_test_{window}\"]\n",
    "        y_test_full  = data[f\"y_test_{window}\"]\n",
    "        meta_test    = pd.DataFrame.from_dict(data[f\"meta_test_{window}\"].item())\n",
    "        meta_test[\"ret_date\"] = pd.to_datetime(meta_test[\"ret_date\"])\n",
    "\n",
    "        total_feat = X_train_init.shape[1]\n",
    "        feat_per_ts = total_feat\n",
    "\n",
    "        # Initial tuning (only window=5 / 2015Q4)\n",
    "        cache_key = f\"LSTM_w{window}\"\n",
    "        if window == TUNED_WINDOW:\n",
    "            tuned_model = load_lstm_model(\"LSTM\", window, 2015, 4)\n",
    "            if tuned_model is not None:\n",
    "                hp = dict(\n",
    "                    hidden_size   = tuned_model.hidden_size,\n",
    "                    num_layers    = tuned_model.num_layers,\n",
    "                    dropout       = tuned_model.dropout,\n",
    "                    learning_rate = tuned_model.learning_rate,\n",
    "                    batch_size    = tuned_model.batch_size,\n",
    "                    max_epochs    = tuned_model.max_epochs,\n",
    "                )\n",
    "                print(\"[Skip-Optuna] hyper-params loaded from existing 2015Q4 model\")\n",
    "            else:\n",
    "                print(\"  - Optuna tuning on initial window...\")\n",
    "                hp = tune_lstm_with_optuna(X_train_init, y_train_init, window, n_trials_optuna)\n",
    "        else:\n",
    "            hp = None\n",
    "        best_params_cache[cache_key] = hp\n",
    "        \n",
    "        model_prev = None\n",
    "        # Quarter loop\n",
    "        for year, quarter in get_quarter_periods(start_year, end_year):\n",
    "            model_cur = load_lstm_model(\"LSTM\", window, year, quarter)\n",
    "            if model_cur is not None:\n",
    "                print(f\"[Skip] Model already trained for window={window}, {year}Q{quarter}\")\n",
    "                continue\n",
    "            if (year == start_year and quarter < 4) or (year == end_year and quarter > 3):\n",
    "                continue  \n",
    "\n",
    "            print(f\"\\n[Window {window}] {year}Q{quarter}\")\n",
    "\n",
    "            # Prepare expanding dataset\n",
    "            if not (year == start_year and quarter == 4):\n",
    "                if quarter == 1:\n",
    "                    py, pq = year - 1, 4\n",
    "                else:\n",
    "                    py, pq = year, quarter - 1\n",
    "                mask_prev = (meta_test[\"ret_date\"].dt.year == py) & (meta_test[\"ret_date\"].dt.quarter == pq)\n",
    "                if mask_prev.any():\n",
    "                    X_prev, y_prev = X_test_full[mask_prev], y_test_full[mask_prev]\n",
    "                    X_train_init = np.vstack([X_train_init, X_prev])\n",
    "                    y_train_init = np.hstack([y_train_init, y_prev])\n",
    "                    print(f\"    +{mask_prev.sum()} obs from {py}Q{pq} -> expanding size {len(y_train_init)}\")\n",
    "\n",
    "            # Hyperparameter copy / re-tune\n",
    "            cache_key = f\"LSTM_w{window}\"\n",
    "            hp = best_params_cache.get(cache_key)\n",
    "            if hp is None and window != TUNED_WINDOW:\n",
    "                hp = best_params_cache[f\"LSTM_w{TUNED_WINDOW}\"].copy()\n",
    "                best_params_cache[cache_key] = hp\n",
    "\n",
    "            if (year, quarter) in quarters_to_tune and window == TUNED_WINDOW:\n",
    "                print(\"    Re-tuning via Optuna...\")\n",
    "                hp = tune_lstm_with_optuna(X_train_init, y_train_init, window, n_trials_optuna)\n",
    "                best_params_cache[cache_key] = hp\n",
    "\n",
    "            # Load previous model for warm-start\n",
    "            model_prev = None\n",
    "            if not (year == start_year and quarter == 4):\n",
    "                if quarter == 1:\n",
    "                    prev_year, prev_quarter = year - 1, 4\n",
    "                else:\n",
    "                    prev_year, prev_quarter = year, quarter - 1\n",
    "\n",
    "                model_prev = load_lstm_model(\"LSTM\", window, prev_year, prev_quarter)\n",
    "\n",
    "            if model_prev is not None:\n",
    "                print(\"    Warm-start from last quarter model ...\")\n",
    "                model_prev.partial_fit(X_train_init, y_train_init, validation_split=0.1, extra_epochs=hp.get(\"warm_start_epochs\", 10))\n",
    "                lstm_wrap = model_prev\n",
    "            else:\n",
    "                print(\"    Cold start training ...\")\n",
    "                lstm_wrap = LSTMWrapper(\n",
    "                    input_size   = total_feat,\n",
    "                    seq_len      = window,\n",
    "                    hidden_size  = hp[\"hidden_size\"],\n",
    "                    num_layers   = hp[\"num_layers\"],\n",
    "                    dropout      = hp[\"dropout\"],\n",
    "                    learning_rate= hp[\"learning_rate\"],\n",
    "                    batch_size   = hp[\"batch_size\"],\n",
    "                    max_epochs   = hp[\"max_epochs\"],\n",
    "                    training_device       = DEVICE_TRAIN,\n",
    "                )\n",
    "                lstm_wrap.fit(X_train_init, y_train_init, validation_split=0.1, warm_start=False)\n",
    "\n",
    "            save_lstm_model(lstm_wrap, \"LSTM\", window, year, quarter)\n",
    "            del lstm_wrap\n",
    "            clear_memory()\n",
    "            \n",
    "    print(\"All LSTM quarterly expanding models trained\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Portfolio Core Class =====\n",
    "# ===== Transaction Cost Settings =====\n",
    "TC_GRID = [0.0005, 0.001, 0.002, 0.003, 0.004]  # 5, 10, 20, 30, 40 bps\n",
    "TC_TAG  = {\n",
    "    0.0005: \"tc5\",\n",
    "    0.001:  \"tc10\", \n",
    "    0.002:  \"tc20\",\n",
    "    0.003:  \"tc30\",\n",
    "    0.004:  \"tc40\"\n",
    "}\n",
    "\n",
    "class PortfolioBacktester:\n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "        \n",
    "    def calc_turnover(self, w_t, r_t, w_tp1):\n",
    "        \n",
    "        if w_t is None:\n",
    "            return np.sum(np.abs(w_tp1))\n",
    "        \n",
    "        gross_ret = np.sum(w_t * r_t)\n",
    "        if abs(1 + gross_ret) < 1e-8:  # Avoid division by zero\n",
    "            return np.sum(np.abs(w_tp1))\n",
    "        \n",
    "        passive_weight = w_t * (1 + r_t) / (1 + gross_ret)\n",
    "        turnover = np.sum(np.abs(w_tp1 - passive_weight))\n",
    "        return turnover\n",
    "    \n",
    "    def create_portfolios_with_permno_tracking(self, signals, market_caps, permnos, top_pct=0.1, bottom_pct=0.1, weight_scheme=\"VW\"):\n",
    "        \"\"\"\n",
    "        Create portfolio weights based on signals, strictly tracking permno alignment.\n",
    "        weight_scheme: 'VW' for value-weighted, 'EW' for equal-weighted\n",
    "        \"\"\"\n",
    "        n_stocks = len(signals)\n",
    "        top_n    = max(1, int(round(n_stocks * top_pct)))\n",
    "        bottom_n = max(1, int(round(n_stocks * bottom_pct)))\n",
    "        \n",
    "        sorted_idx = np.argsort(signals)[::-1]\n",
    "        \n",
    "        top_idx = sorted_idx[:top_n]\n",
    "        bottom_idx = sorted_idx[-bottom_n:]\n",
    "        \n",
    "        portfolio_data = {}\n",
    "        \n",
    "        long_weights = np.zeros(n_stocks)\n",
    "        if len(top_idx) > 0:\n",
    "            if weight_scheme == \"VW\":\n",
    "                top_market_caps = market_caps[top_idx]\n",
    "                if np.sum(top_market_caps) > 0:\n",
    "                    long_weights[top_idx] = top_market_caps / np.sum(top_market_caps)\n",
    "            else:\n",
    "                long_weights[top_idx] = 1.0 / len(top_idx)\n",
    "        \n",
    "        portfolio_data['long_only'] = {\n",
    "            'weights': long_weights,\n",
    "            'permnos': permnos.copy(),\n",
    "            'selected_permnos': permnos[top_idx] if len(top_idx) > 0 else np.array([])\n",
    "        }\n",
    "        \n",
    "        short_weights = np.zeros(n_stocks)\n",
    "        if len(bottom_idx) > 0:\n",
    "            if weight_scheme == \"VW\":\n",
    "                bottom_market_caps = market_caps[bottom_idx]\n",
    "                if np.sum(bottom_market_caps) > 0:\n",
    "                    short_weights[bottom_idx] = -bottom_market_caps / np.sum(bottom_market_caps)\n",
    "            else:\n",
    "                short_weights[bottom_idx] = -1.0 / len(bottom_idx)\n",
    "        \n",
    "        portfolio_data['short_only'] = {\n",
    "            'weights': short_weights,\n",
    "            'permnos': permnos.copy(),\n",
    "            'selected_permnos': permnos[bottom_idx] if len(bottom_idx) > 0 else np.array([])\n",
    "        }\n",
    "        \n",
    "        # Long-Short portfolio (Top long + Bottom short)\n",
    "        ls_raw = long_weights + short_weights\n",
    "\n",
    "        gross_target = 2.0\n",
    "        current_gross = np.sum(np.abs(long_weights)) + np.sum(np.abs(short_weights))\n",
    "        scale = gross_target / current_gross if current_gross > 1e-8 else 0.0\n",
    "        ls_weights = scale * ls_raw\n",
    "\n",
    "        ls_selected_permnos = np.concatenate([\n",
    "            permnos[top_idx] if len(top_idx) > 0 else np.array([]),\n",
    "            permnos[bottom_idx] if len(bottom_idx) > 0 else np.array([])\n",
    "        ])\n",
    "\n",
    "        portfolio_data['long_short'] = {\n",
    "            'weights': ls_weights,\n",
    "            'permnos': permnos.copy(),\n",
    "            'selected_permnos': ls_selected_permnos\n",
    "        }\n",
    "\n",
    "        return portfolio_data\n",
    "    \n",
    "    def calculate_aligned_portfolio_return(self, portfolio_weights, portfolio_permnos, actual_returns, actual_permnos):\n",
    "        \"\"\"Calculate portfolio return strictly aligned by permno\"\"\"\n",
    "        aligned_returns = np.zeros(len(portfolio_permnos))\n",
    "        \n",
    "        return_dict = dict(zip(actual_permnos, actual_returns))\n",
    "        \n",
    "        for i, permno in enumerate(portfolio_permnos):\n",
    "            if permno in return_dict:\n",
    "                aligned_returns[i] = return_dict[permno]\n",
    "        \n",
    "        portfolio_return = np.sum(portfolio_weights * aligned_returns)\n",
    "        return portfolio_return, aligned_returns\n",
    "\n",
    "    def calculate_metrics(self, returns, turnover_series=None):\n",
    "        \"\"\"Calculate portfolio metrics - only returns summary metrics, not long series\"\"\"\n",
    "        returns = np.array(returns)\n",
    "        \n",
    "        annual_return = np.mean(returns) * 252\n",
    "        annual_vol = np.std(returns, ddof=1) * np.sqrt(252)\n",
    "        sharpe = annual_return / annual_vol if annual_vol > 0 else 0\n",
    "        \n",
    "        log_cum = np.cumsum(np.log1p(returns))\n",
    "        peak_log = np.maximum.accumulate(log_cum)\n",
    "        dd_log = peak_log - log_cum\n",
    "        max_drawdown = 1 - np.exp(-dd_log.max()) \n",
    "        max_1d_loss = np.min(returns) \n",
    "        \n",
    "        avg_turnover = np.mean(turnover_series) if turnover_series is not None else 0\n",
    "        \n",
    "        sortino = sortino_ratio(returns)\n",
    "        cvar95  = cvar(returns, alpha=0.95)\n",
    "\n",
    "        result = {\n",
    "            'annual_return': annual_return,\n",
    "            'annual_vol': annual_vol,\n",
    "            'sharpe': sharpe,\n",
    "            'max_drawdown': max_drawdown,\n",
    "            'max_1d_loss': max_1d_loss,\n",
    "            'avg_turnover': avg_turnover,\n",
    "            'sortino': sortino,\n",
    "            'cvar95': cvar95\n",
    "        }\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Main function for daily prediction and next-day rebalancing portfolio simulation ==========\n",
    "\n",
    "def run_portfolio_simulation_daily_rebalance(start_year=2016, end_year=2024, window_sizes=None, model_names=None,\n",
    "                                           npz_path=\"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/all_window_datasets_scaled.npz\"):\n",
    "    \"\"\"\n",
    "Portfolio simulation (daily prediction, next-day rebalancing):\n",
    "    1. Load quarterly models (trained with quarterly expanding window)\n",
    "    2. Daily prediction to daily signals\n",
    "    3. Daily portfolio construction (T+1 rebalancing, strict permno alignment)\n",
    "    4. Separate summary metrics and time series data\n",
    "    \"\"\"\n",
    "    if window_sizes is None:\n",
    "        window_sizes = [5, 21, 252, 512]\n",
    "    if model_names is None:\n",
    "        model_names = [\"LSTM\"]\n",
    "    \n",
    "    print(\"Starting Daily Rebalance Portfolio Backtesting Simulation\")\n",
    "    \n",
    "    backtester = PortfolioBacktester()\n",
    "    datasets = load_datasets(npz_path)\n",
    "    \n",
    "    summary_results = []\n",
    "    daily_series_data = []\n",
    "    pred_rows = []\n",
    "    \n",
    "    WEIGHT_SCHEMES = [\"VW\", \"EW\"]\n",
    "    \n",
    "    for window in window_sizes:\n",
    "        print(f\"Processing window size: {window}\")\n",
    "        \n",
    "        X_test = datasets[f\"X_test_{window}\"]\n",
    "        y_test = datasets[f\"y_test_{window}\"]\n",
    "        input_size = X_test.shape[1]\n",
    "        meta_test_dict = datasets[f\"meta_test_{window}\"].item()\n",
    "        meta_test = pd.DataFrame.from_dict(meta_test_dict)\n",
    "        \n",
    "        permnos_test = meta_test[\"PERMNO\"].values\n",
    "        meta_test[\"signal_date\"]  = pd.to_datetime(meta_test[\"date\"])\n",
    "        meta_test[\"ret_date\"]     = pd.to_datetime(meta_test[\"ret_date\"])\n",
    "        market_caps = meta_test.get(\"MKTCAP\", np.ones(len(permnos_test)))\n",
    "        \n",
    "        meta_test['date'] = pd.to_datetime(meta_test[\"date\"])\n",
    "        dates_test = meta_test['signal_date']\n",
    "        \n",
    "        for model_name in model_names:\n",
    "            for scheme in WEIGHT_SCHEMES:\n",
    "                all_y_true   = []\n",
    "                all_y_pred   = []\n",
    "                all_permnos  = []\n",
    "                all_meta     = []\n",
    "                print(f\"  Model: {model_name}, Scheme: {scheme}\")\n",
    "                \n",
    "                portfolio_daily_data = {\n",
    "                    'long_only': {'returns': [], 'turnovers': [], 'dates': []},\n",
    "                    'short_only': {'returns': [], 'turnovers': [], 'dates': []},\n",
    "                    'long_short': {'returns': [], 'turnovers': [], 'dates': []}\n",
    "                }\n",
    "                \n",
    "                prev_portfolio_data = {'long_only': None, 'short_only': None, 'long_short': None}\n",
    "                \n",
    "                signals_buf = {}\n",
    "                \n",
    "                for year in range(start_year, min(end_year + 1, 2025)):\n",
    "                    for quarter in range(1, 5):\n",
    "                        # Determine model file year and quarter (T+1 logic: use previous quarter's model to predict current quarter)\n",
    "                        if quarter == 1:\n",
    "                            model_file_year, model_file_quarter = year - 1, 4\n",
    "                        else:\n",
    "                            model_file_year, model_file_quarter = year, quarter - 1\n",
    "                            \n",
    "                        pth_path = f\"models/{model_name}_w{window}_{model_file_year}Q{model_file_quarter}.pth\"\n",
    "                        if not os.path.exists(pth_path):\n",
    "                            print(f\"      Skip: Model file not found {pth_path}\")\n",
    "                            continue\n",
    "                        cpu = torch.device('cpu')\n",
    "                        ckpt = torch.load(pth_path, map_location=cpu)\n",
    "\n",
    "                        hp = ckpt.get(\"hyper_params\", {})\n",
    "                        hidden_size  = hp.get(\"hidden_size\", 64)\n",
    "                        num_layers   = hp.get(\"num_layers\", 2)\n",
    "                        dropout      = hp.get(\"dropout\", 0.2)\n",
    "                        batch_size   = hp.get(\"batch_size\", 256)\n",
    "                        learning_rate= hp.get(\"learning_rate\", 1e-3)\n",
    "                        max_epochs   = hp.get(\"max_epochs\", 0)\n",
    "\n",
    "                        lstm = LSTMWrapper(\n",
    "                            input_size=input_size,\n",
    "                            seq_len=window,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            dropout=dropout,\n",
    "                            learning_rate=learning_rate,\n",
    "                            batch_size=batch_size,\n",
    "                            max_epochs=max_epochs,\n",
    "                            training_device=cpu\n",
    "                        )\n",
    "\n",
    "                        if \"state_dict\" in ckpt:\n",
    "                            state_dict = ckpt[\"state_dict\"]\n",
    "                        elif \"model_state_dict\" in ckpt:\n",
    "                            state_dict = ckpt[\"model_state_dict\"]\n",
    "                        else:\n",
    "                            state_dict = ckpt\n",
    "\n",
    "                        lstm.model.load_state_dict(state_dict, strict=False)\n",
    "                        lstm.is_fitted = True\n",
    "                        model = lstm\n",
    "\n",
    "                        quarter_mask = (\n",
    "                            (dates_test.dt.year == year) & \n",
    "                            (dates_test.dt.quarter == quarter)\n",
    "                        )\n",
    "                        if not np.any(quarter_mask):\n",
    "                            continue\n",
    "                        \n",
    "                        X_quarter = X_test[quarter_mask]\n",
    "                        y_quarter = y_test[quarter_mask]\n",
    "                        permnos_quarter = permnos_test[quarter_mask]\n",
    "                        market_caps_quarter = market_caps[quarter_mask]\n",
    "                        dates_quarter = dates_test[quarter_mask]\n",
    "                        ret_dates_quarter = meta_test.loc[quarter_mask, 'ret_date'].values\n",
    "                        \n",
    "                        y_pred_scaled = model.predict(X_quarter)\n",
    "                        \n",
    "                        y_scaler = Y_SCALERS.get(window)\n",
    "                        if y_scaler is not None:\n",
    "                            y_pred_original = inverse_transform_y(y_pred_scaled, y_scaler)\n",
    "                            y_true_original = inverse_transform_y(y_quarter, y_scaler)\n",
    "                            print(f\"[Inverse Transform] Window {window}: predictions and actuals converted to original scale\")\n",
    "                        else:\n",
    "                            print(f\"[Warning] No scaler found for window {window}, using scaled values\")\n",
    "                            y_pred_original = y_pred_scaled\n",
    "                            y_true_original = y_quarter\n",
    "                        \n",
    "                        df_quarter = pd.DataFrame({\n",
    "                            'signal_date': dates_quarter,\n",
    "                            'ret_date': ret_dates_quarter,\n",
    "                            'permno': permnos_quarter,\n",
    "                            'market_cap': market_caps_quarter,\n",
    "                            'actual_return': y_true_original,                 \n",
    "                            'prediction': y_pred_original   \n",
    "                        })\n",
    "                        \n",
    "                        if scheme == 'VW':\n",
    "                            df_q_save = df_quarter[['signal_date','ret_date','permno',\n",
    "                                                    'actual_return','prediction','market_cap']].copy()\n",
    "                            df_q_save.rename(columns={'actual_return':'y_true',\n",
    "                                                      'prediction':'y_pred'}, inplace=True)\n",
    "                            df_q_save['model']  = model_name\n",
    "                            df_q_save['window'] = window\n",
    "                            pred_rows.append(df_q_save)\n",
    "                        \n",
    "                        all_y_true.append(df_quarter['actual_return'].values)\n",
    "                        all_y_pred.append(df_quarter['prediction'].values)\n",
    "                        all_permnos.append(df_quarter['permno'].values)\n",
    "                        all_meta.append(meta_test.loc[quarter_mask, :])   \n",
    "\n",
    "                        for signal_date, sig_grp in df_quarter.groupby('signal_date'):\n",
    "                            daily_signals = (\n",
    "                                sig_grp.groupby('permno')['prediction'].mean()\n",
    "                                      .to_frame('prediction')\n",
    "                                      .join(sig_grp.groupby('permno')['market_cap'].mean())\n",
    "                            )\n",
    "                            signals_buf[signal_date] = daily_signals\n",
    "\n",
    "                            prev_date = signal_date - pd.tseries.offsets.BDay(1)\n",
    "                            if prev_date not in signals_buf:\n",
    "                                continue\n",
    "\n",
    "                            sigs = signals_buf.pop(prev_date)\n",
    "                            if prev_date in signals_buf:\n",
    "                                del signals_buf[prev_date]\n",
    "\n",
    "                            ret_grp = df_quarter[df_quarter['ret_date'] == signal_date]\n",
    "                            if len(ret_grp) == 0:\n",
    "                                continue\n",
    "\n",
    "                            daily_actual_returns = (\n",
    "                                ret_grp.groupby('permno')['actual_return']\n",
    "                                       .mean()\n",
    "                                       .reindex(sigs.index, fill_value=0)\n",
    "                                       .values\n",
    "                            )\n",
    "                            daily_permnos = sigs.index.values\n",
    "\n",
    "                            portfolios_data = backtester.create_portfolios_with_permno_tracking(\n",
    "                                signals      = sigs['prediction'].values,\n",
    "                                market_caps  = sigs['market_cap'].values,\n",
    "                                permnos      = daily_permnos,\n",
    "                                weight_scheme= scheme\n",
    "                            )\n",
    "                            \n",
    "                            for portfolio_type in ['long_only', 'short_only', 'long_short']:\n",
    "                                portfolio_info = portfolios_data[portfolio_type]\n",
    "                                \n",
    "                                portfolio_return, aligned_returns = backtester.calculate_aligned_portfolio_return(\n",
    "                                    portfolio_weights=portfolio_info['weights'],\n",
    "                                    portfolio_permnos=portfolio_info['permnos'],\n",
    "                                    actual_returns=daily_actual_returns,\n",
    "                                    actual_permnos=daily_permnos\n",
    "                                )\n",
    "                                \n",
    "                                if prev_portfolio_data[portfolio_type] is not None:\n",
    "                                    prev_w_ser = pd.Series(\n",
    "                                        prev_portfolio_data[portfolio_type]['weights'],\n",
    "                                        index=prev_portfolio_data[portfolio_type]['permnos']\n",
    "                                    )\n",
    "                                    cur_w_ser = pd.Series(\n",
    "                                        portfolio_info['weights'],\n",
    "                                        index=portfolio_info['permnos']\n",
    "                                    )\n",
    "\n",
    "                                    prev_r_ser = pd.Series(\n",
    "                                        prev_portfolio_data[portfolio_type]['aligned_returns'],\n",
    "                                        index=prev_portfolio_data[portfolio_type]['permnos']\n",
    "                                    )\n",
    "\n",
    "                                    aligned_prev_w = prev_w_ser.reindex(cur_w_ser.index, fill_value=0).values\n",
    "                                    aligned_prev_r = prev_r_ser.reindex(cur_w_ser.index, fill_value=0).values\n",
    "\n",
    "                                    aligned_cur_w = cur_w_ser.values\n",
    "\n",
    "                                    turnover = backtester.calc_turnover(\n",
    "                                        w_t  = aligned_prev_w,\n",
    "                                        r_t  = aligned_prev_r,\n",
    "                                        w_tp1= aligned_cur_w\n",
    "                                    )\n",
    "                                else:\n",
    "                                    turnover = np.sum(np.abs(portfolio_info['weights']))\n",
    "                                \n",
    "                                portfolio_daily_data[portfolio_type]['returns'].append(portfolio_return)\n",
    "                                portfolio_daily_data[portfolio_type]['turnovers'].append(turnover)\n",
    "                                portfolio_daily_data[portfolio_type]['dates'].append(signal_date)\n",
    "                                \n",
    "                                prev_portfolio_data[portfolio_type] = {\n",
    "                                    'weights'        : portfolio_info['weights'],\n",
    "                                    'permnos'        : portfolio_info['permnos'],\n",
    "                                    'aligned_returns': aligned_returns      \n",
    "                                }\n",
    "                \n",
    "                for portfolio_type in ['long_only', 'short_only', 'long_short']:\n",
    "                    portfolio_data = portfolio_daily_data[portfolio_type]\n",
    "                    \n",
    "                    if len(portfolio_data['returns']) > 0:\n",
    "                        metrics = backtester.calculate_metrics(\n",
    "                            returns=portfolio_data['returns'],\n",
    "                            turnover_series=portfolio_data['turnovers']\n",
    "                        )\n",
    "                        \n",
    "                        rets = np.array(portfolio_data['returns'])\n",
    "                        tovs = np.array(portfolio_data['turnovers'])\n",
    "\n",
    "                        for tc in TC_GRID:\n",
    "                            tag = TC_TAG[tc]\n",
    "                            adj = rets - tovs * tc\n",
    "\n",
    "                            ann_ret = adj.mean() * 252\n",
    "                            ann_vol = adj.std(ddof=1) * np.sqrt(252)\n",
    "                            sharpe  = ann_ret / ann_vol if ann_vol > 0 else 0\n",
    "\n",
    "                            cum_adj = np.cumprod(1 + adj)\n",
    "                            mdd = ((cum_adj - np.maximum.accumulate(cum_adj)) /\n",
    "                                   np.maximum.accumulate(cum_adj)).min()\n",
    "\n",
    "                            metrics[f'{tag}_annual_return'] = ann_ret\n",
    "                            metrics[f'{tag}_annual_vol']    = ann_vol\n",
    "                            metrics[f'{tag}_sharpe']        = sharpe\n",
    "                            metrics[f'{tag}_max_drawdown']  = mdd\n",
    "                        \n",
    "                        summary_results.append({\n",
    "                            'scheme': scheme,\n",
    "                            'model': model_name,\n",
    "                            'window': window,\n",
    "                            'portfolio_type': portfolio_type,\n",
    "                            **metrics\n",
    "                        })\n",
    "                        \n",
    "                        rets_arr = np.array(portfolio_data['returns'])\n",
    "                        tovs_arr = np.array(portfolio_data['turnovers'])\n",
    "                        cum_no_tc = np.log1p(rets_arr).cumsum()\n",
    "\n",
    "                        tc_ret_dict = {}\n",
    "                        tc_cum_dict = {}\n",
    "                        for tc in TC_GRID:\n",
    "                            tag = TC_TAG[tc]\n",
    "                            r = rets_arr - tovs_arr * tc\n",
    "                            tc_ret_dict[tag] = r\n",
    "                            tc_cum_dict[tag] = np.log1p(r).cumsum()\n",
    "\n",
    "                        for i, date in enumerate(portfolio_data['dates']):\n",
    "                            row = {\n",
    "                                'scheme'        : scheme,\n",
    "                                'model'         : model_name,\n",
    "                                'window'        : window,\n",
    "                                'portfolio_type': portfolio_type,\n",
    "                                'date'          : str(date),\n",
    "                                'return'        : rets_arr[i],\n",
    "                                'turnover'      : tovs_arr[i],\n",
    "                                'cumulative'    : cum_no_tc[i],\n",
    "                            }\n",
    "                            for tag in TC_TAG.values():\n",
    "                                row[f'{tag}_return']     = tc_ret_dict[tag][i]\n",
    "                                row[f'{tag}_cumulative'] = tc_cum_dict[tag][i]\n",
    "\n",
    "                            daily_series_data.append(row)\n",
    "\n",
    "                if scheme == \"VW\" and len(all_y_true) > 0:\n",
    "                    y_all    = np.concatenate(all_y_true)\n",
    "                    yhat_all = np.concatenate(all_y_pred)\n",
    "                    perm_all = np.concatenate(all_permnos)\n",
    "                    meta_all = pd.concat(all_meta, ignore_index=True)\n",
    "\n",
    "                    k = X_test.shape[1]\n",
    "\n",
    "                    m1_metrics = overall_interval_metrics_method1(\n",
    "                        y_all, yhat_all, k,\n",
    "                        permnos_all=perm_all,\n",
    "                        meta_all=meta_all\n",
    "                    )\n",
    "\n",
    "                    full_pred_df = pd.concat(pred_rows, ignore_index=True)\n",
    "                    mean_ic, t_ic, pos_ic, _ = calc_ic_daily(full_pred_df, method='spearman')\n",
    "                    m1_metrics['RankIC_mean']  = mean_ic\n",
    "                    m1_metrics['RankIC_t']     = t_ic\n",
    "                    m1_metrics['RankIC_pos%']  = pos_ic\n",
    "\n",
    "                    save_metrics(m1_metrics, name=model_name, window=window,\n",
    "                        path=\"portfolio_metrics.csv\")\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_results)\n",
    "    daily_df = pd.DataFrame(daily_series_data) if daily_series_data else pd.DataFrame()\n",
    "    \n",
    "    tc_columns = [c for c in summary_df.columns if c.startswith('tc')]\n",
    "    summary_df[tc_columns] = summary_df[tc_columns].fillna(0.0)\n",
    "    \n",
    "    def save_split_by_scheme(df, base_filename):\n",
    "        \"\"\"Helper function to save files split by scheme\"\"\"\n",
    "        if df.empty:\n",
    "            print(f\"Warning: DataFrame is empty, skipping save for {base_filename}\")\n",
    "            return None, None\n",
    "            \n",
    "        vw_df = df[df['scheme'] == 'VW']\n",
    "        ew_df = df[df['scheme'] == 'EW']\n",
    "        \n",
    "        vw_filename = f\"{base_filename}_VW.csv\"\n",
    "        ew_filename = f\"{base_filename}_EW.csv\"\n",
    "        \n",
    "        vw_df.to_csv(vw_filename, index=False)\n",
    "        ew_df.to_csv(ew_filename, index=False)\n",
    "        \n",
    "        print(f\"VW results saved to {vw_filename}\")\n",
    "        print(f\"EW results saved to {ew_filename}\")\n",
    "        \n",
    "        return vw_filename, ew_filename\n",
    "    \n",
    "    save_split_by_scheme(summary_df, \"portfolio_results_daily_rebalance\")\n",
    "    \n",
    "    if not daily_df.empty:\n",
    "        save_split_by_scheme(daily_df, \"portfolio_daily_series\")\n",
    "    \n",
    "    if pred_rows:\n",
    "        pred_df = pd.concat(pred_rows, ignore_index=True)\n",
    "        pred_df.to_csv(\"predictions_daily.csv\", index=False)\n",
    "        print(f\"Saved {len(pred_df)} prediction rows to predictions_daily.csv\")\n",
    "    \n",
    "    print(f\"Generated {len(summary_results)} portfolio summary records\")\n",
    "    print(f\"Generated {len(daily_series_data)} daily series records\")\n",
    "    \n",
    "    return summary_df, daily_df, backtester\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting **LSTM** quarterly expanding training 2015‑2024\n",
      "\n",
      "=== Window = 5 ===\n",
      "  – Optuna tuning on initial window…\n",
      "Start hyperparameter tuning...\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.890126, Val=0.842051\n",
      "    Epoch   5/10: Train=1.886850, Val=0.843425\n",
      "    Early‑stopped at epoch 7\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.262625, Val=0.915135\n",
      "    Epoch   5/10: Train=1.261065, Val=0.915395\n",
      "    Early‑stopped at epoch 6\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.230929, Val=0.773627\n",
      "    Epoch   5/10: Train=1.229839, Val=0.773783\n",
      "    Epoch  10/10: Train=1.229291, Val=0.772826\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.895309, Val=0.842421\n",
      "    Epoch   5/10: Train=1.889231, Val=0.842859\n",
      "    Early‑stopped at epoch 6\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.265529, Val=0.914276\n",
      "    Epoch   5/10: Train=1.262759, Val=0.914160\n",
      "    Early‑stopped at epoch 9\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.234275, Val=0.773982\n",
      "    Epoch   5/10: Train=1.231713, Val=0.773702\n",
      "    Epoch  10/10: Train=1.231299, Val=0.773684\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.887415, Val=0.842349\n",
      "    Epoch   5/10: Train=1.885453, Val=0.843702\n",
      "    Early‑stopped at epoch 6\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.261371, Val=0.915345\n",
      "    Epoch   5/10: Train=1.259983, Val=0.915532\n",
      "    Early‑stopped at epoch 6\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.229745, Val=0.772733\n",
      "    Epoch   5/10: Train=1.228274, Val=0.772622\n",
      "    Early‑stopped at epoch 9\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.889636, Val=0.842249\n",
      "    Epoch   5/10: Train=1.885201, Val=0.842767\n",
      "    Early‑stopped at epoch 6\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.262928, Val=0.916256\n",
      "    Epoch   5/10: Train=1.260163, Val=0.915368\n",
      "    Epoch  10/10: Train=1.259689, Val=0.915450\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.231006, Val=0.772350\n",
      "    Epoch   5/10: Train=1.228404, Val=0.772670\n",
      "    Early‑stopped at epoch 6\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.891516, Val=0.838443\n",
      "    Epoch   5/10: Train=1.887760, Val=0.839051\n",
      "    Epoch  10/10: Train=1.886643, Val=0.840178\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.263574, Val=0.915513\n",
      "    Epoch   5/10: Train=1.261629, Val=0.915514\n",
      "    Epoch  10/10: Train=1.260322, Val=0.915846\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.231515, Val=0.771153\n",
      "    Epoch   5/10: Train=1.230126, Val=0.770995\n",
      "    Epoch  10/10: Train=1.229695, Val=0.770574\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.891143, Val=0.842255\n",
      "    Epoch   5/10: Train=1.889434, Val=0.842497\n",
      "    Early‑stopped at epoch 6\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.263942, Val=0.914799\n",
      "    Epoch   5/10: Train=1.262826, Val=0.915196\n",
      "    Early‑stopped at epoch 6\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.232384, Val=0.773556\n",
      "    Epoch   5/10: Train=1.230767, Val=0.773700\n",
      "    Early‑stopped at epoch 6\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.891107, Val=0.838824\n",
      "    Epoch   5/10: Train=1.887604, Val=0.838654\n",
      "    Epoch  10/10: Train=1.887190, Val=0.837847\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.263635, Val=0.915726\n",
      "    Epoch   5/10: Train=1.261525, Val=0.915738\n",
      "    Epoch  10/10: Train=1.261122, Val=0.915662\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.231375, Val=0.771171\n",
      "    Epoch   5/10: Train=1.229781, Val=0.771249\n",
      "    Early‑stopped at epoch 6\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.887570, Val=0.842271\n",
      "    Epoch   5/10: Train=1.885230, Val=0.843430\n",
      "    Early‑stopped at epoch 6\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.261557, Val=0.915314\n",
      "    Epoch   5/10: Train=1.260019, Val=0.915331\n",
      "    Early‑stopped at epoch 6\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.229747, Val=0.772781\n",
      "    Epoch   5/10: Train=1.228225, Val=0.772762\n",
      "    Early‑stopped at epoch 8\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.892251, Val=0.842230\n",
      "    Epoch   5/10: Train=1.887652, Val=0.842218\n",
      "    Epoch  10/10: Train=1.888585, Val=0.841704\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.264100, Val=0.915000\n",
      "    Epoch   5/10: Train=1.261716, Val=0.914954\n",
      "    Epoch  10/10: Train=1.258726, Val=0.915282\n",
      "    Early‑stopped at epoch 10\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.232168, Val=0.772961\n",
      "    Epoch   5/10: Train=1.229906, Val=0.772899\n",
      "    Early‑stopped at epoch 9\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.892221, Val=0.842218\n",
      "    Epoch   5/10: Train=1.887301, Val=0.842036\n",
      "    Epoch  10/10: Train=1.885700, Val=0.841359\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.264122, Val=0.915196\n",
      "    Epoch   5/10: Train=1.261321, Val=0.915462\n",
      "    Early‑stopped at epoch 8\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.232149, Val=0.772991\n",
      "    Epoch   5/10: Train=1.229717, Val=0.772611\n",
      "    Early‑stopped at epoch 9\n",
      "[Optuna‑LSTM] best_MSE=0.738933, params={'batch_size': 64, 'learning_rate': 0.0003823475224675188, 'dropout': 0.18355586841671384, 'hidden_size': 128, 'num_layers': 2, 'max_epochs': 25}\n",
      "\n",
      "[Window 5] 2015Q4\n",
      "    Cold start training …\n",
      "    Training LSTM for 25 epochs on mps\n",
      "    Epoch   1/25: Train=1.063382, Val=0.450329\n",
      "    Epoch   5/25: Train=1.060740, Val=0.450566\n",
      "    Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w5_2015Q4.pth\n",
      "\n",
      "[Window 5] 2016Q1\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 196920 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=1.061730, Val=0.450445\n",
      "        Epoch  3/10: Train=1.060871, Val=0.450533\n",
      "        Epoch  5/10: Train=1.060412, Val=0.450582\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w5_2016Q1.pth\n",
      "\n",
      "[Window 5] 2016Q2\n",
      "    +2956 obs from 2016Q1 -> expanding size 199876\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 199876 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=1.050403, Val=0.503064\n",
      "        Epoch  3/10: Train=1.049975, Val=0.503084\n",
      "        Epoch  5/10: Train=1.049569, Val=0.503119\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w5_2016Q2.pth\n",
      "\n",
      "[Window 5] 2016Q3\n",
      "    +3170 obs from 2016Q2 -> expanding size 203046\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 203046 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=1.039978, Val=0.513152\n",
      "        Epoch  3/10: Train=1.039509, Val=0.513140\n",
      "        Epoch  5/10: Train=1.039398, Val=0.513125\n",
      "        Epoch  7/10: Train=1.039223, Val=0.513119\n",
      "        Epoch  9/10: Train=1.038696, Val=0.513172\n",
      "        Epoch 10/10: Train=1.038658, Val=0.513196\n",
      "[Saved] models/LSTM_w5_2016Q3.pth\n",
      "\n",
      "[Window 5] 2016Q4\n",
      "    +3176 obs from 2016Q3 -> expanding size 206222\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 206222 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=1.029802, Val=0.496717\n",
      "        Epoch  3/10: Train=1.029749, Val=0.496746\n",
      "        Epoch  5/10: Train=1.029531, Val=0.496766\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w5_2016Q4.pth\n",
      "\n",
      "[Window 5] 2017Q1\n",
      "    +3123 obs from 2016Q4 -> expanding size 209345\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 209345 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=1.019788, Val=0.508683\n",
      "        Epoch  3/10: Train=1.019762, Val=0.508696\n",
      "        Epoch  5/10: Train=1.019405, Val=0.508773\n",
      "        Epoch  7/10: Train=1.019069, Val=0.508903\n",
      "        Early‑stopped at epoch 7\n",
      "[Saved] models/LSTM_w5_2017Q1.pth\n",
      "\n",
      "[Window 5] 2017Q2\n",
      "    +3083 obs from 2017Q1 -> expanding size 212428\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 212428 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=1.009436, Val=0.506074\n",
      "        Epoch  3/10: Train=1.009289, Val=0.506169\n",
      "        Epoch  5/10: Train=1.008846, Val=0.506257\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w5_2017Q2.pth\n",
      "\n",
      "[Window 5] 2017Q3\n",
      "    +3122 obs from 2017Q2 -> expanding size 215550\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 215550 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=1.004985, Val=0.459740\n",
      "        Epoch  3/10: Train=1.004886, Val=0.459783\n",
      "        Epoch  5/10: Train=1.004505, Val=0.459791\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w5_2017Q3.pth\n",
      "\n",
      "[Window 5] 2017Q4\n",
      "    +3114 obs from 2017Q3 -> expanding size 218664\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 218664 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.998579, Val=0.444871\n",
      "        Epoch  3/10: Train=0.998534, Val=0.444860\n",
      "        Epoch  5/10: Train=0.997958, Val=0.444852\n",
      "        Epoch  7/10: Train=0.997245, Val=0.444830\n",
      "        Epoch  9/10: Train=0.995582, Val=0.445004\n",
      "        Epoch 10/10: Train=0.995473, Val=0.445010\n",
      "[Saved] models/LSTM_w5_2017Q4.pth\n",
      "\n",
      "[Window 5] 2018Q1\n",
      "    +3115 obs from 2017Q4 -> expanding size 221779\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 221779 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.991636, Val=0.410270\n",
      "        Epoch  3/10: Train=0.991474, Val=0.410245\n",
      "        Epoch  5/10: Train=0.990459, Val=0.410215\n",
      "        Epoch  7/10: Train=0.989132, Val=0.410190\n",
      "        Epoch  9/10: Train=0.987433, Val=0.410212\n",
      "        Epoch 10/10: Train=0.985900, Val=0.410180\n",
      "[Saved] models/LSTM_w5_2018Q1.pth\n",
      "\n",
      "[Window 5] 2018Q2\n",
      "    +2996 obs from 2018Q1 -> expanding size 224775\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 224775 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.977327, Val=0.450994\n",
      "        Epoch  3/10: Train=0.974650, Val=0.451027\n",
      "        Epoch  5/10: Train=0.970263, Val=0.451050\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w5_2018Q2.pth\n",
      "\n",
      "[Window 5] 2018Q3\n",
      "    +3160 obs from 2018Q2 -> expanding size 227935\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 227935 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.968004, Val=0.461989\n",
      "        Epoch  3/10: Train=0.966664, Val=0.462012\n",
      "        Epoch  5/10: Train=0.962536, Val=0.462067\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w5_2018Q3.pth\n",
      "\n",
      "[Window 5] 2018Q4\n",
      "    +3125 obs from 2018Q3 -> expanding size 231060\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 231060 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.959043, Val=0.454087\n",
      "        Epoch  3/10: Train=0.958588, Val=0.454043\n",
      "        Epoch  5/10: Train=0.954165, Val=0.454121\n",
      "        Epoch  7/10: Train=0.951308, Val=0.454112\n",
      "        Early‑stopped at epoch 8\n",
      "[Saved] models/LSTM_w5_2018Q4.pth\n",
      "\n",
      "[Window 5] 2019Q1\n",
      "    +3045 obs from 2018Q4 -> expanding size 234105\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 234105 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.948588, Val=0.521302\n",
      "        Epoch  3/10: Train=0.946071, Val=0.521326\n",
      "        Epoch  5/10: Train=0.943453, Val=0.521373\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w5_2019Q1.pth\n",
      "\n",
      "[Window 5] 2019Q2\n",
      "    +3022 obs from 2019Q1 -> expanding size 237127\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 237127 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.938371, Val=0.546428\n",
      "        Epoch  3/10: Train=0.936987, Val=0.546450\n",
      "        Epoch  5/10: Train=0.932980, Val=0.546434\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w5_2019Q2.pth\n",
      "\n",
      "[Window 5] 2019Q3\n",
      "    +3120 obs from 2019Q2 -> expanding size 240247\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 240247 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.930367, Val=0.554056\n",
      "        Epoch  3/10: Train=0.928811, Val=0.554040\n",
      "        Epoch  5/10: Train=0.925311, Val=0.553975\n",
      "        Epoch  7/10: Train=0.922695, Val=0.554030\n",
      "        Epoch  9/10: Train=0.918204, Val=0.554065\n",
      "        Epoch 10/10: Train=0.916425, Val=0.554089\n",
      "        Early‑stopped at epoch 10\n",
      "[Saved] models/LSTM_w5_2019Q3.pth\n",
      "\n",
      "[Window 5] 2019Q4\n",
      "    +3167 obs from 2019Q3 -> expanding size 243414\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 243414 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.916609, Val=0.583605\n",
      "        Epoch  3/10: Train=0.914693, Val=0.583938\n",
      "        Epoch  5/10: Train=0.910807, Val=0.584413\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w5_2019Q4.pth\n",
      "\n",
      "[Window 5] 2020Q1\n",
      "    +3179 obs from 2019Q4 -> expanding size 246593\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 246593 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.909274, Val=0.572554\n",
      "        Epoch  3/10: Train=0.908485, Val=0.572776\n",
      "        Epoch  5/10: Train=0.904705, Val=0.573014\n",
      "        Epoch  7/10: Train=0.901152, Val=0.573371\n",
      "        Early‑stopped at epoch 7\n",
      "[Saved] models/LSTM_w5_2020Q1.pth\n",
      "\n",
      "[Window 5] 2020Q2\n",
      "    +2595 obs from 2020Q1 -> expanding size 249188\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 249188 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.904287, Val=0.643447\n",
      "        Epoch  3/10: Train=0.903543, Val=0.645081\n",
      "        Epoch  5/10: Train=0.899370, Val=0.646648\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w5_2020Q2.pth\n",
      "\n",
      "[Window 5] 2020Q3\n",
      "    +2839 obs from 2020Q2 -> expanding size 252027\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 252027 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.898671, Val=0.780277\n",
      "        Epoch  3/10: Train=0.898079, Val=0.783696\n",
      "        Epoch  5/10: Train=0.895230, Val=0.787681\n",
      "        Epoch  7/10: Train=0.892506, Val=0.787298\n",
      "        Early‑stopped at epoch 7\n",
      "[Saved] models/LSTM_w5_2020Q3.pth\n",
      "\n",
      "[Window 5] 2020Q4\n",
      "    +3151 obs from 2020Q3 -> expanding size 255178\n",
      "    Re‑tuning via Optuna…\n",
      "Start hyperparameter tuning...\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.632615, Val=0.601385\n",
      "    Epoch   5/10: Train=1.630264, Val=0.601157\n",
      "    Epoch  10/10: Train=1.629801, Val=0.601340\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.307346, Val=0.765352\n",
      "    Epoch   5/10: Train=1.305544, Val=0.766029\n",
      "    Early‑stopped at epoch 6\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.081167, Val=0.387775\n",
      "    Epoch   5/10: Train=1.080176, Val=0.387478\n",
      "    Epoch  10/10: Train=1.079440, Val=0.387511\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.635966, Val=0.601503\n",
      "    Epoch   5/10: Train=1.632296, Val=0.601345\n",
      "    Epoch  10/10: Train=1.631742, Val=0.601605\n",
      "    Early‑stopped at epoch 10\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.310563, Val=0.763643\n",
      "    Epoch   5/10: Train=1.308220, Val=0.764180\n",
      "    Epoch  10/10: Train=1.307646, Val=0.763121\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.083878, Val=0.387573\n",
      "    Epoch   5/10: Train=1.082181, Val=0.387498\n",
      "    Epoch  10/10: Train=1.081615, Val=0.387395\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.630646, Val=0.601054\n",
      "    Epoch   5/10: Train=1.629381, Val=0.600701\n",
      "    Epoch  10/10: Train=1.628228, Val=0.600898\n",
      "    Early‑stopped at epoch 10\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.305911, Val=0.766029\n",
      "    Epoch   5/10: Train=1.303849, Val=0.766517\n",
      "    Early‑stopped at epoch 6\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.080126, Val=0.387318\n",
      "    Epoch   5/10: Train=1.078985, Val=0.387402\n",
      "    Early‑stopped at epoch 6\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.633197, Val=0.601419\n",
      "    Epoch   5/10: Train=1.629173, Val=0.601127\n",
      "    Epoch  10/10: Train=1.628453, Val=0.600996\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.307346, Val=0.765346\n",
      "    Epoch   5/10: Train=1.304244, Val=0.765818\n",
      "    Early‑stopped at epoch 6\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.081512, Val=0.387362\n",
      "    Epoch   5/10: Train=1.079203, Val=0.387365\n",
      "    Epoch  10/10: Train=1.078295, Val=0.387376\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.635330, Val=0.603048\n",
      "    Epoch   5/10: Train=1.631981, Val=0.602967\n",
      "    Early‑stopped at epoch 8\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.307870, Val=0.762867\n",
      "    Epoch   5/10: Train=1.306105, Val=0.764016\n",
      "    Early‑stopped at epoch 7\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.081859, Val=0.387939\n",
      "    Epoch   5/10: Train=1.080667, Val=0.388024\n",
      "    Early‑stopped at epoch 7\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.633989, Val=0.601461\n",
      "    Epoch   5/10: Train=1.631593, Val=0.601444\n",
      "    Epoch  10/10: Train=1.630387, Val=0.600987\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.308631, Val=0.763718\n",
      "    Epoch   5/10: Train=1.306996, Val=0.764337\n",
      "    Early‑stopped at epoch 6\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.082574, Val=0.387855\n",
      "    Epoch   5/10: Train=1.081326, Val=0.387362\n",
      "    Epoch  10/10: Train=1.081181, Val=0.387380\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.635194, Val=0.603038\n",
      "    Epoch   5/10: Train=1.631821, Val=0.602838\n",
      "    Epoch  10/10: Train=1.631271, Val=0.602759\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.307839, Val=0.762913\n",
      "    Epoch   5/10: Train=1.305920, Val=0.765310\n",
      "    Early‑stopped at epoch 6\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.081529, Val=0.388030\n",
      "    Epoch   5/10: Train=1.080579, Val=0.387859\n",
      "    Epoch  10/10: Train=1.080158, Val=0.387734\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.630739, Val=0.601089\n",
      "    Epoch   5/10: Train=1.629264, Val=0.600767\n",
      "    Early‑stopped at epoch 9\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.305946, Val=0.765971\n",
      "    Epoch   5/10: Train=1.303971, Val=0.766446\n",
      "    Early‑stopped at epoch 6\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.080330, Val=0.387353\n",
      "    Epoch   5/10: Train=1.078942, Val=0.387379\n",
      "    Early‑stopped at epoch 7\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.634064, Val=0.601343\n",
      "    Epoch   5/10: Train=1.630777, Val=0.601283\n",
      "    Epoch  10/10: Train=1.630084, Val=0.601159\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.308588, Val=0.762985\n",
      "    Epoch   5/10: Train=1.305533, Val=0.763327\n",
      "    Early‑stopped at epoch 8\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.082226, Val=0.387547\n",
      "    Epoch   5/10: Train=1.080001, Val=0.387510\n",
      "    Early‑stopped at epoch 8\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.634944, Val=0.601372\n",
      "    Epoch   5/10: Train=1.630405, Val=0.601174\n",
      "    Epoch  10/10: Train=1.628787, Val=0.601327\n",
      "    Early‑stopped at epoch 10\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.308325, Val=0.763254\n",
      "    Epoch   5/10: Train=1.305255, Val=0.763676\n",
      "    Early‑stopped at epoch 8\n",
      "    Training LSTM for 10 epochs on cpu\n",
      "    Epoch   1/10: Train=1.082110, Val=0.387480\n",
      "    Epoch   5/10: Train=1.079981, Val=0.387452\n",
      "    Early‑stopped at epoch 8\n",
      "[Optuna‑LSTM] best_MSE=0.711009, params={'batch_size': 64, 'learning_rate': 0.008706020878304856, 'dropout': 0.2497327922401265, 'hidden_size': 64, 'num_layers': 1, 'max_epochs': 25}\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 255178 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.891229, Val=0.840906\n",
      "        Epoch  3/10: Train=0.889469, Val=0.840860\n",
      "        Epoch  5/10: Train=0.886887, Val=0.842230\n",
      "        Epoch  7/10: Train=0.883744, Val=0.845525\n",
      "        Early‑stopped at epoch 8\n",
      "[Saved] models/LSTM_w5_2020Q4.pth\n",
      "\n",
      "[Window 5] 2021Q1\n",
      "    +3114 obs from 2020Q4 -> expanding size 258292\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 258292 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.884665, Val=0.897409\n",
      "        Epoch  3/10: Train=0.882685, Val=0.894963\n",
      "        Epoch  5/10: Train=0.880107, Val=0.896870\n",
      "        Epoch  7/10: Train=0.876853, Val=0.898161\n",
      "        Epoch  9/10: Train=0.875849, Val=0.898757\n",
      "        Early‑stopped at epoch 9\n",
      "[Saved] models/LSTM_w5_2021Q1.pth\n",
      "\n",
      "[Window 5] 2021Q2\n",
      "    +2989 obs from 2021Q1 -> expanding size 261281\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 261281 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.879736, Val=0.911809\n",
      "        Epoch  3/10: Train=0.877565, Val=0.913186\n",
      "        Epoch  5/10: Train=0.873052, Val=0.919250\n",
      "        Epoch  7/10: Train=0.871753, Val=0.919784\n",
      "        Early‑stopped at epoch 7\n",
      "[Saved] models/LSTM_w5_2021Q2.pth\n",
      "\n",
      "[Window 5] 2021Q3\n",
      "    +3132 obs from 2021Q2 -> expanding size 264413\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 264413 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.871430, Val=0.921652\n",
      "        Epoch  3/10: Train=0.868328, Val=0.926066\n",
      "        Epoch  5/10: Train=0.865782, Val=0.926517\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w5_2021Q3.pth\n",
      "\n",
      "[Window 5] 2021Q4\n",
      "    +3173 obs from 2021Q3 -> expanding size 267586\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 267586 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.863798, Val=0.932701\n",
      "        Epoch  3/10: Train=0.864141, Val=0.938879\n",
      "        Epoch  5/10: Train=0.861569, Val=0.940688\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w5_2021Q4.pth\n",
      "\n",
      "[Window 5] 2022Q1\n",
      "    +3153 obs from 2021Q4 -> expanding size 270739\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 270739 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.861123, Val=0.917206\n",
      "        Epoch  3/10: Train=0.862288, Val=0.918944\n",
      "        Epoch  5/10: Train=0.859199, Val=0.923060\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w5_2022Q1.pth\n",
      "\n",
      "[Window 5] 2022Q2\n",
      "    +3029 obs from 2022Q1 -> expanding size 273768\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 273768 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.856323, Val=0.990624\n",
      "        Epoch  3/10: Train=0.857195, Val=0.993776\n",
      "        Epoch  5/10: Train=0.852842, Val=0.993409\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w5_2022Q2.pth\n",
      "\n",
      "[Window 5] 2022Q3\n",
      "    +2969 obs from 2022Q2 -> expanding size 276737\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 276737 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.858248, Val=1.008061\n",
      "        Epoch  3/10: Train=0.858347, Val=1.009938\n",
      "        Epoch  5/10: Train=0.854870, Val=1.011997\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w5_2022Q3.pth\n",
      "\n",
      "[Window 5] 2022Q4\n",
      "    +3152 obs from 2022Q3 -> expanding size 279889\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 279889 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.866453, Val=0.911824\n",
      "        Epoch  3/10: Train=0.881431, Val=0.913240\n",
      "        Epoch  5/10: Train=0.875644, Val=0.916939\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w5_2022Q4.pth\n",
      "\n",
      "[Window 5] 2023Q1\n",
      "    +3070 obs from 2022Q4 -> expanding size 282959\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 282959 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.901979, Val=0.911347\n",
      "        Epoch  3/10: Train=0.878133, Val=0.912191\n",
      "        Epoch  5/10: Train=0.867787, Val=0.913677\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w5_2023Q1.pth\n",
      "\n",
      "[Window 5] 2023Q2\n",
      "    +3064 obs from 2023Q1 -> expanding size 286023\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 286023 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.902355, Val=0.876577\n",
      "        Epoch  3/10: Train=0.874082, Val=0.877744\n",
      "        Epoch  5/10: Train=0.870054, Val=0.879411\n",
      "        Epoch  7/10: Train=0.864008, Val=0.881513\n",
      "        Early‑stopped at epoch 7\n",
      "[Saved] models/LSTM_w5_2023Q2.pth\n",
      "\n",
      "[Window 5] 2023Q3\n",
      "    +3069 obs from 2023Q2 -> expanding size 289092\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 289092 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.875517, Val=0.856880\n",
      "        Epoch  3/10: Train=0.869381, Val=0.857316\n",
      "        Epoch  5/10: Train=0.864762, Val=0.861575\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w5_2023Q3.pth\n",
      "\n",
      "[Window 5] 2023Q4\n",
      "    +3121 obs from 2023Q3 -> expanding size 292213\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 292213 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.872822, Val=0.824856\n",
      "        Epoch  3/10: Train=0.868507, Val=0.826071\n",
      "        Epoch  5/10: Train=0.864338, Val=0.825958\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w5_2023Q4.pth\n",
      "\n",
      "[Window 5] 2024Q1\n",
      "    +3113 obs from 2023Q4 -> expanding size 295326\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 295326 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.867271, Val=0.841274\n",
      "        Epoch  3/10: Train=0.863702, Val=0.843315\n",
      "        Epoch  5/10: Train=0.860683, Val=0.843246\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w5_2024Q1.pth\n",
      "\n",
      "[Window 5] 2024Q2\n",
      "    +3026 obs from 2024Q1 -> expanding size 298352\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 298352 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.862478, Val=0.839023\n",
      "        Epoch  3/10: Train=0.860376, Val=0.839951\n",
      "        Epoch  5/10: Train=0.855040, Val=0.841059\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w5_2024Q2.pth\n",
      "\n",
      "[Window 5] 2024Q3\n",
      "    +3112 obs from 2024Q2 -> expanding size 301464\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 301464 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.860456, Val=0.818578\n",
      "        Epoch  3/10: Train=0.858859, Val=0.819716\n",
      "        Epoch  5/10: Train=0.853449, Val=0.821666\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w5_2024Q3.pth\n",
      "\n",
      "=== Window = 21 ===\n",
      "\n",
      "[Window 21] 2015Q4\n",
      "    Cold start training …\n",
      "    Training LSTM for 25 epochs on mps\n",
      "    Epoch   1/25: Train=1.065071, Val=0.455169\n",
      "    Epoch   5/25: Train=1.063151, Val=0.454876\n",
      "    Epoch  10/25: Train=1.062811, Val=0.454320\n",
      "    Epoch  15/25: Train=1.062776, Val=0.454255\n",
      "    Epoch  20/25: Train=1.059491, Val=0.454455\n",
      "    Early‑stopped at epoch 22\n",
      "[Saved] models/LSTM_w21_2015Q4.pth\n",
      "\n",
      "[Window 21] 2016Q1\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 196120 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=1.062376, Val=0.454229\n",
      "        Epoch  3/10: Train=1.062348, Val=0.454162\n",
      "        Epoch  5/10: Train=1.061578, Val=0.454272\n",
      "        Epoch  7/10: Train=1.061004, Val=0.454134\n",
      "        Epoch  9/10: Train=1.060935, Val=0.454053\n",
      "        Epoch 10/10: Train=1.060990, Val=0.454197\n",
      "[Saved] models/LSTM_w21_2016Q1.pth\n",
      "\n",
      "[Window 21] 2016Q2\n",
      "    +2956 obs from 2016Q1 -> expanding size 199076\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 199076 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=1.049390, Val=0.507541\n",
      "        Epoch  3/10: Train=1.049403, Val=0.507398\n",
      "        Epoch  5/10: Train=1.049219, Val=0.507510\n",
      "        Epoch  7/10: Train=1.049068, Val=0.507284\n",
      "        Epoch  9/10: Train=1.051965, Val=0.507778\n",
      "        Epoch 10/10: Train=1.049350, Val=0.507151\n",
      "[Saved] models/LSTM_w21_2016Q2.pth\n",
      "\n",
      "[Window 21] 2016Q3\n",
      "    +3170 obs from 2016Q2 -> expanding size 202246\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 202246 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=1.038439, Val=0.517937\n",
      "        Epoch  3/10: Train=1.039097, Val=0.518184\n",
      "        Epoch  5/10: Train=1.037449, Val=0.517805\n",
      "        Epoch  7/10: Train=1.036762, Val=0.518126\n",
      "        Epoch  9/10: Train=1.038077, Val=0.518057\n",
      "        Epoch 10/10: Train=1.037893, Val=0.517852\n",
      "        Early‑stopped at epoch 10\n",
      "[Saved] models/LSTM_w21_2016Q3.pth\n",
      "\n",
      "[Window 21] 2016Q4\n",
      "    +3176 obs from 2016Q3 -> expanding size 205422\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 205422 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=1.027359, Val=0.500864\n",
      "        Epoch  3/10: Train=1.028557, Val=0.501223\n",
      "        Epoch  5/10: Train=1.030052, Val=0.501542\n",
      "        Epoch  7/10: Train=1.030625, Val=0.501049\n",
      "        Early‑stopped at epoch 7\n",
      "[Saved] models/LSTM_w21_2016Q4.pth\n",
      "\n",
      "[Window 21] 2017Q1\n",
      "    +3123 obs from 2016Q4 -> expanding size 208545\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 208545 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=1.018215, Val=0.512638\n",
      "        Epoch  3/10: Train=1.019117, Val=0.512887\n",
      "        Epoch  5/10: Train=1.018506, Val=0.513066\n",
      "        Epoch  7/10: Train=1.018396, Val=0.513211\n",
      "        Epoch  9/10: Train=1.029511, Val=0.515921\n",
      "        Early‑stopped at epoch 9\n",
      "[Saved] models/LSTM_w21_2017Q1.pth\n",
      "\n",
      "[Window 21] 2017Q2\n",
      "    +3083 obs from 2017Q1 -> expanding size 211628\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 211628 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=1.006810, Val=0.508671\n",
      "        Epoch  3/10: Train=1.008498, Val=0.507968\n",
      "        Epoch  5/10: Train=1.007426, Val=0.508033\n",
      "        Epoch  7/10: Train=1.007003, Val=0.508191\n",
      "        Early‑stopped at epoch 8\n",
      "[Saved] models/LSTM_w21_2017Q2.pth\n",
      "\n",
      "[Window 21] 2017Q3\n",
      "    +3122 obs from 2017Q2 -> expanding size 214750\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 214750 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=1.003193, Val=0.465542\n",
      "        Epoch  3/10: Train=1.006753, Val=0.464942\n",
      "        Epoch  5/10: Train=1.005863, Val=0.465342\n",
      "        Epoch  7/10: Train=1.006669, Val=0.464719\n",
      "        Epoch  9/10: Train=1.005639, Val=0.464343\n",
      "        Epoch 10/10: Train=1.004673, Val=0.464750\n",
      "[Saved] models/LSTM_w21_2017Q3.pth\n",
      "\n",
      "[Window 21] 2017Q4\n",
      "    +3114 obs from 2017Q3 -> expanding size 217864\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 217864 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.997108, Val=0.446887\n",
      "        Epoch  3/10: Train=0.997848, Val=0.447048\n",
      "        Epoch  5/10: Train=0.999733, Val=0.447336\n",
      "        Epoch  7/10: Train=0.999556, Val=0.446839\n",
      "        Early‑stopped at epoch 7\n",
      "[Saved] models/LSTM_w21_2017Q4.pth\n",
      "\n",
      "[Window 21] 2018Q1\n",
      "    +3115 obs from 2017Q4 -> expanding size 220979\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 220979 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.993791, Val=0.413564\n",
      "        Epoch  3/10: Train=0.993634, Val=0.413340\n",
      "        Epoch  5/10: Train=0.995633, Val=0.413923\n",
      "        Epoch  7/10: Train=0.994061, Val=0.413671\n",
      "        Early‑stopped at epoch 7\n",
      "[Saved] models/LSTM_w21_2018Q1.pth\n",
      "\n",
      "[Window 21] 2018Q2\n",
      "    +2996 obs from 2018Q1 -> expanding size 223975\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 223975 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.985460, Val=0.453410\n",
      "        Epoch  3/10: Train=0.985375, Val=0.453258\n",
      "        Epoch  5/10: Train=0.986822, Val=0.453489\n",
      "        Epoch  7/10: Train=0.986564, Val=0.453340\n",
      "        Early‑stopped at epoch 8\n",
      "[Saved] models/LSTM_w21_2018Q2.pth\n",
      "\n",
      "[Window 21] 2018Q3\n",
      "    +3160 obs from 2018Q2 -> expanding size 227135\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 227135 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.977125, Val=0.467244\n",
      "        Epoch  3/10: Train=0.977493, Val=0.467596\n",
      "        Epoch  5/10: Train=0.979988, Val=0.467491\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w21_2018Q3.pth\n",
      "\n",
      "[Window 21] 2018Q4\n",
      "    +3125 obs from 2018Q3 -> expanding size 230260\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 230260 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.968491, Val=0.458327\n",
      "        Epoch  3/10: Train=0.972732, Val=0.458416\n",
      "        Epoch  5/10: Train=0.971145, Val=0.458390\n",
      "        Epoch  7/10: Train=0.972284, Val=0.458481\n",
      "        Early‑stopped at epoch 7\n",
      "[Saved] models/LSTM_w21_2018Q4.pth\n",
      "\n",
      "[Window 21] 2019Q1\n",
      "    +3045 obs from 2018Q4 -> expanding size 233305\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 233305 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.961780, Val=0.526439\n",
      "        Epoch  3/10: Train=0.969287, Val=0.526314\n",
      "        Epoch  5/10: Train=0.967949, Val=0.526204\n",
      "        Epoch  7/10: Train=0.968163, Val=0.526261\n",
      "        Epoch  9/10: Train=0.967622, Val=0.526281\n",
      "        Epoch 10/10: Train=0.968187, Val=0.526365\n",
      "[Saved] models/LSTM_w21_2019Q1.pth\n",
      "\n",
      "[Window 21] 2019Q2\n",
      "    +3022 obs from 2019Q1 -> expanding size 236327\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 236327 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.957583, Val=0.550549\n",
      "        Epoch  3/10: Train=0.958442, Val=0.550633\n",
      "        Epoch  5/10: Train=0.958081, Val=0.550672\n",
      "        Epoch  7/10: Train=0.957503, Val=0.550616\n",
      "        Epoch  9/10: Train=0.954693, Val=0.550667\n",
      "        Early‑stopped at epoch 9\n",
      "[Saved] models/LSTM_w21_2019Q2.pth\n",
      "\n",
      "[Window 21] 2019Q3\n",
      "    +3120 obs from 2019Q2 -> expanding size 239447\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 239447 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.948909, Val=0.558272\n",
      "        Epoch  3/10: Train=0.950774, Val=0.558215\n",
      "        Epoch  5/10: Train=0.950811, Val=0.558115\n",
      "        Epoch  7/10: Train=0.949269, Val=0.558165\n",
      "        Epoch  9/10: Train=0.949472, Val=0.558410\n",
      "        Epoch 10/10: Train=0.950671, Val=0.558527\n",
      "        Early‑stopped at epoch 10\n",
      "[Saved] models/LSTM_w21_2019Q3.pth\n",
      "\n",
      "[Window 21] 2019Q4\n",
      "    +3167 obs from 2019Q3 -> expanding size 242614\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 242614 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.942208, Val=0.588892\n",
      "        Epoch  3/10: Train=0.944981, Val=0.588840\n",
      "        Epoch  5/10: Train=0.944538, Val=0.589201\n",
      "        Epoch  7/10: Train=0.944162, Val=0.588929\n",
      "        Early‑stopped at epoch 7\n",
      "[Saved] models/LSTM_w21_2019Q4.pth\n",
      "\n",
      "[Window 21] 2020Q1\n",
      "    +3179 obs from 2019Q4 -> expanding size 245793\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 245793 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.935774, Val=0.579573\n",
      "        Epoch  3/10: Train=0.939196, Val=0.579602\n",
      "        Epoch  5/10: Train=0.936848, Val=0.579559\n",
      "        Epoch  7/10: Train=0.936580, Val=0.579117\n",
      "        Epoch  9/10: Train=0.936655, Val=0.579614\n",
      "        Epoch 10/10: Train=0.935780, Val=0.579167\n",
      "[Saved] models/LSTM_w21_2020Q1.pth\n",
      "\n",
      "[Window 21] 2020Q2\n",
      "    +2595 obs from 2020Q1 -> expanding size 248388\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 248388 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.931834, Val=0.661423\n",
      "        Epoch  3/10: Train=0.935945, Val=0.651854\n",
      "        Epoch  5/10: Train=0.936567, Val=0.654322\n",
      "        Epoch  7/10: Train=0.936862, Val=0.652481\n",
      "        Early‑stopped at epoch 8\n",
      "[Saved] models/LSTM_w21_2020Q2.pth\n",
      "\n",
      "[Window 21] 2020Q3\n",
      "    +2839 obs from 2020Q2 -> expanding size 251227\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 251227 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.929257, Val=0.771752\n",
      "        Epoch  3/10: Train=0.933302, Val=0.771473\n",
      "        Epoch  5/10: Train=0.931918, Val=0.771438\n",
      "        Epoch  7/10: Train=0.931671, Val=0.770057\n",
      "        Epoch  9/10: Train=0.933714, Val=0.772269\n",
      "        Epoch 10/10: Train=0.932864, Val=0.770251\n",
      "[Saved] models/LSTM_w21_2020Q3.pth\n",
      "\n",
      "[Window 21] 2020Q4\n",
      "    +3151 obs from 2020Q3 -> expanding size 254378\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 254378 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.924888, Val=0.839797\n",
      "        Epoch  3/10: Train=0.931085, Val=0.841054\n",
      "        Epoch  5/10: Train=0.935745, Val=0.836425\n",
      "        Epoch  7/10: Train=0.935076, Val=0.836339\n",
      "        Epoch  9/10: Train=0.933714, Val=0.837414\n",
      "        Epoch 10/10: Train=0.932466, Val=0.839438\n",
      "[Saved] models/LSTM_w21_2020Q4.pth\n",
      "\n",
      "[Window 21] 2021Q1\n",
      "    +3114 obs from 2020Q4 -> expanding size 257492\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 257492 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.929191, Val=0.879507\n",
      "        Epoch  3/10: Train=0.932523, Val=0.879313\n",
      "        Epoch  5/10: Train=0.932069, Val=0.880506\n",
      "        Epoch  7/10: Train=0.934486, Val=0.881250\n",
      "        Early‑stopped at epoch 8\n",
      "[Saved] models/LSTM_w21_2021Q1.pth\n",
      "\n",
      "[Window 21] 2021Q2\n",
      "    +2989 obs from 2021Q1 -> expanding size 260481\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 260481 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.929228, Val=0.894504\n",
      "        Epoch  3/10: Train=0.930917, Val=0.895929\n",
      "        Epoch  5/10: Train=0.935178, Val=0.893456\n",
      "        Epoch  7/10: Train=0.932982, Val=0.896165\n",
      "        Epoch  9/10: Train=0.934162, Val=0.898026\n",
      "        Epoch 10/10: Train=0.935411, Val=0.901714\n",
      "        Early‑stopped at epoch 10\n",
      "[Saved] models/LSTM_w21_2021Q2.pth\n",
      "\n",
      "[Window 21] 2021Q3\n",
      "    +3132 obs from 2021Q2 -> expanding size 263613\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 263613 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.926688, Val=0.909839\n",
      "        Epoch  3/10: Train=0.929189, Val=0.912494\n",
      "        Epoch  5/10: Train=0.928769, Val=0.910389\n",
      "        Epoch  7/10: Train=0.927787, Val=0.910837\n",
      "        Epoch  9/10: Train=0.929265, Val=0.908546\n",
      "        Epoch 10/10: Train=0.929155, Val=0.913245\n",
      "[Saved] models/LSTM_w21_2021Q3.pth\n",
      "\n",
      "[Window 21] 2021Q4\n",
      "    +3173 obs from 2021Q3 -> expanding size 266786\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 266786 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.921961, Val=0.925045\n",
      "        Epoch  3/10: Train=0.923498, Val=0.921276\n",
      "        Epoch  5/10: Train=0.924456, Val=0.930250\n",
      "        Epoch  7/10: Train=0.926130, Val=0.929624\n",
      "        Early‑stopped at epoch 8\n",
      "[Saved] models/LSTM_w21_2021Q4.pth\n",
      "\n",
      "[Window 21] 2022Q1\n",
      "    +3153 obs from 2021Q4 -> expanding size 269939\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 269939 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.920559, Val=0.907513\n",
      "        Epoch  3/10: Train=0.920370, Val=0.909724\n",
      "        Epoch  5/10: Train=0.923401, Val=0.906771\n",
      "        Epoch  7/10: Train=0.924228, Val=0.908842\n",
      "        Epoch  9/10: Train=0.923967, Val=0.907588\n",
      "        Epoch 10/10: Train=0.924385, Val=0.909857\n",
      "        Early‑stopped at epoch 10\n",
      "[Saved] models/LSTM_w21_2022Q1.pth\n",
      "\n",
      "[Window 21] 2022Q2\n",
      "    +3029 obs from 2022Q1 -> expanding size 272968\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 272968 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.918422, Val=0.979812\n",
      "        Epoch  3/10: Train=0.916792, Val=0.982320\n",
      "        Epoch  5/10: Train=0.919573, Val=0.978784\n",
      "        Epoch  7/10: Train=0.918913, Val=0.980151\n",
      "        Epoch  9/10: Train=0.916220, Val=0.979248\n",
      "        Epoch 10/10: Train=0.918060, Val=0.978248\n",
      "[Saved] models/LSTM_w21_2022Q2.pth\n",
      "\n",
      "[Window 21] 2022Q3\n",
      "    +2969 obs from 2022Q2 -> expanding size 275937\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 275937 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.919659, Val=1.152800\n",
      "        Epoch  3/10: Train=0.921204, Val=1.139641\n",
      "        Epoch  5/10: Train=0.920437, Val=1.120248\n",
      "        Epoch  7/10: Train=0.921726, Val=1.147488\n",
      "        Early‑stopped at epoch 7\n",
      "[Saved] models/LSTM_w21_2022Q3.pth\n",
      "\n",
      "[Window 21] 2022Q4\n",
      "    +3152 obs from 2022Q3 -> expanding size 279089\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 279089 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.928184, Val=0.938813\n",
      "        Epoch  3/10: Train=0.930323, Val=0.938426\n",
      "        Epoch  5/10: Train=0.930656, Val=0.941377\n",
      "        Epoch  7/10: Train=0.933162, Val=0.929342\n",
      "        Epoch  9/10: Train=0.935841, Val=0.925896\n",
      "        Epoch 10/10: Train=0.934925, Val=0.928588\n",
      "[Saved] models/LSTM_w21_2022Q4.pth\n",
      "\n",
      "[Window 21] 2023Q1\n",
      "    +3070 obs from 2022Q4 -> expanding size 282159\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 282159 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.935573, Val=0.924233\n",
      "        Epoch  3/10: Train=0.932986, Val=0.925463\n",
      "        Epoch  5/10: Train=0.933012, Val=0.924801\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w21_2023Q1.pth\n",
      "\n",
      "[Window 21] 2023Q2\n",
      "    +3064 obs from 2023Q1 -> expanding size 285223\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 285223 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.936873, Val=0.905397\n",
      "        Epoch  3/10: Train=0.933530, Val=0.904699\n",
      "        Epoch  5/10: Train=0.935928, Val=0.907485\n",
      "        Epoch  7/10: Train=0.935338, Val=0.909909\n",
      "        Early‑stopped at epoch 7\n",
      "[Saved] models/LSTM_w21_2023Q2.pth\n",
      "\n",
      "[Window 21] 2023Q3\n",
      "    +3069 obs from 2023Q2 -> expanding size 288292\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 288292 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.932589, Val=0.871263\n",
      "        Epoch  3/10: Train=0.933095, Val=0.867497\n",
      "        Epoch  5/10: Train=0.935304, Val=0.867131\n",
      "        Epoch  7/10: Train=0.934926, Val=0.871764\n",
      "        Early‑stopped at epoch 7\n",
      "[Saved] models/LSTM_w21_2023Q3.pth\n",
      "\n",
      "[Window 21] 2023Q4\n",
      "    +3121 obs from 2023Q3 -> expanding size 291413\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 291413 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.930324, Val=0.834546\n",
      "        Epoch  3/10: Train=0.930736, Val=0.834558\n",
      "        Epoch  5/10: Train=0.929188, Val=0.833207\n",
      "        Epoch  7/10: Train=0.930372, Val=0.835523\n",
      "        Early‑stopped at epoch 7\n",
      "[Saved] models/LSTM_w21_2023Q4.pth\n",
      "\n",
      "[Window 21] 2024Q1\n",
      "    +3113 obs from 2023Q4 -> expanding size 294526\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 294526 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.926981, Val=0.853590\n",
      "        Epoch  3/10: Train=0.929122, Val=0.851662\n",
      "        Epoch  5/10: Train=0.928832, Val=0.859345\n",
      "        Epoch  7/10: Train=0.929116, Val=0.850774\n",
      "        Epoch  9/10: Train=0.927351, Val=0.850445\n",
      "        Epoch 10/10: Train=0.927099, Val=0.852310\n",
      "[Saved] models/LSTM_w21_2024Q1.pth\n",
      "\n",
      "[Window 21] 2024Q2\n",
      "    +3026 obs from 2024Q1 -> expanding size 297552\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 297552 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.922617, Val=0.849336\n",
      "        Epoch  3/10: Train=0.924815, Val=0.849321\n",
      "        Epoch  5/10: Train=0.926023, Val=0.857811\n",
      "        Epoch  7/10: Train=0.925358, Val=0.850657\n",
      "        Epoch  9/10: Train=0.917395, Val=0.845523\n",
      "        Epoch 10/10: Train=0.917058, Val=0.845227\n",
      "[Saved] models/LSTM_w21_2024Q2.pth\n",
      "\n",
      "[Window 21] 2024Q3\n",
      "    +3112 obs from 2024Q2 -> expanding size 300664\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 300664 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.925663, Val=0.829622\n",
      "        Epoch  3/10: Train=0.930174, Val=0.833024\n",
      "        Epoch  5/10: Train=0.925937, Val=0.828454\n",
      "        Epoch  7/10: Train=0.925542, Val=0.828961\n",
      "        Epoch  9/10: Train=0.924451, Val=0.833996\n",
      "        Epoch 10/10: Train=0.926647, Val=0.830407\n",
      "[Saved] models/LSTM_w21_2024Q3.pth\n",
      "\n",
      "=== Window = 252 ===\n",
      "\n",
      "[Window 252] 2015Q4\n",
      "    Cold start training …\n",
      "    Training LSTM for 25 epochs on mps\n",
      "    Epoch   1/25: Train=1.059519, Val=0.516349\n",
      "    Epoch   5/25: Train=1.056790, Val=0.515844\n",
      "    Epoch  10/25: Train=1.056344, Val=0.515150\n",
      "    Epoch  15/25: Train=1.056018, Val=0.515230\n",
      "    Early‑stopped at epoch 18\n",
      "[Saved] models/LSTM_w252_2015Q4.pth\n",
      "\n",
      "[Window 252] 2016Q1\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 184570 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=1.056267, Val=0.515094\n",
      "        Epoch  3/10: Train=1.055948, Val=0.514959\n",
      "        Epoch  5/10: Train=1.056031, Val=0.515268\n",
      "        Epoch  7/10: Train=1.056097, Val=0.515175\n",
      "        Early‑stopped at epoch 8\n",
      "[Saved] models/LSTM_w252_2016Q1.pth\n",
      "\n",
      "[Window 252] 2016Q2\n",
      "    +2956 obs from 2016Q1 -> expanding size 187526\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 187526 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=1.045191, Val=0.575135\n",
      "        Epoch  3/10: Train=1.045569, Val=0.575143\n",
      "        Epoch  5/10: Train=1.044833, Val=0.575231\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w252_2016Q2.pth\n",
      "\n",
      "[Window 252] 2016Q3\n",
      "    +3170 obs from 2016Q2 -> expanding size 190696\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 190696 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=1.036606, Val=0.580081\n",
      "        Epoch  3/10: Train=1.036623, Val=0.580087\n",
      "        Epoch  5/10: Train=1.036370, Val=0.579960\n",
      "        Epoch  7/10: Train=1.037236, Val=0.580499\n",
      "        Epoch  9/10: Train=1.036630, Val=0.580624\n",
      "        Epoch 10/10: Train=1.036735, Val=0.580315\n",
      "        Early‑stopped at epoch 10\n",
      "[Saved] models/LSTM_w252_2016Q3.pth\n",
      "\n",
      "[Window 252] 2016Q4\n",
      "    +3176 obs from 2016Q3 -> expanding size 193872\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 193872 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=1.027352, Val=0.562899\n",
      "        Epoch  3/10: Train=1.026901, Val=0.563531\n",
      "        Epoch  5/10: Train=1.027208, Val=0.563511\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w252_2016Q4.pth\n",
      "\n",
      "[Window 252] 2017Q1\n",
      "    +3123 obs from 2016Q4 -> expanding size 196995\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 196995 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=1.016596, Val=0.576085\n",
      "        Epoch  3/10: Train=1.016862, Val=0.575758\n",
      "        Epoch  5/10: Train=1.016540, Val=0.575748\n",
      "        Epoch  7/10: Train=1.016965, Val=0.575578\n",
      "        Epoch  9/10: Train=1.016636, Val=0.575565\n",
      "        Epoch 10/10: Train=1.016822, Val=0.575896\n",
      "[Saved] models/LSTM_w252_2017Q1.pth\n",
      "\n",
      "[Window 252] 2017Q2\n",
      "    +3083 obs from 2017Q1 -> expanding size 200078\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 200078 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=1.008420, Val=0.546210\n",
      "        Epoch  3/10: Train=1.008458, Val=0.546001\n",
      "        Epoch  5/10: Train=1.009273, Val=0.546152\n",
      "        Epoch  7/10: Train=1.008365, Val=0.546181\n",
      "        Early‑stopped at epoch 8\n",
      "[Saved] models/LSTM_w252_2017Q2.pth\n",
      "\n",
      "[Window 252] 2017Q3\n",
      "    +3122 obs from 2017Q2 -> expanding size 203200\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 203200 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=1.004510, Val=0.505935\n",
      "        Epoch  3/10: Train=1.003845, Val=0.505740\n",
      "        Epoch  5/10: Train=1.004692, Val=0.505695\n",
      "        Epoch  7/10: Train=1.004091, Val=0.505444\n",
      "        Epoch  9/10: Train=1.004761, Val=0.505414\n",
      "        Epoch 10/10: Train=1.004886, Val=0.505339\n",
      "[Saved] models/LSTM_w252_2017Q3.pth\n",
      "\n",
      "[Window 252] 2017Q4\n",
      "    +3114 obs from 2017Q3 -> expanding size 206314\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 206314 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=1.000982, Val=0.464391\n",
      "        Epoch  3/10: Train=1.001076, Val=0.464757\n",
      "        Epoch  5/10: Train=1.001076, Val=0.465375\n",
      "        Epoch  7/10: Train=1.000812, Val=0.464879\n",
      "        Early‑stopped at epoch 7\n",
      "[Saved] models/LSTM_w252_2017Q4.pth\n",
      "\n",
      "[Window 252] 2018Q1\n",
      "    +3115 obs from 2017Q4 -> expanding size 209429\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 209429 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.994210, Val=0.459628\n",
      "        Epoch  3/10: Train=0.995269, Val=0.459645\n",
      "        Epoch  5/10: Train=0.994574, Val=0.459802\n",
      "        Epoch  7/10: Train=0.994796, Val=0.459623\n",
      "        Epoch  9/10: Train=0.995116, Val=0.459744\n",
      "        Epoch 10/10: Train=0.995224, Val=0.459824\n",
      "[Saved] models/LSTM_w252_2018Q1.pth\n",
      "\n",
      "[Window 252] 2018Q2\n",
      "    +2996 obs from 2018Q1 -> expanding size 212425\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 212425 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.988402, Val=0.494849\n",
      "        Epoch  3/10: Train=0.987809, Val=0.494827\n",
      "        Epoch  5/10: Train=0.988668, Val=0.494758\n",
      "        Epoch  7/10: Train=0.988241, Val=0.494969\n",
      "        Epoch  9/10: Train=0.986096, Val=0.494942\n",
      "        Early‑stopped at epoch 9\n",
      "[Saved] models/LSTM_w252_2018Q2.pth\n",
      "\n",
      "[Window 252] 2018Q3\n",
      "    +3160 obs from 2018Q2 -> expanding size 215585\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 215585 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.979464, Val=0.519805\n",
      "        Epoch  3/10: Train=0.979526, Val=0.520180\n",
      "        Epoch  5/10: Train=0.978671, Val=0.520396\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w252_2018Q3.pth\n",
      "\n",
      "[Window 252] 2018Q4\n",
      "    +3125 obs from 2018Q3 -> expanding size 218710\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 218710 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.972513, Val=0.503108\n",
      "        Epoch  3/10: Train=0.972628, Val=0.503260\n",
      "        Epoch  5/10: Train=0.973842, Val=0.503225\n",
      "        Epoch  7/10: Train=0.973632, Val=0.503094\n",
      "        Epoch  9/10: Train=0.970696, Val=0.503328\n",
      "        Epoch 10/10: Train=0.970391, Val=0.503425\n",
      "[Saved] models/LSTM_w252_2018Q4.pth\n",
      "\n",
      "[Window 252] 2019Q1\n",
      "    +3045 obs from 2018Q4 -> expanding size 221755\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 221755 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.962228, Val=0.593504\n",
      "        Epoch  3/10: Train=0.964494, Val=0.593236\n",
      "        Epoch  5/10: Train=0.964071, Val=0.593135\n",
      "        Epoch  7/10: Train=0.963654, Val=0.593230\n",
      "        Epoch  9/10: Train=0.963871, Val=0.593136\n",
      "        Epoch 10/10: Train=0.965254, Val=0.592935\n",
      "[Saved] models/LSTM_w252_2019Q1.pth\n",
      "\n",
      "[Window 252] 2019Q2\n",
      "    +3022 obs from 2019Q1 -> expanding size 224777\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 224777 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.955862, Val=0.613387\n",
      "        Epoch  3/10: Train=0.957138, Val=0.613535\n",
      "        Epoch  5/10: Train=0.956529, Val=0.613559\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w252_2019Q2.pth\n",
      "\n",
      "[Window 252] 2019Q3\n",
      "    +3120 obs from 2019Q2 -> expanding size 227897\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 227897 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.949805, Val=0.619080\n",
      "        Epoch  3/10: Train=0.950588, Val=0.619144\n",
      "        Epoch  5/10: Train=0.949486, Val=0.619192\n",
      "        Epoch  7/10: Train=0.953617, Val=0.619079\n",
      "        Epoch  9/10: Train=0.953238, Val=0.619177\n",
      "        Early‑stopped at epoch 9\n",
      "[Saved] models/LSTM_w252_2019Q3.pth\n",
      "\n",
      "[Window 252] 2019Q4\n",
      "    +3167 obs from 2019Q3 -> expanding size 231064\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 231064 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.941999, Val=0.655310\n",
      "        Epoch  3/10: Train=0.944357, Val=0.655318\n",
      "        Epoch  5/10: Train=0.944069, Val=0.656014\n",
      "        Epoch  7/10: Train=0.943649, Val=0.655413\n",
      "        Early‑stopped at epoch 7\n",
      "[Saved] models/LSTM_w252_2019Q4.pth\n",
      "\n",
      "[Window 252] 2020Q1\n",
      "    +3179 obs from 2019Q4 -> expanding size 234243\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 234243 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.939535, Val=0.628003\n",
      "        Epoch  3/10: Train=0.939183, Val=0.627697\n",
      "        Epoch  5/10: Train=0.940786, Val=0.627581\n",
      "        Epoch  7/10: Train=0.940435, Val=0.627517\n",
      "        Epoch  9/10: Train=0.940492, Val=0.627836\n",
      "        Epoch 10/10: Train=0.940694, Val=0.627593\n",
      "[Saved] models/LSTM_w252_2020Q1.pth\n",
      "\n",
      "[Window 252] 2020Q2\n",
      "    +2595 obs from 2020Q1 -> expanding size 236838\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 236838 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.937527, Val=0.708978\n",
      "        Epoch  3/10: Train=0.937735, Val=0.709931\n",
      "        Epoch  5/10: Train=0.937640, Val=0.709026\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w252_2020Q2.pth\n",
      "\n",
      "[Window 252] 2020Q3\n",
      "    +2839 obs from 2020Q2 -> expanding size 239677\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 239677 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.932042, Val=0.868726\n",
      "        Epoch  3/10: Train=0.934745, Val=0.868626\n",
      "        Epoch  5/10: Train=0.934089, Val=0.868877\n",
      "        Epoch  7/10: Train=0.933198, Val=0.868484\n",
      "        Early‑stopped at epoch 7\n",
      "[Saved] models/LSTM_w252_2020Q3.pth\n",
      "\n",
      "[Window 252] 2020Q4\n",
      "    +3151 obs from 2020Q3 -> expanding size 242828\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 242828 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.927591, Val=0.949279\n",
      "        Epoch  3/10: Train=0.930330, Val=0.950745\n",
      "        Epoch  5/10: Train=0.928681, Val=0.950890\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w252_2020Q4.pth\n",
      "\n",
      "[Window 252] 2021Q1\n",
      "    +3114 obs from 2020Q4 -> expanding size 245942\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 245942 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.927673, Val=0.958219\n",
      "        Epoch  3/10: Train=0.929679, Val=0.962147\n",
      "        Epoch  5/10: Train=0.928438, Val=0.961455\n",
      "        Epoch  7/10: Train=0.928224, Val=0.958270\n",
      "        Early‑stopped at epoch 7\n",
      "[Saved] models/LSTM_w252_2021Q1.pth\n",
      "\n",
      "[Window 252] 2021Q2\n",
      "    +2989 obs from 2021Q1 -> expanding size 248931\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 248931 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.925442, Val=1.008078\n",
      "        Epoch  3/10: Train=0.925413, Val=1.009056\n",
      "        Epoch  5/10: Train=0.927138, Val=1.008708\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w252_2021Q2.pth\n",
      "\n",
      "[Window 252] 2021Q3\n",
      "    +3132 obs from 2021Q2 -> expanding size 252063\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 252063 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.919967, Val=1.017022\n",
      "        Epoch  3/10: Train=0.922073, Val=1.017715\n",
      "        Epoch  5/10: Train=0.920547, Val=1.019380\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w252_2021Q3.pth\n",
      "\n",
      "[Window 252] 2021Q4\n",
      "    +3173 obs from 2021Q3 -> expanding size 255236\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 255236 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.916913, Val=1.010085\n",
      "        Epoch  3/10: Train=0.919758, Val=1.009602\n",
      "        Epoch  5/10: Train=0.918954, Val=1.009352\n",
      "        Epoch  7/10: Train=0.918656, Val=1.011099\n",
      "        Epoch  9/10: Train=0.920582, Val=1.009614\n",
      "        Early‑stopped at epoch 9\n",
      "[Saved] models/LSTM_w252_2021Q4.pth\n",
      "\n",
      "[Window 252] 2022Q1\n",
      "    +3153 obs from 2021Q4 -> expanding size 258389\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 258389 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.916189, Val=1.018493\n",
      "        Epoch  3/10: Train=0.916657, Val=1.019215\n",
      "        Epoch  5/10: Train=0.916001, Val=1.020844\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w252_2022Q1.pth\n",
      "\n",
      "[Window 252] 2022Q2\n",
      "    +3029 obs from 2022Q1 -> expanding size 261418\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 261418 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.910800, Val=1.105349\n",
      "        Epoch  3/10: Train=0.915384, Val=1.104380\n",
      "        Epoch  5/10: Train=0.911531, Val=1.108406\n",
      "        Epoch  7/10: Train=0.914081, Val=1.106013\n",
      "        Early‑stopped at epoch 7\n",
      "[Saved] models/LSTM_w252_2022Q2.pth\n",
      "\n",
      "[Window 252] 2022Q3\n",
      "    +2969 obs from 2022Q2 -> expanding size 264387\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 264387 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.927132, Val=1.078357\n",
      "        Epoch  3/10: Train=0.927090, Val=1.074474\n",
      "        Epoch  5/10: Train=0.926351, Val=1.077786\n",
      "        Epoch  7/10: Train=0.926707, Val=1.080968\n",
      "        Early‑stopped at epoch 8\n",
      "[Saved] models/LSTM_w252_2022Q3.pth\n",
      "\n",
      "[Window 252] 2022Q4\n",
      "    +3152 obs from 2022Q3 -> expanding size 267539\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 267539 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.934358, Val=1.012435\n",
      "        Epoch  3/10: Train=0.941236, Val=1.009207\n",
      "        Epoch  5/10: Train=0.947770, Val=1.008449\n",
      "        Epoch  7/10: Train=0.948487, Val=1.005514\n",
      "        Epoch  9/10: Train=0.948216, Val=1.004902\n",
      "        Epoch 10/10: Train=0.948316, Val=1.006007\n",
      "[Saved] models/LSTM_w252_2022Q4.pth\n",
      "\n",
      "[Window 252] 2023Q1\n",
      "    +3070 obs from 2022Q4 -> expanding size 270609\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 270609 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.966687, Val=1.027260\n",
      "        Epoch  3/10: Train=0.954212, Val=1.029473\n",
      "        Epoch  5/10: Train=0.954542, Val=1.032992\n",
      "        Epoch  7/10: Train=0.954111, Val=1.031809\n",
      "        Epoch  9/10: Train=0.954535, Val=1.040040\n",
      "        Early‑stopped at epoch 9\n",
      "[Saved] models/LSTM_w252_2023Q1.pth\n",
      "\n",
      "[Window 252] 2023Q2\n",
      "    +3064 obs from 2023Q1 -> expanding size 273673\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 273673 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.950150, Val=0.986886\n",
      "        Epoch  3/10: Train=0.951020, Val=0.988737\n",
      "        Epoch  5/10: Train=0.951443, Val=0.987275\n",
      "        Epoch  7/10: Train=0.951143, Val=0.985319\n",
      "        Early‑stopped at epoch 7\n",
      "[Saved] models/LSTM_w252_2023Q2.pth\n",
      "\n",
      "[Window 252] 2023Q3\n",
      "    +3069 obs from 2023Q2 -> expanding size 276742\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 276742 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.951310, Val=0.960788\n",
      "        Epoch  3/10: Train=0.952103, Val=0.972841\n",
      "        Epoch  5/10: Train=0.952345, Val=0.960907\n",
      "        Epoch  7/10: Train=0.952289, Val=0.960100\n",
      "        Early‑stopped at epoch 7\n",
      "[Saved] models/LSTM_w252_2023Q3.pth\n",
      "\n",
      "[Window 252] 2023Q4\n",
      "    +3121 obs from 2023Q3 -> expanding size 279863\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 279863 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.949551, Val=0.931799\n",
      "        Epoch  3/10: Train=0.950806, Val=0.932052\n",
      "        Epoch  5/10: Train=0.951282, Val=0.933576\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w252_2023Q4.pth\n",
      "\n",
      "[Window 252] 2024Q1\n",
      "    +3113 obs from 2023Q4 -> expanding size 282976\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 282976 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.946439, Val=0.949637\n",
      "        Epoch  3/10: Train=0.946817, Val=0.948125\n",
      "        Epoch  5/10: Train=0.947877, Val=0.946989\n",
      "        Epoch  7/10: Train=0.947285, Val=0.948994\n",
      "        Epoch  9/10: Train=0.947340, Val=0.948112\n",
      "        Epoch 10/10: Train=0.947329, Val=0.948133\n",
      "        Early‑stopped at epoch 10\n",
      "[Saved] models/LSTM_w252_2024Q1.pth\n",
      "\n",
      "[Window 252] 2024Q2\n",
      "    +3026 obs from 2024Q1 -> expanding size 286002\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 286002 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.943908, Val=0.930433\n",
      "        Epoch  3/10: Train=0.944932, Val=0.930587\n",
      "        Epoch  5/10: Train=0.945144, Val=0.932339\n",
      "        Epoch  7/10: Train=0.945363, Val=0.931366\n",
      "        Epoch  9/10: Train=0.945536, Val=0.934073\n",
      "        Early‑stopped at epoch 9\n",
      "[Saved] models/LSTM_w252_2024Q2.pth\n",
      "\n",
      "[Window 252] 2024Q3\n",
      "    +3112 obs from 2024Q2 -> expanding size 289114\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 289114 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.944959, Val=0.903965\n",
      "        Epoch  3/10: Train=0.946185, Val=0.903516\n",
      "        Epoch  5/10: Train=0.946254, Val=0.899880\n",
      "        Epoch  7/10: Train=0.945793, Val=0.903694\n",
      "        Epoch  9/10: Train=0.945752, Val=0.902848\n",
      "        Epoch 10/10: Train=0.945699, Val=0.909243\n",
      "        Early‑stopped at epoch 10\n",
      "[Saved] models/LSTM_w252_2024Q3.pth\n",
      "\n",
      "=== Window = 512 ===\n",
      "\n",
      "[Window 512] 2015Q4\n",
      "    Cold start training …\n",
      "    Training LSTM for 25 epochs on mps\n",
      "    Epoch   1/25: Train=1.052920, Val=0.568991\n",
      "    Epoch   5/25: Train=1.051345, Val=0.568564\n",
      "    Epoch  10/25: Train=1.050602, Val=0.567694\n",
      "    Epoch  15/25: Train=1.050641, Val=0.567551\n",
      "    Epoch  20/25: Train=1.050430, Val=0.567889\n",
      "    Early‑stopped at epoch 20\n",
      "[Saved] models/LSTM_w512_2015Q4.pth\n",
      "\n",
      "[Window 512] 2016Q1\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 171570 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=1.050692, Val=0.567606\n",
      "        Epoch  3/10: Train=1.050313, Val=0.567517\n",
      "        Epoch  5/10: Train=1.050249, Val=0.567698\n",
      "        Epoch  7/10: Train=1.050314, Val=0.567696\n",
      "        Early‑stopped at epoch 7\n",
      "[Saved] models/LSTM_w512_2016Q1.pth\n",
      "\n",
      "[Window 512] 2016Q2\n",
      "    +2956 obs from 2016Q1 -> expanding size 174526\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 174526 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=1.040959, Val=0.620334\n",
      "        Epoch  3/10: Train=1.041165, Val=0.620355\n",
      "        Epoch  5/10: Train=1.041065, Val=0.620328\n",
      "        Epoch  7/10: Train=1.041139, Val=0.620679\n",
      "        Epoch  9/10: Train=1.039087, Val=0.620514\n",
      "        Epoch 10/10: Train=1.038994, Val=0.620638\n",
      "        Early‑stopped at epoch 10\n",
      "[Saved] models/LSTM_w512_2016Q2.pth\n",
      "\n",
      "[Window 512] 2016Q3\n",
      "    +3170 obs from 2016Q2 -> expanding size 177696\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 177696 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=1.031039, Val=0.635431\n",
      "        Epoch  3/10: Train=1.031039, Val=0.635679\n",
      "        Epoch  5/10: Train=1.030947, Val=0.635514\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w512_2016Q3.pth\n",
      "\n",
      "[Window 512] 2016Q4\n",
      "    +3176 obs from 2016Q3 -> expanding size 180872\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 180872 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=1.020946, Val=0.625030\n",
      "        Epoch  3/10: Train=1.021012, Val=0.624792\n",
      "        Epoch  5/10: Train=1.020740, Val=0.625027\n",
      "        Epoch  7/10: Train=1.019916, Val=0.625263\n",
      "        Epoch  9/10: Train=1.019397, Val=0.625539\n",
      "        Early‑stopped at epoch 9\n",
      "[Saved] models/LSTM_w512_2016Q4.pth\n",
      "\n",
      "[Window 512] 2017Q1\n",
      "    +3123 obs from 2016Q4 -> expanding size 183995\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 183995 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=1.009960, Val=0.641519\n",
      "        Epoch  3/10: Train=1.009852, Val=0.641732\n",
      "        Epoch  5/10: Train=1.009554, Val=0.641472\n",
      "        Epoch  7/10: Train=1.009766, Val=0.642476\n",
      "        Epoch  9/10: Train=1.007050, Val=0.642793\n",
      "        Epoch 10/10: Train=1.006652, Val=0.642899\n",
      "        Early‑stopped at epoch 10\n",
      "[Saved] models/LSTM_w512_2017Q1.pth\n",
      "\n",
      "[Window 512] 2017Q2\n",
      "    +3083 obs from 2017Q1 -> expanding size 187078\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 187078 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=1.006145, Val=0.577886\n",
      "        Epoch  3/10: Train=1.005743, Val=0.577775\n",
      "        Epoch  5/10: Train=1.005940, Val=0.577851\n",
      "        Epoch  7/10: Train=1.004091, Val=0.577659\n",
      "        Early‑stopped at epoch 7\n",
      "[Saved] models/LSTM_w512_2017Q2.pth\n",
      "\n",
      "[Window 512] 2017Q3\n",
      "    +3122 obs from 2017Q2 -> expanding size 190200\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 190200 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=1.001165, Val=0.534315\n",
      "        Epoch  3/10: Train=1.002273, Val=0.534259\n",
      "        Epoch  5/10: Train=1.001036, Val=0.534168\n",
      "        Epoch  7/10: Train=1.001750, Val=0.534069\n",
      "        Epoch  9/10: Train=1.001362, Val=0.534389\n",
      "        Epoch 10/10: Train=1.001276, Val=0.534499\n",
      "[Saved] models/LSTM_w512_2017Q3.pth\n",
      "\n",
      "[Window 512] 2017Q4\n",
      "    +3114 obs from 2017Q3 -> expanding size 193314\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 193314 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.999486, Val=0.480603\n",
      "        Epoch  3/10: Train=0.999408, Val=0.481158\n",
      "        Epoch  5/10: Train=0.999687, Val=0.481340\n",
      "        Epoch  7/10: Train=1.000385, Val=0.481384\n",
      "        Epoch  9/10: Train=1.000633, Val=0.480919\n",
      "        Epoch 10/10: Train=1.000928, Val=0.480415\n",
      "[Saved] models/LSTM_w512_2017Q4.pth\n",
      "\n",
      "[Window 512] 2018Q1\n",
      "    +3115 obs from 2017Q4 -> expanding size 196429\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 196429 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.991208, Val=0.495800\n",
      "        Epoch  3/10: Train=0.991303, Val=0.495877\n",
      "        Epoch  5/10: Train=0.991147, Val=0.495736\n",
      "        Epoch  7/10: Train=0.990355, Val=0.495975\n",
      "        Epoch  9/10: Train=0.990471, Val=0.496135\n",
      "        Epoch 10/10: Train=0.990344, Val=0.496162\n",
      "        Early‑stopped at epoch 10\n",
      "[Saved] models/LSTM_w512_2018Q1.pth\n",
      "\n",
      "[Window 512] 2018Q2\n",
      "    +2996 obs from 2018Q1 -> expanding size 199425\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 199425 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.983004, Val=0.546342\n",
      "        Epoch  3/10: Train=0.983652, Val=0.545869\n",
      "        Epoch  5/10: Train=0.983783, Val=0.545828\n",
      "        Epoch  7/10: Train=0.985805, Val=0.546056\n",
      "        Epoch  9/10: Train=0.986215, Val=0.546250\n",
      "        Early‑stopped at epoch 9\n",
      "[Saved] models/LSTM_w512_2018Q2.pth\n",
      "\n",
      "[Window 512] 2018Q3\n",
      "    +3160 obs from 2018Q2 -> expanding size 202585\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 202585 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.975167, Val=0.568560\n",
      "        Epoch  3/10: Train=0.980207, Val=0.566917\n",
      "        Epoch  5/10: Train=0.979280, Val=0.567162\n",
      "        Epoch  7/10: Train=0.979048, Val=0.567156\n",
      "        Early‑stopped at epoch 7\n",
      "[Saved] models/LSTM_w512_2018Q3.pth\n",
      "\n",
      "[Window 512] 2018Q4\n",
      "    +3125 obs from 2018Q3 -> expanding size 205710\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 205710 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.970841, Val=0.556856\n",
      "        Epoch  3/10: Train=0.970776, Val=0.556637\n",
      "        Epoch  5/10: Train=0.970462, Val=0.557007\n",
      "        Epoch  7/10: Train=0.969920, Val=0.556943\n",
      "        Early‑stopped at epoch 8\n",
      "[Saved] models/LSTM_w512_2018Q4.pth\n",
      "\n",
      "[Window 512] 2019Q1\n",
      "    +3045 obs from 2018Q4 -> expanding size 208755\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 208755 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.964112, Val=0.664018\n",
      "        Epoch  3/10: Train=0.965938, Val=0.664513\n",
      "        Epoch  5/10: Train=0.961920, Val=0.664626\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w512_2019Q1.pth\n",
      "\n",
      "[Window 512] 2019Q2\n",
      "    +3022 obs from 2019Q1 -> expanding size 211777\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 211777 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.954560, Val=0.674756\n",
      "        Epoch  3/10: Train=0.954830, Val=0.674922\n",
      "        Epoch  5/10: Train=0.954878, Val=0.674901\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w512_2019Q2.pth\n",
      "\n",
      "[Window 512] 2019Q3\n",
      "    +3120 obs from 2019Q2 -> expanding size 214897\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 214897 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.947978, Val=0.684923\n",
      "        Epoch  3/10: Train=0.948431, Val=0.684564\n",
      "        Epoch  5/10: Train=0.949069, Val=0.684321\n",
      "        Epoch  7/10: Train=0.947347, Val=0.684537\n",
      "        Epoch  9/10: Train=0.947803, Val=0.684563\n",
      "        Epoch 10/10: Train=0.947530, Val=0.684578\n",
      "        Early‑stopped at epoch 10\n",
      "[Saved] models/LSTM_w512_2019Q3.pth\n",
      "\n",
      "[Window 512] 2019Q4\n",
      "    +3167 obs from 2019Q3 -> expanding size 218064\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 218064 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.944219, Val=0.718699\n",
      "        Epoch  3/10: Train=0.943436, Val=0.718982\n",
      "        Epoch  5/10: Train=0.942659, Val=0.719392\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w512_2019Q4.pth\n",
      "\n",
      "[Window 512] 2020Q1\n",
      "    +3179 obs from 2019Q4 -> expanding size 221243\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 221243 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.940767, Val=0.680195\n",
      "        Epoch  3/10: Train=0.940233, Val=0.680143\n",
      "        Epoch  5/10: Train=0.940461, Val=0.680212\n",
      "        Epoch  7/10: Train=0.940454, Val=0.680403\n",
      "        Early‑stopped at epoch 8\n",
      "[Saved] models/LSTM_w512_2020Q1.pth\n",
      "\n",
      "[Window 512] 2020Q2\n",
      "    +2595 obs from 2020Q1 -> expanding size 223838\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 223838 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.936872, Val=0.782470\n",
      "        Epoch  3/10: Train=0.937913, Val=0.783540\n",
      "        Epoch  5/10: Train=0.937596, Val=0.780802\n",
      "        Epoch  7/10: Train=0.937423, Val=0.783704\n",
      "        Epoch  9/10: Train=0.936700, Val=0.783354\n",
      "        Epoch 10/10: Train=0.937473, Val=0.781508\n",
      "        Early‑stopped at epoch 10\n",
      "[Saved] models/LSTM_w512_2020Q2.pth\n",
      "\n",
      "[Window 512] 2020Q3\n",
      "    +2839 obs from 2020Q2 -> expanding size 226677\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 226677 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.931110, Val=0.972511\n",
      "        Epoch  3/10: Train=0.931787, Val=0.972907\n",
      "        Epoch  5/10: Train=0.932019, Val=0.973278\n",
      "        Epoch  7/10: Train=0.931890, Val=0.971364\n",
      "        Epoch  9/10: Train=0.931346, Val=0.974530\n",
      "        Epoch 10/10: Train=0.930762, Val=0.975345\n",
      "[Saved] models/LSTM_w512_2020Q3.pth\n",
      "\n",
      "[Window 512] 2020Q4\n",
      "    +3151 obs from 2020Q3 -> expanding size 229828\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 229828 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.927263, Val=1.037726\n",
      "        Epoch  3/10: Train=0.927903, Val=1.034736\n",
      "        Epoch  5/10: Train=0.927534, Val=1.036401\n",
      "        Epoch  7/10: Train=0.928783, Val=1.041780\n",
      "        Epoch  9/10: Train=0.928732, Val=1.032100\n",
      "        Epoch 10/10: Train=0.927639, Val=1.033799\n",
      "[Saved] models/LSTM_w512_2020Q4.pth\n",
      "\n",
      "[Window 512] 2021Q1\n",
      "    +3114 obs from 2020Q4 -> expanding size 232942\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 232942 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.928631, Val=1.050130\n",
      "        Epoch  3/10: Train=0.933639, Val=1.049717\n",
      "        Epoch  5/10: Train=0.933249, Val=1.051154\n",
      "        Epoch  7/10: Train=0.933162, Val=1.050684\n",
      "        Early‑stopped at epoch 7\n",
      "[Saved] models/LSTM_w512_2021Q1.pth\n",
      "\n",
      "[Window 512] 2021Q2\n",
      "    +2989 obs from 2021Q1 -> expanding size 235931\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 235931 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.923588, Val=1.131244\n",
      "        Epoch  3/10: Train=0.934246, Val=1.134922\n",
      "        Epoch  5/10: Train=0.929790, Val=1.132907\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w512_2021Q2.pth\n",
      "\n",
      "[Window 512] 2021Q3\n",
      "    +3132 obs from 2021Q2 -> expanding size 239063\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 239063 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.919222, Val=1.134342\n",
      "        Epoch  3/10: Train=0.919968, Val=1.135936\n",
      "        Epoch  5/10: Train=0.923141, Val=1.137150\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w512_2021Q3.pth\n",
      "\n",
      "[Window 512] 2021Q4\n",
      "    +3173 obs from 2021Q3 -> expanding size 242236\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 242236 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.919143, Val=1.122228\n",
      "        Epoch  3/10: Train=0.923283, Val=1.116008\n",
      "        Epoch  5/10: Train=0.922025, Val=1.114095\n",
      "        Epoch  7/10: Train=0.920725, Val=1.117139\n",
      "        Epoch  9/10: Train=0.920861, Val=1.116011\n",
      "        Epoch 10/10: Train=0.920463, Val=1.118415\n",
      "        Early‑stopped at epoch 10\n",
      "[Saved] models/LSTM_w512_2021Q4.pth\n",
      "\n",
      "[Window 512] 2022Q1\n",
      "    +3153 obs from 2021Q4 -> expanding size 245389\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 245389 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.916532, Val=1.137713\n",
      "        Epoch  3/10: Train=0.916576, Val=1.139242\n",
      "        Epoch  5/10: Train=0.916561, Val=1.138403\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w512_2022Q1.pth\n",
      "\n",
      "[Window 512] 2022Q2\n",
      "    +3029 obs from 2022Q1 -> expanding size 248418\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 248418 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.920223, Val=1.161823\n",
      "        Epoch  3/10: Train=0.920382, Val=1.158957\n",
      "        Epoch  5/10: Train=0.920824, Val=1.165767\n",
      "        Epoch  7/10: Train=0.920419, Val=1.163697\n",
      "        Early‑stopped at epoch 7\n",
      "[Saved] models/LSTM_w512_2022Q2.pth\n",
      "\n",
      "[Window 512] 2022Q3\n",
      "    +2969 obs from 2022Q2 -> expanding size 251387\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 251387 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.937672, Val=1.094978\n",
      "        Epoch  3/10: Train=0.936966, Val=1.096100\n",
      "        Epoch  5/10: Train=0.935889, Val=1.095994\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w512_2022Q3.pth\n",
      "\n",
      "[Window 512] 2022Q4\n",
      "    +3152 obs from 2022Q3 -> expanding size 254539\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 254539 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.940099, Val=1.098230\n",
      "        Epoch  3/10: Train=0.940618, Val=1.101159\n",
      "        Epoch  5/10: Train=0.941286, Val=1.096027\n",
      "        Epoch  7/10: Train=0.942568, Val=1.093677\n",
      "        Early‑stopped at epoch 7\n",
      "[Saved] models/LSTM_w512_2022Q4.pth\n",
      "\n",
      "[Window 512] 2023Q1\n",
      "    +3070 obs from 2022Q4 -> expanding size 257609\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 257609 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.945765, Val=1.078709\n",
      "        Epoch  3/10: Train=0.944711, Val=1.079839\n",
      "        Epoch  5/10: Train=0.945004, Val=1.077921\n",
      "        Epoch  7/10: Train=0.943754, Val=1.078381\n",
      "        Epoch  9/10: Train=0.943533, Val=1.078770\n",
      "        Epoch 10/10: Train=0.943944, Val=1.079244\n",
      "        Early‑stopped at epoch 10\n",
      "[Saved] models/LSTM_w512_2023Q1.pth\n",
      "\n",
      "[Window 512] 2023Q2\n",
      "    +3064 obs from 2023Q1 -> expanding size 260673\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 260673 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.944805, Val=1.069131\n",
      "        Epoch  3/10: Train=0.948195, Val=1.068581\n",
      "        Epoch  5/10: Train=0.953907, Val=1.069719\n",
      "        Epoch  7/10: Train=0.947480, Val=1.071918\n",
      "        Early‑stopped at epoch 8\n",
      "[Saved] models/LSTM_w512_2023Q2.pth\n",
      "\n",
      "[Window 512] 2023Q3\n",
      "    +3069 obs from 2023Q2 -> expanding size 263742\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 263742 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.945008, Val=1.051624\n",
      "        Epoch  3/10: Train=0.945795, Val=1.052921\n",
      "        Epoch  5/10: Train=0.945603, Val=1.054482\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w512_2023Q3.pth\n",
      "\n",
      "[Window 512] 2023Q4\n",
      "    +3121 obs from 2023Q3 -> expanding size 266863\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 266863 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.943299, Val=1.032845\n",
      "        Epoch  3/10: Train=0.942372, Val=1.035212\n",
      "        Epoch  5/10: Train=0.942643, Val=1.032969\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w512_2023Q4.pth\n",
      "\n",
      "[Window 512] 2024Q1\n",
      "    +3113 obs from 2023Q4 -> expanding size 269976\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 269976 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.939729, Val=1.044896\n",
      "        Epoch  3/10: Train=0.940353, Val=1.045893\n",
      "        Epoch  5/10: Train=0.939217, Val=1.044720\n",
      "        Epoch  7/10: Train=0.939686, Val=1.044827\n",
      "        Epoch  9/10: Train=0.939270, Val=1.044416\n",
      "        Epoch 10/10: Train=0.939744, Val=1.044216\n",
      "[Saved] models/LSTM_w512_2024Q1.pth\n",
      "\n",
      "[Window 512] 2024Q2\n",
      "    +3026 obs from 2024Q1 -> expanding size 273002\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 273002 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.939198, Val=1.016517\n",
      "        Epoch  3/10: Train=0.938946, Val=1.018161\n",
      "        Epoch  5/10: Train=0.939707, Val=1.017199\n",
      "        Epoch  7/10: Train=0.939260, Val=1.017538\n",
      "        Epoch  9/10: Train=0.938728, Val=1.023912\n",
      "        Early‑stopped at epoch 9\n",
      "[Saved] models/LSTM_w512_2024Q2.pth\n",
      "\n",
      "[Window 512] 2024Q3\n",
      "    +3112 obs from 2024Q2 -> expanding size 276114\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 276114 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.942683, Val=0.949015\n",
      "        Epoch  3/10: Train=0.945132, Val=0.950196\n",
      "        Epoch  5/10: Train=0.945755, Val=0.949050\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w512_2024Q3.pth\n",
      "All LSTM quarterly expanding models trained \n"
     ]
    }
   ],
   "source": [
    "train_lstm_models_expanding_quarterly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Daily Rebalance Portfolio Backtesting Simulation\n",
      "Processing window size: 5\n",
      "  Model: LSTM, Scheme: VW\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Create] New metrics file created with LSTM w=5\n",
      "  Model: LSTM, Scheme: EW\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 5: predictions and actuals converted to original scale\n",
      "Processing window size: 21\n",
      "  Model: LSTM, Scheme: VW\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Update] Metrics updated for LSTM w=21\n",
      "  Model: LSTM, Scheme: EW\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 21: predictions and actuals converted to original scale\n",
      "Processing window size: 252\n",
      "  Model: LSTM, Scheme: VW\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Update] Metrics updated for LSTM w=252\n",
      "  Model: LSTM, Scheme: EW\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 252: predictions and actuals converted to original scale\n",
      "Processing window size: 512\n",
      "  Model: LSTM, Scheme: VW\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Update] Metrics updated for LSTM w=512\n",
      "  Model: LSTM, Scheme: EW\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "[Inverse Transform] Window 512: predictions and actuals converted to original scale\n",
      "VW results saved to portfolio_results_daily_rebalance_VW.csv\n",
      "EW results saved to portfolio_results_daily_rebalance_EW.csv\n",
      "VW results saved to portfolio_daily_series_VW.csv\n",
      "EW results saved to portfolio_daily_series_EW.csv\n",
      "Saved 443400 prediction rows to predictions_daily.csv\n",
      "Generated 24 portfolio summary records\n",
      "Generated 51672 daily series records\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   scheme model  window portfolio_type  annual_return  annual_vol    sharpe  \\\n",
       " 0      VW  LSTM       5      long_only       0.186154    0.225748  0.824609   \n",
       " 1      VW  LSTM       5     short_only      -0.096422    0.202947 -0.475109   \n",
       " 2      VW  LSTM       5     long_short       0.089732    0.237942  0.377118   \n",
       " 3      EW  LSTM       5      long_only       0.152240    0.236384  0.644036   \n",
       " 4      EW  LSTM       5     short_only      -0.143782    0.210357 -0.683514   \n",
       " 5      EW  LSTM       5     long_short       0.008458    0.225746  0.037466   \n",
       " 6      VW  LSTM      21      long_only       0.218543    0.217522  1.004692   \n",
       " 7      VW  LSTM      21     short_only      -0.055203    0.171268 -0.322323   \n",
       " 8      VW  LSTM      21     long_short       0.163339    0.205997  0.792919   \n",
       " 9      EW  LSTM      21      long_only       0.231820    0.230541  1.005547   \n",
       " 10     EW  LSTM      21     short_only      -0.027958    0.167504 -0.166907   \n",
       " 11     EW  LSTM      21     long_short       0.203863    0.192724  1.057795   \n",
       " 12     VW  LSTM     252      long_only       0.185604    0.214948  0.863484   \n",
       " 13     VW  LSTM     252     short_only      -0.087210    0.182653 -0.477462   \n",
       " 14     VW  LSTM     252     long_short       0.098394    0.213599  0.460648   \n",
       " 15     EW  LSTM     252      long_only       0.174326    0.226675  0.769056   \n",
       " 16     EW  LSTM     252     short_only      -0.134235    0.182152 -0.736938   \n",
       " 17     EW  LSTM     252     long_short       0.040091    0.202299  0.198178   \n",
       " 18     VW  LSTM     512      long_only       0.179817    0.230644  0.779630   \n",
       " 19     VW  LSTM     512     short_only      -0.094059    0.161722 -0.581611   \n",
       " 20     VW  LSTM     512     long_short       0.085757    0.217155  0.394914   \n",
       " 21     EW  LSTM     512      long_only       0.170793    0.249329  0.685010   \n",
       " 22     EW  LSTM     512     short_only      -0.065714    0.161124 -0.407847   \n",
       " 23     EW  LSTM     512     long_short       0.105079    0.220098  0.477417   \n",
       " \n",
       "     max_drawdown  max_1d_loss  avg_turnover  ...  tc20_sharpe  \\\n",
       " 0       0.382641    -0.077032      1.459228  ...    -2.427557   \n",
       " 1       0.745157    -0.073426      1.465104  ...    -4.094253   \n",
       " 2       0.589568    -0.075438      2.924232  ...    -5.756143   \n",
       " 3       0.398823    -0.075105      1.244448  ...    -2.005052   \n",
       " 4       0.791845    -0.073426      1.296631  ...    -3.778995   \n",
       " 5       0.613698    -0.092666      2.541514  ...    -5.584771   \n",
       " 6       0.275246    -0.059510      1.413937  ...    -2.259661   \n",
       " 7       0.551158    -0.042883      1.482968  ...    -4.670284   \n",
       " 8       0.386336    -0.060785      2.896752  ...    -6.223049   \n",
       " 9       0.302377    -0.063023      1.216230  ...    -1.645559   \n",
       " 10      0.418753    -0.049972      1.390750  ...    -4.328445   \n",
       " 11      0.274783    -0.062291      2.606789  ...    -5.691523   \n",
       " 12      0.277270    -0.077703      1.263690  ...    -2.088295   \n",
       " 13      0.707062    -0.073426      1.400267  ...    -4.327459   \n",
       " 14      0.313606    -0.086101      2.663842  ...    -5.781936   \n",
       " 15      0.308772    -0.079536      1.065287  ...    -1.594008   \n",
       " 16      0.762626    -0.073426      1.266658  ...    -4.233059   \n",
       " 17      0.332526    -0.079536      2.332367  ...    -5.559869   \n",
       " 18      0.412878    -0.061587      1.132009  ...    -1.686340   \n",
       " 19      0.671441    -0.046705      1.318335  ...    -4.666440   \n",
       " 20      0.516543    -0.061587      2.450527  ...    -5.237935   \n",
       " 21      0.437400    -0.061060      0.887241  ...    -1.106282   \n",
       " 22      0.562416    -0.051135      1.188389  ...    -4.111681   \n",
       " 23      0.436300    -0.061060      2.076038  ...    -4.254855   \n",
       " \n",
       "     tc20_max_drawdown  tc30_annual_return  tc30_annual_vol  tc30_sharpe  \\\n",
       " 0           -0.993030           -0.917022         0.227045    -4.038948   \n",
       " 1           -0.999361           -1.204041         0.204990    -5.873666   \n",
       " 2           -0.999994           -2.120987         0.242830    -8.734467   \n",
       " 3           -0.986402           -0.788563         0.237463    -3.320787   \n",
       " 4           -0.999135           -1.124035         0.211676    -5.310172   \n",
       " 5           -0.999985           -1.912927         0.229795    -8.324505   \n",
       " 6           -0.987991           -0.850394         0.219870    -3.867715   \n",
       " 7           -0.999124           -1.176328         0.172923    -6.802603   \n",
       " 8           -0.999987           -2.026605         0.211223    -9.594637   \n",
       " 9           -0.970521           -0.687650         0.232674    -2.955421   \n",
       " 10          -0.998342           -1.079364         0.169407    -6.371416   \n",
       " 11          -0.999936           -1.766870         0.197756    -8.934595   \n",
       " 12          -0.982968           -0.769746         0.217408    -3.540561   \n",
       " 13          -0.999073           -1.145812         0.184246    -6.218925   \n",
       " 14          -0.999981           -1.915470         0.217592    -8.803045   \n",
       " 15          -0.963916           -0.631031         0.228338    -2.763582   \n",
       " 16          -0.998903           -1.091828         0.183175    -5.960583   \n",
       " 17          -0.999949           -1.723178         0.206419    -8.347961   \n",
       " 18          -0.972869           -0.675982         0.232852    -2.903055   \n",
       " 19          -0.998730           -1.090721         0.163767    -6.660197   \n",
       " 20          -0.999957           -1.766841         0.221837    -7.964585   \n",
       " 21          -0.932076           -0.499961         0.250380    -1.996812   \n",
       " 22          -0.997163           -0.964136         0.162372    -5.937816   \n",
       " 23          -0.999739           -1.464406         0.222470    -6.582477   \n",
       " \n",
       "     tc30_max_drawdown  tc40_annual_return  tc40_annual_vol  tc40_sharpe  \\\n",
       " 0           -0.999688           -1.284748         0.228147    -5.631221   \n",
       " 1           -0.999973           -1.573247         0.206472    -7.619650   \n",
       " 2           -1.000000           -2.857893         0.245928   -11.620877   \n",
       " 3           -0.999066           -1.102164         0.238261    -4.625868   \n",
       " 4           -0.999947           -1.450786         0.212629    -6.823079   \n",
       " 5           -1.000000           -2.553389         0.232325   -10.990606   \n",
       " 6           -0.999434           -1.206706         0.221512    -5.447584   \n",
       " 7           -0.999964           -1.550036         0.174493    -8.883070   \n",
       " 8           -1.000000           -2.756587         0.215156   -12.812048   \n",
       " 9           -0.997772           -0.994140         0.234047    -4.247608   \n",
       " 10          -0.999917           -1.429833         0.170786    -8.372070   \n",
       " 11          -1.000000           -2.423781         0.201499   -12.028724   \n",
       " 12          -0.998865           -1.088196         0.219179    -4.964882   \n",
       " 13          -0.999954           -1.498679         0.185727    -8.069279   \n",
       " 14          -1.000000           -2.586758         0.221034   -11.702979   \n",
       " 15          -0.996324           -0.899483         0.229528    -3.918833   \n",
       " 16          -0.999928           -1.411026         0.184133    -7.663061   \n",
       " 17          -1.000000           -2.310935         0.209418   -11.035060   \n",
       " 18          -0.997599           -0.961248         0.234423    -4.100485   \n",
       " 19          -0.999924           -1.422941         0.165518    -8.596896   \n",
       " 20          -1.000000           -2.384374         0.225077   -10.593595   \n",
       " 21          -0.989376           -0.723546         0.251138    -2.881069   \n",
       " 22          -0.999776           -1.263610         0.163391    -7.733642   \n",
       " 23          -0.999997           -1.987568         0.224178    -8.866028   \n",
       " \n",
       "     tc40_max_drawdown  \n",
       " 0           -0.999987  \n",
       " 1           -0.999999  \n",
       " 2           -1.000000  \n",
       " 3           -0.999937  \n",
       " 4           -0.999997  \n",
       " 5           -1.000000  \n",
       " 6           -0.999973  \n",
       " 7           -0.999999  \n",
       " 8           -1.000000  \n",
       " 9           -0.999839  \n",
       " 10          -0.999996  \n",
       " 11          -1.000000  \n",
       " 12          -0.999926  \n",
       " 13          -0.999998  \n",
       " 14          -1.000000  \n",
       " 15          -0.999632  \n",
       " 16          -0.999995  \n",
       " 17          -1.000000  \n",
       " 18          -0.999788  \n",
       " 19          -0.999996  \n",
       " 20          -1.000000  \n",
       " 21          -0.998430  \n",
       " 22          -0.999983  \n",
       " 23          -1.000000  \n",
       " \n",
       " [24 rows x 32 columns],\n",
       "       scheme model  window portfolio_type                 date    return  \\\n",
       " 0         VW  LSTM       5      long_only  2016-01-05 00:00:00 -0.006821   \n",
       " 1         VW  LSTM       5      long_only  2016-01-06 00:00:00 -0.009845   \n",
       " 2         VW  LSTM       5      long_only  2016-01-07 00:00:00  0.002894   \n",
       " 3         VW  LSTM       5      long_only  2016-01-08 00:00:00 -0.001119   \n",
       " 4         VW  LSTM       5      long_only  2016-01-11 00:00:00 -0.005552   \n",
       " ...      ...   ...     ...            ...                  ...       ...   \n",
       " 51667     EW  LSTM     512     long_short  2024-12-20 00:00:00 -0.004618   \n",
       " 51668     EW  LSTM     512     long_short  2024-12-23 00:00:00 -0.004992   \n",
       " 51669     EW  LSTM     512     long_short  2024-12-24 00:00:00  0.004984   \n",
       " 51670     EW  LSTM     512     long_short  2024-12-27 00:00:00  0.007147   \n",
       " 51671     EW  LSTM     512     long_short  2024-12-30 00:00:00 -0.014354   \n",
       " \n",
       "        turnover  cumulative  tc5_return  tc5_cumulative  tc10_return  \\\n",
       " 0      1.000000   -0.006844   -0.007321       -0.007348    -0.007821   \n",
       " 1      1.226258   -0.016738   -0.010458       -0.017861    -0.011072   \n",
       " 2      0.508445   -0.013849    0.002640       -0.015225     0.002385   \n",
       " 3      1.528312   -0.014968   -0.001883       -0.017110    -0.002647   \n",
       " 4      1.546078   -0.020536   -0.006325       -0.023455    -0.007098   \n",
       " ...         ...         ...         ...             ...          ...   \n",
       " 51667  2.429103    0.698657   -0.005833       -1.534208    -0.007047   \n",
       " 51668  0.830470    0.693652   -0.005407       -1.539630    -0.005823   \n",
       " 51669  1.610287    0.698624    0.004179       -1.535459     0.003374   \n",
       " 51670  1.212176    0.705745    0.006541       -1.528940     0.005935   \n",
       " 51671  2.005421    0.691287   -0.015357       -1.544416    -0.016359   \n",
       " \n",
       "        tc10_cumulative  tc20_return  tc20_cumulative  tc30_return  \\\n",
       " 0            -0.007852    -0.008821        -0.008860    -0.009821   \n",
       " 1            -0.018985    -0.012298        -0.021234    -0.013524   \n",
       " 2            -0.016602     0.001877        -0.019359     0.001368   \n",
       " 3            -0.019253    -0.004176        -0.023543    -0.005704   \n",
       " 4            -0.026377    -0.008644        -0.032225    -0.010190   \n",
       " ...                ...          ...              ...          ...   \n",
       " 51667        -3.769616    -0.009476        -8.248090    -0.011905   \n",
       " 51668        -3.775456    -0.006653        -8.254766    -0.007484   \n",
       " 51669        -3.772087     0.001764        -8.253003     0.000153   \n",
       " 51670        -3.766170     0.004722        -8.248292     0.003510   \n",
       " 51671        -3.782665    -0.018365        -8.266828    -0.020370   \n",
       " \n",
       "        tc30_cumulative  tc40_return  tc40_cumulative  \n",
       " 0            -0.009869    -0.010821        -0.010880  \n",
       " 1            -0.023486    -0.014750        -0.025740  \n",
       " 2            -0.022118     0.000860        -0.024880  \n",
       " 3            -0.027839    -0.007232        -0.032139  \n",
       " 4            -0.038081    -0.011736        -0.043945  \n",
       " ...                ...          ...              ...  \n",
       " 51667       -12.736816    -0.014334       -17.235844  \n",
       " 51668       -12.744328    -0.008314       -17.244193  \n",
       " 51669       -12.744174    -0.001457       -17.245651  \n",
       " 51670       -12.740670     0.002298       -17.243355  \n",
       " 51671       -12.761251    -0.022376       -17.265985  \n",
       " \n",
       " [51672 rows x 18 columns],\n",
       " <__main__.PortfolioBacktester at 0x37cc80770>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_portfolio_simulation_daily_rebalance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] 5_factor_analysis_VW_gross.csv \n",
      "[Saved] 5_factor_analysis_VW_net.csv \n",
      "[Saved] 5_factor_analysis_EW_gross.csv \n",
      "[Saved] 5_factor_analysis_EW_net.csv \n"
     ]
    }
   ],
   "source": [
    "# ---------- Main function for 5-factor regression -----------\n",
    "def run_factor_regression(port_ret, factors, use_excess=True):\n",
    "    df = pd.concat([port_ret, factors], axis=1, join='inner').dropna()\n",
    "    df.columns = ['ret'] + list(factors.columns)\n",
    "    \n",
    "    if use_excess:\n",
    "        y = df['ret'].values\n",
    "    else:\n",
    "        y = df['ret'].values - df['rf'].values\n",
    "    \n",
    "    X = df[['mktrf','smb','hml','rmw','cma','umd']].values\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    model = sm.OLS(y, X)\n",
    "    res = model.fit()\n",
    "    alpha = res.params[0]          \n",
    "    resid_std = res.resid.std(ddof=1)\n",
    "\n",
    "    ir_daily = alpha / resid_std          \n",
    "    ir_annual = ir_daily * np.sqrt(252)   \n",
    "\n",
    "    y_hat = np.asarray(res.fittedvalues)  \n",
    "    \n",
    "    out = {\n",
    "        'N_obs'            : len(y),\n",
    "        'alpha_daily'      : alpha,\n",
    "        'alpha_annual'     : alpha*252,      \n",
    "        't_alpha'          : res.tvalues[0],\n",
    "        'IR_daily'         : ir_daily,\n",
    "        'IR_annual'        : ir_annual,\n",
    "        'R2_zero'          : r2_zero(y, y_hat),\n",
    "    }\n",
    "    \n",
    "    factor_names = ['MKT','SMB','HML','RMW','CMA','UMD']\n",
    "    for i, fac in enumerate(factor_names, start=1):\n",
    "        out[f'beta_{fac}'] = res.params[i]\n",
    "        out[f't_{fac}']    = res.tvalues[i]\n",
    "    \n",
    "    return out\n",
    "\n",
    "# ---------- Batch run (EW/VW, three portfolio types) ----------\n",
    "def batch_factor_analysis(\n",
    "    daily_df: pd.DataFrame,\n",
    "    factors_path: str,\n",
    "    scheme: str,\n",
    "    tc_levels=(0, 5, 10, 20, 40),\n",
    "    portfolio_types=('long_only','short_only','long_short'),\n",
    "    model_filter=None,\n",
    "    window_filter=None,\n",
    "    gross_only=False,            \n",
    "    out_dir='factor_IR_results',\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a CSV containing IR results.\n",
    "    gross_only=True  → only tc=0; False → all tc_levels.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    fac = (pd.read_csv(factors_path, parse_dates=['date'])\n",
    "             .set_index('date')\n",
    "             .sort_index())\n",
    "\n",
    "    sub = daily_df[daily_df['scheme'] == scheme].copy()\n",
    "    if model_filter is not None:\n",
    "        sub = sub[sub['model'].isin(model_filter)]\n",
    "    if window_filter is not None:\n",
    "        sub = sub[sub['window'].isin(window_filter)]\n",
    "\n",
    "    tc_iter = (0,) if gross_only else tc_levels\n",
    "    results = []\n",
    "\n",
    "    for (model, win, ptype), g in sub.groupby(['model','window','portfolio_type']):\n",
    "        g = g.sort_values('date').set_index(pd.to_datetime(g['date']))\n",
    "\n",
    "        for tc in tc_iter:\n",
    "            col = 'return' if tc == 0 else f'tc{tc}_return'\n",
    "            if col not in g.columns:\n",
    "                continue  \n",
    "            port_ret = g[col]\n",
    "            stats = run_factor_regression(port_ret, fac, use_excess=True)\n",
    "            stats.update({\n",
    "                'scheme'        : scheme,\n",
    "                'model'         : model,\n",
    "                'window'        : win,\n",
    "                'portfolio_type': ptype,\n",
    "                'tc_bps'        : tc,\n",
    "            })\n",
    "            results.append(stats)\n",
    "\n",
    "    df_out = pd.DataFrame(results)[[\n",
    "        'scheme','model','window','portfolio_type','tc_bps','N_obs',\n",
    "        'alpha_daily','alpha_annual','t_alpha',\n",
    "        'IR_daily','IR_annual','R2_zero',\n",
    "        'beta_MKT','t_MKT','beta_SMB','t_SMB',\n",
    "        'beta_HML','t_HML','beta_RMW','t_RMW',\n",
    "        'beta_CMA','t_CMA','beta_UMD','t_UMD'\n",
    "    ]]\n",
    "\n",
    "    tag = 'gross' if gross_only else 'net'\n",
    "    fname = f'5_factor_analysis_{scheme}_{tag}.csv'\n",
    "    df_out.to_csv(os.path.join(out_dir, fname), index=False)\n",
    "    print(f'[Saved] {fname}')\n",
    "    return df_out\n",
    "\n",
    "\n",
    "\n",
    "def run_all_factor_tests(vw_csv=\"portfolio_daily_series_VW.csv\",\n",
    "                         ew_csv=\"portfolio_daily_series_EW.csv\",\n",
    "                         factor_csv=\"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/5_Factors_Plus_Momentum.csv\",\n",
    "                         save_dir=\"results\",\n",
    "                         y_is_excess=True,\n",
    "                         hac_lags=5,\n",
    "                         save_txt=True):\n",
    "    vw_df = pd.read_csv(vw_csv)\n",
    "    ew_df = pd.read_csv(ew_csv)\n",
    "\n",
    "    vw_gross = batch_factor_analysis(\n",
    "        vw_df, factor_csv, scheme='VW', gross_only=True)    \n",
    "    vw_net   = batch_factor_analysis(\n",
    "        vw_df, factor_csv, scheme='VW', gross_only=False)   \n",
    "\n",
    "    ew_gross = batch_factor_analysis(\n",
    "        ew_df, factor_csv, scheme='EW', gross_only=True)\n",
    "    ew_net   = batch_factor_analysis(\n",
    "        ew_df, factor_csv, scheme='EW', gross_only=False)\n",
    "\n",
    "    return vw_gross, vw_net, ew_gross, ew_net\n",
    "    \n",
    "\n",
    "vw_gross, vw_net, ew_gross, ew_net = run_all_factor_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish: portfolio_daily_series_VW_with_rf.csv\n",
      "Finish: portfolio_daily_series_EW_with_rf.csv\n"
     ]
    }
   ],
   "source": [
    "# === File Paths ===\n",
    "rf_file = \"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/CRSP_2016_2024_top50_with_exret.csv\"\n",
    "vw_file = \"portfolio_daily_series_VW.csv\"\n",
    "ew_file = \"portfolio_daily_series_EW.csv\"\n",
    "\n",
    "# === Load risk-free rate (rf) data ===\n",
    "\n",
    "rf_df = pd.read_csv(rf_file, usecols=[\"date\", \"rf\"])\n",
    "rf_df[\"date\"] = pd.to_datetime(rf_df[\"date\"])\n",
    "rf_dict = dict(zip(rf_df[\"date\"], rf_df[\"rf\"]))\n",
    "\n",
    "\n",
    "def adjust_returns_with_rf_grouped(file_path, output_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Handle different date formats\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], format='mixed', dayfirst=True)\n",
    "\n",
    "    # Find all return columns (excluding cumulative columns)\n",
    "    return_cols = [col for col in df.columns if \"return\" in col and \"cumul\" not in col]\n",
    "\n",
    "    # Set portfolio_type order to avoid groupby sorting issues\n",
    "    order = [\"long_only\", \"short_only\", \"long_short\"]\n",
    "    df[\"portfolio_type\"] = pd.Categorical(df[\"portfolio_type\"], categories=order, ordered=True)\n",
    "\n",
    "    df_list = []\n",
    "    # Group by scheme/model/window/portfolio_type, add rf and recalculate cumulative\n",
    "    for _, group in df.groupby([\"scheme\", \"model\", \"window\", \"portfolio_type\"], sort=False):\n",
    "        group = group.sort_values(\"date\").copy()\n",
    "        for col in return_cols:\n",
    "            group[col] = group.apply(lambda row: row[col] + rf_dict.get(row[\"date\"], 0), axis=1)\n",
    "            cum_col = col.replace(\"return\", \"cumulative\")\n",
    "            group[cum_col] = np.log1p(group[col]).cumsum()\n",
    "        df_list.append(group)\n",
    "\n",
    "    df_new = pd.concat(df_list).sort_values([\"scheme\", \"model\", \"window\", \"portfolio_type\", \"date\"])\n",
    "    df_new.to_csv(output_path, index=False)\n",
    "    print(f\"Finished: {output_path}\")\n",
    "\n",
    "# Process VW and EW files\n",
    "adjust_returns_with_rf_grouped(vw_file, \"portfolio_daily_series_VW_with_rf.csv\")\n",
    "adjust_returns_with_rf_grouped(ew_file, \"portfolio_daily_series_EW_with_rf.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All figures have been generated and saved to: Baseline_Portfolio/\n"
     ]
    }
   ],
   "source": [
    "# ======== Download S&P500 (2016-2024) ========\n",
    "sp500 = yf.download(\"^GSPC\", start=\"2016-01-01\", end=\"2024-12-31\")\n",
    "price_col = \"Adj Close\" if \"Adj Close\" in sp500.columns else \"Close\"\n",
    "sp500[\"daily_return\"] = sp500[price_col].pct_change().fillna(0)\n",
    "sp500[\"cum_return\"] = np.cumsum(np.log1p(sp500[\"daily_return\"]))\n",
    "sp500 = sp500[[\"cum_return\"]]\n",
    "sp500.index = pd.to_datetime(sp500.index)\n",
    "\n",
    "# ======== Configuration ========\n",
    "files = [\n",
    "    (\"VW\", \"portfolio_daily_series_VW_with_rf.csv\"),\n",
    "    (\"EW\", \"portfolio_daily_series_EW_with_rf.csv\")\n",
    "]\n",
    "tc_levels = [0, 5, 10, 20, 40]      # Transaction cost (bps)\n",
    "windows = [5, 21, 252, 512]         # Window sizes\n",
    "strategies = [\"long_only\", \"short_only\", \"long_short\"]\n",
    "\n",
    "output_dir = \"Baseline_Portfolio\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Economic crisis periods (for shading in plots)\n",
    "crisis_periods = [\n",
    "    (datetime(2018, 6, 1), datetime(2019, 1, 1), \"US-China Trade War\"),\n",
    "    (datetime(2020, 2, 1), datetime(2020, 7, 1), \"COVID-19\"),\n",
    "    (datetime(2022, 2, 1), datetime(2022, 6, 1), \"Russia-Ukraine War\"),\n",
    "    (datetime(2023, 1, 1), datetime(2023, 4, 1), \"US Bank Crisis\"),\n",
    "]\n",
    "\n",
    "def plot_comparison_styled(df, scheme, tc, window):\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    model_names = df[\"model\"].unique()\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(model_names)))\n",
    "\n",
    "    offset_step = 0.02\n",
    "\n",
    "    for i, strat in enumerate(strategies, 1):\n",
    "        ax = plt.subplot(3, 1, i)\n",
    "\n",
    "        plt.plot(sp500.index, sp500[\"cum_return\"],\n",
    "                 color=\"black\", lw=2.5, label=\"S&P500 (Total Return)\", zorder=10)\n",
    "\n",
    "        for idx, model_name in enumerate(model_names):\n",
    "            sub = df[(df[\"window\"] == window) &\n",
    "                     (df[\"portfolio_type\"] == strat) &\n",
    "                     (df[\"model\"] == model_name)].sort_values(\"date\")\n",
    "            if sub.empty:\n",
    "                continue\n",
    "\n",
    "            if tc == 0:\n",
    "                ret_col = \"return\"\n",
    "            else:\n",
    "                ret_col = f\"tc{tc}_return\"\n",
    "\n",
    "            if ret_col not in sub.columns:\n",
    "                continue\n",
    "\n",
    "            log_cum = np.cumsum(np.log1p(sub[ret_col].values))\n",
    "\n",
    "            y_shift = idx * offset_step\n",
    "            plt.plot(sub[\"date\"], log_cum + y_shift,\n",
    "                     label=f\"{model_name} ({strat.replace('_',' ').title()})\",\n",
    "                     lw=2, color=colors[idx], alpha=0.9)\n",
    "\n",
    "        for start, end, label in crisis_periods:\n",
    "            ax.axvspan(start, end, color='grey', alpha=0.3)\n",
    "            ax.text(start + pd.Timedelta(days=10),\n",
    "                    ax.get_ylim()[1]*0.92, label, fontsize=8, color='grey')\n",
    "        ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "        ax.set_ylabel(\"Cumulative log return (start = 0)\")\n",
    "        ax.set_title(f\"{scheme} | Window={window} | Strategy={strat} | TC={tc} bps\")\n",
    "        ax.grid(alpha=0.3)\n",
    "        plt.xticks(rotation=30)\n",
    "        plt.legend(bbox_to_anchor=(1.04, 1), loc='upper left', fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fname = f\"{scheme}_window{window}_TC{tc}_logreturn_offset.png\"\n",
    "    plt.savefig(os.path.join(output_dir, fname), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# ======== Main loop to generate all figures ========\n",
    "for scheme, file_path in files:\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    for tc in tc_levels:\n",
    "        for window in windows:\n",
    "            plot_comparison_styled(df, scheme, tc, window)\n",
    "\n",
    "print(f\"All figures have been generated and saved to: {output_dir}/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Update] ΔSharpe has been written to portfolio_results_daily_rebalance_VW.csv\n",
      "[Update] ΔSharpe has been written to portfolio_results_daily_rebalance_EW.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load R²_zero from portfolio_metrics.csv\n",
    "metrics_df = pd.read_csv(\"portfolio_metrics.csv\")[[\"Model\", \"Window\", \"R²_zero\"]]\n",
    "metrics_df.rename(columns={\"Model\": \"model\", \"Window\": \"window\"}, inplace=True)\n",
    "\n",
    "# Process VW/EW files\n",
    "for fname in [\"portfolio_results_daily_rebalance_VW.csv\", \"portfolio_results_daily_rebalance_EW.csv\"]:\n",
    "    df = pd.read_csv(fname)\n",
    "\n",
    "    # Merge R²_zero by model and window\n",
    "    df = df.merge(metrics_df, on=[\"model\", \"window\"], how=\"left\")\n",
    "\n",
    "    rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        r2 = float(row[\"R²_zero\"]) if not pd.isna(row[\"R²_zero\"]) else 0.0\n",
    "        if row[\"portfolio_type\"] == \"long_only\":\n",
    "            d_sr, sr_star = delta_sharpe(r2, SR_MKT_EX)\n",
    "            row[\"ΔSharpe\"]  = d_sr\n",
    "            row[\"Sharpe*\"]  = sr_star\n",
    "            row[\"baseline\"] = f\"SPX_excess ({SR_MKT_EX:.2f})\"\n",
    "        else:\n",
    "            d_sr, sr_star = delta_sharpe(r2, 0)\n",
    "            row[\"ΔSharpe\"]  = d_sr\n",
    "            row[\"Sharpe*\"]  = sr_star\n",
    "            row[\"baseline\"] = \"cash (0)\"\n",
    "        rows.append(row)\n",
    "\n",
    "    pd.DataFrame(rows).to_csv(fname, index=False)\n",
    "    print(f\"[Update] ΔSharpe has been written to {fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wj/7nbyftys3g77fcp37r3t0_lm0000gn/T/ipykernel_61715/1104344638.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ics = (df_group.groupby(\"signal_date\", observed=True).apply(_day_ic).dropna())\n",
      "/var/folders/wj/7nbyftys3g77fcp37r3t0_lm0000gn/T/ipykernel_61715/1104344638.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ics = (df_group.groupby(\"signal_date\", observed=True).apply(_day_ic).dropna())\n",
      "/var/folders/wj/7nbyftys3g77fcp37r3t0_lm0000gn/T/ipykernel_61715/1104344638.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ics = (df_group.groupby(\"signal_date\", observed=True).apply(_day_ic).dropna())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Overwrote portfolio_metrics.csv with new RankIC )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wj/7nbyftys3g77fcp37r3t0_lm0000gn/T/ipykernel_61715/1104344638.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ics = (df_group.groupby(\"signal_date\", observed=True).apply(_day_ic).dropna())\n",
      "/var/folders/wj/7nbyftys3g77fcp37r3t0_lm0000gn/T/ipykernel_61715/1104344638.py:33: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(rankic_stats)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "PRED_PATH = \"predictions_daily.csv\"\n",
    "METRICS_PATH = \"portfolio_metrics.csv\"\n",
    "TREAT_CONSTANT_DAY_AS_ZERO = False\n",
    "MIN_DAYS_FOR_STATS = 1\n",
    "\n",
    "def _day_ic(g):\n",
    "    if g[\"y_pred\"].nunique(dropna=True) <= 1 or g[\"y_true\"].nunique(dropna=True) <= 1:\n",
    "        return 0.0 if TREAT_CONSTANT_DAY_AS_ZERO else np.nan\n",
    "    return g[\"y_pred\"].corr(g[\"y_true\"], method=\"spearman\")\n",
    "\n",
    "def rankic_stats(df_group):\n",
    "    ics = (df_group.groupby(\"signal_date\", observed=True).apply(_day_ic).dropna())\n",
    "    n = int(ics.shape[0])\n",
    "    if n < MIN_DAYS_FOR_STATS:\n",
    "        return pd.Series({\"RankIC_mean\": np.nan, \"RankIC_t\": np.nan, \"RankIC_pos%\": np.nan, \"N_days\": n})\n",
    "    mean_ic = float(ics.mean())\n",
    "    std_ic  = float(ics.std(ddof=1))\n",
    "    t_ic    = mean_ic / (std_ic / np.sqrt(n)) if std_ic > 0 else np.nan\n",
    "    pos_pct = float((ics > 0).mean())\n",
    "    return pd.Series({\"RankIC_mean\": mean_ic, \"RankIC_t\": t_ic, \"RankIC_pos%\": pos_pct, \"N_days\": n})\n",
    "\n",
    "# Read data and calculate RankIC\n",
    "pred = pd.read_csv(PRED_PATH)\n",
    "pred[\"signal_date\"] = pd.to_datetime(pred[\"signal_date\"], errors=\"coerce\")\n",
    "pred = pred.dropna(subset=[\"signal_date\", \"y_true\", \"y_pred\", \"model\", \"window\"])\n",
    "pred[\"window\"] = pd.to_numeric(pred[\"window\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "rankic_df = (pred.groupby([\"model\", \"window\"], dropna=False)\n",
    "                .apply(rankic_stats)\n",
    "                .reset_index()\n",
    "                .rename(columns={\"model\":\"Model\",\"window\":\"Window\"}))\n",
    "\n",
    "# Merge: keep new RankIC columns, add _old suffix to original metrics columns\n",
    "metrics = pd.read_csv(METRICS_PATH)\n",
    "metrics[\"Window\"] = pd.to_numeric(metrics[\"Window\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "merged = metrics.merge(rankic_df, on=[\"Model\",\"Window\"], how=\"left\", suffixes=(\"_old\",\"\"))\n",
    "\n",
    "# Drop old columns with _old suffix\n",
    "to_drop = [c for c in merged.columns if c.endswith(\"_old\")]\n",
    "merged = merged.drop(columns=to_drop)\n",
    "\n",
    "# Save and overwrite\n",
    "merged.to_csv(METRICS_PATH, index=False)\n",
    "print(\"[OK] Overwrote portfolio_metrics.csv with new RankIC\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-mac)",
   "language": "python",
   "name": "tf-mac"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
