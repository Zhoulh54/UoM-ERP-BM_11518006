{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps (Apple Metal GPU)\n",
      " MPS cache cleared\n"
     ]
    }
   ],
   "source": [
    "# Numerical computation & data processing\n",
    "import os\n",
    "import copy\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine learning & deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import optuna\n",
    "\n",
    "# Statistics & finance tools\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import f as f_dist\n",
    "import yfinance as yf\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration - prefer MPS (Mac GPU), then CUDA, then CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(f\"Using device: {device} (Apple Metal GPU)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Using device: {device} (NVIDIA GPU)\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(f\"Using device: {device} (CPU only)\")\n",
    "\n",
    "# MPS specific settings\n",
    "if device.type == 'mps':\n",
    "    torch.mps.empty_cache()\n",
    "    print(\"MPS cache cleared\")\n",
    "\n",
    "def get_device_info():\n",
    "    if device.type == 'mps':\n",
    "        return f\"Apple Metal GPU (MPS) - Mac M-series chip\"\n",
    "    elif device.type == 'cuda':\n",
    "        return f\"NVIDIA GPU: {torch.cuda.get_device_name()}\"\n",
    "    else:\n",
    "        return \"CPU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MLP Model Definition =====\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping mechanism\"\"\"\n",
    "    def __init__(self, patience=20, min_delta=0, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            \n",
    "        if self.counter >= self.patience:\n",
    "            if self.restore_best_weights:\n",
    "                model.load_state_dict(self.best_weights)\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def save_checkpoint(self, model):\n",
    "        \"\"\"Save best weights\"\"\"\n",
    "        self.best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "class MLPNet(nn.Module):\n",
    "    \"\"\"MLP neural network model\"\"\"\n",
    "    def __init__(self, input_size, hidden_sizes, dropout_rate=0.2, activation='relu'):\n",
    "        super(MLPNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        activations = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'tanh': nn.Tanh(),\n",
    "            'leaky_relu': nn.LeakyReLU()\n",
    "        }\n",
    "        self.activation = activations.get(activation, nn.ReLU())\n",
    "            \n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                self.activation,\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "            \n",
    "        layers.append(nn.Linear(prev_size, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, m):\n",
    "        \"\"\"Xavier initialization\"\"\"\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze()\n",
    "\n",
    "def get_mlp_model(model_name, input_size, dropout_rate=0.1, training_device='cpu'):\n",
    "    \"\"\"Create MLP model, allow specifying training device\"\"\"\n",
    "    hidden_configs = {\n",
    "        'NN1': [32],\n",
    "        'NN2': [64,32],\n",
    "        'NN3': [128,64,32],\n",
    "        'NN4': [256,128,64,32],\n",
    "        'NN5': [512,256,128,64,32]\n",
    "    }\n",
    "    return MLPNet(input_size=input_size,\n",
    "                  hidden_sizes=hidden_configs[model_name],\n",
    "                  dropout_rate=dropout_rate).to(training_device)\n",
    "\n",
    "# ===== Core Training and Prediction Functions =====\n",
    "\n",
    "class MLPWrapper:\n",
    "    \"\"\"MLP model wrapper with training logic\"\"\"\n",
    "    def __init__(self, model_name, input_size, learning_rate=0.001, batch_size=512, max_epochs=100, warm_start_epochs=15, dropout_rate=0.1, training_device=None):\n",
    "        self.model_name = model_name\n",
    "        self.input_size = input_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.max_epochs = max_epochs\n",
    "        self.warm_start_epochs = warm_start_epochs\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.training_device = training_device if training_device is not None else device\n",
    "        \n",
    "        self.model = get_mlp_model(model_name, input_size, dropout_rate, self.training_device)\n",
    "        self._init_training_components()\n",
    "        self.is_fitted = False\n",
    "        \n",
    "    def _init_training_components(self):\n",
    "        \"\"\"Initialize training components\"\"\"\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode='min', factor=0.5, patience=5, verbose=False)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.early_stopping = EarlyStopping(patience=5)\n",
    "        \n",
    "    def fit(self, X, y, validation_split=0.1, warm_start=False, verbose=True):\n",
    "        \"\"\"Train model\"\"\"\n",
    "        if not warm_start:\n",
    "            self.model = get_mlp_model(self.model_name, self.input_size, self.dropout_rate, self.training_device)\n",
    "            self._init_training_components()\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"    [Warm Start] Retaining model weights, continuing training...\")\n",
    "            self.early_stopping = EarlyStopping(patience=5)\n",
    "        \n",
    "        val_size = int(len(X) * validation_split) if validation_split > 0 else max(1, int(len(X)*0.1))\n",
    "        X_train, X_val = X[:-val_size], X[-val_size:]\n",
    "        y_train, y_val = y[:-val_size], y[-val_size:]\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            TensorDataset(torch.FloatTensor(X_train).to(self.training_device), torch.FloatTensor(y_train).to(self.training_device)),\n",
    "            batch_size=self.batch_size, shuffle=False, drop_last=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            TensorDataset(torch.FloatTensor(X_val).to(self.training_device), torch.FloatTensor(y_val).to(self.training_device)),\n",
    "            batch_size=self.batch_size, shuffle=False\n",
    "        )\n",
    "        \n",
    "        actual_epochs = self.warm_start_epochs if warm_start else self.max_epochs\n",
    "        if verbose:\n",
    "            print(f\"    Training for {actual_epochs} epochs ({'warm-start fine-tuning' if warm_start else 'full training'}) on {self.training_device}\")\n",
    "        \n",
    "        losses = {'train': [], 'val': []}\n",
    "        \n",
    "        for epoch in range(actual_epochs):\n",
    "            self.model.train()\n",
    "            train_loss = sum(\n",
    "                self._train_batch(batch_X, batch_y) for batch_X, batch_y in train_loader\n",
    "            ) / len(train_loader)\n",
    "            \n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = sum(\n",
    "                    self.criterion(self.model(batch_X), batch_y).item() \n",
    "                    for batch_X, batch_y in val_loader\n",
    "                ) / len(val_loader)\n",
    "            \n",
    "            losses['train'].append(train_loss)\n",
    "            losses['val'].append(val_loss)\n",
    "            \n",
    "            if verbose and ((epoch + 1) % 5 == 0 or epoch == 0):\n",
    "                print(f\"    Epoch {epoch+1}/{actual_epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "            \n",
    "            self.scheduler.step(val_loss)\n",
    "            \n",
    "            if self.early_stopping(val_loss, self.model):\n",
    "                if verbose:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "                break\n",
    "            \n",
    "            if self.training_device.type == 'mps' and (epoch + 1) % 10 == 0:\n",
    "                torch.mps.empty_cache()\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"    Final - Train Loss: {losses['train'][-1]:.6f}, Val Loss: {losses['val'][-1]:.6f}\")\n",
    "        \n",
    "        self.loss_history = losses\n",
    "        if self.training_device.type == 'mps':\n",
    "            torch.mps.empty_cache()\n",
    "        self.is_fitted = True\n",
    "        \n",
    "    def clear_loss_history(self):\n",
    "        \"\"\"Clear loss history to reduce file size\"\"\"\n",
    "        if hasattr(self, 'loss_history'):\n",
    "            del self.loss_history\n",
    "        \n",
    "    def _train_batch(self, batch_X, batch_y):\n",
    "        \"\"\"Train a single batch\"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        outputs = self.model(batch_X)\n",
    "        loss = self.criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model must be fitted before predicting\")\n",
    "            \n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.FloatTensor(X).to(self.training_device)\n",
    "            predictions = self.model(X_tensor).cpu().numpy()\n",
    "            \n",
    "        return predictions\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        \"\"\"Set hyperparameters\"\"\"\n",
    "        for key, value in params.items():\n",
    "            if hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "        self._init_training_components()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] S&P500 Excess Sharpe (2016–24) = 0.652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ========== Core Function Definitions ==========\n",
    "# === Zero-based R² ===\n",
    "def r2_zero(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute zero-based R² (baseline is 0)\n",
    "    y_true: array of true values (N,)\n",
    "    y_pred: array of predicted values (N,)\n",
    "    \"\"\"\n",
    "    rss = np.sum((y_true - y_pred)**2)  \n",
    "    tss = np.sum(y_true**2)            \n",
    "    return 1 - rss / tss\n",
    "\n",
    "def calc_ic_daily(df, method='spearman'):\n",
    "    \"\"\"\n",
    "    Calculate daily cross-sectional RankIC.\n",
    "    df: must contain ['signal_date','y_true','y_pred']\n",
    "    \"\"\"\n",
    "    ics = (df.groupby('signal_date')\n",
    "             .apply(lambda g: g['y_pred'].corr(g['y_true'], method=method))\n",
    "             .dropna())\n",
    "    mean_ic = ics.mean()\n",
    "    std_ic  = ics.std(ddof=1)\n",
    "    t_ic    = mean_ic / (std_ic / np.sqrt(len(ics))) if std_ic > 0 else np.nan\n",
    "    pos_ratio = (ics > 0).mean()\n",
    "    return mean_ic, t_ic, pos_ratio, ics\n",
    "\n",
    "def annual_sharpe(rets, freq=252):\n",
    "    mu = float(np.mean(rets)) * freq\n",
    "    sd = float(np.std(rets, ddof=1)) * np.sqrt(freq)\n",
    "    return mu / sd if sd > 0 else 0\n",
    "\n",
    "def delta_sharpe(r2_zero, sr_base):\n",
    "    sr_star = np.sqrt(sr_base**2 + r2_zero) / np.sqrt(1 - r2_zero)\n",
    "    return sr_star - sr_base, sr_star\n",
    "\n",
    "# === 1. Load risk-free rate & calculate S&P500 Excess Sharpe ===\n",
    "rf_file = \"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/CRSP_2016_2024_top50_with_exret.csv\"\n",
    "rf_df = pd.read_csv(rf_file, usecols=[\"date\", \"rf\"])\n",
    "rf_df[\"date\"] = pd.to_datetime(rf_df[\"date\"])\n",
    "rf_df = rf_df.drop_duplicates(\"date\").set_index(\"date\").sort_index()\n",
    "rf_series = rf_df[\"rf\"].astype(float)\n",
    "\n",
    "px = yf.download(\"^GSPC\", start=\"2016-01-01\", end=\"2024-12-31\")[\"Close\"]\n",
    "sp_ret = px.pct_change().dropna()\n",
    "rf_align = rf_series.reindex(sp_ret.index).fillna(method=\"ffill\")\n",
    "sp_excess = sp_ret.values - rf_align.values\n",
    "\n",
    "SR_MKT_EX = annual_sharpe(sp_excess)\n",
    "print(f\"[INFO] S&P500 Excess Sharpe (2016–24) = {SR_MKT_EX:.3f}\")\n",
    "\n",
    "def calc_directional_metrics(y_true, y_pred, permnos=None):\n",
    "    \"\"\"\n",
    "    Improved version:\n",
    "    - Sample-level sign prediction\n",
    "    - If grouped by stock, compute Overall, Up, Down for each stock and then average\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    if permnos is None:\n",
    "        s_true = np.sign(y_true)\n",
    "        s_pred = np.sign(y_pred)\n",
    "        mask = s_true != 0\n",
    "        s_true = s_true[mask]\n",
    "        s_pred = s_pred[mask]\n",
    "\n",
    "        overall_acc = np.mean(s_true == s_pred)\n",
    "\n",
    "        up_mask = s_true > 0\n",
    "        down_mask = s_true < 0\n",
    "        up_acc = np.mean(s_true[up_mask] == s_pred[up_mask]) if np.any(up_mask) else 0\n",
    "        down_acc = np.mean(s_true[down_mask] == s_pred[down_mask]) if np.any(down_mask) else 0\n",
    "\n",
    "    else:\n",
    "        df = pd.DataFrame({\"permno\": permnos, \"yt\": y_true, \"yp\": y_pred})\n",
    "        overall_accs = []\n",
    "        up_accs = []\n",
    "        down_accs = []\n",
    "\n",
    "        for _, g in df.groupby(\"permno\"):\n",
    "            s_true = np.sign(g[\"yt\"].values)\n",
    "            s_pred = np.sign(g[\"yp\"].values)\n",
    "            mask = s_true != 0\n",
    "            s_true = s_true[mask]\n",
    "            s_pred = s_pred[mask]\n",
    "            if len(s_true) == 0:\n",
    "                continue\n",
    "            overall_accs.append(np.mean(s_true == s_pred))\n",
    "\n",
    "            up_mask = s_true > 0\n",
    "            down_mask = s_true < 0\n",
    "            up_accs.append(np.mean(s_true[up_mask] == s_pred[up_mask]) if np.any(up_mask) else np.nan)\n",
    "            down_accs.append(np.mean(s_true[down_mask] == s_pred[down_mask]) if np.any(down_mask) else np.nan)\n",
    "\n",
    "        overall_acc = np.nanmean(overall_accs)\n",
    "        up_acc = np.nanmean(up_accs)\n",
    "        down_acc = np.nanmean(down_accs)\n",
    "\n",
    "    return overall_acc, up_acc, down_acc\n",
    "\n",
    "\n",
    "# === Combined regression metrics ===\n",
    "def regression_metrics(y_true, y_pred, k, meta=None, permnos=None):\n",
    "    \"\"\"\n",
    "    Includes:\n",
    "    - Regression metrics\n",
    "    - Pointwise directional accuracy\n",
    "    - Market cap group metrics\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    n = len(y_true)\n",
    "\n",
    "    r2 = r2_zero(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "\n",
    "    dir_acc, up_acc, down_acc = calc_directional_metrics(y_true, y_pred, permnos)\n",
    "\n",
    "    metrics = {\n",
    "        \"R²_zero\": r2,\n",
    "        \"RMSE\": rmse,\n",
    "        \"MAE\": mae,\n",
    "        \"MSE\": mse,\n",
    "        \"Directional Accuracy\": dir_acc,\n",
    "        \"Up_Directional_Acc\": up_acc,\n",
    "        \"Down_Directional_Acc\": down_acc\n",
    "    }\n",
    "\n",
    "    if meta is not None and \"MKTCAP_PERCENTILE\" in meta:\n",
    "        top_mask = meta[\"MKTCAP_PERCENTILE\"] >= 0.75\n",
    "        bottom_mask = meta[\"MKTCAP_PERCENTILE\"] <= 0.25\n",
    "\n",
    "        if np.any(top_mask):\n",
    "            yt_top = y_true[top_mask]\n",
    "            yp_top = y_pred[top_mask]\n",
    "            perm_top = permnos[top_mask] if permnos is not None else None\n",
    "            r2_top = r2_zero(yt_top, yp_top)\n",
    "            rmse_top = np.sqrt(mean_squared_error(yt_top, yp_top))\n",
    "            mae_top = mean_absolute_error(yt_top, yp_top)\n",
    "            mse_top = mean_squared_error(yt_top, yp_top)\n",
    "            dir_top, up_top, down_top = calc_directional_metrics(yt_top, yp_top, perm_top)\n",
    "            metrics.update({\n",
    "                \"Top25_R2_zero\": r2_top,\n",
    "                \"Top25_RMSE\": rmse_top,\n",
    "                \"Top25_MAE\": mae_top,\n",
    "                \"Top25_MSE\": mse_top,\n",
    "                \"Top25_Dir_Acc\": dir_top,\n",
    "                \"Top25_Up_Acc\": up_top,\n",
    "                \"Top25_Down_Acc\": down_top\n",
    "            })\n",
    "\n",
    "        if np.any(bottom_mask):\n",
    "            yt_bot = y_true[bottom_mask]\n",
    "            yp_bot = y_pred[bottom_mask]\n",
    "            perm_bot = permnos[bottom_mask] if permnos is not None else None\n",
    "            r2_bot = r2_zero(yt_bot, yp_bot)\n",
    "            rmse_bot = np.sqrt(mean_squared_error(yt_bot, yp_bot))\n",
    "            mae_bot = mean_absolute_error(yt_bot, yp_bot)\n",
    "            mse_bot = mean_squared_error(yt_bot, yp_bot)\n",
    "            dir_bot, up_bot, down_bot = calc_directional_metrics(yt_bot, yp_bot, perm_bot)\n",
    "            metrics.update({\n",
    "                \"Bottom25_R2_zero\": r2_bot,\n",
    "                \"Bottom25_RMSE\": rmse_bot,\n",
    "                \"Bottom25_MAE\": mae_bot,\n",
    "                \"Bottom25_MSE\": mse_bot,\n",
    "                \"Bottom25_Dir_Acc\": dir_bot,\n",
    "                \"Bottom25_Up_Acc\": up_bot,\n",
    "                \"Bottom25_Down_Acc\": down_bot\n",
    "            })\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def f_statistic(y_true, y_pred, k):\n",
    "    \"\"\"Return F statistic and corresponding p-value\"\"\"\n",
    "    n   = len(y_true)\n",
    "    rss = np.sum((y_true - y_pred) ** 2)\n",
    "    tss = np.sum(y_true ** 2)\n",
    "    r2  = 1 - rss / tss\n",
    "    if (r2 <= 0) or (n <= k):\n",
    "        return 0.0, 1.0\n",
    "    F = (r2 / k) / ((1 - r2) / (n - k))\n",
    "    p = f_dist.sf(F, k, n - k)\n",
    "    return F, p\n",
    "\n",
    "def overall_interval_metrics_method1(y_all, yhat_all, k, permnos_all=None, meta_all=None):\n",
    "    \"\"\"\n",
    "    Method 1: Compute metrics for the entire interval at once (2016-2024, all samples concatenated)\n",
    "    Returns: a dict, can be directly passed to save_metrics()\n",
    "    \"\"\"\n",
    "    base = regression_metrics(\n",
    "        y_true=y_all, \n",
    "        y_pred=yhat_all, \n",
    "        k=k, \n",
    "        meta=meta_all, \n",
    "        permnos=permnos_all\n",
    "    )\n",
    "    F, p = f_statistic(y_all, yhat_all, k)\n",
    "    base[\"F_stat\"]     = F\n",
    "    base[\"F_pvalue\"]   = p\n",
    "    base[\"N_obs\"] = len(y_all)\n",
    "    \n",
    "    delta_cash, sr_star_cash = delta_sharpe(base[\"R²_zero\"], sr_base=0)\n",
    "    base[\"ΔSharpe_cash\"]      = delta_cash\n",
    "    base[\"Sharpe*_cash\"]      = sr_star_cash\n",
    "\n",
    "    delta_mkt , sr_star_mkt  = delta_sharpe(base[\"R²_zero\"], sr_base=SR_MKT_EX)\n",
    "    base[\"ΔSharpe_mkt\"]       = delta_mkt\n",
    "    base[\"Sharpe*_mkt\"]       = sr_star_mkt\n",
    "    \n",
    "    return base\n",
    "\n",
    "def sortino_ratio(rets, freq=252):\n",
    "    \"\"\"Compute Sortino Ratio\"\"\"\n",
    "    downside = rets[rets < 0]\n",
    "    if len(downside) == 0:\n",
    "        return np.inf\n",
    "    mu = rets.mean() * freq\n",
    "    sigma = np.sqrt((downside ** 2).mean()) * np.sqrt(freq)\n",
    "    return mu / sigma\n",
    "\n",
    "def cvar(rets, alpha=0.95):\n",
    "    \"\"\"Compute CVaR\"\"\"\n",
    "    q = np.quantile(rets, 1 - alpha)\n",
    "    return rets[rets <= q].mean()\n",
    "\n",
    "def save_predictions(model_name, window_size, y_true, y_pred, permnos, path=\"predictions/\"):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        \"PERMNO\": permnos,\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred\n",
    "    })\n",
    "\n",
    "    filename = f\"{model_name}_w{window_size}.csv\"\n",
    "    df.to_csv(os.path.join(path, filename), index=False)\n",
    "    print(f\"[Save] {filename}\")\n",
    "\n",
    "def save_metrics(metrics_dict, name, window, path=\"results.csv\"):\n",
    "    \"\"\"Save evaluation metrics\"\"\"\n",
    "    row = pd.DataFrame([metrics_dict])\n",
    "    row.insert(0, \"Model\", name)\n",
    "    row.insert(1, \"Window\", window)\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path)\n",
    "        df = df[~((df[\"Model\"] == name) & (df[\"Window\"] == window))]\n",
    "        df = pd.concat([df, row], ignore_index=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        print(f\"[Update] Metrics updated for {name} w={window}\")\n",
    "    else:\n",
    "        row.to_csv(path, index=False)\n",
    "        print(f\"[Create] New metrics file created with {name} w={window}\")\n",
    "\n",
    "def get_quarter_periods(start_year=2015, end_year=2024):\n",
    "    \"\"\"Generate quarter sequence\"\"\"\n",
    "    quarters = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for q in range(1, 5):\n",
    "            quarters.append((year, q))\n",
    "    return quarters\n",
    "\n",
    "def save_model_with_quarter(mlp_wrapper, name, window, year, quarter,\n",
    "                            path=\"models/\"):\n",
    "    \"\"\"\n",
    "    Safely save MLP model parameters, handle MPS device compatibility and file size optimization\n",
    "    \"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    mlp_wrapper.clear_loss_history()\n",
    "    \n",
    "    original_device = mlp_wrapper.training_device\n",
    "    mlp_wrapper.model.to('cpu')\n",
    "    \n",
    "    pth_file = f\"{name}_w{window}_{year}Q{quarter}.pth\"\n",
    "    torch.save(mlp_wrapper.model.state_dict(),\n",
    "               os.path.join(path, pth_file))\n",
    "    \n",
    "    mlp_wrapper.model.to(original_device)\n",
    "    \n",
    "    print(f\"[Saved] {pth_file}\")\n",
    "    \n",
    "def load_datasets(npz_path):\n",
    "    \"\"\"Load datasets\"\"\"\n",
    "    data = np.load(npz_path, allow_pickle=True) \n",
    "    datasets = {}\n",
    "    for key in data.files:\n",
    "        datasets[key] = data[key]\n",
    "    return datasets\n",
    "\n",
    "def find_coef_step(model):\n",
    "    \"\"\"\n",
    "    Get model coefficients, handle Pipeline and standalone models.\n",
    "    For nonlinear models (e.g., MLP), return None to indicate no coefficients.\n",
    "    \"\"\"\n",
    "    if hasattr(model, 'named_steps'):\n",
    "        for name, est in model.named_steps.items():\n",
    "            if hasattr(est, 'coef_'):\n",
    "                return name, est\n",
    "            if isinstance(est, Pipeline):\n",
    "                for subname, subest in est.named_steps.items():\n",
    "                    if hasattr(subest, 'coef_'):\n",
    "                        return f\"{name}__{subname}\", subest\n",
    "        return None, None\n",
    "    else:\n",
    "        if hasattr(model, 'coef_'):\n",
    "            return 'model', model\n",
    "    \n",
    "    raise ValueError(\"No estimator with coef_ found in model\")\n",
    "    \n",
    "def train_or_skip(model, train_loader, valid_loader, window_size, year, quarter, **train_kwargs):\n",
    "    save_path = f\"models/NN1_w{window_size}_{year}Q{quarter}.pth\"\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"[Skip Training] Model already exists: {save_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"[Training] Start training model: {save_path}\")\n",
    "    train_model(model, train_loader, valid_loader, **train_kwargs)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"[Done] Model saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "TUNED_MODELS = {\"NN1\"}  # Only tune NN1\n",
    "\n",
    "def tune_mlp_with_optuna(model_name, X, y, n_trials=25, verbose=False):\n",
    "    \"\"\"Optuna hyperparameter tuning for MLP model\"\"\"\n",
    "    if model_name not in TUNED_MODELS:\n",
    "        print(f\"Skip {model_name} - not tunable\")\n",
    "        return None\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    input_size = X.shape[1]\n",
    "\n",
    "    def objective(trial):\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "        batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
    "        dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.5)\n",
    "        \n",
    "        mlp = MLPWrapper(\n",
    "            model_name=model_name,\n",
    "            input_size=input_size,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size,\n",
    "            max_epochs=20,\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "        \n",
    "        cv_mse = []\n",
    "        for train_idx, val_idx in tscv.split(X):\n",
    "            X_tr, X_val = X[train_idx], X[val_idx]\n",
    "            y_tr, y_val = y[train_idx], y[val_idx]\n",
    "            \n",
    "            mlp.fit(X_tr, y_tr, validation_split=0.0, warm_start=False, verbose=verbose)\n",
    "            preds = mlp.predict(X_val)\n",
    "            cv_mse.append(mean_squared_error(y_val, preds))\n",
    "            \n",
    "        return np.mean(cv_mse)\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\",\n",
    "                                sampler=optuna.samplers.TPESampler(seed=42))\n",
    "    study.optimize(objective, n_trials=n_trials, n_jobs=-1)\n",
    "\n",
    "    print(f\"[Hyper] {model_name}: best_MSE={study.best_value:.6f}, params={study.best_params}\")\n",
    "    return study.best_params\n",
    "\n",
    "def train_mlp_models_expanding_quarterly(start_year=2015, end_year=2024, window_sizes=None, model_names=None, \n",
    "                          npz_path=\"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/all_window_datasets_scaled.npz\"):\n",
    "    \"\"\"\n",
    "    MLP expanding window training by quarter:\n",
    "    1. 2015Q4 (first quarter): use 2000-2015 data for tuning and training\n",
    "    2. 2015Q4: use 2000-2015Q2 data (train) + 2015Q3 data (test previous quarter) for training\n",
    "    3. Continue to add previous quarter's test data to the training set each quarter\n",
    "    4. Only tune hyperparameters for NN1, other models share the same hyperparameters\n",
    "    \"\"\"\n",
    "    if window_sizes is None:\n",
    "        window_sizes = [5, 21, 252, 512]\n",
    "    if model_names is None:\n",
    "        model_names = [\"NN1\", \"NN2\", \"NN3\", \"NN4\", \"NN5\"]\n",
    "    \n",
    "    print(f\"Starting MLP Quarterly Expanding Window Training ({start_year}-{end_year})\")\n",
    "    \n",
    "    datasets = load_datasets(npz_path)\n",
    "    \n",
    "    best_params_cache = {}\n",
    "    quarters_to_tune = {(2015, 4), (2020, 4)}\n",
    "    \n",
    "    for window in window_sizes:\n",
    "        print(f\"Processing window size: {window}\")\n",
    "        \n",
    "        X_train_initial = datasets[f\"X_train_{window}\"]\n",
    "        y_train_initial = datasets[f\"y_train_{window}\"]\n",
    "        X_test_full = datasets[f\"X_test_{window}\"]\n",
    "        y_test_full = datasets[f\"y_test_{window}\"]\n",
    "        \n",
    "        meta_train_dict = datasets[f\"meta_train_{window}\"].item()\n",
    "        meta_test_dict = datasets[f\"meta_test_{window}\"].item()\n",
    "        meta_train = pd.DataFrame.from_dict(meta_train_dict)\n",
    "        meta_test = pd.DataFrame.from_dict(meta_test_dict)\n",
    "        meta_test['ret_date'] = pd.to_datetime(meta_test['ret_date'])\n",
    "        \n",
    "        input_size = X_train_initial.shape[1]\n",
    "        \n",
    "        X_expanding = X_train_initial.copy()\n",
    "        y_expanding = y_train_initial.copy()\n",
    "\n",
    "        for model_name in model_names:\n",
    "            cache_key = f\"{model_name}_w{window}\"\n",
    "            default_params = {'learning_rate': 0.001, 'batch_size': 512, 'dropout_rate': 0.1}\n",
    "            best_params_cache[cache_key] = default_params\n",
    "        \n",
    "        mlp_models = {}\n",
    "        \n",
    "        quarter_periods = get_quarter_periods(start_year, end_year)\n",
    "\n",
    "        quarter_periods = [\n",
    "            (y, q) for (y, q) in quarter_periods\n",
    "            if not (y == start_year and q < 4)\n",
    "            and not (y == end_year and q > 3)\n",
    "        ]\n",
    "        \n",
    "        for year, quarter in quarter_periods:\n",
    "            print(f\"  Quarter {year}Q{quarter}: Training models with expanding data\")\n",
    "            \n",
    "            if not (year == start_year and quarter == 1):\n",
    "                if quarter == 1:\n",
    "                    prev_year, prev_quarter = year - 1, 4\n",
    "                else:\n",
    "                    prev_year, prev_quarter = year, quarter - 1\n",
    "                \n",
    "                prev_quarter_mask = (\n",
    "                    (meta_test['ret_date'].dt.year == prev_year) & \n",
    "                    (meta_test['ret_date'].dt.quarter == prev_quarter)\n",
    "                )\n",
    "                if np.any(prev_quarter_mask):\n",
    "                    X_prev_quarter = X_test_full[prev_quarter_mask]\n",
    "                    y_prev_quarter = y_test_full[prev_quarter_mask]\n",
    "                    \n",
    "                    X_expanding = np.vstack([X_expanding, X_prev_quarter])\n",
    "                    y_expanding = np.hstack([y_expanding, y_prev_quarter])\n",
    "                    \n",
    "                    print(f\"    Added {np.sum(prev_quarter_mask)} samples from {prev_year}Q{prev_quarter} to training set\")\n",
    "            \n",
    "            for model_name in model_names:\n",
    "                model_path = f\"models/{model_name}_w{window}_{year}Q{quarter}.pth\"\n",
    "                if os.path.exists(model_path):\n",
    "                    print(f\"    [Skip Training] {model_path} already exists\")\n",
    "                    continue\n",
    "                cache_key = f\"{model_name}_w{window}\"\n",
    "\n",
    "                if cache_key in best_params_cache:\n",
    "                    best_params = best_params_cache[cache_key]\n",
    "                else:\n",
    "                    best_params = {'learning_rate': 0.001, 'batch_size': 512, 'dropout_rate': 0.1}\n",
    "\n",
    "                need_rebuild = False\n",
    "                if ((year, quarter) in quarters_to_tune) and (model_name == \"NN1\"):\n",
    "                    print(f\"    [Re-tuning] {model_name} for {year}Q{quarter}\")\n",
    "                    new_params = tune_mlp_with_optuna(model_name, X_expanding, y_expanding, n_trials=5)\n",
    "                    if new_params is not None:\n",
    "                        old_params = best_params_cache.get(cache_key, {})\n",
    "                        if 'dropout_rate' in new_params and old_params.get('dropout_rate') != new_params.get('dropout_rate'):\n",
    "                            print(f\"    [Structure Change] dropout_rate changed: {old_params.get('dropout_rate', 'N/A')} -> {new_params.get('dropout_rate')}\")\n",
    "                            need_rebuild = True\n",
    "                            \n",
    "                        best_params = new_params\n",
    "                        for mn in model_names:\n",
    "                            best_params_cache[f\"{mn}_w{window}\"] = best_params\n",
    "\n",
    "                if (year == start_year and quarter == 4) or model_name not in mlp_models or need_rebuild:\n",
    "                    if need_rebuild:\n",
    "                        print(f\"    Rebuilding {model_name} model due to structure change\")\n",
    "                    else:\n",
    "                        print(f\"    Creating new {model_name} model\")\n",
    "                        \n",
    "                    mlp = MLPWrapper(\n",
    "                        model_name=model_name,\n",
    "                        input_size=input_size,\n",
    "                        learning_rate=best_params.get('learning_rate', 0.001),\n",
    "                        batch_size=best_params.get('batch_size', 512),\n",
    "                        max_epochs=50,\n",
    "                        warm_start_epochs=10,\n",
    "                        dropout_rate=best_params.get('dropout_rate', 0.1)   \n",
    "                    )\n",
    "                    \n",
    "                    mlp_models[model_name] = mlp\n",
    "                    warm_start = False\n",
    "                else:\n",
    "                    print(f\"    Using warm-start for {model_name} model (retaining previous weights)\")\n",
    "                    mlp = mlp_models[model_name]\n",
    "                    mlp.learning_rate = best_params.get('learning_rate', 0.001)\n",
    "                    mlp.batch_size = best_params.get('batch_size', 512)\n",
    "                    mlp.max_epochs = 50\n",
    "                    mlp.warm_start_epochs = 10\n",
    "                    mlp.optimizer = optim.Adam(mlp.model.parameters(), lr=mlp.learning_rate)\n",
    "                    mlp.scheduler = ReduceLROnPlateau(mlp.optimizer, mode='min', factor=0.5, patience=10, verbose=False)\n",
    "                    mlp.early_stopping = EarlyStopping(patience=10)\n",
    "                    warm_start = True\n",
    "\n",
    "                mlp.fit(X_expanding, y_expanding, validation_split=0.2, warm_start=warm_start)\n",
    "                \n",
    "                save_model_with_quarter(mlp, model_name, window, year, quarter)\n",
    "                \n",
    "            print(f\"    Training data size: {len(X_expanding)} samples\")\n",
    "            gc.collect()\n",
    "            if torch.backends.mps.is_available():\n",
    "                torch.mps.empty_cache()\n",
    "            elif torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    print(\"MLP Quarterly Expanding Window Training Completed\")\n",
    "    return best_params_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Portfolio Core Class =====\n",
    "# ===== Transaction Cost Settings =====\n",
    "TC_GRID = [0.0005, 0.001, 0.002, 0.003, 0.004]  # 5, 10, 20, 30, 40 bps\n",
    "TC_TAG  = {\n",
    "    0.0005: \"tc5\",\n",
    "    0.001:  \"tc10\", \n",
    "    0.002:  \"tc20\",\n",
    "    0.003:  \"tc30\",\n",
    "    0.004:  \"tc40\"\n",
    "}\n",
    "\n",
    "class PortfolioBacktester:\n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "        \n",
    "    def calc_turnover(self, w_t, r_t, w_tp1):\n",
    "        \"\"\"Calculate turnover using the standard formula\"\"\"\n",
    "        if w_t is None:\n",
    "            return np.sum(np.abs(w_tp1))\n",
    "        \n",
    "        gross_ret = np.sum(w_t * r_t)\n",
    "        if abs(1 + gross_ret) < 1e-8:\n",
    "            return np.sum(np.abs(w_tp1))\n",
    "        \n",
    "        passive_weight = w_t * (1 + r_t) / (1 + gross_ret)\n",
    "        turnover = np.sum(np.abs(w_tp1 - passive_weight))\n",
    "        return turnover\n",
    "    \n",
    "    def create_portfolios_with_permno_tracking(self, signals, market_caps, permnos, top_pct=0.1, bottom_pct=0.1, weight_scheme=\"VW\"):\n",
    "        \"\"\"\n",
    "        Create portfolio weights based on signals, strictly tracking permno alignment.\n",
    "        weight_scheme: 'VW' for value-weighted, 'EW' for equal-weighted\n",
    "        \"\"\"\n",
    "        n_stocks = len(signals)\n",
    "        top_n    = max(1, int(round(n_stocks * top_pct)))\n",
    "        bottom_n = max(1, int(round(n_stocks * bottom_pct)))\n",
    "        \n",
    "        sorted_idx = np.argsort(signals)[::-1]\n",
    "        \n",
    "        top_idx = sorted_idx[:top_n]\n",
    "        bottom_idx = sorted_idx[-bottom_n:]\n",
    "        \n",
    "        portfolio_data = {}\n",
    "        \n",
    "        # Long-only portfolio (Top 10%)\n",
    "        long_weights = np.zeros(n_stocks)\n",
    "        if len(top_idx) > 0:\n",
    "            if weight_scheme == \"VW\":\n",
    "                top_market_caps = market_caps[top_idx]\n",
    "                if np.sum(top_market_caps) > 0:\n",
    "                    long_weights[top_idx] = top_market_caps / np.sum(top_market_caps)\n",
    "            else:\n",
    "                long_weights[top_idx] = 1.0 / len(top_idx)\n",
    "        \n",
    "        portfolio_data['long_only'] = {\n",
    "            'weights': long_weights,\n",
    "            'permnos': permnos.copy(),\n",
    "            'selected_permnos': permnos[top_idx] if len(top_idx) > 0 else np.array([])\n",
    "        }\n",
    "        \n",
    "        # Short-only portfolio (Bottom 10%)\n",
    "        short_weights = np.zeros(n_stocks)\n",
    "        if len(bottom_idx) > 0:\n",
    "            if weight_scheme == \"VW\":\n",
    "                bottom_market_caps = market_caps[bottom_idx]\n",
    "                if np.sum(bottom_market_caps) > 0:\n",
    "                    short_weights[bottom_idx] = -bottom_market_caps / np.sum(bottom_market_caps)\n",
    "            else:\n",
    "                short_weights[bottom_idx] = -1.0 / len(bottom_idx)\n",
    "        \n",
    "        portfolio_data['short_only'] = {\n",
    "            'weights': short_weights,\n",
    "            'permnos': permnos.copy(),\n",
    "            'selected_permnos': permnos[bottom_idx] if len(bottom_idx) > 0 else np.array([])\n",
    "        }\n",
    "        \n",
    "        # Long-Short portfolio (Top long + Bottom short)\n",
    "        ls_raw = long_weights + short_weights\n",
    "\n",
    "        gross_target = 2.0\n",
    "        current_gross = np.sum(np.abs(long_weights)) + np.sum(np.abs(short_weights))\n",
    "        scale = gross_target / current_gross if current_gross > 1e-8 else 0.0\n",
    "        ls_weights = scale * ls_raw\n",
    "\n",
    "        ls_selected_permnos = np.concatenate([\n",
    "            permnos[top_idx] if len(top_idx) > 0 else np.array([]),\n",
    "            permnos[bottom_idx] if len(bottom_idx) > 0 else np.array([])\n",
    "        ])\n",
    "\n",
    "        portfolio_data['long_short'] = {\n",
    "            'weights': ls_weights,\n",
    "            'permnos': permnos.copy(),\n",
    "            'selected_permnos': ls_selected_permnos\n",
    "        }\n",
    "\n",
    "        return portfolio_data\n",
    "    \n",
    "    def calculate_aligned_portfolio_return(self, portfolio_weights, portfolio_permnos, actual_returns, actual_permnos):\n",
    "        \"\"\"Calculate portfolio return strictly aligned by permno\"\"\"\n",
    "        aligned_returns = np.zeros(len(portfolio_permnos))\n",
    "        \n",
    "        return_dict = dict(zip(actual_permnos, actual_returns))\n",
    "        \n",
    "        for i, permno in enumerate(portfolio_permnos):\n",
    "            if permno in return_dict:\n",
    "                aligned_returns[i] = return_dict[permno]\n",
    "        \n",
    "        portfolio_return = np.sum(portfolio_weights * aligned_returns)\n",
    "        return portfolio_return, aligned_returns\n",
    "\n",
    "    def calculate_metrics(self, returns, turnover_series=None):\n",
    "        \"\"\"Calculate portfolio metrics - only returns summary metrics, not full series\"\"\"\n",
    "        returns = np.array(returns)\n",
    "        \n",
    "        annual_return = np.mean(returns) * 252\n",
    "        annual_vol = np.std(returns, ddof=1) * np.sqrt(252)\n",
    "        sharpe = annual_return / annual_vol if annual_vol > 0 else 0\n",
    "        \n",
    "        log_cum = np.cumsum(np.log1p(returns))\n",
    "        peak_log = np.maximum.accumulate(log_cum)\n",
    "        dd_log = peak_log - log_cum\n",
    "        max_drawdown = 1 - np.exp(-dd_log.max()) \n",
    "        max_1d_loss = np.min(returns) \n",
    "        \n",
    "        avg_turnover = np.mean(turnover_series) if turnover_series is not None else 0\n",
    "        \n",
    "        sortino = sortino_ratio(returns)\n",
    "        cvar95  = cvar(returns, alpha=0.95)\n",
    "\n",
    "        result = {\n",
    "            'annual_return': annual_return,\n",
    "            'annual_vol': annual_vol,\n",
    "            'sharpe': sharpe,\n",
    "            'max_drawdown': max_drawdown,\n",
    "            'max_1d_loss': max_1d_loss,\n",
    "            'avg_turnover': avg_turnover,\n",
    "            'sortino': sortino,\n",
    "            'cvar95': cvar95\n",
    "        }\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_portfolio_simulation_daily_rebalance(start_year=2016, end_year=2024, window_sizes=None, model_names=None,\n",
    "                                           npz_path=\"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/all_window_datasets_scaled.npz\"):\n",
    "    \"\"\"\n",
    "    Portfolio simulation (daily prediction, next-day rebalancing):\n",
    "        1. Load quarterly models (trained with quarterly expanding window)\n",
    "        2. Daily prediction to daily signals\n",
    "        3. Daily portfolio construction (T+1 rebalancing, strict permno alignment)\n",
    "        4. Separate summary metrics and time series data\n",
    "    \"\"\"\n",
    "    if window_sizes is None:\n",
    "        window_sizes = [5, 21, 252, 512]\n",
    "    if model_names is None:\n",
    "        model_names = [\"NN1\", \"NN2\", \"NN3\", \"NN4\", \"NN5\"]\n",
    "    \n",
    "    print(\"Starting daily rebalance portfolio backtesting simulation\")\n",
    "    \n",
    "    backtester = PortfolioBacktester()\n",
    "    datasets = load_datasets(npz_path)\n",
    "    \n",
    "    import joblib\n",
    "    scaler_path_base = \"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/\"\n",
    "    y_scalers = {}\n",
    "    \n",
    "    for window in window_sizes:\n",
    "        scaler_file = f\"{scaler_path_base}scaler_y_window_{window}.pkl\"\n",
    "        try:\n",
    "            y_scalers[window] = joblib.load(scaler_file)\n",
    "            print(f\"Loaded y scaler for window {window}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Y scaler file not found for window {window}: {scaler_file}\")\n",
    "            y_scalers[window] = None\n",
    "    \n",
    "    summary_results = []\n",
    "    daily_series_data = []\n",
    "    pred_rows = []\n",
    "    \n",
    "    WEIGHT_SCHEMES = [\"VW\", \"EW\"]\n",
    "    \n",
    "    for window in window_sizes:\n",
    "        print(f\"Processing window size: {window}\")\n",
    "        \n",
    "        X_test = datasets[f\"X_test_{window}\"]\n",
    "        y_test = datasets[f\"y_test_{window}\"]\n",
    "        input_size = X_test.shape[1]\n",
    "        meta_test_dict = datasets[f\"meta_test_{window}\"].item()\n",
    "        meta_test = pd.DataFrame.from_dict(meta_test_dict)\n",
    "        \n",
    "        permnos_test = meta_test[\"PERMNO\"].values\n",
    "        meta_test[\"signal_date\"]  = pd.to_datetime(meta_test[\"date\"])\n",
    "        meta_test[\"ret_date\"]     = pd.to_datetime(meta_test[\"ret_date\"])\n",
    "        market_caps = meta_test.get(\"MKTCAP\", np.ones(len(permnos_test)))\n",
    "        \n",
    "        meta_test['date'] = pd.to_datetime(meta_test[\"date\"])\n",
    "        dates_test = meta_test['signal_date']\n",
    "        \n",
    "        for model_name in model_names:\n",
    "            for scheme in WEIGHT_SCHEMES:\n",
    "                all_y_true   = []\n",
    "                all_y_pred   = []\n",
    "                all_permnos  = []\n",
    "                all_meta     = []\n",
    "                print(f\"  Model: {model_name}, Scheme: {scheme}\")\n",
    "                \n",
    "                portfolio_daily_data = {\n",
    "                    'long_only': {'returns': [], 'turnovers': [], 'dates': []},\n",
    "                    'short_only': {'returns': [], 'turnovers': [], 'dates': []},\n",
    "                    'long_short': {'returns': [], 'turnovers': [], 'dates': []}\n",
    "                }\n",
    "                \n",
    "                prev_portfolio_data = {'long_only': None, 'short_only': None, 'long_short': None}\n",
    "                \n",
    "                signals_buf = {}\n",
    "                \n",
    "                for year in range(start_year, min(end_year + 1, 2025)):\n",
    "                    for quarter in range(1, 5):\n",
    "                        # Determine model file year and quarter (T+1 logic: use previous quarter's model to predict current quarter)\n",
    "                        if quarter == 1:\n",
    "                            model_file_year, model_file_quarter = year - 1, 4\n",
    "                        else:\n",
    "                            model_file_year, model_file_quarter = year, quarter - 1\n",
    "                            \n",
    "                        pth_path = f\"models/{model_name}_w{window}_{model_file_year}Q{model_file_quarter}.pth\"\n",
    "                        if not os.path.exists(pth_path):\n",
    "                            print(f\"      Skip: Model file not found {pth_path}\")\n",
    "                            continue\n",
    "\n",
    "                        mlp = MLPWrapper(model_name=model_name,\n",
    "                                         input_size=input_size,\n",
    "                                         max_epochs=0)\n",
    "                        mlp.model.load_state_dict(torch.load(pth_path, map_location=device))\n",
    "                        mlp.is_fitted = True\n",
    "                        model = mlp\n",
    "                        \n",
    "                        quarter_mask = (\n",
    "                            (dates_test.dt.year == year) & \n",
    "                            (dates_test.dt.quarter == quarter)\n",
    "                        )\n",
    "                        if not np.any(quarter_mask):\n",
    "                            continue\n",
    "                        \n",
    "                        X_quarter = X_test[quarter_mask]\n",
    "                        y_quarter = y_test[quarter_mask]\n",
    "                        permnos_quarter = permnos_test[quarter_mask]\n",
    "                        market_caps_quarter = market_caps[quarter_mask]\n",
    "                        dates_quarter = dates_test[quarter_mask]\n",
    "                        ret_dates_quarter = meta_test.loc[quarter_mask, 'ret_date'].values\n",
    "                        \n",
    "                        predictions_raw = model.predict(X_quarter)\n",
    "                        \n",
    "                        if y_scalers[window] is not None:\n",
    "                            predictions_raw_reshaped = predictions_raw.reshape(-1, 1)\n",
    "                            predictions_inverse = y_scalers[window].inverse_transform(predictions_raw_reshaped).flatten()\n",
    "                            \n",
    "                            y_quarter_reshaped = y_quarter.reshape(-1, 1) \n",
    "                            actual_returns_inverse = y_scalers[window].inverse_transform(y_quarter_reshaped).flatten()\n",
    "                            \n",
    "                            print(f\"      Applied inverse scaling for window {window} - {len(predictions_inverse)} samples\")\n",
    "                        else:\n",
    "                            predictions_inverse = predictions_raw\n",
    "                            actual_returns_inverse = y_quarter\n",
    "                            print(f\"      No scaler available for window {window}, using original scaled data\")\n",
    "                        \n",
    "                        df_quarter = pd.DataFrame({\n",
    "                            'signal_date': dates_quarter,\n",
    "                            'ret_date': ret_dates_quarter,\n",
    "                            'permno': permnos_quarter,\n",
    "                            'market_cap': market_caps_quarter,\n",
    "                            'actual_return': actual_returns_inverse,                 \n",
    "                            'prediction': predictions_inverse   \n",
    "                        })\n",
    "                        \n",
    "                        if scheme == 'VW':\n",
    "                            df_q_save = df_quarter[['signal_date','ret_date','permno',\n",
    "                                                    'actual_return','prediction','market_cap']].copy()\n",
    "                            df_q_save.rename(columns={'actual_return':'y_true',\n",
    "                                                      'prediction':'y_pred'}, inplace=True)\n",
    "                            df_q_save['model']  = model_name\n",
    "                            df_q_save['window'] = window\n",
    "                            pred_rows.append(df_q_save)\n",
    "                        \n",
    "                        all_y_true.append(df_quarter['actual_return'].values)\n",
    "                        all_y_pred.append(df_quarter['prediction'].values)\n",
    "                        all_permnos.append(df_quarter['permno'].values)\n",
    "                        all_meta.append(meta_test.loc[quarter_mask, :])   \n",
    "\n",
    "                        for signal_date, sig_grp in df_quarter.groupby('signal_date'):\n",
    "                            daily_signals = (\n",
    "                                sig_grp.groupby('permno')['prediction'].mean()\n",
    "                                      .to_frame('prediction')\n",
    "                                      .join(sig_grp.groupby('permno')['market_cap'].mean())\n",
    "                            )\n",
    "                            signals_buf[signal_date] = daily_signals\n",
    "\n",
    "                            prev_date = signal_date - pd.tseries.offsets.BDay(1)\n",
    "                            if prev_date not in signals_buf:\n",
    "                                continue\n",
    "\n",
    "                            sigs = signals_buf.pop(prev_date)\n",
    "                            if prev_date in signals_buf:\n",
    "                                del signals_buf[prev_date]\n",
    "\n",
    "                            ret_grp = df_quarter[df_quarter['ret_date'] == signal_date]\n",
    "                            if len(ret_grp) == 0:\n",
    "                                continue\n",
    "\n",
    "                            daily_actual_returns = (\n",
    "                                ret_grp.groupby('permno')['actual_return']\n",
    "                                       .mean()\n",
    "                                       .reindex(sigs.index, fill_value=0)\n",
    "                                       .values\n",
    "                            )\n",
    "                            daily_permnos = sigs.index.values\n",
    "\n",
    "                            portfolios_data = backtester.create_portfolios_with_permno_tracking(\n",
    "                                signals      = sigs['prediction'].values,\n",
    "                                market_caps  = sigs['market_cap'].values,\n",
    "                                permnos      = daily_permnos,\n",
    "                                weight_scheme= scheme\n",
    "                            )\n",
    "                            \n",
    "                            for portfolio_type in ['long_only', 'short_only', 'long_short']:\n",
    "                                portfolio_info = portfolios_data[portfolio_type]\n",
    "                                \n",
    "                                portfolio_return, aligned_returns = backtester.calculate_aligned_portfolio_return(\n",
    "                                    portfolio_weights=portfolio_info['weights'],\n",
    "                                    portfolio_permnos=portfolio_info['permnos'],\n",
    "                                    actual_returns=daily_actual_returns,\n",
    "                                    actual_permnos=daily_permnos\n",
    "                                )\n",
    "                                \n",
    "                                if prev_portfolio_data[portfolio_type] is not None:\n",
    "                                    prev_w_ser = pd.Series(\n",
    "                                        prev_portfolio_data[portfolio_type]['weights'],\n",
    "                                        index=prev_portfolio_data[portfolio_type]['permnos']\n",
    "                                    )\n",
    "                                    cur_w_ser = pd.Series(\n",
    "                                        portfolio_info['weights'],\n",
    "                                        index=portfolio_info['permnos']\n",
    "                                    )\n",
    "\n",
    "                                    prev_r_ser = pd.Series(\n",
    "                                        prev_portfolio_data[portfolio_type]['aligned_returns'],\n",
    "                                        index=prev_portfolio_data[portfolio_type]['permnos']\n",
    "                                    )\n",
    "\n",
    "                                    aligned_prev_w = prev_w_ser.reindex(cur_w_ser.index, fill_value=0).values\n",
    "                                    aligned_prev_r = prev_r_ser.reindex(cur_w_ser.index, fill_value=0).values\n",
    "\n",
    "                                    aligned_cur_w = cur_w_ser.values\n",
    "\n",
    "                                    turnover = backtester.calc_turnover(\n",
    "                                        w_t  = aligned_prev_w,\n",
    "                                        r_t  = aligned_prev_r,\n",
    "                                        w_tp1= aligned_cur_w\n",
    "                                    )\n",
    "                                else:\n",
    "                                    turnover = np.sum(np.abs(portfolio_info['weights']))\n",
    "                                \n",
    "                                portfolio_daily_data[portfolio_type]['returns'].append(portfolio_return)\n",
    "                                portfolio_daily_data[portfolio_type]['turnovers'].append(turnover)\n",
    "                                portfolio_daily_data[portfolio_type]['dates'].append(signal_date)\n",
    "                                \n",
    "                                prev_portfolio_data[portfolio_type] = {\n",
    "                                    'weights'        : portfolio_info['weights'],\n",
    "                                    'permnos'        : portfolio_info['permnos'],\n",
    "                                    'aligned_returns': aligned_returns      \n",
    "                                }\n",
    "                \n",
    "                for portfolio_type in ['long_only', 'short_only', 'long_short']:\n",
    "                    portfolio_data = portfolio_daily_data[portfolio_type]\n",
    "                    \n",
    "                    if len(portfolio_data['returns']) > 0:\n",
    "                        metrics = backtester.calculate_metrics(\n",
    "                            returns=portfolio_data['returns'],\n",
    "                            turnover_series=portfolio_data['turnovers']\n",
    "                        )\n",
    "                        \n",
    "                        rets = np.array(portfolio_data['returns'])\n",
    "                        tovs = np.array(portfolio_data['turnovers'])\n",
    "\n",
    "                        for tc in TC_GRID:\n",
    "                            tag = TC_TAG[tc]\n",
    "                            adj = rets - tovs * tc\n",
    "\n",
    "                            ann_ret = adj.mean() * 252\n",
    "                            ann_vol = adj.std(ddof=1) * np.sqrt(252)\n",
    "                            sharpe  = ann_ret / ann_vol if ann_vol > 0 else 0\n",
    "\n",
    "                            cum_adj = np.cumprod(1 + adj)\n",
    "                            mdd = ((cum_adj - np.maximum.accumulate(cum_adj)) /\n",
    "                                   np.maximum.accumulate(cum_adj)).min()\n",
    "\n",
    "                            metrics[f'{tag}_annual_return'] = ann_ret\n",
    "                            metrics[f'{tag}_annual_vol']    = ann_vol\n",
    "                            metrics[f'{tag}_sharpe']        = sharpe\n",
    "                            metrics[f'{tag}_max_drawdown']  = mdd\n",
    "                        \n",
    "                        summary_results.append({\n",
    "                            'scheme': scheme,\n",
    "                            'model': model_name,\n",
    "                            'window': window,\n",
    "                            'portfolio_type': portfolio_type,\n",
    "                            **metrics\n",
    "                        })\n",
    "                        \n",
    "                        rets_arr = np.array(portfolio_data['returns'])\n",
    "                        tovs_arr = np.array(portfolio_data['turnovers'])\n",
    "                        cum_no_tc = np.log1p(rets_arr).cumsum()\n",
    "\n",
    "                        tc_ret_dict = {}\n",
    "                        tc_cum_dict = {}\n",
    "                        for tc in TC_GRID:\n",
    "                            tag = TC_TAG[tc]\n",
    "                            r = rets_arr - tovs_arr * tc\n",
    "                            tc_ret_dict[tag] = r\n",
    "                            tc_cum_dict[tag] = np.log1p(r).cumsum()\n",
    "\n",
    "                        for i, date in enumerate(portfolio_data['dates']):\n",
    "                            row = {\n",
    "                                'scheme'        : scheme,\n",
    "                                'model'         : model_name,\n",
    "                                'window'        : window,\n",
    "                                'portfolio_type': portfolio_type,\n",
    "                                'date'          : str(date),\n",
    "                                'return'        : rets_arr[i],\n",
    "                                'turnover'      : tovs_arr[i],\n",
    "                                'cumulative'    : cum_no_tc[i],\n",
    "                            }\n",
    "                            for tag in TC_TAG.values():\n",
    "                                row[f'{tag}_return']     = tc_ret_dict[tag][i]\n",
    "                                row[f'{tag}_cumulative'] = tc_cum_dict[tag][i]\n",
    "\n",
    "                            daily_series_data.append(row)\n",
    "\n",
    "                if scheme == \"VW\" and len(all_y_true) > 0:\n",
    "                    y_all    = np.concatenate(all_y_true)\n",
    "                    yhat_all = np.concatenate(all_y_pred)\n",
    "                    perm_all = np.concatenate(all_permnos)\n",
    "                    meta_all = pd.concat(all_meta, ignore_index=True)\n",
    "\n",
    "                    k = X_test.shape[1]\n",
    "\n",
    "                    m1_metrics = overall_interval_metrics_method1(\n",
    "                        y_all, yhat_all, k,\n",
    "                        permnos_all=perm_all,\n",
    "                        meta_all=meta_all\n",
    "                    )\n",
    "\n",
    "                if scheme == \"VW\" and len(all_y_true) > 0:\n",
    "                    y_all    = np.concatenate(all_y_true)\n",
    "                    yhat_all = np.concatenate(all_y_pred)\n",
    "                    perm_all = np.concatenate(all_permnos)\n",
    "                    meta_all = pd.concat(all_meta, ignore_index=True)\n",
    "\n",
    "                    k = X_test.shape[1]\n",
    "\n",
    "                    m1_metrics = overall_interval_metrics_method1(\n",
    "                        y_all, yhat_all, k,\n",
    "                        permnos_all=perm_all,\n",
    "                        meta_all=meta_all\n",
    "                    )\n",
    "\n",
    "                    full_pred_df = pd.concat(pred_rows, ignore_index=True)\n",
    "                    full_pred_df['signal_date'] = pd.to_datetime(full_pred_df['signal_date'], errors='coerce')\n",
    "\n",
    "                    cur = full_pred_df.loc[\n",
    "                        (full_pred_df['window'] == window) &\n",
    "                        (full_pred_df['model'] == model_name),\n",
    "                        ['signal_date', 'y_true', 'y_pred']\n",
    "                    ].dropna()\n",
    "\n",
    "                    if len(cur) >= 30:\n",
    "                        mean_ic, t_ic, pos_ic, _ = calc_ic_daily(cur, method='spearman')\n",
    "                    else:\n",
    "                        mean_ic, t_ic, pos_ic = np.nan, np.nan, np.nan\n",
    "\n",
    "                    m1_metrics['RankIC_mean']  = mean_ic\n",
    "                    m1_metrics['RankIC_t']     = t_ic\n",
    "                    m1_metrics['RankIC_pos%']  = pos_ic\n",
    "\n",
    "                    save_metrics(m1_metrics, name=model_name, window=window,\n",
    "                        path=\"portfolio_metrics.csv\")\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_results)\n",
    "    daily_df = pd.DataFrame(daily_series_data) if daily_series_data else pd.DataFrame()\n",
    "    \n",
    "    tc_columns = [c for c in summary_df.columns if c.startswith('tc')]\n",
    "    summary_df[tc_columns] = summary_df[tc_columns].fillna(0.0)\n",
    "    \n",
    "    def save_split_by_scheme(df, base_filename):\n",
    "        \"\"\"Helper function to save files split by scheme\"\"\"\n",
    "        if df.empty:\n",
    "            print(f\"Warning: DataFrame is empty, skipping save for {base_filename}\")\n",
    "            return None, None\n",
    "            \n",
    "        vw_df = df[df['scheme'] == 'VW']\n",
    "        ew_df = df[df['scheme'] == 'EW']\n",
    "        \n",
    "        vw_filename = f\"{base_filename}_VW.csv\"\n",
    "        ew_filename = f\"{base_filename}_EW.csv\"\n",
    "        \n",
    "        vw_df.to_csv(vw_filename, index=False)\n",
    "        ew_df.to_csv(ew_filename, index=False)\n",
    "        \n",
    "        print(f\"VW results saved to {vw_filename}\")\n",
    "        print(f\"EW results saved to {ew_filename}\")\n",
    "        \n",
    "        return vw_filename, ew_filename\n",
    "    \n",
    "    save_split_by_scheme(summary_df, \"portfolio_results_daily_rebalance\")\n",
    "    \n",
    "    if not daily_df.empty:\n",
    "        save_split_by_scheme(daily_df, \"portfolio_daily_series\")\n",
    "    \n",
    "    if pred_rows:\n",
    "        pred_df = pd.concat(pred_rows, ignore_index=True)\n",
    "        pred_df.to_csv(\"predictions_daily.csv\", index=False)\n",
    "        print(f\"Saved {len(pred_df)} prediction rows to predictions_daily.csv\")\n",
    "    \n",
    "    print(f\"Generated {len(summary_results)} portfolio summary records\")\n",
    "    print(f\"Generated {len(daily_series_data)} daily series records\")\n",
    "    \n",
    "    return summary_df, daily_df, backtester\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MLP Quarterly Expanding Window Training (2015-2024)\n",
      "Processing window size: 5\n",
      "  Quarter 2015Q4: Training models with expanding data\n",
      "    [Re-tuning] NN1 for 2015Q4\n",
      "[Hyper] NN1: best_MSE=0.742246, params={'learning_rate': 0.00019603807684547063, 'batch_size': 64, 'dropout_rate': 0.48472181142528964}\n",
      "    [Structure Change] dropout_rate changed: 0.1 -> 0.48472181142528964\n",
      "    Rebuilding NN1 model due to structure change\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 1.587145, Val Loss: 0.437690\n",
      "    Epoch 5/50, Train Loss: 1.138599, Val Loss: 0.433211\n",
      "    Epoch 10/50, Train Loss: 1.136151, Val Loss: 0.434096\n",
      "Early stopping at epoch 12, Train Loss: 1.136425, Val Loss: 0.434145\n",
      "    Final - Train Loss: 1.136425, Val Loss: 0.434145\n",
      "[Saved] NN1_w5_2015Q4.pth\n",
      "    Creating new NN2 model\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 1.647200, Val Loss: 0.436540\n",
      "    Epoch 5/50, Train Loss: 1.139938, Val Loss: 0.430264\n",
      "Early stopping at epoch 9, Train Loss: 1.136391, Val Loss: 0.431948\n",
      "    Final - Train Loss: 1.136391, Val Loss: 0.431948\n",
      "[Saved] NN2_w5_2015Q4.pth\n",
      "    Creating new NN3 model\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 1.517318, Val Loss: 0.431595\n",
      "    Epoch 5/50, Train Loss: 1.139239, Val Loss: 0.430720\n",
      "Early stopping at epoch 8, Train Loss: 1.137175, Val Loss: 0.431446\n",
      "    Final - Train Loss: 1.137175, Val Loss: 0.431446\n",
      "[Saved] NN3_w5_2015Q4.pth\n",
      "    Creating new NN4 model\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 1.526353, Val Loss: 0.428816\n",
      "    Epoch 5/50, Train Loss: 1.140287, Val Loss: 0.429877\n",
      "Early stopping at epoch 8, Train Loss: 1.136959, Val Loss: 0.431275\n",
      "    Final - Train Loss: 1.136959, Val Loss: 0.431275\n",
      "[Saved] NN4_w5_2015Q4.pth\n",
      "    Creating new NN5 model\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 1.616950, Val Loss: 0.429692\n",
      "    Epoch 5/50, Train Loss: 1.141522, Val Loss: 0.429477\n",
      "Early stopping at epoch 8, Train Loss: 1.136938, Val Loss: 0.431953\n",
      "    Final - Train Loss: 1.136938, Val Loss: 0.431953\n",
      "[Saved] NN5_w5_2015Q4.pth\n",
      "    Training data size: 196920 samples\n",
      "  Quarter 2016Q1: Training models with expanding data\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.136783, Val Loss: 0.433810\n",
      "    Epoch 5/20, Train Loss: 1.136123, Val Loss: 0.434084\n",
      "Early stopping at epoch 9, Train Loss: 1.135708, Val Loss: 0.434089\n",
      "    Final - Train Loss: 1.135708, Val Loss: 0.434089\n",
      "[Saved] NN1_w5_2016Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.139629, Val Loss: 0.430386\n",
      "    Epoch 5/20, Train Loss: 1.137053, Val Loss: 0.431585\n",
      "Early stopping at epoch 6, Train Loss: 1.136362, Val Loss: 0.431896\n",
      "    Final - Train Loss: 1.136362, Val Loss: 0.431896\n",
      "[Saved] NN2_w5_2016Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.141413, Val Loss: 0.430312\n",
      "    Epoch 5/20, Train Loss: 1.136650, Val Loss: 0.431545\n",
      "Early stopping at epoch 6, Train Loss: 1.136625, Val Loss: 0.431567\n",
      "    Final - Train Loss: 1.136625, Val Loss: 0.431567\n",
      "[Saved] NN3_w5_2016Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.142603, Val Loss: 0.429144\n",
      "    Epoch 5/20, Train Loss: 1.137244, Val Loss: 0.431264\n",
      "Early stopping at epoch 6, Train Loss: 1.137049, Val Loss: 0.431179\n",
      "    Final - Train Loss: 1.137049, Val Loss: 0.431179\n",
      "[Saved] NN4_w5_2016Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.143995, Val Loss: 0.429040\n",
      "    Epoch 5/20, Train Loss: 1.136847, Val Loss: 0.431208\n",
      "Early stopping at epoch 6, Train Loss: 1.136762, Val Loss: 0.431833\n",
      "    Final - Train Loss: 1.136762, Val Loss: 0.431833\n",
      "[Saved] NN5_w5_2016Q1.pth\n",
      "    Training data size: 196920 samples\n",
      "  Quarter 2016Q2: Training models with expanding data\n",
      "    Added 2956 samples from 2016Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.125646, Val Loss: 0.454825\n",
      "    Epoch 5/20, Train Loss: 1.125039, Val Loss: 0.455381\n",
      "Early stopping at epoch 6, Train Loss: 1.125383, Val Loss: 0.455352\n",
      "    Final - Train Loss: 1.125383, Val Loss: 0.455352\n",
      "[Saved] NN1_w5_2016Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.128017, Val Loss: 0.450208\n",
      "    Epoch 5/20, Train Loss: 1.126386, Val Loss: 0.451555\n",
      "Early stopping at epoch 6, Train Loss: 1.125982, Val Loss: 0.451844\n",
      "    Final - Train Loss: 1.125982, Val Loss: 0.451844\n",
      "[Saved] NN2_w5_2016Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.128153, Val Loss: 0.451027\n",
      "    Epoch 5/20, Train Loss: 1.125793, Val Loss: 0.451683\n",
      "Early stopping at epoch 7, Train Loss: 1.125201, Val Loss: 0.452179\n",
      "    Final - Train Loss: 1.125201, Val Loss: 0.452179\n",
      "[Saved] NN3_w5_2016Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.129012, Val Loss: 0.449631\n",
      "    Epoch 5/20, Train Loss: 1.125953, Val Loss: 0.451092\n",
      "Early stopping at epoch 6, Train Loss: 1.125506, Val Loss: 0.451206\n",
      "    Final - Train Loss: 1.125506, Val Loss: 0.451206\n",
      "[Saved] NN4_w5_2016Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.130183, Val Loss: 0.448572\n",
      "    Epoch 5/20, Train Loss: 1.126469, Val Loss: 0.450119\n",
      "Early stopping at epoch 6, Train Loss: 1.125694, Val Loss: 0.450039\n",
      "    Final - Train Loss: 1.125694, Val Loss: 0.450039\n",
      "[Saved] NN5_w5_2016Q2.pth\n",
      "    Training data size: 199876 samples\n",
      "  Quarter 2016Q3: Training models with expanding data\n",
      "    Added 3170 samples from 2016Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.113910, Val Loss: 0.463163\n",
      "    Epoch 5/20, Train Loss: 1.113180, Val Loss: 0.463622\n",
      "    Epoch 10/20, Train Loss: 1.112761, Val Loss: 0.463640\n",
      "Early stopping at epoch 13, Train Loss: 1.113107, Val Loss: 0.462604\n",
      "    Final - Train Loss: 1.113107, Val Loss: 0.462604\n",
      "[Saved] NN1_w5_2016Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.115335, Val Loss: 0.458801\n",
      "    Epoch 5/20, Train Loss: 1.114240, Val Loss: 0.458930\n",
      "Early stopping at epoch 6, Train Loss: 1.113701, Val Loss: 0.459851\n",
      "    Final - Train Loss: 1.113701, Val Loss: 0.459851\n",
      "[Saved] NN2_w5_2016Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.115041, Val Loss: 0.458876\n",
      "    Epoch 5/20, Train Loss: 1.113297, Val Loss: 0.459367\n",
      "Early stopping at epoch 6, Train Loss: 1.113118, Val Loss: 0.459680\n",
      "    Final - Train Loss: 1.113118, Val Loss: 0.459680\n",
      "[Saved] NN3_w5_2016Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.115699, Val Loss: 0.457478\n",
      "    Epoch 5/20, Train Loss: 1.114294, Val Loss: 0.458314\n",
      "Early stopping at epoch 6, Train Loss: 1.113436, Val Loss: 0.458691\n",
      "    Final - Train Loss: 1.113436, Val Loss: 0.458691\n",
      "[Saved] NN4_w5_2016Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.117009, Val Loss: 0.456885\n",
      "    Epoch 5/20, Train Loss: 1.114064, Val Loss: 0.458183\n",
      "Early stopping at epoch 6, Train Loss: 1.113623, Val Loss: 0.458299\n",
      "    Final - Train Loss: 1.113623, Val Loss: 0.458299\n",
      "[Saved] NN5_w5_2016Q3.pth\n",
      "    Training data size: 203046 samples\n",
      "  Quarter 2016Q4: Training models with expanding data\n",
      "    Added 3176 samples from 2016Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.103631, Val Loss: 0.450656\n",
      "    Epoch 5/20, Train Loss: 1.103122, Val Loss: 0.450776\n",
      "Early stopping at epoch 8, Train Loss: 1.102850, Val Loss: 0.451086\n",
      "    Final - Train Loss: 1.102850, Val Loss: 0.451086\n",
      "[Saved] NN1_w5_2016Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.104968, Val Loss: 0.448470\n",
      "    Epoch 5/20, Train Loss: 1.104169, Val Loss: 0.448784\n",
      "Early stopping at epoch 7, Train Loss: 1.104088, Val Loss: 0.448989\n",
      "    Final - Train Loss: 1.104088, Val Loss: 0.448989\n",
      "[Saved] NN2_w5_2016Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.104517, Val Loss: 0.448910\n",
      "    Epoch 5/20, Train Loss: 1.103447, Val Loss: 0.449295\n",
      "Early stopping at epoch 6, Train Loss: 1.103500, Val Loss: 0.449339\n",
      "    Final - Train Loss: 1.103500, Val Loss: 0.449339\n",
      "[Saved] NN3_w5_2016Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.105611, Val Loss: 0.447540\n",
      "    Epoch 5/20, Train Loss: 1.103949, Val Loss: 0.448162\n",
      "Early stopping at epoch 6, Train Loss: 1.103215, Val Loss: 0.448729\n",
      "    Final - Train Loss: 1.103215, Val Loss: 0.448729\n",
      "[Saved] NN4_w5_2016Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.106000, Val Loss: 0.447128\n",
      "    Epoch 5/20, Train Loss: 1.103557, Val Loss: 0.448440\n",
      "Early stopping at epoch 6, Train Loss: 1.103474, Val Loss: 0.448714\n",
      "    Final - Train Loss: 1.103474, Val Loss: 0.448714\n",
      "[Saved] NN5_w5_2016Q4.pth\n",
      "    Training data size: 206222 samples\n",
      "  Quarter 2017Q1: Training models with expanding data\n",
      "    Added 3123 samples from 2016Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.091877, Val Loss: 0.455310\n",
      "    Epoch 5/20, Train Loss: 1.092118, Val Loss: 0.455449\n",
      "Early stopping at epoch 7, Train Loss: 1.092246, Val Loss: 0.454908\n",
      "    Final - Train Loss: 1.092246, Val Loss: 0.454908\n",
      "[Saved] NN1_w5_2017Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.093587, Val Loss: 0.452402\n",
      "    Epoch 5/20, Train Loss: 1.092790, Val Loss: 0.452521\n",
      "    Epoch 10/20, Train Loss: 1.092374, Val Loss: 0.452768\n",
      "Early stopping at epoch 11, Train Loss: 1.092447, Val Loss: 0.452478\n",
      "    Final - Train Loss: 1.092447, Val Loss: 0.452478\n",
      "[Saved] NN2_w5_2017Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.093285, Val Loss: 0.452888\n",
      "    Epoch 5/20, Train Loss: 1.092501, Val Loss: 0.452417\n",
      "Early stopping at epoch 7, Train Loss: 1.092159, Val Loss: 0.452941\n",
      "    Final - Train Loss: 1.092159, Val Loss: 0.452941\n",
      "[Saved] NN3_w5_2017Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.093698, Val Loss: 0.452094\n",
      "    Epoch 5/20, Train Loss: 1.092906, Val Loss: 0.451797\n",
      "    Epoch 10/20, Train Loss: 1.092215, Val Loss: 0.452528\n",
      "Early stopping at epoch 10, Train Loss: 1.092215, Val Loss: 0.452528\n",
      "    Final - Train Loss: 1.092215, Val Loss: 0.452528\n",
      "[Saved] NN4_w5_2017Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.093843, Val Loss: 0.451585\n",
      "    Epoch 5/20, Train Loss: 1.092975, Val Loss: 0.451479\n",
      "Early stopping at epoch 7, Train Loss: 1.092548, Val Loss: 0.451624\n",
      "    Final - Train Loss: 1.092548, Val Loss: 0.451624\n",
      "[Saved] NN5_w5_2017Q1.pth\n",
      "    Training data size: 209345 samples\n",
      "  Quarter 2017Q2: Training models with expanding data\n",
      "    Added 3083 samples from 2017Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.081920, Val Loss: 0.449245\n",
      "    Epoch 5/20, Train Loss: 1.081924, Val Loss: 0.449089\n",
      "    Epoch 10/20, Train Loss: 1.081725, Val Loss: 0.448780\n",
      "Early stopping at epoch 13, Train Loss: 1.081810, Val Loss: 0.448829\n",
      "    Final - Train Loss: 1.081810, Val Loss: 0.448829\n",
      "[Saved] NN1_w5_2017Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.082689, Val Loss: 0.447901\n",
      "    Epoch 5/20, Train Loss: 1.081994, Val Loss: 0.448203\n",
      "Early stopping at epoch 8, Train Loss: 1.082143, Val Loss: 0.447962\n",
      "    Final - Train Loss: 1.082143, Val Loss: 0.447962\n",
      "[Saved] NN2_w5_2017Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.082298, Val Loss: 0.447667\n",
      "    Epoch 5/20, Train Loss: 1.081796, Val Loss: 0.447715\n",
      "Early stopping at epoch 7, Train Loss: 1.081566, Val Loss: 0.447861\n",
      "    Final - Train Loss: 1.081566, Val Loss: 0.447861\n",
      "[Saved] NN3_w5_2017Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.081861, Val Loss: 0.446588\n",
      "    Epoch 5/20, Train Loss: 1.081728, Val Loss: 0.447188\n",
      "Early stopping at epoch 6, Train Loss: 1.081874, Val Loss: 0.446929\n",
      "    Final - Train Loss: 1.081874, Val Loss: 0.446929\n",
      "[Saved] NN4_w5_2017Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.082670, Val Loss: 0.445853\n",
      "    Epoch 5/20, Train Loss: 1.081906, Val Loss: 0.446067\n",
      "Early stopping at epoch 6, Train Loss: 1.081347, Val Loss: 0.446073\n",
      "    Final - Train Loss: 1.081347, Val Loss: 0.446073\n",
      "[Saved] NN5_w5_2017Q2.pth\n",
      "    Training data size: 212428 samples\n",
      "  Quarter 2017Q3: Training models with expanding data\n",
      "    Added 3122 samples from 2017Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.070570, Val Loss: 0.450228\n",
      "    Epoch 5/20, Train Loss: 1.070762, Val Loss: 0.449405\n",
      "    Epoch 10/20, Train Loss: 1.070799, Val Loss: 0.449215\n",
      "    Epoch 15/20, Train Loss: 1.070640, Val Loss: 0.449320\n",
      "    Epoch 20/20, Train Loss: 1.070411, Val Loss: 0.449178\n",
      "    Final - Train Loss: 1.070411, Val Loss: 0.449178\n",
      "[Saved] NN1_w5_2017Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.071919, Val Loss: 0.447756\n",
      "    Epoch 5/20, Train Loss: 1.071386, Val Loss: 0.448146\n",
      "Early stopping at epoch 6, Train Loss: 1.071519, Val Loss: 0.448111\n",
      "    Final - Train Loss: 1.071519, Val Loss: 0.448111\n",
      "[Saved] NN2_w5_2017Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.071580, Val Loss: 0.448561\n",
      "    Epoch 5/20, Train Loss: 1.071507, Val Loss: 0.448278\n",
      "Early stopping at epoch 7, Train Loss: 1.071122, Val Loss: 0.448219\n",
      "    Final - Train Loss: 1.071122, Val Loss: 0.448219\n",
      "[Saved] NN3_w5_2017Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.071773, Val Loss: 0.447649\n",
      "    Epoch 5/20, Train Loss: 1.071224, Val Loss: 0.448287\n",
      "Early stopping at epoch 7, Train Loss: 1.070952, Val Loss: 0.448139\n",
      "    Final - Train Loss: 1.070952, Val Loss: 0.448139\n",
      "[Saved] NN4_w5_2017Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.071610, Val Loss: 0.446889\n",
      "    Epoch 5/20, Train Loss: 1.071101, Val Loss: 0.447094\n",
      "Early stopping at epoch 6, Train Loss: 1.071209, Val Loss: 0.447363\n",
      "    Final - Train Loss: 1.071209, Val Loss: 0.447363\n",
      "[Saved] NN5_w5_2017Q3.pth\n",
      "    Training data size: 215550 samples\n",
      "  Quarter 2017Q4: Training models with expanding data\n",
      "    Added 3114 samples from 2017Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.061113, Val Loss: 0.447097\n",
      "    Epoch 5/20, Train Loss: 1.061069, Val Loss: 0.447192\n",
      "    Epoch 10/20, Train Loss: 1.061488, Val Loss: 0.447320\n",
      "Early stopping at epoch 13, Train Loss: 1.061195, Val Loss: 0.447135\n",
      "    Final - Train Loss: 1.061195, Val Loss: 0.447135\n",
      "[Saved] NN1_w5_2017Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.062495, Val Loss: 0.446684\n",
      "    Epoch 5/20, Train Loss: 1.062466, Val Loss: 0.446872\n",
      "Early stopping at epoch 8, Train Loss: 1.062501, Val Loss: 0.446676\n",
      "    Final - Train Loss: 1.062501, Val Loss: 0.446676\n",
      "[Saved] NN2_w5_2017Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.062793, Val Loss: 0.446878\n",
      "    Epoch 5/20, Train Loss: 1.062516, Val Loss: 0.447365\n",
      "Early stopping at epoch 7, Train Loss: 1.062072, Val Loss: 0.446971\n",
      "    Final - Train Loss: 1.062072, Val Loss: 0.446971\n",
      "[Saved] NN3_w5_2017Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.062344, Val Loss: 0.446139\n",
      "    Epoch 5/20, Train Loss: 1.062046, Val Loss: 0.446681\n",
      "Early stopping at epoch 7, Train Loss: 1.061616, Val Loss: 0.446546\n",
      "    Final - Train Loss: 1.061616, Val Loss: 0.446546\n",
      "[Saved] NN4_w5_2017Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.062872, Val Loss: 0.445773\n",
      "    Epoch 5/20, Train Loss: 1.062182, Val Loss: 0.445519\n",
      "Early stopping at epoch 8, Train Loss: 1.061294, Val Loss: 0.445938\n",
      "    Final - Train Loss: 1.061294, Val Loss: 0.445938\n",
      "[Saved] NN5_w5_2017Q4.pth\n",
      "    Training data size: 218664 samples\n",
      "  Quarter 2018Q1: Training models with expanding data\n",
      "    Added 3115 samples from 2017Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.051662, Val Loss: 0.452952\n",
      "    Epoch 5/20, Train Loss: 1.051811, Val Loss: 0.453287\n",
      "Early stopping at epoch 9, Train Loss: 1.051902, Val Loss: 0.452861\n",
      "    Final - Train Loss: 1.051902, Val Loss: 0.452861\n",
      "[Saved] NN1_w5_2018Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.053032, Val Loss: 0.452711\n",
      "    Epoch 5/20, Train Loss: 1.052812, Val Loss: 0.452921\n",
      "Early stopping at epoch 8, Train Loss: 1.052378, Val Loss: 0.453331\n",
      "    Final - Train Loss: 1.052378, Val Loss: 0.453331\n",
      "[Saved] NN2_w5_2018Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.053138, Val Loss: 0.452610\n",
      "    Epoch 5/20, Train Loss: 1.052689, Val Loss: 0.453625\n",
      "    Epoch 10/20, Train Loss: 1.051871, Val Loss: 0.453521\n",
      "Early stopping at epoch 14, Train Loss: 1.052138, Val Loss: 0.453464\n",
      "    Final - Train Loss: 1.052138, Val Loss: 0.453464\n",
      "[Saved] NN3_w5_2018Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.052537, Val Loss: 0.452652\n",
      "    Epoch 5/20, Train Loss: 1.052542, Val Loss: 0.452542\n",
      "Early stopping at epoch 8, Train Loss: 1.052183, Val Loss: 0.452876\n",
      "    Final - Train Loss: 1.052183, Val Loss: 0.452876\n",
      "[Saved] NN4_w5_2018Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.052643, Val Loss: 0.451758\n",
      "    Epoch 5/20, Train Loss: 1.051328, Val Loss: 0.452060\n",
      "Early stopping at epoch 8, Train Loss: 1.051425, Val Loss: 0.452118\n",
      "    Final - Train Loss: 1.051425, Val Loss: 0.452118\n",
      "[Saved] NN5_w5_2018Q1.pth\n",
      "    Training data size: 221779 samples\n",
      "  Quarter 2018Q2: Training models with expanding data\n",
      "    Added 2996 samples from 2018Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.042391, Val Loss: 0.476380\n",
      "    Epoch 5/20, Train Loss: 1.042468, Val Loss: 0.476484\n",
      "Early stopping at epoch 6, Train Loss: 1.042036, Val Loss: 0.476643\n",
      "    Final - Train Loss: 1.042036, Val Loss: 0.476643\n",
      "[Saved] NN1_w5_2018Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.043213, Val Loss: 0.475675\n",
      "    Epoch 5/20, Train Loss: 1.043270, Val Loss: 0.475954\n",
      "    Epoch 10/20, Train Loss: 1.043156, Val Loss: 0.475710\n",
      "Early stopping at epoch 11, Train Loss: 1.043277, Val Loss: 0.475587\n",
      "    Final - Train Loss: 1.043277, Val Loss: 0.475587\n",
      "[Saved] NN2_w5_2018Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.042705, Val Loss: 0.475709\n",
      "    Epoch 5/20, Train Loss: 1.042769, Val Loss: 0.475857\n",
      "Early stopping at epoch 7, Train Loss: 1.042337, Val Loss: 0.476251\n",
      "    Final - Train Loss: 1.042337, Val Loss: 0.476251\n",
      "[Saved] NN3_w5_2018Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.042136, Val Loss: 0.476545\n",
      "    Epoch 5/20, Train Loss: 1.042599, Val Loss: 0.475383\n",
      "    Epoch 10/20, Train Loss: 1.042247, Val Loss: 0.475706\n",
      "Early stopping at epoch 10, Train Loss: 1.042247, Val Loss: 0.475706\n",
      "    Final - Train Loss: 1.042247, Val Loss: 0.475706\n",
      "[Saved] NN4_w5_2018Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.042701, Val Loss: 0.474450\n",
      "    Epoch 5/20, Train Loss: 1.041958, Val Loss: 0.474950\n",
      "    Epoch 10/20, Train Loss: 1.040872, Val Loss: 0.473822\n",
      "    Epoch 15/20, Train Loss: 1.040270, Val Loss: 0.473572\n",
      "    Epoch 20/20, Train Loss: 1.038634, Val Loss: 0.474468\n",
      "Early stopping at epoch 20, Train Loss: 1.038634, Val Loss: 0.474468\n",
      "    Final - Train Loss: 1.038634, Val Loss: 0.474468\n",
      "[Saved] NN5_w5_2018Q2.pth\n",
      "    Training data size: 224775 samples\n",
      "  Quarter 2018Q3: Training models with expanding data\n",
      "    Added 3160 samples from 2018Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.033348, Val Loss: 0.481492\n",
      "    Epoch 5/20, Train Loss: 1.033636, Val Loss: 0.482214\n",
      "Early stopping at epoch 6, Train Loss: 1.033369, Val Loss: 0.481675\n",
      "    Final - Train Loss: 1.033369, Val Loss: 0.481675\n",
      "[Saved] NN1_w5_2018Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.034371, Val Loss: 0.480939\n",
      "    Epoch 5/20, Train Loss: 1.033830, Val Loss: 0.481484\n",
      "Early stopping at epoch 6, Train Loss: 1.034563, Val Loss: 0.481177\n",
      "    Final - Train Loss: 1.034563, Val Loss: 0.481177\n",
      "[Saved] NN2_w5_2018Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.033849, Val Loss: 0.481152\n",
      "    Epoch 5/20, Train Loss: 1.033909, Val Loss: 0.480285\n",
      "    Epoch 10/20, Train Loss: 1.033272, Val Loss: 0.481244\n",
      "Early stopping at epoch 10, Train Loss: 1.033272, Val Loss: 0.481244\n",
      "    Final - Train Loss: 1.033272, Val Loss: 0.481244\n",
      "[Saved] NN3_w5_2018Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.033136, Val Loss: 0.481008\n",
      "    Epoch 5/20, Train Loss: 1.033099, Val Loss: 0.480952\n",
      "    Epoch 10/20, Train Loss: 1.032648, Val Loss: 0.481152\n",
      "Early stopping at epoch 11, Train Loss: 1.033027, Val Loss: 0.481027\n",
      "    Final - Train Loss: 1.033027, Val Loss: 0.481027\n",
      "[Saved] NN4_w5_2018Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.030967, Val Loss: 0.479474\n",
      "    Epoch 5/20, Train Loss: 1.031344, Val Loss: 0.479054\n",
      "    Epoch 10/20, Train Loss: 1.029848, Val Loss: 0.479022\n",
      "    Epoch 15/20, Train Loss: 1.029738, Val Loss: 0.478900\n",
      "Early stopping at epoch 16, Train Loss: 1.029350, Val Loss: 0.478884\n",
      "    Final - Train Loss: 1.029350, Val Loss: 0.478884\n",
      "[Saved] NN5_w5_2018Q3.pth\n",
      "    Training data size: 227935 samples\n",
      "  Quarter 2018Q4: Training models with expanding data\n",
      "    Added 3125 samples from 2018Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.024785, Val Loss: 0.478027\n",
      "    Epoch 5/20, Train Loss: 1.024508, Val Loss: 0.478381\n",
      "Early stopping at epoch 6, Train Loss: 1.025063, Val Loss: 0.478722\n",
      "    Final - Train Loss: 1.025063, Val Loss: 0.478722\n",
      "[Saved] NN1_w5_2018Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.025356, Val Loss: 0.478663\n",
      "    Epoch 5/20, Train Loss: 1.025291, Val Loss: 0.478857\n",
      "Early stopping at epoch 7, Train Loss: 1.024794, Val Loss: 0.478999\n",
      "    Final - Train Loss: 1.024794, Val Loss: 0.478999\n",
      "[Saved] NN2_w5_2018Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.024536, Val Loss: 0.479429\n",
      "    Epoch 5/20, Train Loss: 1.024433, Val Loss: 0.479868\n",
      "Early stopping at epoch 7, Train Loss: 1.025096, Val Loss: 0.479372\n",
      "    Final - Train Loss: 1.025096, Val Loss: 0.479372\n",
      "[Saved] NN3_w5_2018Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.023860, Val Loss: 0.478901\n",
      "    Epoch 5/20, Train Loss: 1.024087, Val Loss: 0.479471\n",
      "Early stopping at epoch 7, Train Loss: 1.024132, Val Loss: 0.479096\n",
      "    Final - Train Loss: 1.024132, Val Loss: 0.479096\n",
      "[Saved] NN4_w5_2018Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.020312, Val Loss: 0.478098\n",
      "    Epoch 5/20, Train Loss: 1.020181, Val Loss: 0.478435\n",
      "Early stopping at epoch 7, Train Loss: 1.019281, Val Loss: 0.479208\n",
      "    Final - Train Loss: 1.019281, Val Loss: 0.479208\n",
      "[Saved] NN5_w5_2018Q4.pth\n",
      "    Training data size: 231060 samples\n",
      "  Quarter 2019Q1: Training models with expanding data\n",
      "    Added 3045 samples from 2018Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.016787, Val Loss: 0.512717\n",
      "    Epoch 5/20, Train Loss: 1.016507, Val Loss: 0.513232\n",
      "Early stopping at epoch 6, Train Loss: 1.016717, Val Loss: 0.513469\n",
      "    Final - Train Loss: 1.016717, Val Loss: 0.513469\n",
      "[Saved] NN1_w5_2019Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.017864, Val Loss: 0.512395\n",
      "    Epoch 5/20, Train Loss: 1.017503, Val Loss: 0.512406\n",
      "    Epoch 10/20, Train Loss: 1.017484, Val Loss: 0.512310\n",
      "Early stopping at epoch 12, Train Loss: 1.017575, Val Loss: 0.512300\n",
      "    Final - Train Loss: 1.017575, Val Loss: 0.512300\n",
      "[Saved] NN2_w5_2019Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.016156, Val Loss: 0.512220\n",
      "    Epoch 5/20, Train Loss: 1.017217, Val Loss: 0.512425\n",
      "Early stopping at epoch 7, Train Loss: 1.016436, Val Loss: 0.511941\n",
      "    Final - Train Loss: 1.016436, Val Loss: 0.511941\n",
      "[Saved] NN3_w5_2019Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.016823, Val Loss: 0.511852\n",
      "    Epoch 5/20, Train Loss: 1.016775, Val Loss: 0.512100\n",
      "    Epoch 10/20, Train Loss: 1.016068, Val Loss: 0.512025\n",
      "Early stopping at epoch 11, Train Loss: 1.016266, Val Loss: 0.512204\n",
      "    Final - Train Loss: 1.016266, Val Loss: 0.512204\n",
      "[Saved] NN4_w5_2019Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.013244, Val Loss: 0.510041\n",
      "    Epoch 5/20, Train Loss: 1.012860, Val Loss: 0.509551\n",
      "Early stopping at epoch 7, Train Loss: 1.011346, Val Loss: 0.510530\n",
      "    Final - Train Loss: 1.011346, Val Loss: 0.510530\n",
      "[Saved] NN5_w5_2019Q1.pth\n",
      "    Training data size: 234105 samples\n",
      "  Quarter 2019Q2: Training models with expanding data\n",
      "    Added 3022 samples from 2019Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.008457, Val Loss: 0.522303\n",
      "    Epoch 5/20, Train Loss: 1.007883, Val Loss: 0.521631\n",
      "    Epoch 10/20, Train Loss: 1.007803, Val Loss: 0.522644\n",
      "Early stopping at epoch 10, Train Loss: 1.007803, Val Loss: 0.522644\n",
      "    Final - Train Loss: 1.007803, Val Loss: 0.522644\n",
      "[Saved] NN1_w5_2019Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.009050, Val Loss: 0.521460\n",
      "    Epoch 5/20, Train Loss: 1.008643, Val Loss: 0.521691\n",
      "Early stopping at epoch 7, Train Loss: 1.008628, Val Loss: 0.521421\n",
      "    Final - Train Loss: 1.008628, Val Loss: 0.521421\n",
      "[Saved] NN2_w5_2019Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.007801, Val Loss: 0.520961\n",
      "    Epoch 5/20, Train Loss: 1.008414, Val Loss: 0.520465\n",
      "    Epoch 10/20, Train Loss: 1.007822, Val Loss: 0.521017\n",
      "Early stopping at epoch 10, Train Loss: 1.007822, Val Loss: 0.521017\n",
      "    Final - Train Loss: 1.007822, Val Loss: 0.521017\n",
      "[Saved] NN3_w5_2019Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.007614, Val Loss: 0.520985\n",
      "    Epoch 5/20, Train Loss: 1.007034, Val Loss: 0.521479\n",
      "    Epoch 10/20, Train Loss: 1.006897, Val Loss: 0.522501\n",
      "    Epoch 15/20, Train Loss: 1.006884, Val Loss: 0.520818\n",
      "Early stopping at epoch 16, Train Loss: 1.007016, Val Loss: 0.520658\n",
      "    Final - Train Loss: 1.007016, Val Loss: 0.520658\n",
      "[Saved] NN4_w5_2019Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.003472, Val Loss: 0.520123\n",
      "    Epoch 5/20, Train Loss: 1.003092, Val Loss: 0.519674\n",
      "Early stopping at epoch 9, Train Loss: 1.003390, Val Loss: 0.519561\n",
      "    Final - Train Loss: 1.003390, Val Loss: 0.519561\n",
      "[Saved] NN5_w5_2019Q2.pth\n",
      "    Training data size: 237127 samples\n",
      "  Quarter 2019Q3: Training models with expanding data\n",
      "    Added 3120 samples from 2019Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.999319, Val Loss: 0.527817\n",
      "    Epoch 5/20, Train Loss: 0.999661, Val Loss: 0.527768\n",
      "    Epoch 10/20, Train Loss: 0.999172, Val Loss: 0.528070\n",
      "    Epoch 15/20, Train Loss: 0.999716, Val Loss: 0.527514\n",
      "    Epoch 20/20, Train Loss: 0.998906, Val Loss: 0.528181\n",
      "Early stopping at epoch 20, Train Loss: 0.998906, Val Loss: 0.528181\n",
      "    Final - Train Loss: 0.998906, Val Loss: 0.528181\n",
      "[Saved] NN1_w5_2019Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.000326, Val Loss: 0.527182\n",
      "    Epoch 5/20, Train Loss: 0.999797, Val Loss: 0.527304\n",
      "Early stopping at epoch 6, Train Loss: 1.000028, Val Loss: 0.527468\n",
      "    Final - Train Loss: 1.000028, Val Loss: 0.527468\n",
      "[Saved] NN2_w5_2019Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.999470, Val Loss: 0.526463\n",
      "    Epoch 5/20, Train Loss: 0.999089, Val Loss: 0.525987\n",
      "Early stopping at epoch 7, Train Loss: 0.999010, Val Loss: 0.526111\n",
      "    Final - Train Loss: 0.999010, Val Loss: 0.526111\n",
      "[Saved] NN3_w5_2019Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.997698, Val Loss: 0.526432\n",
      "    Epoch 5/20, Train Loss: 0.998357, Val Loss: 0.525872\n",
      "    Epoch 10/20, Train Loss: 0.998036, Val Loss: 0.525541\n",
      "    Epoch 15/20, Train Loss: 0.997181, Val Loss: 0.524953\n",
      "    Epoch 20/20, Train Loss: 0.996986, Val Loss: 0.525255\n",
      "Early stopping at epoch 20, Train Loss: 0.996986, Val Loss: 0.525255\n",
      "    Final - Train Loss: 0.996986, Val Loss: 0.525255\n",
      "[Saved] NN4_w5_2019Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.994871, Val Loss: 0.524617\n",
      "    Epoch 5/20, Train Loss: 0.994242, Val Loss: 0.524180\n",
      "    Epoch 10/20, Train Loss: 0.993583, Val Loss: 0.524360\n",
      "Early stopping at epoch 10, Train Loss: 0.993583, Val Loss: 0.524360\n",
      "    Final - Train Loss: 0.993583, Val Loss: 0.524360\n",
      "[Saved] NN5_w5_2019Q3.pth\n",
      "    Training data size: 240247 samples\n",
      "  Quarter 2019Q4: Training models with expanding data\n",
      "    Added 3167 samples from 2019Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.996422, Val Loss: 0.522505\n",
      "    Epoch 5/20, Train Loss: 0.996395, Val Loss: 0.522831\n",
      "    Epoch 10/20, Train Loss: 0.996988, Val Loss: 0.522279\n",
      "Early stopping at epoch 12, Train Loss: 0.997004, Val Loss: 0.522425\n",
      "    Final - Train Loss: 0.997004, Val Loss: 0.522425\n",
      "[Saved] NN1_w5_2019Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.997616, Val Loss: 0.522770\n",
      "    Epoch 5/20, Train Loss: 0.997287, Val Loss: 0.522391\n",
      "Early stopping at epoch 7, Train Loss: 0.997129, Val Loss: 0.522785\n",
      "    Final - Train Loss: 0.997129, Val Loss: 0.522785\n",
      "[Saved] NN2_w5_2019Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.996696, Val Loss: 0.522464\n",
      "    Epoch 5/20, Train Loss: 0.996294, Val Loss: 0.522744\n",
      "Early stopping at epoch 7, Train Loss: 0.996052, Val Loss: 0.522840\n",
      "    Final - Train Loss: 0.996052, Val Loss: 0.522840\n",
      "[Saved] NN3_w5_2019Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.995186, Val Loss: 0.522440\n",
      "    Epoch 5/20, Train Loss: 0.994206, Val Loss: 0.521609\n",
      "    Epoch 10/20, Train Loss: 0.994084, Val Loss: 0.521552\n",
      "Early stopping at epoch 12, Train Loss: 0.994451, Val Loss: 0.521016\n",
      "    Final - Train Loss: 0.994451, Val Loss: 0.521016\n",
      "[Saved] NN4_w5_2019Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.991258, Val Loss: 0.520598\n",
      "    Epoch 5/20, Train Loss: 0.990970, Val Loss: 0.520335\n",
      "    Epoch 10/20, Train Loss: 0.990702, Val Loss: 0.520391\n",
      "Early stopping at epoch 12, Train Loss: 0.990044, Val Loss: 0.520215\n",
      "    Final - Train Loss: 0.990044, Val Loss: 0.520215\n",
      "[Saved] NN5_w5_2019Q4.pth\n",
      "    Training data size: 243414 samples\n",
      "  Quarter 2020Q1: Training models with expanding data\n",
      "    Added 3179 samples from 2019Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.991615, Val Loss: 0.509768\n",
      "    Epoch 5/20, Train Loss: 0.991898, Val Loss: 0.509729\n",
      "    Epoch 10/20, Train Loss: 0.992080, Val Loss: 0.509914\n",
      "Early stopping at epoch 10, Train Loss: 0.992080, Val Loss: 0.509914\n",
      "    Final - Train Loss: 0.992080, Val Loss: 0.509914\n",
      "[Saved] NN1_w5_2020Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.992356, Val Loss: 0.509504\n",
      "    Epoch 5/20, Train Loss: 0.992616, Val Loss: 0.509761\n",
      "Early stopping at epoch 6, Train Loss: 0.992326, Val Loss: 0.510135\n",
      "    Final - Train Loss: 0.992326, Val Loss: 0.510135\n",
      "[Saved] NN2_w5_2020Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.991125, Val Loss: 0.509551\n",
      "    Epoch 5/20, Train Loss: 0.991422, Val Loss: 0.509272\n",
      "    Epoch 10/20, Train Loss: 0.990635, Val Loss: 0.509573\n",
      "Early stopping at epoch 11, Train Loss: 0.991020, Val Loss: 0.509690\n",
      "    Final - Train Loss: 0.991020, Val Loss: 0.509690\n",
      "[Saved] NN3_w5_2020Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.988975, Val Loss: 0.509020\n",
      "    Epoch 5/20, Train Loss: 0.989318, Val Loss: 0.509051\n",
      "Early stopping at epoch 6, Train Loss: 0.988724, Val Loss: 0.509344\n",
      "    Final - Train Loss: 0.988724, Val Loss: 0.509344\n",
      "[Saved] NN4_w5_2020Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.984810, Val Loss: 0.509182\n",
      "    Epoch 5/20, Train Loss: 0.984488, Val Loss: 0.508959\n",
      "Early stopping at epoch 7, Train Loss: 0.984962, Val Loss: 0.508761\n",
      "    Final - Train Loss: 0.984962, Val Loss: 0.508761\n",
      "[Saved] NN5_w5_2020Q1.pth\n",
      "    Training data size: 246593 samples\n",
      "  Quarter 2020Q2: Training models with expanding data\n",
      "    Added 2595 samples from 2020Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.989371, Val Loss: 0.543137\n",
      "    Epoch 5/20, Train Loss: 0.989680, Val Loss: 0.543196\n",
      "Early stopping at epoch 6, Train Loss: 0.989564, Val Loss: 0.543379\n",
      "    Final - Train Loss: 0.989564, Val Loss: 0.543379\n",
      "[Saved] NN1_w5_2020Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.990034, Val Loss: 0.543074\n",
      "    Epoch 5/20, Train Loss: 0.989948, Val Loss: 0.542753\n",
      "    Epoch 10/20, Train Loss: 0.989945, Val Loss: 0.542505\n",
      "Early stopping at epoch 12, Train Loss: 0.989846, Val Loss: 0.542719\n",
      "    Final - Train Loss: 0.989846, Val Loss: 0.542719\n",
      "[Saved] NN2_w5_2020Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.988384, Val Loss: 0.542886\n",
      "    Epoch 5/20, Train Loss: 0.988744, Val Loss: 0.542401\n",
      "    Epoch 10/20, Train Loss: 0.988697, Val Loss: 0.542241\n",
      "Early stopping at epoch 12, Train Loss: 0.988326, Val Loss: 0.542276\n",
      "    Final - Train Loss: 0.988326, Val Loss: 0.542276\n",
      "[Saved] NN3_w5_2020Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.986896, Val Loss: 0.541608\n",
      "    Epoch 5/20, Train Loss: 0.987089, Val Loss: 0.541699\n",
      "Early stopping at epoch 6, Train Loss: 0.985997, Val Loss: 0.541662\n",
      "    Final - Train Loss: 0.985997, Val Loss: 0.541662\n",
      "[Saved] NN4_w5_2020Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.983567, Val Loss: 0.539124\n",
      "    Epoch 5/20, Train Loss: 0.982389, Val Loss: 0.539577\n",
      "    Epoch 10/20, Train Loss: 0.982068, Val Loss: 0.539977\n",
      "    Epoch 15/20, Train Loss: 0.981575, Val Loss: 0.539376\n",
      "Early stopping at epoch 16, Train Loss: 0.981606, Val Loss: 0.539446\n",
      "    Final - Train Loss: 0.981606, Val Loss: 0.539446\n",
      "[Saved] NN5_w5_2020Q2.pth\n",
      "    Training data size: 249188 samples\n",
      "  Quarter 2020Q3: Training models with expanding data\n",
      "    Added 2839 samples from 2020Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.982783, Val Loss: 0.613630\n",
      "    Epoch 5/20, Train Loss: 0.983017, Val Loss: 0.613405\n",
      "    Epoch 10/20, Train Loss: 0.982458, Val Loss: 0.614087\n",
      "Early stopping at epoch 13, Train Loss: 0.982423, Val Loss: 0.613632\n",
      "    Final - Train Loss: 0.982423, Val Loss: 0.613632\n",
      "[Saved] NN1_w5_2020Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.983346, Val Loss: 0.613343\n",
      "    Epoch 5/20, Train Loss: 0.983513, Val Loss: 0.613131\n",
      "Early stopping at epoch 8, Train Loss: 0.983116, Val Loss: 0.613315\n",
      "    Final - Train Loss: 0.983116, Val Loss: 0.613315\n",
      "[Saved] NN2_w5_2020Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.982489, Val Loss: 0.612631\n",
      "    Epoch 5/20, Train Loss: 0.981441, Val Loss: 0.612197\n",
      "    Epoch 10/20, Train Loss: 0.981279, Val Loss: 0.612625\n",
      "Early stopping at epoch 12, Train Loss: 0.981549, Val Loss: 0.612977\n",
      "    Final - Train Loss: 0.981549, Val Loss: 0.612977\n",
      "[Saved] NN3_w5_2020Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.980748, Val Loss: 0.612100\n",
      "    Epoch 5/20, Train Loss: 0.979448, Val Loss: 0.612488\n",
      "Early stopping at epoch 6, Train Loss: 0.979733, Val Loss: 0.612270\n",
      "    Final - Train Loss: 0.979733, Val Loss: 0.612270\n",
      "[Saved] NN4_w5_2020Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.975412, Val Loss: 0.611013\n",
      "    Epoch 5/20, Train Loss: 0.973968, Val Loss: 0.611300\n",
      "Early stopping at epoch 6, Train Loss: 0.974363, Val Loss: 0.611039\n",
      "    Final - Train Loss: 0.974363, Val Loss: 0.611039\n",
      "[Saved] NN5_w5_2020Q3.pth\n",
      "    Training data size: 252027 samples\n",
      "  Quarter 2020Q4: Training models with expanding data\n",
      "    Added 3151 samples from 2020Q3 to training set\n",
      "    [Re-tuning] NN1 for 2020Q4\n",
      "[Hyper] NN1: best_MSE=0.715474, params={'learning_rate': 0.001363296120391928, 'batch_size': 64, 'dropout_rate': 0.31201103153955173}\n",
      "    [Structure Change] dropout_rate changed: 0.48472181142528964 -> 0.31201103153955173\n",
      "    Rebuilding NN1 model due to structure change\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 1.055386, Val Loss: 0.646647\n",
      "    Epoch 5/50, Train Loss: 0.977664, Val Loss: 0.645720\n",
      "    Epoch 10/50, Train Loss: 0.976328, Val Loss: 0.646297\n",
      "Early stopping at epoch 10, Train Loss: 0.976328, Val Loss: 0.646297\n",
      "    Final - Train Loss: 0.976328, Val Loss: 0.646297\n",
      "[Saved] NN1_w5_2020Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.978248, Val Loss: 0.643978\n",
      "    Epoch 5/20, Train Loss: 0.977651, Val Loss: 0.642319\n",
      "    Epoch 10/20, Train Loss: 0.977082, Val Loss: 0.642805\n",
      "Early stopping at epoch 12, Train Loss: 0.976889, Val Loss: 0.642966\n",
      "    Final - Train Loss: 0.976889, Val Loss: 0.642966\n",
      "[Saved] NN2_w5_2020Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.977332, Val Loss: 0.641759\n",
      "    Epoch 5/20, Train Loss: 0.976147, Val Loss: 0.641832\n",
      "Early stopping at epoch 9, Train Loss: 0.975739, Val Loss: 0.642099\n",
      "    Final - Train Loss: 0.975739, Val Loss: 0.642099\n",
      "[Saved] NN3_w5_2020Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.977841, Val Loss: 0.641019\n",
      "    Epoch 5/20, Train Loss: 0.976451, Val Loss: 0.641506\n",
      "Early stopping at epoch 6, Train Loss: 0.976300, Val Loss: 0.641554\n",
      "    Final - Train Loss: 0.976300, Val Loss: 0.641554\n",
      "[Saved] NN4_w5_2020Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.977412, Val Loss: 0.641064\n",
      "    Epoch 5/20, Train Loss: 0.975796, Val Loss: 0.641586\n",
      "Early stopping at epoch 7, Train Loss: 0.975389, Val Loss: 0.640819\n",
      "    Final - Train Loss: 0.975389, Val Loss: 0.640819\n",
      "[Saved] NN5_w5_2020Q4.pth\n",
      "    Training data size: 255178 samples\n",
      "  Quarter 2021Q1: Training models with expanding data\n",
      "    Added 3114 samples from 2020Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.969088, Val Loss: 0.679485\n",
      "    Epoch 5/20, Train Loss: 0.969013, Val Loss: 0.679243\n",
      "    Epoch 10/20, Train Loss: 0.968873, Val Loss: 0.680590\n",
      "Early stopping at epoch 14, Train Loss: 0.968485, Val Loss: 0.679850\n",
      "    Final - Train Loss: 0.968485, Val Loss: 0.679850\n",
      "[Saved] NN1_w5_2021Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.969853, Val Loss: 0.676420\n",
      "    Epoch 5/20, Train Loss: 0.969447, Val Loss: 0.676733\n",
      "Early stopping at epoch 8, Train Loss: 0.969367, Val Loss: 0.676611\n",
      "    Final - Train Loss: 0.969367, Val Loss: 0.676611\n",
      "[Saved] NN2_w5_2021Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.969040, Val Loss: 0.676267\n",
      "    Epoch 5/20, Train Loss: 0.968725, Val Loss: 0.676788\n",
      "Early stopping at epoch 8, Train Loss: 0.968030, Val Loss: 0.676993\n",
      "    Final - Train Loss: 0.968030, Val Loss: 0.676993\n",
      "[Saved] NN3_w5_2021Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.969912, Val Loss: 0.677110\n",
      "    Epoch 5/20, Train Loss: 0.969331, Val Loss: 0.677394\n",
      "Early stopping at epoch 8, Train Loss: 0.968381, Val Loss: 0.677228\n",
      "    Final - Train Loss: 0.968381, Val Loss: 0.677228\n",
      "[Saved] NN4_w5_2021Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.969370, Val Loss: 0.675086\n",
      "    Epoch 5/20, Train Loss: 0.967544, Val Loss: 0.676083\n",
      "Early stopping at epoch 8, Train Loss: 0.967187, Val Loss: 0.677034\n",
      "    Final - Train Loss: 0.967187, Val Loss: 0.677034\n",
      "[Saved] NN5_w5_2021Q1.pth\n",
      "    Training data size: 258292 samples\n",
      "  Quarter 2021Q2: Training models with expanding data\n",
      "    Added 2989 samples from 2021Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.963175, Val Loss: 0.705328\n",
      "    Epoch 5/20, Train Loss: 0.963080, Val Loss: 0.705128\n",
      "    Epoch 10/20, Train Loss: 0.963462, Val Loss: 0.704843\n",
      "    Epoch 15/20, Train Loss: 0.962893, Val Loss: 0.704958\n",
      "Early stopping at epoch 16, Train Loss: 0.963219, Val Loss: 0.704807\n",
      "    Final - Train Loss: 0.963219, Val Loss: 0.704807\n",
      "[Saved] NN1_w5_2021Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.963946, Val Loss: 0.703069\n",
      "    Epoch 5/20, Train Loss: 0.964084, Val Loss: 0.703128\n",
      "Early stopping at epoch 9, Train Loss: 0.963918, Val Loss: 0.703038\n",
      "    Final - Train Loss: 0.963918, Val Loss: 0.703038\n",
      "[Saved] NN2_w5_2021Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.962570, Val Loss: 0.702666\n",
      "    Epoch 5/20, Train Loss: 0.962440, Val Loss: 0.703077\n",
      "Early stopping at epoch 6, Train Loss: 0.961422, Val Loss: 0.702815\n",
      "    Final - Train Loss: 0.961422, Val Loss: 0.702815\n",
      "[Saved] NN3_w5_2021Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.963419, Val Loss: 0.702651\n",
      "    Epoch 5/20, Train Loss: 0.963060, Val Loss: 0.702838\n",
      "    Epoch 10/20, Train Loss: 0.962440, Val Loss: 0.703108\n",
      "Early stopping at epoch 13, Train Loss: 0.961663, Val Loss: 0.702552\n",
      "    Final - Train Loss: 0.961663, Val Loss: 0.702552\n",
      "[Saved] NN4_w5_2021Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.962316, Val Loss: 0.702715\n",
      "    Epoch 5/20, Train Loss: 0.961148, Val Loss: 0.702343\n",
      "    Epoch 10/20, Train Loss: 0.959985, Val Loss: 0.702817\n",
      "Early stopping at epoch 10, Train Loss: 0.959985, Val Loss: 0.702817\n",
      "    Final - Train Loss: 0.959985, Val Loss: 0.702817\n",
      "[Saved] NN5_w5_2021Q2.pth\n",
      "    Training data size: 261281 samples\n",
      "  Quarter 2021Q3: Training models with expanding data\n",
      "    Added 3132 samples from 2021Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.955812, Val Loss: 0.715542\n",
      "    Epoch 5/20, Train Loss: 0.955905, Val Loss: 0.715316\n",
      "    Epoch 10/20, Train Loss: 0.956043, Val Loss: 0.715136\n",
      "Early stopping at epoch 11, Train Loss: 0.955800, Val Loss: 0.715093\n",
      "    Final - Train Loss: 0.955800, Val Loss: 0.715093\n",
      "[Saved] NN1_w5_2021Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.956578, Val Loss: 0.711459\n",
      "    Epoch 5/20, Train Loss: 0.956477, Val Loss: 0.711460\n",
      "    Epoch 10/20, Train Loss: 0.955710, Val Loss: 0.711822\n",
      "Early stopping at epoch 13, Train Loss: 0.956176, Val Loss: 0.711876\n",
      "    Final - Train Loss: 0.956176, Val Loss: 0.711876\n",
      "[Saved] NN2_w5_2021Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.955286, Val Loss: 0.710681\n",
      "    Epoch 5/20, Train Loss: 0.954931, Val Loss: 0.710918\n",
      "Early stopping at epoch 6, Train Loss: 0.954747, Val Loss: 0.710739\n",
      "    Final - Train Loss: 0.954747, Val Loss: 0.710739\n",
      "[Saved] NN3_w5_2021Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.954831, Val Loss: 0.710899\n",
      "    Epoch 5/20, Train Loss: 0.954036, Val Loss: 0.710667\n",
      "    Epoch 10/20, Train Loss: 0.953119, Val Loss: 0.711461\n",
      "Early stopping at epoch 10, Train Loss: 0.953119, Val Loss: 0.711461\n",
      "    Final - Train Loss: 0.953119, Val Loss: 0.711461\n",
      "[Saved] NN4_w5_2021Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.953766, Val Loss: 0.710800\n",
      "    Epoch 5/20, Train Loss: 0.952477, Val Loss: 0.711411\n",
      "Early stopping at epoch 6, Train Loss: 0.951768, Val Loss: 0.710962\n",
      "    Final - Train Loss: 0.951768, Val Loss: 0.710962\n",
      "[Saved] NN5_w5_2021Q3.pth\n",
      "    Training data size: 264413 samples\n",
      "  Quarter 2021Q4: Training models with expanding data\n",
      "    Added 3173 samples from 2021Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.948156, Val Loss: 0.728352\n",
      "    Epoch 5/20, Train Loss: 0.948002, Val Loss: 0.728933\n",
      "Early stopping at epoch 6, Train Loss: 0.948281, Val Loss: 0.729179\n",
      "    Final - Train Loss: 0.948281, Val Loss: 0.729179\n",
      "[Saved] NN1_w5_2021Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.948612, Val Loss: 0.724970\n",
      "    Epoch 5/20, Train Loss: 0.949066, Val Loss: 0.724890\n",
      "Early stopping at epoch 7, Train Loss: 0.948032, Val Loss: 0.725524\n",
      "    Final - Train Loss: 0.948032, Val Loss: 0.725524\n",
      "[Saved] NN2_w5_2021Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.948296, Val Loss: 0.723962\n",
      "    Epoch 5/20, Train Loss: 0.947361, Val Loss: 0.723920\n",
      "    Epoch 10/20, Train Loss: 0.946327, Val Loss: 0.723759\n",
      "    Epoch 15/20, Train Loss: 0.945920, Val Loss: 0.723562\n",
      "Early stopping at epoch 19, Train Loss: 0.945292, Val Loss: 0.723622\n",
      "    Final - Train Loss: 0.945292, Val Loss: 0.723622\n",
      "[Saved] NN3_w5_2021Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.946923, Val Loss: 0.723928\n",
      "    Epoch 5/20, Train Loss: 0.945872, Val Loss: 0.724267\n",
      "    Epoch 10/20, Train Loss: 0.944933, Val Loss: 0.724785\n",
      "Early stopping at epoch 14, Train Loss: 0.944493, Val Loss: 0.724259\n",
      "    Final - Train Loss: 0.944493, Val Loss: 0.724259\n",
      "[Saved] NN4_w5_2021Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.945829, Val Loss: 0.724370\n",
      "    Epoch 5/20, Train Loss: 0.944814, Val Loss: 0.724586\n",
      "Early stopping at epoch 7, Train Loss: 0.944728, Val Loss: 0.724360\n",
      "    Final - Train Loss: 0.944728, Val Loss: 0.724360\n",
      "[Saved] NN5_w5_2021Q4.pth\n",
      "    Training data size: 267586 samples\n",
      "  Quarter 2022Q1: Training models with expanding data\n",
      "    Added 3153 samples from 2021Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.942119, Val Loss: 0.737600\n",
      "    Epoch 5/20, Train Loss: 0.942082, Val Loss: 0.738480\n",
      "    Epoch 10/20, Train Loss: 0.942106, Val Loss: 0.737569\n",
      "Early stopping at epoch 12, Train Loss: 0.942118, Val Loss: 0.737678\n",
      "    Final - Train Loss: 0.942118, Val Loss: 0.737678\n",
      "[Saved] NN1_w5_2022Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.942294, Val Loss: 0.735770\n",
      "    Epoch 5/20, Train Loss: 0.942045, Val Loss: 0.736058\n",
      "Early stopping at epoch 7, Train Loss: 0.942017, Val Loss: 0.735816\n",
      "    Final - Train Loss: 0.942017, Val Loss: 0.735816\n",
      "[Saved] NN2_w5_2022Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.939641, Val Loss: 0.735159\n",
      "    Epoch 5/20, Train Loss: 0.939067, Val Loss: 0.734943\n",
      "    Epoch 10/20, Train Loss: 0.938378, Val Loss: 0.735348\n",
      "Early stopping at epoch 11, Train Loss: 0.938398, Val Loss: 0.735192\n",
      "    Final - Train Loss: 0.938398, Val Loss: 0.735192\n",
      "[Saved] NN3_w5_2022Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.938910, Val Loss: 0.735655\n",
      "    Epoch 5/20, Train Loss: 0.938382, Val Loss: 0.735031\n",
      "    Epoch 10/20, Train Loss: 0.938507, Val Loss: 0.734845\n",
      "    Epoch 15/20, Train Loss: 0.937273, Val Loss: 0.735005\n",
      "Early stopping at epoch 15, Train Loss: 0.937273, Val Loss: 0.735005\n",
      "    Final - Train Loss: 0.937273, Val Loss: 0.735005\n",
      "[Saved] NN4_w5_2022Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.938929, Val Loss: 0.735411\n",
      "    Epoch 5/20, Train Loss: 0.939159, Val Loss: 0.735487\n",
      "Early stopping at epoch 9, Train Loss: 0.937786, Val Loss: 0.735434\n",
      "    Final - Train Loss: 0.937786, Val Loss: 0.735434\n",
      "[Saved] NN5_w5_2022Q1.pth\n",
      "    Training data size: 270739 samples\n",
      "  Quarter 2022Q2: Training models with expanding data\n",
      "    Added 3029 samples from 2022Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.936140, Val Loss: 0.772122\n",
      "    Epoch 5/20, Train Loss: 0.936680, Val Loss: 0.772043\n",
      "    Epoch 10/20, Train Loss: 0.936355, Val Loss: 0.772133\n",
      "Early stopping at epoch 10, Train Loss: 0.936355, Val Loss: 0.772133\n",
      "    Final - Train Loss: 0.936355, Val Loss: 0.772133\n",
      "[Saved] NN1_w5_2022Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.936700, Val Loss: 0.768921\n",
      "    Epoch 5/20, Train Loss: 0.937011, Val Loss: 0.769036\n",
      "Early stopping at epoch 7, Train Loss: 0.936509, Val Loss: 0.769457\n",
      "    Final - Train Loss: 0.936509, Val Loss: 0.769457\n",
      "[Saved] NN2_w5_2022Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.933470, Val Loss: 0.768135\n",
      "    Epoch 5/20, Train Loss: 0.933911, Val Loss: 0.768544\n",
      "Early stopping at epoch 6, Train Loss: 0.933157, Val Loss: 0.768784\n",
      "    Final - Train Loss: 0.933157, Val Loss: 0.768784\n",
      "[Saved] NN3_w5_2022Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.932119, Val Loss: 0.768307\n",
      "    Epoch 5/20, Train Loss: 0.932091, Val Loss: 0.768751\n",
      "Early stopping at epoch 6, Train Loss: 0.931862, Val Loss: 0.768616\n",
      "    Final - Train Loss: 0.931862, Val Loss: 0.768616\n",
      "[Saved] NN4_w5_2022Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.931588, Val Loss: 0.768734\n",
      "    Epoch 5/20, Train Loss: 0.931954, Val Loss: 0.769200\n",
      "Early stopping at epoch 9, Train Loss: 0.931387, Val Loss: 0.769068\n",
      "    Final - Train Loss: 0.931387, Val Loss: 0.769068\n",
      "[Saved] NN5_w5_2022Q2.pth\n",
      "    Training data size: 273768 samples\n",
      "  Quarter 2022Q3: Training models with expanding data\n",
      "    Added 2969 samples from 2022Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.931938, Val Loss: 0.812453\n",
      "    Epoch 5/20, Train Loss: 0.931916, Val Loss: 0.811906\n",
      "    Epoch 10/20, Train Loss: 0.932323, Val Loss: 0.812400\n",
      "Early stopping at epoch 12, Train Loss: 0.932074, Val Loss: 0.812274\n",
      "    Final - Train Loss: 0.932074, Val Loss: 0.812274\n",
      "[Saved] NN1_w5_2022Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.932348, Val Loss: 0.809076\n",
      "    Epoch 5/20, Train Loss: 0.932179, Val Loss: 0.809063\n",
      "    Epoch 10/20, Train Loss: 0.932134, Val Loss: 0.809155\n",
      "Early stopping at epoch 14, Train Loss: 0.932359, Val Loss: 0.809141\n",
      "    Final - Train Loss: 0.932359, Val Loss: 0.809141\n",
      "[Saved] NN2_w5_2022Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.928422, Val Loss: 0.808655\n",
      "    Epoch 5/20, Train Loss: 0.929148, Val Loss: 0.809098\n",
      "Early stopping at epoch 6, Train Loss: 0.928875, Val Loss: 0.809046\n",
      "    Final - Train Loss: 0.928875, Val Loss: 0.809046\n",
      "[Saved] NN3_w5_2022Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.927759, Val Loss: 0.809073\n",
      "    Epoch 5/20, Train Loss: 0.927718, Val Loss: 0.808806\n",
      "Early stopping at epoch 9, Train Loss: 0.927576, Val Loss: 0.808682\n",
      "    Final - Train Loss: 0.927576, Val Loss: 0.808682\n",
      "[Saved] NN4_w5_2022Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.927727, Val Loss: 0.809487\n",
      "    Epoch 5/20, Train Loss: 0.927943, Val Loss: 0.809024\n",
      "Early stopping at epoch 8, Train Loss: 0.926807, Val Loss: 0.808805\n",
      "    Final - Train Loss: 0.926807, Val Loss: 0.808805\n",
      "[Saved] NN5_w5_2022Q3.pth\n",
      "    Training data size: 276737 samples\n",
      "  Quarter 2022Q4: Training models with expanding data\n",
      "    Added 3152 samples from 2022Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.928660, Val Loss: 0.828224\n",
      "    Epoch 5/20, Train Loss: 0.929000, Val Loss: 0.827953\n",
      "    Epoch 10/20, Train Loss: 0.929123, Val Loss: 0.828199\n",
      "    Epoch 15/20, Train Loss: 0.928671, Val Loss: 0.827897\n",
      "Early stopping at epoch 17, Train Loss: 0.928955, Val Loss: 0.828048\n",
      "    Final - Train Loss: 0.928955, Val Loss: 0.828048\n",
      "[Saved] NN1_w5_2022Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.928675, Val Loss: 0.825303\n",
      "    Epoch 5/20, Train Loss: 0.928938, Val Loss: 0.825756\n",
      "Early stopping at epoch 6, Train Loss: 0.929004, Val Loss: 0.825497\n",
      "    Final - Train Loss: 0.929004, Val Loss: 0.825497\n",
      "[Saved] NN2_w5_2022Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.925811, Val Loss: 0.825986\n",
      "    Epoch 5/20, Train Loss: 0.925496, Val Loss: 0.825830\n",
      "Early stopping at epoch 7, Train Loss: 0.925538, Val Loss: 0.826198\n",
      "    Final - Train Loss: 0.925538, Val Loss: 0.826198\n",
      "[Saved] NN3_w5_2022Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.924179, Val Loss: 0.825949\n",
      "    Epoch 5/20, Train Loss: 0.924707, Val Loss: 0.826449\n",
      "    Epoch 10/20, Train Loss: 0.924104, Val Loss: 0.825200\n",
      "    Epoch 15/20, Train Loss: 0.924427, Val Loss: 0.826681\n",
      "Early stopping at epoch 15, Train Loss: 0.924427, Val Loss: 0.826681\n",
      "    Final - Train Loss: 0.924427, Val Loss: 0.826681\n",
      "[Saved] NN4_w5_2022Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.924763, Val Loss: 0.825359\n",
      "    Epoch 5/20, Train Loss: 0.923140, Val Loss: 0.825381\n",
      "Early stopping at epoch 8, Train Loss: 0.922525, Val Loss: 0.825399\n",
      "    Final - Train Loss: 0.922525, Val Loss: 0.825399\n",
      "[Saved] NN5_w5_2022Q4.pth\n",
      "    Training data size: 279889 samples\n",
      "  Quarter 2023Q1: Training models with expanding data\n",
      "    Added 3070 samples from 2022Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.924951, Val Loss: 0.848040\n",
      "    Epoch 5/20, Train Loss: 0.924881, Val Loss: 0.848358\n",
      "Early stopping at epoch 7, Train Loss: 0.925330, Val Loss: 0.847867\n",
      "    Final - Train Loss: 0.925330, Val Loss: 0.847867\n",
      "[Saved] NN1_w5_2023Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.924756, Val Loss: 0.845197\n",
      "    Epoch 5/20, Train Loss: 0.925300, Val Loss: 0.845333\n",
      "Early stopping at epoch 6, Train Loss: 0.925333, Val Loss: 0.845393\n",
      "    Final - Train Loss: 0.925333, Val Loss: 0.845393\n",
      "[Saved] NN2_w5_2023Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.921879, Val Loss: 0.844797\n",
      "    Epoch 5/20, Train Loss: 0.921363, Val Loss: 0.844866\n",
      "Early stopping at epoch 8, Train Loss: 0.922058, Val Loss: 0.844804\n",
      "    Final - Train Loss: 0.922058, Val Loss: 0.844804\n",
      "[Saved] NN3_w5_2023Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.921034, Val Loss: 0.844895\n",
      "    Epoch 5/20, Train Loss: 0.920280, Val Loss: 0.845016\n",
      "Early stopping at epoch 6, Train Loss: 0.919411, Val Loss: 0.844938\n",
      "    Final - Train Loss: 0.919411, Val Loss: 0.844938\n",
      "[Saved] NN4_w5_2023Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.919761, Val Loss: 0.844864\n",
      "    Epoch 5/20, Train Loss: 0.918873, Val Loss: 0.845330\n",
      "Early stopping at epoch 6, Train Loss: 0.919650, Val Loss: 0.845708\n",
      "    Final - Train Loss: 0.919650, Val Loss: 0.845708\n",
      "[Saved] NN5_w5_2023Q1.pth\n",
      "    Training data size: 282959 samples\n",
      "  Quarter 2023Q2: Training models with expanding data\n",
      "    Added 3064 samples from 2023Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.919399, Val Loss: 0.863125\n",
      "    Epoch 5/20, Train Loss: 0.919707, Val Loss: 0.862295\n",
      "    Epoch 10/20, Train Loss: 0.919716, Val Loss: 0.863208\n",
      "Early stopping at epoch 12, Train Loss: 0.919749, Val Loss: 0.862201\n",
      "    Final - Train Loss: 0.919749, Val Loss: 0.862201\n",
      "[Saved] NN1_w5_2023Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.919819, Val Loss: 0.859022\n",
      "    Epoch 5/20, Train Loss: 0.919171, Val Loss: 0.859856\n",
      "Early stopping at epoch 6, Train Loss: 0.919545, Val Loss: 0.859821\n",
      "    Final - Train Loss: 0.919545, Val Loss: 0.859821\n",
      "[Saved] NN2_w5_2023Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.916177, Val Loss: 0.858862\n",
      "    Epoch 5/20, Train Loss: 0.915862, Val Loss: 0.858977\n",
      "Early stopping at epoch 7, Train Loss: 0.916119, Val Loss: 0.858677\n",
      "    Final - Train Loss: 0.916119, Val Loss: 0.858677\n",
      "[Saved] NN3_w5_2023Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.914137, Val Loss: 0.858532\n",
      "    Epoch 5/20, Train Loss: 0.915942, Val Loss: 0.859028\n",
      "Early stopping at epoch 6, Train Loss: 0.914109, Val Loss: 0.858735\n",
      "    Final - Train Loss: 0.914109, Val Loss: 0.858735\n",
      "[Saved] NN4_w5_2023Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.914475, Val Loss: 0.859155\n",
      "    Epoch 5/20, Train Loss: 0.913575, Val Loss: 0.859406\n",
      "Early stopping at epoch 9, Train Loss: 0.913276, Val Loss: 0.859528\n",
      "    Final - Train Loss: 0.913276, Val Loss: 0.859528\n",
      "[Saved] NN5_w5_2023Q2.pth\n",
      "    Training data size: 286023 samples\n",
      "  Quarter 2023Q3: Training models with expanding data\n",
      "    Added 3069 samples from 2023Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.914447, Val Loss: 0.871429\n",
      "    Epoch 5/20, Train Loss: 0.914362, Val Loss: 0.871903\n",
      "    Epoch 10/20, Train Loss: 0.914441, Val Loss: 0.870962\n",
      "    Epoch 15/20, Train Loss: 0.913994, Val Loss: 0.871764\n",
      "Early stopping at epoch 16, Train Loss: 0.914174, Val Loss: 0.871602\n",
      "    Final - Train Loss: 0.914174, Val Loss: 0.871602\n",
      "[Saved] NN1_w5_2023Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.914134, Val Loss: 0.868476\n",
      "    Epoch 5/20, Train Loss: 0.913719, Val Loss: 0.868460\n",
      "    Epoch 10/20, Train Loss: 0.913966, Val Loss: 0.868009\n",
      "Early stopping at epoch 12, Train Loss: 0.914342, Val Loss: 0.868185\n",
      "    Final - Train Loss: 0.914342, Val Loss: 0.868185\n",
      "[Saved] NN2_w5_2023Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.910993, Val Loss: 0.867187\n",
      "    Epoch 5/20, Train Loss: 0.909942, Val Loss: 0.867031\n",
      "    Epoch 10/20, Train Loss: 0.910526, Val Loss: 0.867312\n",
      "Early stopping at epoch 11, Train Loss: 0.910437, Val Loss: 0.867405\n",
      "    Final - Train Loss: 0.910437, Val Loss: 0.867405\n",
      "[Saved] NN3_w5_2023Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.908931, Val Loss: 0.867555\n",
      "    Epoch 5/20, Train Loss: 0.909192, Val Loss: 0.868293\n",
      "    Epoch 10/20, Train Loss: 0.908474, Val Loss: 0.867523\n",
      "Early stopping at epoch 12, Train Loss: 0.909032, Val Loss: 0.867661\n",
      "    Final - Train Loss: 0.909032, Val Loss: 0.867661\n",
      "[Saved] NN4_w5_2023Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.908730, Val Loss: 0.867766\n",
      "    Epoch 5/20, Train Loss: 0.907569, Val Loss: 0.867302\n",
      "    Epoch 10/20, Train Loss: 0.907942, Val Loss: 0.868069\n",
      "Early stopping at epoch 10, Train Loss: 0.907942, Val Loss: 0.868069\n",
      "    Final - Train Loss: 0.907942, Val Loss: 0.868069\n",
      "[Saved] NN5_w5_2023Q3.pth\n",
      "    Training data size: 289092 samples\n",
      "  Quarter 2023Q4: Training models with expanding data\n",
      "    Added 3121 samples from 2023Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.914083, Val Loss: 0.847400\n",
      "    Epoch 5/20, Train Loss: 0.913912, Val Loss: 0.847677\n",
      "Early stopping at epoch 6, Train Loss: 0.914135, Val Loss: 0.847839\n",
      "    Final - Train Loss: 0.914135, Val Loss: 0.847839\n",
      "[Saved] NN1_w5_2023Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.914329, Val Loss: 0.846790\n",
      "    Epoch 5/20, Train Loss: 0.913974, Val Loss: 0.846641\n",
      "    Epoch 10/20, Train Loss: 0.913740, Val Loss: 0.846134\n",
      "    Epoch 15/20, Train Loss: 0.914126, Val Loss: 0.846383\n",
      "Early stopping at epoch 16, Train Loss: 0.913664, Val Loss: 0.846415\n",
      "    Final - Train Loss: 0.913664, Val Loss: 0.846415\n",
      "[Saved] NN2_w5_2023Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.909426, Val Loss: 0.846011\n",
      "    Epoch 5/20, Train Loss: 0.910148, Val Loss: 0.845292\n",
      "    Epoch 10/20, Train Loss: 0.909988, Val Loss: 0.846374\n",
      "Early stopping at epoch 12, Train Loss: 0.909702, Val Loss: 0.845805\n",
      "    Final - Train Loss: 0.909702, Val Loss: 0.845805\n",
      "[Saved] NN3_w5_2023Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.908526, Val Loss: 0.845574\n",
      "    Epoch 5/20, Train Loss: 0.908572, Val Loss: 0.846497\n",
      "Early stopping at epoch 6, Train Loss: 0.908362, Val Loss: 0.846248\n",
      "    Final - Train Loss: 0.908362, Val Loss: 0.846248\n",
      "[Saved] NN4_w5_2023Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.907851, Val Loss: 0.845696\n",
      "    Epoch 5/20, Train Loss: 0.908305, Val Loss: 0.845677\n",
      "    Epoch 10/20, Train Loss: 0.906394, Val Loss: 0.846148\n",
      "Early stopping at epoch 12, Train Loss: 0.907125, Val Loss: 0.845462\n",
      "    Final - Train Loss: 0.907125, Val Loss: 0.845462\n",
      "[Saved] NN5_w5_2023Q4.pth\n",
      "    Training data size: 292213 samples\n",
      "  Quarter 2024Q1: Training models with expanding data\n",
      "    Added 3113 samples from 2023Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.910998, Val Loss: 0.851887\n",
      "    Epoch 5/20, Train Loss: 0.911197, Val Loss: 0.851727\n",
      "Early stopping at epoch 8, Train Loss: 0.911363, Val Loss: 0.851818\n",
      "    Final - Train Loss: 0.911363, Val Loss: 0.851818\n",
      "[Saved] NN1_w5_2024Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.911034, Val Loss: 0.850017\n",
      "    Epoch 5/20, Train Loss: 0.911154, Val Loss: 0.850338\n",
      "Early stopping at epoch 9, Train Loss: 0.911039, Val Loss: 0.850157\n",
      "    Final - Train Loss: 0.911039, Val Loss: 0.850157\n",
      "[Saved] NN2_w5_2024Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.907439, Val Loss: 0.848472\n",
      "    Epoch 5/20, Train Loss: 0.906804, Val Loss: 0.848490\n",
      "    Epoch 10/20, Train Loss: 0.906785, Val Loss: 0.848570\n",
      "Early stopping at epoch 13, Train Loss: 0.907117, Val Loss: 0.849043\n",
      "    Final - Train Loss: 0.907117, Val Loss: 0.849043\n",
      "[Saved] NN3_w5_2024Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.906729, Val Loss: 0.848708\n",
      "    Epoch 5/20, Train Loss: 0.906319, Val Loss: 0.848788\n",
      "Early stopping at epoch 6, Train Loss: 0.905870, Val Loss: 0.849347\n",
      "    Final - Train Loss: 0.905870, Val Loss: 0.849347\n",
      "[Saved] NN4_w5_2024Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.903825, Val Loss: 0.848943\n",
      "    Epoch 5/20, Train Loss: 0.903283, Val Loss: 0.849331\n",
      "Early stopping at epoch 7, Train Loss: 0.903933, Val Loss: 0.848945\n",
      "    Final - Train Loss: 0.903933, Val Loss: 0.848945\n",
      "[Saved] NN5_w5_2024Q1.pth\n",
      "    Training data size: 295326 samples\n",
      "  Quarter 2024Q2: Training models with expanding data\n",
      "    Added 3026 samples from 2024Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.906309, Val Loss: 0.854155\n",
      "    Epoch 5/20, Train Loss: 0.906439, Val Loss: 0.853760\n",
      "Early stopping at epoch 9, Train Loss: 0.906853, Val Loss: 0.853896\n",
      "    Final - Train Loss: 0.906853, Val Loss: 0.853896\n",
      "[Saved] NN1_w5_2024Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.905745, Val Loss: 0.853071\n",
      "    Epoch 5/20, Train Loss: 0.905958, Val Loss: 0.853160\n",
      "Early stopping at epoch 8, Train Loss: 0.905543, Val Loss: 0.854948\n",
      "    Final - Train Loss: 0.905543, Val Loss: 0.854948\n",
      "[Saved] NN2_w5_2024Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.902295, Val Loss: 0.851993\n",
      "    Epoch 5/20, Train Loss: 0.902650, Val Loss: 0.851757\n",
      "Early stopping at epoch 9, Train Loss: 0.902004, Val Loss: 0.851601\n",
      "    Final - Train Loss: 0.902004, Val Loss: 0.851601\n",
      "[Saved] NN3_w5_2024Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.900708, Val Loss: 0.852883\n",
      "    Epoch 5/20, Train Loss: 0.901010, Val Loss: 0.850938\n",
      "Early stopping at epoch 9, Train Loss: 0.899654, Val Loss: 0.852092\n",
      "    Final - Train Loss: 0.899654, Val Loss: 0.852092\n",
      "[Saved] NN4_w5_2024Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.900091, Val Loss: 0.852223\n",
      "    Epoch 5/20, Train Loss: 0.899495, Val Loss: 0.852040\n",
      "    Epoch 10/20, Train Loss: 0.898233, Val Loss: 0.851240\n",
      "    Epoch 15/20, Train Loss: 0.898609, Val Loss: 0.851536\n",
      "Early stopping at epoch 15, Train Loss: 0.898609, Val Loss: 0.851536\n",
      "    Final - Train Loss: 0.898609, Val Loss: 0.851536\n",
      "[Saved] NN5_w5_2024Q2.pth\n",
      "    Training data size: 298352 samples\n",
      "  Quarter 2024Q3: Training models with expanding data\n",
      "    Added 3112 samples from 2024Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.902144, Val Loss: 0.855319\n",
      "    Epoch 5/20, Train Loss: 0.901347, Val Loss: 0.855018\n",
      "    Epoch 10/20, Train Loss: 0.901869, Val Loss: 0.855608\n",
      "Early stopping at epoch 10, Train Loss: 0.901869, Val Loss: 0.855608\n",
      "    Final - Train Loss: 0.901869, Val Loss: 0.855608\n",
      "[Saved] NN1_w5_2024Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.901108, Val Loss: 0.853961\n",
      "    Epoch 5/20, Train Loss: 0.901610, Val Loss: 0.853989\n",
      "Early stopping at epoch 9, Train Loss: 0.901177, Val Loss: 0.854150\n",
      "    Final - Train Loss: 0.901177, Val Loss: 0.854150\n",
      "[Saved] NN2_w5_2024Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.897236, Val Loss: 0.853006\n",
      "    Epoch 5/20, Train Loss: 0.898598, Val Loss: 0.853333\n",
      "Early stopping at epoch 7, Train Loss: 0.897729, Val Loss: 0.853182\n",
      "    Final - Train Loss: 0.897729, Val Loss: 0.853182\n",
      "[Saved] NN3_w5_2024Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.896634, Val Loss: 0.853568\n",
      "    Epoch 5/20, Train Loss: 0.896342, Val Loss: 0.852503\n",
      "    Epoch 10/20, Train Loss: 0.896827, Val Loss: 0.852852\n",
      "Early stopping at epoch 10, Train Loss: 0.896827, Val Loss: 0.852852\n",
      "    Final - Train Loss: 0.896827, Val Loss: 0.852852\n",
      "[Saved] NN4_w5_2024Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.893845, Val Loss: 0.853049\n",
      "    Epoch 5/20, Train Loss: 0.893601, Val Loss: 0.854563\n",
      "Early stopping at epoch 6, Train Loss: 0.892577, Val Loss: 0.853201\n",
      "    Final - Train Loss: 0.892577, Val Loss: 0.853201\n",
      "[Saved] NN5_w5_2024Q3.pth\n",
      "    Training data size: 301464 samples\n",
      "Processing window size: 21\n",
      "  Quarter 2015Q4: Training models with expanding data\n",
      "    [Re-tuning] NN1 for 2015Q4\n",
      "[Hyper] NN1: best_MSE=0.750133, params={'learning_rate': 0.0006117951693436601, 'batch_size': 64, 'dropout_rate': 0.4069508665834103}\n",
      "    [Structure Change] dropout_rate changed: 0.1 -> 0.4069508665834103\n",
      "    Rebuilding NN1 model due to structure change\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 1.319135, Val Loss: 0.437675\n",
      "    Epoch 5/50, Train Loss: 1.137057, Val Loss: 0.436273\n",
      "Early stopping at epoch 7, Train Loss: 1.135871, Val Loss: 0.436530\n",
      "    Final - Train Loss: 1.135871, Val Loss: 0.436530\n",
      "[Saved] NN1_w21_2015Q4.pth\n",
      "    Creating new NN2 model\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 1.298814, Val Loss: 0.432788\n",
      "    Epoch 5/50, Train Loss: 1.137170, Val Loss: 0.433018\n",
      "Early stopping at epoch 7, Train Loss: 1.136607, Val Loss: 0.433596\n",
      "    Final - Train Loss: 1.136607, Val Loss: 0.433596\n",
      "[Saved] NN2_w21_2015Q4.pth\n",
      "    Creating new NN3 model\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 1.319444, Val Loss: 0.432091\n",
      "    Epoch 5/50, Train Loss: 1.137661, Val Loss: 0.433493\n",
      "Early stopping at epoch 7, Train Loss: 1.135796, Val Loss: 0.434358\n",
      "    Final - Train Loss: 1.135796, Val Loss: 0.434358\n",
      "[Saved] NN3_w21_2015Q4.pth\n",
      "    Creating new NN4 model\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 1.282065, Val Loss: 0.431178\n",
      "    Epoch 5/50, Train Loss: 1.136148, Val Loss: 0.433106\n",
      "Early stopping at epoch 6, Train Loss: 1.135342, Val Loss: 0.433996\n",
      "    Final - Train Loss: 1.135342, Val Loss: 0.433996\n",
      "[Saved] NN4_w21_2015Q4.pth\n",
      "    Creating new NN5 model\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 1.294330, Val Loss: 0.431706\n",
      "    Epoch 5/50, Train Loss: 1.136065, Val Loss: 0.432691\n",
      "Early stopping at epoch 7, Train Loss: 1.134823, Val Loss: 0.433776\n",
      "    Final - Train Loss: 1.134823, Val Loss: 0.433776\n",
      "[Saved] NN5_w21_2015Q4.pth\n",
      "    Training data size: 196120 samples\n",
      "  Quarter 2016Q1: Training models with expanding data\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.138507, Val Loss: 0.435860\n",
      "    Epoch 5/20, Train Loss: 1.136546, Val Loss: 0.436219\n",
      "Early stopping at epoch 7, Train Loss: 1.135555, Val Loss: 0.436108\n",
      "    Final - Train Loss: 1.135555, Val Loss: 0.436108\n",
      "[Saved] NN1_w21_2016Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.139781, Val Loss: 0.432017\n",
      "    Epoch 5/20, Train Loss: 1.135680, Val Loss: 0.433720\n",
      "Early stopping at epoch 6, Train Loss: 1.135325, Val Loss: 0.433904\n",
      "    Final - Train Loss: 1.135325, Val Loss: 0.433904\n",
      "[Saved] NN2_w21_2016Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.140198, Val Loss: 0.432361\n",
      "    Epoch 5/20, Train Loss: 1.135657, Val Loss: 0.434108\n",
      "Early stopping at epoch 6, Train Loss: 1.134664, Val Loss: 0.434188\n",
      "    Final - Train Loss: 1.134664, Val Loss: 0.434188\n",
      "[Saved] NN3_w21_2016Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.143282, Val Loss: 0.431742\n",
      "    Epoch 5/20, Train Loss: 1.135263, Val Loss: 0.434335\n",
      "Early stopping at epoch 6, Train Loss: 1.134473, Val Loss: 0.434429\n",
      "    Final - Train Loss: 1.134473, Val Loss: 0.434429\n",
      "[Saved] NN4_w21_2016Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.139397, Val Loss: 0.432053\n",
      "    Epoch 5/20, Train Loss: 1.134305, Val Loss: 0.434228\n",
      "Early stopping at epoch 6, Train Loss: 1.134052, Val Loss: 0.433986\n",
      "    Final - Train Loss: 1.134052, Val Loss: 0.433986\n",
      "[Saved] NN5_w21_2016Q1.pth\n",
      "    Training data size: 196120 samples\n",
      "  Quarter 2016Q2: Training models with expanding data\n",
      "    Added 2956 samples from 2016Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.126676, Val Loss: 0.455555\n",
      "    Epoch 5/20, Train Loss: 1.125239, Val Loss: 0.455532\n",
      "    Epoch 10/20, Train Loss: 1.124023, Val Loss: 0.456270\n",
      "Early stopping at epoch 10, Train Loss: 1.124023, Val Loss: 0.456270\n",
      "    Final - Train Loss: 1.124023, Val Loss: 0.456270\n",
      "[Saved] NN1_w21_2016Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.127886, Val Loss: 0.452739\n",
      "    Epoch 5/20, Train Loss: 1.125407, Val Loss: 0.453578\n",
      "Early stopping at epoch 7, Train Loss: 1.124570, Val Loss: 0.453516\n",
      "    Final - Train Loss: 1.124570, Val Loss: 0.453516\n",
      "[Saved] NN2_w21_2016Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.127243, Val Loss: 0.452689\n",
      "    Epoch 5/20, Train Loss: 1.124541, Val Loss: 0.453366\n",
      "Early stopping at epoch 6, Train Loss: 1.123912, Val Loss: 0.454143\n",
      "    Final - Train Loss: 1.123912, Val Loss: 0.454143\n",
      "[Saved] NN3_w21_2016Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.127919, Val Loss: 0.452527\n",
      "    Epoch 5/20, Train Loss: 1.123652, Val Loss: 0.454533\n",
      "Early stopping at epoch 6, Train Loss: 1.123588, Val Loss: 0.454571\n",
      "    Final - Train Loss: 1.123588, Val Loss: 0.454571\n",
      "[Saved] NN4_w21_2016Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.126094, Val Loss: 0.452255\n",
      "    Epoch 5/20, Train Loss: 1.123116, Val Loss: 0.453871\n",
      "Early stopping at epoch 6, Train Loss: 1.122760, Val Loss: 0.454207\n",
      "    Final - Train Loss: 1.122760, Val Loss: 0.454207\n",
      "[Saved] NN5_w21_2016Q2.pth\n",
      "    Training data size: 199076 samples\n",
      "  Quarter 2016Q3: Training models with expanding data\n",
      "    Added 3170 samples from 2016Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.113318, Val Loss: 0.464240\n",
      "    Epoch 5/20, Train Loss: 1.112644, Val Loss: 0.464811\n",
      "Early stopping at epoch 9, Train Loss: 1.111901, Val Loss: 0.465043\n",
      "    Final - Train Loss: 1.111901, Val Loss: 0.465043\n",
      "[Saved] NN1_w21_2016Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.114166, Val Loss: 0.460885\n",
      "    Epoch 5/20, Train Loss: 1.112828, Val Loss: 0.461627\n",
      "Early stopping at epoch 6, Train Loss: 1.112514, Val Loss: 0.462019\n",
      "    Final - Train Loss: 1.112514, Val Loss: 0.462019\n",
      "[Saved] NN2_w21_2016Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.113952, Val Loss: 0.460445\n",
      "    Epoch 5/20, Train Loss: 1.112508, Val Loss: 0.461439\n",
      "Early stopping at epoch 6, Train Loss: 1.112267, Val Loss: 0.462207\n",
      "    Final - Train Loss: 1.112267, Val Loss: 0.462207\n",
      "[Saved] NN3_w21_2016Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.114406, Val Loss: 0.460319\n",
      "    Epoch 5/20, Train Loss: 1.110748, Val Loss: 0.461565\n",
      "Early stopping at epoch 6, Train Loss: 1.111034, Val Loss: 0.462249\n",
      "    Final - Train Loss: 1.111034, Val Loss: 0.462249\n",
      "[Saved] NN4_w21_2016Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.113084, Val Loss: 0.459865\n",
      "    Epoch 5/20, Train Loss: 1.110422, Val Loss: 0.463288\n",
      "Early stopping at epoch 6, Train Loss: 1.109930, Val Loss: 0.462817\n",
      "    Final - Train Loss: 1.109930, Val Loss: 0.462817\n",
      "[Saved] NN5_w21_2016Q3.pth\n",
      "    Training data size: 202246 samples\n",
      "  Quarter 2016Q4: Training models with expanding data\n",
      "    Added 3176 samples from 2016Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.102259, Val Loss: 0.452804\n",
      "    Epoch 5/20, Train Loss: 1.102298, Val Loss: 0.452918\n",
      "Early stopping at epoch 7, Train Loss: 1.101393, Val Loss: 0.453242\n",
      "    Final - Train Loss: 1.101393, Val Loss: 0.453242\n",
      "[Saved] NN1_w21_2016Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.104363, Val Loss: 0.450543\n",
      "    Epoch 5/20, Train Loss: 1.103223, Val Loss: 0.450937\n",
      "Early stopping at epoch 7, Train Loss: 1.102546, Val Loss: 0.450886\n",
      "    Final - Train Loss: 1.102546, Val Loss: 0.450886\n",
      "[Saved] NN2_w21_2016Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.104036, Val Loss: 0.450134\n",
      "    Epoch 5/20, Train Loss: 1.102607, Val Loss: 0.450770\n",
      "Early stopping at epoch 6, Train Loss: 1.102503, Val Loss: 0.452091\n",
      "    Final - Train Loss: 1.102503, Val Loss: 0.452091\n",
      "[Saved] NN3_w21_2016Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.104005, Val Loss: 0.449848\n",
      "    Epoch 5/20, Train Loss: 1.101550, Val Loss: 0.451762\n",
      "Early stopping at epoch 6, Train Loss: 1.101265, Val Loss: 0.451881\n",
      "    Final - Train Loss: 1.101265, Val Loss: 0.451881\n",
      "[Saved] NN4_w21_2016Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.103012, Val Loss: 0.450505\n",
      "    Epoch 5/20, Train Loss: 1.101115, Val Loss: 0.451702\n",
      "    Epoch 10/20, Train Loss: 1.098109, Val Loss: 0.450606\n",
      "Early stopping at epoch 13, Train Loss: 1.095041, Val Loss: 0.449948\n",
      "    Final - Train Loss: 1.095041, Val Loss: 0.449948\n",
      "[Saved] NN5_w21_2016Q4.pth\n",
      "    Training data size: 205422 samples\n",
      "  Quarter 2017Q1: Training models with expanding data\n",
      "    Added 3123 samples from 2016Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.091180, Val Loss: 0.459106\n",
      "    Epoch 5/20, Train Loss: 1.090774, Val Loss: 0.458457\n",
      "    Epoch 10/20, Train Loss: 1.090221, Val Loss: 0.459495\n",
      "Early stopping at epoch 12, Train Loss: 1.089949, Val Loss: 0.459766\n",
      "    Final - Train Loss: 1.089949, Val Loss: 0.459766\n",
      "[Saved] NN1_w21_2017Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.092415, Val Loss: 0.455743\n",
      "    Epoch 5/20, Train Loss: 1.091203, Val Loss: 0.456768\n",
      "Early stopping at epoch 6, Train Loss: 1.091463, Val Loss: 0.456476\n",
      "    Final - Train Loss: 1.091463, Val Loss: 0.456476\n",
      "[Saved] NN2_w21_2017Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.092516, Val Loss: 0.455669\n",
      "    Epoch 5/20, Train Loss: 1.090949, Val Loss: 0.456599\n",
      "Early stopping at epoch 6, Train Loss: 1.091110, Val Loss: 0.456052\n",
      "    Final - Train Loss: 1.091110, Val Loss: 0.456052\n",
      "[Saved] NN3_w21_2017Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.091538, Val Loss: 0.456163\n",
      "    Epoch 5/20, Train Loss: 1.090426, Val Loss: 0.456530\n",
      "Early stopping at epoch 8, Train Loss: 1.089539, Val Loss: 0.456552\n",
      "    Final - Train Loss: 1.089539, Val Loss: 0.456552\n",
      "[Saved] NN4_w21_2017Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.085583, Val Loss: 0.457180\n",
      "    Epoch 5/20, Train Loss: 1.083939, Val Loss: 0.457978\n",
      "Early stopping at epoch 7, Train Loss: 1.083161, Val Loss: 0.457122\n",
      "    Final - Train Loss: 1.083161, Val Loss: 0.457122\n",
      "[Saved] NN5_w21_2017Q1.pth\n",
      "    Training data size: 208545 samples\n",
      "  Quarter 2017Q2: Training models with expanding data\n",
      "    Added 3083 samples from 2017Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.080057, Val Loss: 0.450903\n",
      "    Epoch 5/20, Train Loss: 1.079478, Val Loss: 0.452226\n",
      "Early stopping at epoch 6, Train Loss: 1.078971, Val Loss: 0.452209\n",
      "    Final - Train Loss: 1.078971, Val Loss: 0.452209\n",
      "[Saved] NN1_w21_2017Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.082183, Val Loss: 0.449399\n",
      "    Epoch 5/20, Train Loss: 1.081771, Val Loss: 0.449067\n",
      "    Epoch 10/20, Train Loss: 1.080245, Val Loss: 0.449075\n",
      "Early stopping at epoch 14, Train Loss: 1.079896, Val Loss: 0.449288\n",
      "    Final - Train Loss: 1.079896, Val Loss: 0.449288\n",
      "[Saved] NN2_w21_2017Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.082178, Val Loss: 0.448525\n",
      "    Epoch 5/20, Train Loss: 1.080903, Val Loss: 0.449378\n",
      "Early stopping at epoch 6, Train Loss: 1.080890, Val Loss: 0.449407\n",
      "    Final - Train Loss: 1.080890, Val Loss: 0.449407\n",
      "[Saved] NN3_w21_2017Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.079959, Val Loss: 0.449350\n",
      "    Epoch 5/20, Train Loss: 1.079097, Val Loss: 0.449398\n",
      "Early stopping at epoch 7, Train Loss: 1.078226, Val Loss: 0.448923\n",
      "    Final - Train Loss: 1.078226, Val Loss: 0.448923\n",
      "[Saved] NN4_w21_2017Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.076090, Val Loss: 0.447758\n",
      "    Epoch 5/20, Train Loss: 1.072151, Val Loss: 0.448249\n",
      "Early stopping at epoch 6, Train Loss: 1.072807, Val Loss: 0.448079\n",
      "    Final - Train Loss: 1.072807, Val Loss: 0.448079\n",
      "[Saved] NN5_w21_2017Q2.pth\n",
      "    Training data size: 211628 samples\n",
      "  Quarter 2017Q3: Training models with expanding data\n",
      "    Added 3122 samples from 2017Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.069425, Val Loss: 0.452320\n",
      "    Epoch 5/20, Train Loss: 1.068986, Val Loss: 0.452874\n",
      "Early stopping at epoch 7, Train Loss: 1.069135, Val Loss: 0.452961\n",
      "    Final - Train Loss: 1.069135, Val Loss: 0.452961\n",
      "[Saved] NN1_w21_2017Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.070029, Val Loss: 0.449748\n",
      "    Epoch 5/20, Train Loss: 1.068842, Val Loss: 0.450518\n",
      "    Epoch 10/20, Train Loss: 1.068823, Val Loss: 0.450381\n",
      "Early stopping at epoch 11, Train Loss: 1.069123, Val Loss: 0.450064\n",
      "    Final - Train Loss: 1.069123, Val Loss: 0.450064\n",
      "[Saved] NN2_w21_2017Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.071218, Val Loss: 0.449494\n",
      "    Epoch 5/20, Train Loss: 1.069789, Val Loss: 0.450032\n",
      "Early stopping at epoch 7, Train Loss: 1.068805, Val Loss: 0.450267\n",
      "    Final - Train Loss: 1.068805, Val Loss: 0.450267\n",
      "[Saved] NN3_w21_2017Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.069216, Val Loss: 0.450049\n",
      "    Epoch 5/20, Train Loss: 1.067796, Val Loss: 0.450287\n",
      "Early stopping at epoch 9, Train Loss: 1.065987, Val Loss: 0.450469\n",
      "    Final - Train Loss: 1.065987, Val Loss: 0.450469\n",
      "[Saved] NN4_w21_2017Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.064796, Val Loss: 0.452299\n",
      "    Epoch 5/20, Train Loss: 1.061596, Val Loss: 0.450591\n",
      "    Epoch 10/20, Train Loss: 1.057626, Val Loss: 0.450058\n",
      "Early stopping at epoch 14, Train Loss: 1.055741, Val Loss: 0.449645\n",
      "    Final - Train Loss: 1.055741, Val Loss: 0.449645\n",
      "[Saved] NN5_w21_2017Q3.pth\n",
      "    Training data size: 214750 samples\n",
      "  Quarter 2017Q4: Training models with expanding data\n",
      "    Added 3114 samples from 2017Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.059936, Val Loss: 0.450092\n",
      "    Epoch 5/20, Train Loss: 1.060656, Val Loss: 0.451133\n",
      "Early stopping at epoch 6, Train Loss: 1.060420, Val Loss: 0.450896\n",
      "    Final - Train Loss: 1.060420, Val Loss: 0.450896\n",
      "[Saved] NN1_w21_2017Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.059954, Val Loss: 0.448542\n",
      "    Epoch 5/20, Train Loss: 1.059400, Val Loss: 0.448630\n",
      "Early stopping at epoch 6, Train Loss: 1.059531, Val Loss: 0.448981\n",
      "    Final - Train Loss: 1.059531, Val Loss: 0.448981\n",
      "[Saved] NN2_w21_2017Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.061350, Val Loss: 0.448573\n",
      "    Epoch 5/20, Train Loss: 1.060639, Val Loss: 0.448489\n",
      "Early stopping at epoch 8, Train Loss: 1.060292, Val Loss: 0.449120\n",
      "    Final - Train Loss: 1.060292, Val Loss: 0.449120\n",
      "[Saved] NN3_w21_2017Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.058587, Val Loss: 0.448616\n",
      "    Epoch 5/20, Train Loss: 1.057074, Val Loss: 0.448719\n",
      "Early stopping at epoch 7, Train Loss: 1.056964, Val Loss: 0.447873\n",
      "    Final - Train Loss: 1.056964, Val Loss: 0.447873\n",
      "[Saved] NN4_w21_2017Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.049963, Val Loss: 0.448160\n",
      "    Epoch 5/20, Train Loss: 1.048745, Val Loss: 0.449215\n",
      "Early stopping at epoch 6, Train Loss: 1.046717, Val Loss: 0.448752\n",
      "    Final - Train Loss: 1.046717, Val Loss: 0.448752\n",
      "[Saved] NN5_w21_2017Q4.pth\n",
      "    Training data size: 217864 samples\n",
      "  Quarter 2018Q1: Training models with expanding data\n",
      "    Added 3115 samples from 2017Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.050896, Val Loss: 0.457385\n",
      "    Epoch 5/20, Train Loss: 1.050176, Val Loss: 0.458934\n",
      "Early stopping at epoch 6, Train Loss: 1.049732, Val Loss: 0.459355\n",
      "    Final - Train Loss: 1.049732, Val Loss: 0.459355\n",
      "[Saved] NN1_w21_2018Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.050276, Val Loss: 0.455548\n",
      "    Epoch 5/20, Train Loss: 1.049691, Val Loss: 0.456587\n",
      "Early stopping at epoch 6, Train Loss: 1.050022, Val Loss: 0.457454\n",
      "    Final - Train Loss: 1.050022, Val Loss: 0.457454\n",
      "[Saved] NN2_w21_2018Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.051401, Val Loss: 0.454218\n",
      "    Epoch 5/20, Train Loss: 1.050850, Val Loss: 0.454728\n",
      "Early stopping at epoch 6, Train Loss: 1.050128, Val Loss: 0.454633\n",
      "    Final - Train Loss: 1.050128, Val Loss: 0.454633\n",
      "[Saved] NN3_w21_2018Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.048396, Val Loss: 0.454335\n",
      "    Epoch 5/20, Train Loss: 1.046682, Val Loss: 0.454845\n",
      "    Epoch 10/20, Train Loss: 1.043965, Val Loss: 0.454367\n",
      "Early stopping at epoch 14, Train Loss: 1.042212, Val Loss: 0.454243\n",
      "    Final - Train Loss: 1.042212, Val Loss: 0.454243\n",
      "[Saved] NN4_w21_2018Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.038316, Val Loss: 0.455303\n",
      "    Epoch 5/20, Train Loss: 1.036955, Val Loss: 0.455673\n",
      "    Epoch 10/20, Train Loss: 1.034207, Val Loss: 0.459034\n",
      "Early stopping at epoch 12, Train Loss: 1.035239, Val Loss: 0.457091\n",
      "    Final - Train Loss: 1.035239, Val Loss: 0.457091\n",
      "[Saved] NN5_w21_2018Q1.pth\n",
      "    Training data size: 220979 samples\n",
      "  Quarter 2018Q2: Training models with expanding data\n",
      "    Added 2996 samples from 2018Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.040689, Val Loss: 0.481244\n",
      "    Epoch 5/20, Train Loss: 1.040248, Val Loss: 0.481633\n",
      "Early stopping at epoch 6, Train Loss: 1.040848, Val Loss: 0.482132\n",
      "    Final - Train Loss: 1.040848, Val Loss: 0.482132\n",
      "[Saved] NN1_w21_2018Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.040954, Val Loss: 0.479728\n",
      "    Epoch 5/20, Train Loss: 1.040203, Val Loss: 0.479680\n",
      "Early stopping at epoch 9, Train Loss: 1.039558, Val Loss: 0.479210\n",
      "    Final - Train Loss: 1.039558, Val Loss: 0.479210\n",
      "[Saved] NN2_w21_2018Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.041318, Val Loss: 0.477832\n",
      "    Epoch 5/20, Train Loss: 1.039758, Val Loss: 0.478194\n",
      "Early stopping at epoch 6, Train Loss: 1.040225, Val Loss: 0.478280\n",
      "    Final - Train Loss: 1.040225, Val Loss: 0.478280\n",
      "[Saved] NN3_w21_2018Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.033321, Val Loss: 0.477013\n",
      "    Epoch 5/20, Train Loss: 1.032887, Val Loss: 0.477425\n",
      "Early stopping at epoch 6, Train Loss: 1.031513, Val Loss: 0.477297\n",
      "    Final - Train Loss: 1.031513, Val Loss: 0.477297\n",
      "[Saved] NN4_w21_2018Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.026135, Val Loss: 0.480645\n",
      "    Epoch 5/20, Train Loss: 1.024206, Val Loss: 0.480612\n",
      "Early stopping at epoch 8, Train Loss: 1.019417, Val Loss: 0.482120\n",
      "    Final - Train Loss: 1.019417, Val Loss: 0.482120\n",
      "[Saved] NN5_w21_2018Q2.pth\n",
      "    Training data size: 223975 samples\n",
      "  Quarter 2018Q3: Training models with expanding data\n",
      "    Added 3160 samples from 2018Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.031991, Val Loss: 0.486552\n",
      "    Epoch 5/20, Train Loss: 1.031975, Val Loss: 0.487127\n",
      "Early stopping at epoch 6, Train Loss: 1.031274, Val Loss: 0.486748\n",
      "    Final - Train Loss: 1.031274, Val Loss: 0.486748\n",
      "[Saved] NN1_w21_2018Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.032031, Val Loss: 0.484492\n",
      "    Epoch 5/20, Train Loss: 1.031245, Val Loss: 0.484978\n",
      "Early stopping at epoch 6, Train Loss: 1.030700, Val Loss: 0.485055\n",
      "    Final - Train Loss: 1.030700, Val Loss: 0.485055\n",
      "[Saved] NN2_w21_2018Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.032457, Val Loss: 0.483262\n",
      "    Epoch 5/20, Train Loss: 1.031967, Val Loss: 0.484122\n",
      "Early stopping at epoch 7, Train Loss: 1.030872, Val Loss: 0.483972\n",
      "    Final - Train Loss: 1.030872, Val Loss: 0.483972\n",
      "[Saved] NN3_w21_2018Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.025216, Val Loss: 0.482521\n",
      "    Epoch 5/20, Train Loss: 1.023443, Val Loss: 0.482813\n",
      "Early stopping at epoch 7, Train Loss: 1.022586, Val Loss: 0.482566\n",
      "    Final - Train Loss: 1.022586, Val Loss: 0.482566\n",
      "[Saved] NN4_w21_2018Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.012612, Val Loss: 0.484300\n",
      "    Epoch 5/20, Train Loss: 1.013707, Val Loss: 0.483252\n",
      "    Epoch 10/20, Train Loss: 1.008644, Val Loss: 0.482565\n",
      "    Epoch 15/20, Train Loss: 1.007088, Val Loss: 0.482729\n",
      "Early stopping at epoch 16, Train Loss: 1.007095, Val Loss: 0.486558\n",
      "    Final - Train Loss: 1.007095, Val Loss: 0.486558\n",
      "[Saved] NN5_w21_2018Q3.pth\n",
      "    Training data size: 227135 samples\n",
      "  Quarter 2018Q4: Training models with expanding data\n",
      "    Added 3125 samples from 2018Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.023046, Val Loss: 0.484634\n",
      "    Epoch 5/20, Train Loss: 1.022594, Val Loss: 0.484827\n",
      "Early stopping at epoch 9, Train Loss: 1.023361, Val Loss: 0.484367\n",
      "    Final - Train Loss: 1.023361, Val Loss: 0.484367\n",
      "[Saved] NN1_w21_2018Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.022462, Val Loss: 0.483560\n",
      "    Epoch 5/20, Train Loss: 1.022083, Val Loss: 0.484208\n",
      "Early stopping at epoch 7, Train Loss: 1.022494, Val Loss: 0.483872\n",
      "    Final - Train Loss: 1.022494, Val Loss: 0.483872\n",
      "[Saved] NN2_w21_2018Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.023005, Val Loss: 0.482436\n",
      "    Epoch 5/20, Train Loss: 1.022444, Val Loss: 0.481983\n",
      "    Epoch 10/20, Train Loss: 1.021397, Val Loss: 0.482713\n",
      "Early stopping at epoch 10, Train Loss: 1.021397, Val Loss: 0.482713\n",
      "    Final - Train Loss: 1.021397, Val Loss: 0.482713\n",
      "[Saved] NN3_w21_2018Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.016954, Val Loss: 0.482286\n",
      "    Epoch 5/20, Train Loss: 1.015216, Val Loss: 0.481549\n",
      "    Epoch 10/20, Train Loss: 1.013377, Val Loss: 0.484773\n",
      "Early stopping at epoch 14, Train Loss: 1.010192, Val Loss: 0.482395\n",
      "    Final - Train Loss: 1.010192, Val Loss: 0.482395\n",
      "[Saved] NN4_w21_2018Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.999046, Val Loss: 0.481553\n",
      "    Epoch 5/20, Train Loss: 0.995857, Val Loss: 0.482319\n",
      "Early stopping at epoch 6, Train Loss: 0.996993, Val Loss: 0.482176\n",
      "    Final - Train Loss: 0.996993, Val Loss: 0.482176\n",
      "[Saved] NN5_w21_2018Q4.pth\n",
      "    Training data size: 230260 samples\n",
      "  Quarter 2019Q1: Training models with expanding data\n",
      "    Added 3045 samples from 2018Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.014753, Val Loss: 0.519753\n",
      "    Epoch 5/20, Train Loss: 1.014890, Val Loss: 0.519545\n",
      "    Epoch 10/20, Train Loss: 1.014741, Val Loss: 0.521385\n",
      "Early stopping at epoch 10, Train Loss: 1.014741, Val Loss: 0.521385\n",
      "    Final - Train Loss: 1.014741, Val Loss: 0.521385\n",
      "[Saved] NN1_w21_2019Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.014451, Val Loss: 0.517027\n",
      "    Epoch 5/20, Train Loss: 1.013774, Val Loss: 0.519151\n",
      "Early stopping at epoch 7, Train Loss: 1.013757, Val Loss: 0.518501\n",
      "    Final - Train Loss: 1.013757, Val Loss: 0.518501\n",
      "[Saved] NN2_w21_2019Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.013950, Val Loss: 0.515214\n",
      "    Epoch 5/20, Train Loss: 1.013441, Val Loss: 0.514454\n",
      "    Epoch 10/20, Train Loss: 1.012130, Val Loss: 0.515104\n",
      "Early stopping at epoch 10, Train Loss: 1.012130, Val Loss: 0.515104\n",
      "    Final - Train Loss: 1.012130, Val Loss: 0.515104\n",
      "[Saved] NN3_w21_2019Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.003842, Val Loss: 0.515688\n",
      "    Epoch 5/20, Train Loss: 1.001693, Val Loss: 0.518612\n",
      "Early stopping at epoch 7, Train Loss: 1.002384, Val Loss: 0.516116\n",
      "    Final - Train Loss: 1.002384, Val Loss: 0.516116\n",
      "[Saved] NN4_w21_2019Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.992813, Val Loss: 0.514054\n",
      "    Epoch 5/20, Train Loss: 0.986903, Val Loss: 0.515055\n",
      "Early stopping at epoch 8, Train Loss: 0.987266, Val Loss: 0.514205\n",
      "    Final - Train Loss: 0.987266, Val Loss: 0.514205\n",
      "[Saved] NN5_w21_2019Q1.pth\n",
      "    Training data size: 233305 samples\n",
      "  Quarter 2019Q2: Training models with expanding data\n",
      "    Added 3022 samples from 2019Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.005707, Val Loss: 0.530620\n",
      "    Epoch 5/20, Train Loss: 1.005533, Val Loss: 0.530425\n",
      "Early stopping at epoch 9, Train Loss: 1.005686, Val Loss: 0.531263\n",
      "    Final - Train Loss: 1.005686, Val Loss: 0.531263\n",
      "[Saved] NN1_w21_2019Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.005213, Val Loss: 0.528779\n",
      "    Epoch 5/20, Train Loss: 1.005186, Val Loss: 0.528946\n",
      "Early stopping at epoch 9, Train Loss: 1.005209, Val Loss: 0.529110\n",
      "    Final - Train Loss: 1.005209, Val Loss: 0.529110\n",
      "[Saved] NN2_w21_2019Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.004605, Val Loss: 0.525225\n",
      "    Epoch 5/20, Train Loss: 1.003369, Val Loss: 0.524744\n",
      "    Epoch 10/20, Train Loss: 1.002707, Val Loss: 0.527422\n",
      "Early stopping at epoch 10, Train Loss: 1.002707, Val Loss: 0.527422\n",
      "    Final - Train Loss: 1.002707, Val Loss: 0.527422\n",
      "[Saved] NN3_w21_2019Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.992592, Val Loss: 0.522856\n",
      "    Epoch 5/20, Train Loss: 0.993530, Val Loss: 0.524732\n",
      "Early stopping at epoch 6, Train Loss: 0.991634, Val Loss: 0.524255\n",
      "    Final - Train Loss: 0.991634, Val Loss: 0.524255\n",
      "[Saved] NN4_w21_2019Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.978136, Val Loss: 0.528594\n",
      "    Epoch 5/20, Train Loss: 0.978302, Val Loss: 0.525917\n",
      "    Epoch 10/20, Train Loss: 0.975450, Val Loss: 0.526028\n",
      "Early stopping at epoch 12, Train Loss: 0.976201, Val Loss: 0.526628\n",
      "    Final - Train Loss: 0.976201, Val Loss: 0.526628\n",
      "[Saved] NN5_w21_2019Q2.pth\n",
      "    Training data size: 236327 samples\n",
      "  Quarter 2019Q3: Training models with expanding data\n",
      "    Added 3120 samples from 2019Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.999130, Val Loss: 0.528515\n",
      "    Epoch 5/20, Train Loss: 0.998355, Val Loss: 0.528783\n",
      "Early stopping at epoch 6, Train Loss: 0.998242, Val Loss: 0.529029\n",
      "    Final - Train Loss: 0.998242, Val Loss: 0.529029\n",
      "[Saved] NN1_w21_2019Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.997406, Val Loss: 0.527256\n",
      "    Epoch 5/20, Train Loss: 0.997562, Val Loss: 0.527993\n",
      "    Epoch 10/20, Train Loss: 0.998034, Val Loss: 0.526880\n",
      "    Epoch 15/20, Train Loss: 0.997023, Val Loss: 0.527560\n",
      "Early stopping at epoch 15, Train Loss: 0.997023, Val Loss: 0.527560\n",
      "    Final - Train Loss: 0.997023, Val Loss: 0.527560\n",
      "[Saved] NN2_w21_2019Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.995533, Val Loss: 0.525253\n",
      "    Epoch 5/20, Train Loss: 0.995424, Val Loss: 0.524364\n",
      "    Epoch 10/20, Train Loss: 0.993424, Val Loss: 0.524558\n",
      "Early stopping at epoch 12, Train Loss: 0.992301, Val Loss: 0.525028\n",
      "    Final - Train Loss: 0.992301, Val Loss: 0.525028\n",
      "[Saved] NN3_w21_2019Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.988098, Val Loss: 0.523506\n",
      "    Epoch 5/20, Train Loss: 0.984591, Val Loss: 0.523739\n",
      "Early stopping at epoch 7, Train Loss: 0.984187, Val Loss: 0.524535\n",
      "    Final - Train Loss: 0.984187, Val Loss: 0.524535\n",
      "[Saved] NN4_w21_2019Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.968071, Val Loss: 0.525186\n",
      "    Epoch 5/20, Train Loss: 0.967865, Val Loss: 0.525012\n",
      "    Epoch 10/20, Train Loss: 0.966959, Val Loss: 0.527441\n",
      "Early stopping at epoch 10, Train Loss: 0.966959, Val Loss: 0.527441\n",
      "    Final - Train Loss: 0.966959, Val Loss: 0.527441\n",
      "[Saved] NN5_w21_2019Q3.pth\n",
      "    Training data size: 239447 samples\n",
      "  Quarter 2019Q4: Training models with expanding data\n",
      "    Added 3167 samples from 2019Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.994528, Val Loss: 0.527122\n",
      "    Epoch 5/20, Train Loss: 0.994557, Val Loss: 0.527008\n",
      "Early stopping at epoch 8, Train Loss: 0.994385, Val Loss: 0.526981\n",
      "    Final - Train Loss: 0.994385, Val Loss: 0.526981\n",
      "[Saved] NN1_w21_2019Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.993377, Val Loss: 0.526309\n",
      "    Epoch 5/20, Train Loss: 0.993241, Val Loss: 0.526448\n",
      "Early stopping at epoch 7, Train Loss: 0.993401, Val Loss: 0.526215\n",
      "    Final - Train Loss: 0.993401, Val Loss: 0.526215\n",
      "[Saved] NN2_w21_2019Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.991012, Val Loss: 0.524857\n",
      "    Epoch 5/20, Train Loss: 0.988905, Val Loss: 0.525327\n",
      "Early stopping at epoch 6, Train Loss: 0.988904, Val Loss: 0.525325\n",
      "    Final - Train Loss: 0.988904, Val Loss: 0.525325\n",
      "[Saved] NN3_w21_2019Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.984491, Val Loss: 0.523550\n",
      "    Epoch 5/20, Train Loss: 0.981354, Val Loss: 0.525057\n",
      "Early stopping at epoch 8, Train Loss: 0.979026, Val Loss: 0.523750\n",
      "    Final - Train Loss: 0.979026, Val Loss: 0.523750\n",
      "[Saved] NN4_w21_2019Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.964502, Val Loss: 0.524016\n",
      "    Epoch 5/20, Train Loss: 0.960062, Val Loss: 0.523445\n",
      "    Epoch 10/20, Train Loss: 0.959966, Val Loss: 0.523906\n",
      "Early stopping at epoch 10, Train Loss: 0.959966, Val Loss: 0.523906\n",
      "    Final - Train Loss: 0.959966, Val Loss: 0.523906\n",
      "[Saved] NN5_w21_2019Q4.pth\n",
      "    Training data size: 242614 samples\n",
      "  Quarter 2020Q1: Training models with expanding data\n",
      "    Added 3179 samples from 2019Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.990262, Val Loss: 0.515270\n",
      "    Epoch 5/20, Train Loss: 0.990160, Val Loss: 0.515530\n",
      "Early stopping at epoch 6, Train Loss: 0.989679, Val Loss: 0.516167\n",
      "    Final - Train Loss: 0.989679, Val Loss: 0.516167\n",
      "[Saved] NN1_w21_2020Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.988643, Val Loss: 0.514865\n",
      "    Epoch 5/20, Train Loss: 0.988896, Val Loss: 0.514518\n",
      "    Epoch 10/20, Train Loss: 0.987881, Val Loss: 0.515000\n",
      "Early stopping at epoch 10, Train Loss: 0.987881, Val Loss: 0.515000\n",
      "    Final - Train Loss: 0.987881, Val Loss: 0.515000\n",
      "[Saved] NN2_w21_2020Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.985844, Val Loss: 0.514335\n",
      "    Epoch 5/20, Train Loss: 0.984534, Val Loss: 0.512511\n",
      "    Epoch 10/20, Train Loss: 0.982526, Val Loss: 0.513186\n",
      "Early stopping at epoch 10, Train Loss: 0.982526, Val Loss: 0.513186\n",
      "    Final - Train Loss: 0.982526, Val Loss: 0.513186\n",
      "[Saved] NN3_w21_2020Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.978700, Val Loss: 0.510271\n",
      "    Epoch 5/20, Train Loss: 0.975181, Val Loss: 0.511203\n",
      "Early stopping at epoch 6, Train Loss: 0.973079, Val Loss: 0.510333\n",
      "    Final - Train Loss: 0.973079, Val Loss: 0.510333\n",
      "[Saved] NN4_w21_2020Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.953564, Val Loss: 0.511629\n",
      "    Epoch 5/20, Train Loss: 0.954617, Val Loss: 0.512633\n",
      "Early stopping at epoch 9, Train Loss: 0.954569, Val Loss: 0.513259\n",
      "    Final - Train Loss: 0.954569, Val Loss: 0.513259\n",
      "[Saved] NN5_w21_2020Q1.pth\n",
      "    Training data size: 245793 samples\n",
      "  Quarter 2020Q2: Training models with expanding data\n",
      "    Added 2595 samples from 2020Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.987392, Val Loss: 0.550001\n",
      "    Epoch 5/20, Train Loss: 0.987135, Val Loss: 0.550304\n",
      "    Epoch 10/20, Train Loss: 0.987607, Val Loss: 0.550535\n",
      "Early stopping at epoch 14, Train Loss: 0.986587, Val Loss: 0.549707\n",
      "    Final - Train Loss: 0.986587, Val Loss: 0.549707\n",
      "[Saved] NN1_w21_2020Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.985645, Val Loss: 0.549757\n",
      "    Epoch 5/20, Train Loss: 0.985525, Val Loss: 0.549878\n",
      "    Epoch 10/20, Train Loss: 0.985302, Val Loss: 0.549756\n",
      "    Epoch 15/20, Train Loss: 0.984813, Val Loss: 0.548500\n",
      "Early stopping at epoch 17, Train Loss: 0.984705, Val Loss: 0.549125\n",
      "    Final - Train Loss: 0.984705, Val Loss: 0.549125\n",
      "[Saved] NN2_w21_2020Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.981494, Val Loss: 0.547875\n",
      "    Epoch 5/20, Train Loss: 0.980129, Val Loss: 0.546011\n",
      "    Epoch 10/20, Train Loss: 0.979053, Val Loss: 0.547025\n",
      "    Epoch 15/20, Train Loss: 0.978115, Val Loss: 0.544518\n",
      "Early stopping at epoch 18, Train Loss: 0.976261, Val Loss: 0.546319\n",
      "    Final - Train Loss: 0.976261, Val Loss: 0.546319\n",
      "[Saved] NN3_w21_2020Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.975041, Val Loss: 0.544537\n",
      "    Epoch 5/20, Train Loss: 0.973167, Val Loss: 0.544552\n",
      "Early stopping at epoch 9, Train Loss: 0.970180, Val Loss: 0.544566\n",
      "    Final - Train Loss: 0.970180, Val Loss: 0.544566\n",
      "[Saved] NN4_w21_2020Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.952392, Val Loss: 0.544392\n",
      "    Epoch 5/20, Train Loss: 0.950617, Val Loss: 0.546151\n",
      "Early stopping at epoch 6, Train Loss: 0.949612, Val Loss: 0.546523\n",
      "    Final - Train Loss: 0.949612, Val Loss: 0.546523\n",
      "[Saved] NN5_w21_2020Q2.pth\n",
      "    Training data size: 248388 samples\n",
      "  Quarter 2020Q3: Training models with expanding data\n",
      "    Added 2839 samples from 2020Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.980429, Val Loss: 0.623726\n",
      "    Epoch 5/20, Train Loss: 0.980387, Val Loss: 0.624595\n",
      "Early stopping at epoch 6, Train Loss: 0.980486, Val Loss: 0.624958\n",
      "    Final - Train Loss: 0.980486, Val Loss: 0.624958\n",
      "[Saved] NN1_w21_2020Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.978641, Val Loss: 0.620331\n",
      "    Epoch 5/20, Train Loss: 0.978082, Val Loss: 0.623038\n",
      "Early stopping at epoch 6, Train Loss: 0.977791, Val Loss: 0.621968\n",
      "    Final - Train Loss: 0.977791, Val Loss: 0.621968\n",
      "[Saved] NN2_w21_2020Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.970431, Val Loss: 0.615705\n",
      "    Epoch 5/20, Train Loss: 0.970256, Val Loss: 0.616744\n",
      "Early stopping at epoch 7, Train Loss: 0.970653, Val Loss: 0.616990\n",
      "    Final - Train Loss: 0.970653, Val Loss: 0.616990\n",
      "[Saved] NN3_w21_2020Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.966490, Val Loss: 0.615950\n",
      "    Epoch 5/20, Train Loss: 0.963473, Val Loss: 0.615704\n",
      "    Epoch 10/20, Train Loss: 0.964529, Val Loss: 0.617796\n",
      "Early stopping at epoch 12, Train Loss: 0.963259, Val Loss: 0.619386\n",
      "    Final - Train Loss: 0.963259, Val Loss: 0.619386\n",
      "[Saved] NN4_w21_2020Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.949819, Val Loss: 0.615871\n",
      "    Epoch 5/20, Train Loss: 0.943121, Val Loss: 0.616992\n",
      "Early stopping at epoch 6, Train Loss: 0.943584, Val Loss: 0.616631\n",
      "    Final - Train Loss: 0.943584, Val Loss: 0.616631\n",
      "[Saved] NN5_w21_2020Q3.pth\n",
      "    Training data size: 251227 samples\n",
      "  Quarter 2020Q4: Training models with expanding data\n",
      "    Added 3151 samples from 2020Q3 to training set\n",
      "    [Re-tuning] NN1 for 2020Q4\n",
      "[Hyper] NN1: best_MSE=0.717257, params={'learning_rate': 0.007962341411229049, 'batch_size': 64, 'dropout_rate': 0.316043700121437}\n",
      "    [Structure Change] dropout_rate changed: 0.4069508665834103 -> 0.316043700121437\n",
      "    Rebuilding NN1 model due to structure change\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 0.999189, Val Loss: 0.648769\n",
      "    Epoch 5/50, Train Loss: 0.981616, Val Loss: 0.648610\n",
      "Early stopping at epoch 7, Train Loss: 0.981527, Val Loss: 0.648504\n",
      "    Final - Train Loss: 0.981527, Val Loss: 0.648504\n",
      "[Saved] NN1_w21_2020Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.981850, Val Loss: 0.647626\n",
      "    Epoch 5/20, Train Loss: 0.982496, Val Loss: 0.647623\n",
      "    Epoch 10/20, Train Loss: 0.981661, Val Loss: 0.648095\n",
      "Early stopping at epoch 11, Train Loss: 0.981621, Val Loss: 0.648186\n",
      "    Final - Train Loss: 0.981621, Val Loss: 0.648186\n",
      "[Saved] NN2_w21_2020Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.982370, Val Loss: 0.646811\n",
      "    Epoch 5/20, Train Loss: 0.982385, Val Loss: 0.647716\n",
      "Early stopping at epoch 7, Train Loss: 0.981761, Val Loss: 0.647584\n",
      "    Final - Train Loss: 0.981761, Val Loss: 0.647584\n",
      "[Saved] NN3_w21_2020Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.982824, Val Loss: 0.647571\n",
      "    Epoch 5/20, Train Loss: 0.982169, Val Loss: 0.648878\n",
      "    Epoch 10/20, Train Loss: 0.981622, Val Loss: 0.646646\n",
      "Early stopping at epoch 12, Train Loss: 0.980910, Val Loss: 0.647921\n",
      "    Final - Train Loss: 0.980910, Val Loss: 0.647921\n",
      "[Saved] NN4_w21_2020Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.983089, Val Loss: 0.647231\n",
      "    Epoch 5/20, Train Loss: 0.982761, Val Loss: 0.648630\n",
      "Early stopping at epoch 6, Train Loss: 0.982609, Val Loss: 0.647439\n",
      "    Final - Train Loss: 0.982609, Val Loss: 0.647439\n",
      "[Saved] NN5_w21_2020Q4.pth\n",
      "    Training data size: 254378 samples\n",
      "  Quarter 2021Q1: Training models with expanding data\n",
      "    Added 3114 samples from 2020Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.974161, Val Loss: 0.682699\n",
      "    Epoch 5/20, Train Loss: 0.973915, Val Loss: 0.681321\n",
      "Early stopping at epoch 8, Train Loss: 0.973398, Val Loss: 0.682222\n",
      "    Final - Train Loss: 0.973398, Val Loss: 0.682222\n",
      "[Saved] NN1_w21_2021Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.974250, Val Loss: 0.682290\n",
      "    Epoch 5/20, Train Loss: 0.973925, Val Loss: 0.681554\n",
      "Early stopping at epoch 7, Train Loss: 0.973899, Val Loss: 0.682496\n",
      "    Final - Train Loss: 0.973899, Val Loss: 0.682496\n",
      "[Saved] NN2_w21_2021Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.974885, Val Loss: 0.682385\n",
      "    Epoch 5/20, Train Loss: 0.973840, Val Loss: 0.682577\n",
      "Early stopping at epoch 6, Train Loss: 0.973872, Val Loss: 0.682544\n",
      "    Final - Train Loss: 0.973872, Val Loss: 0.682544\n",
      "[Saved] NN3_w21_2021Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.973507, Val Loss: 0.681461\n",
      "    Epoch 5/20, Train Loss: 0.973281, Val Loss: 0.682559\n",
      "    Epoch 10/20, Train Loss: 0.971656, Val Loss: 0.682881\n",
      "Early stopping at epoch 14, Train Loss: 0.971080, Val Loss: 0.683035\n",
      "    Final - Train Loss: 0.971080, Val Loss: 0.683035\n",
      "[Saved] NN4_w21_2021Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.975802, Val Loss: 0.680638\n",
      "    Epoch 5/20, Train Loss: 0.974874, Val Loss: 0.681423\n",
      "Early stopping at epoch 6, Train Loss: 0.975294, Val Loss: 0.681403\n",
      "    Final - Train Loss: 0.975294, Val Loss: 0.681403\n",
      "[Saved] NN5_w21_2021Q1.pth\n",
      "    Training data size: 257492 samples\n",
      "  Quarter 2021Q2: Training models with expanding data\n",
      "    Added 2989 samples from 2021Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.968521, Val Loss: 0.707453\n",
      "    Epoch 5/20, Train Loss: 0.968195, Val Loss: 0.707206\n",
      "    Epoch 10/20, Train Loss: 0.968592, Val Loss: 0.707110\n",
      "    Epoch 15/20, Train Loss: 0.967920, Val Loss: 0.707472\n",
      "Early stopping at epoch 15, Train Loss: 0.967920, Val Loss: 0.707472\n",
      "    Final - Train Loss: 0.967920, Val Loss: 0.707472\n",
      "[Saved] NN1_w21_2021Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.968558, Val Loss: 0.707369\n",
      "    Epoch 5/20, Train Loss: 0.968401, Val Loss: 0.707591\n",
      "Early stopping at epoch 6, Train Loss: 0.968345, Val Loss: 0.707453\n",
      "    Final - Train Loss: 0.968345, Val Loss: 0.707453\n",
      "[Saved] NN2_w21_2021Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.969662, Val Loss: 0.708362\n",
      "    Epoch 5/20, Train Loss: 0.969197, Val Loss: 0.707622\n",
      "Early stopping at epoch 9, Train Loss: 0.968477, Val Loss: 0.708066\n",
      "    Final - Train Loss: 0.968477, Val Loss: 0.708066\n",
      "[Saved] NN3_w21_2021Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.965661, Val Loss: 0.707472\n",
      "    Epoch 5/20, Train Loss: 0.965761, Val Loss: 0.707796\n",
      "Early stopping at epoch 7, Train Loss: 0.965168, Val Loss: 0.707474\n",
      "    Final - Train Loss: 0.965168, Val Loss: 0.707474\n",
      "[Saved] NN4_w21_2021Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.969857, Val Loss: 0.707801\n",
      "    Epoch 5/20, Train Loss: 0.968495, Val Loss: 0.708236\n",
      "Early stopping at epoch 9, Train Loss: 0.968142, Val Loss: 0.708063\n",
      "    Final - Train Loss: 0.968142, Val Loss: 0.708063\n",
      "[Saved] NN5_w21_2021Q2.pth\n",
      "    Training data size: 260481 samples\n",
      "  Quarter 2021Q3: Training models with expanding data\n",
      "    Added 3132 samples from 2021Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.960955, Val Loss: 0.716654\n",
      "    Epoch 5/20, Train Loss: 0.960881, Val Loss: 0.716698\n",
      "Early stopping at epoch 7, Train Loss: 0.960853, Val Loss: 0.716717\n",
      "    Final - Train Loss: 0.960853, Val Loss: 0.716717\n",
      "[Saved] NN1_w21_2021Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.960813, Val Loss: 0.716367\n",
      "    Epoch 5/20, Train Loss: 0.960576, Val Loss: 0.716588\n",
      "Early stopping at epoch 6, Train Loss: 0.961108, Val Loss: 0.716597\n",
      "    Final - Train Loss: 0.961108, Val Loss: 0.716597\n",
      "[Saved] NN2_w21_2021Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.960875, Val Loss: 0.716721\n",
      "    Epoch 5/20, Train Loss: 0.960748, Val Loss: 0.716752\n",
      "    Epoch 10/20, Train Loss: 0.960079, Val Loss: 0.716625\n",
      "Early stopping at epoch 14, Train Loss: 0.958880, Val Loss: 0.716708\n",
      "    Final - Train Loss: 0.958880, Val Loss: 0.716708\n",
      "[Saved] NN3_w21_2021Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.957898, Val Loss: 0.716356\n",
      "    Epoch 5/20, Train Loss: 0.957554, Val Loss: 0.716517\n",
      "Early stopping at epoch 6, Train Loss: 0.957379, Val Loss: 0.716787\n",
      "    Final - Train Loss: 0.957379, Val Loss: 0.716787\n",
      "[Saved] NN4_w21_2021Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.960873, Val Loss: 0.716737\n",
      "    Epoch 5/20, Train Loss: 0.960908, Val Loss: 0.716764\n",
      "Early stopping at epoch 7, Train Loss: 0.959383, Val Loss: 0.716909\n",
      "    Final - Train Loss: 0.959383, Val Loss: 0.716909\n",
      "[Saved] NN5_w21_2021Q3.pth\n",
      "    Training data size: 263613 samples\n",
      "  Quarter 2021Q4: Training models with expanding data\n",
      "    Added 3173 samples from 2021Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.953184, Val Loss: 0.729551\n",
      "    Epoch 5/20, Train Loss: 0.953291, Val Loss: 0.729450\n",
      "    Epoch 10/20, Train Loss: 0.953912, Val Loss: 0.729421\n",
      "    Epoch 15/20, Train Loss: 0.953558, Val Loss: 0.729514\n",
      "Early stopping at epoch 15, Train Loss: 0.953558, Val Loss: 0.729514\n",
      "    Final - Train Loss: 0.953558, Val Loss: 0.729514\n",
      "[Saved] NN1_w21_2021Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.953115, Val Loss: 0.729762\n",
      "    Epoch 5/20, Train Loss: 0.953133, Val Loss: 0.730091\n",
      "Early stopping at epoch 9, Train Loss: 0.952907, Val Loss: 0.729829\n",
      "    Final - Train Loss: 0.952907, Val Loss: 0.729829\n",
      "[Saved] NN2_w21_2021Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.951657, Val Loss: 0.729835\n",
      "    Epoch 5/20, Train Loss: 0.951196, Val Loss: 0.729861\n",
      "    Epoch 10/20, Train Loss: 0.950826, Val Loss: 0.730120\n",
      "Early stopping at epoch 11, Train Loss: 0.950230, Val Loss: 0.730209\n",
      "    Final - Train Loss: 0.950230, Val Loss: 0.730209\n",
      "[Saved] NN3_w21_2021Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.949999, Val Loss: 0.729771\n",
      "    Epoch 5/20, Train Loss: 0.950432, Val Loss: 0.730063\n",
      "Early stopping at epoch 8, Train Loss: 0.948754, Val Loss: 0.735258\n",
      "    Final - Train Loss: 0.948754, Val Loss: 0.735258\n",
      "[Saved] NN4_w21_2021Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.953548, Val Loss: 0.729607\n",
      "    Epoch 5/20, Train Loss: 0.951817, Val Loss: 0.730274\n",
      "Early stopping at epoch 6, Train Loss: 0.952175, Val Loss: 0.729965\n",
      "    Final - Train Loss: 0.952175, Val Loss: 0.729965\n",
      "[Saved] NN5_w21_2021Q4.pth\n",
      "    Training data size: 266786 samples\n",
      "  Quarter 2022Q1: Training models with expanding data\n",
      "    Added 3153 samples from 2021Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.946973, Val Loss: 0.740462\n",
      "    Epoch 5/20, Train Loss: 0.947195, Val Loss: 0.740440\n",
      "Early stopping at epoch 7, Train Loss: 0.946992, Val Loss: 0.740529\n",
      "    Final - Train Loss: 0.946992, Val Loss: 0.740529\n",
      "[Saved] NN1_w21_2022Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.946981, Val Loss: 0.740917\n",
      "    Epoch 5/20, Train Loss: 0.947390, Val Loss: 0.740817\n",
      "Early stopping at epoch 8, Train Loss: 0.946598, Val Loss: 0.740861\n",
      "    Final - Train Loss: 0.946598, Val Loss: 0.740861\n",
      "[Saved] NN2_w21_2022Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.945010, Val Loss: 0.740824\n",
      "    Epoch 5/20, Train Loss: 0.944517, Val Loss: 0.741053\n",
      "Early stopping at epoch 8, Train Loss: 0.943827, Val Loss: 0.741275\n",
      "    Final - Train Loss: 0.943827, Val Loss: 0.741275\n",
      "[Saved] NN3_w21_2022Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.944704, Val Loss: 0.740703\n",
      "    Epoch 5/20, Train Loss: 0.943831, Val Loss: 0.741473\n",
      "Early stopping at epoch 6, Train Loss: 0.943202, Val Loss: 0.741559\n",
      "    Final - Train Loss: 0.943202, Val Loss: 0.741559\n",
      "[Saved] NN4_w21_2022Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.946206, Val Loss: 0.740961\n",
      "    Epoch 5/20, Train Loss: 0.945698, Val Loss: 0.741235\n",
      "Early stopping at epoch 7, Train Loss: 0.945858, Val Loss: 0.741292\n",
      "    Final - Train Loss: 0.945858, Val Loss: 0.741292\n",
      "[Saved] NN5_w21_2022Q1.pth\n",
      "    Training data size: 269939 samples\n",
      "  Quarter 2022Q2: Training models with expanding data\n",
      "    Added 3029 samples from 2022Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.941584, Val Loss: 0.774037\n",
      "    Epoch 5/20, Train Loss: 0.941648, Val Loss: 0.774100\n",
      "Early stopping at epoch 6, Train Loss: 0.941569, Val Loss: 0.774132\n",
      "    Final - Train Loss: 0.941569, Val Loss: 0.774132\n",
      "[Saved] NN1_w21_2022Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.941038, Val Loss: 0.774139\n",
      "    Epoch 5/20, Train Loss: 0.941226, Val Loss: 0.774447\n",
      "Early stopping at epoch 6, Train Loss: 0.941137, Val Loss: 0.774533\n",
      "    Final - Train Loss: 0.941137, Val Loss: 0.774533\n",
      "[Saved] NN2_w21_2022Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.937508, Val Loss: 0.774260\n",
      "    Epoch 5/20, Train Loss: 0.937721, Val Loss: 0.774223\n",
      "Early stopping at epoch 9, Train Loss: 0.937267, Val Loss: 0.774368\n",
      "    Final - Train Loss: 0.937267, Val Loss: 0.774368\n",
      "[Saved] NN3_w21_2022Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.938033, Val Loss: 0.774042\n",
      "    Epoch 5/20, Train Loss: 0.936990, Val Loss: 0.773812\n",
      "    Epoch 10/20, Train Loss: 0.936617, Val Loss: 0.774811\n",
      "Early stopping at epoch 10, Train Loss: 0.936617, Val Loss: 0.774811\n",
      "    Final - Train Loss: 0.936617, Val Loss: 0.774811\n",
      "[Saved] NN4_w21_2022Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.940309, Val Loss: 0.774193\n",
      "    Epoch 5/20, Train Loss: 0.938453, Val Loss: 0.773972\n",
      "    Epoch 10/20, Train Loss: 0.937276, Val Loss: 0.773912\n",
      "    Epoch 15/20, Train Loss: 0.936640, Val Loss: 0.774385\n",
      "Early stopping at epoch 15, Train Loss: 0.936640, Val Loss: 0.774385\n",
      "    Final - Train Loss: 0.936640, Val Loss: 0.774385\n",
      "[Saved] NN5_w21_2022Q2.pth\n",
      "    Training data size: 272968 samples\n",
      "  Quarter 2022Q3: Training models with expanding data\n",
      "    Added 2969 samples from 2022Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.936460, Val Loss: 0.819246\n",
      "    Epoch 5/20, Train Loss: 0.937253, Val Loss: 0.818610\n",
      "    Epoch 10/20, Train Loss: 0.937249, Val Loss: 0.818845\n",
      "Early stopping at epoch 12, Train Loss: 0.936799, Val Loss: 0.818489\n",
      "    Final - Train Loss: 0.936799, Val Loss: 0.818489\n",
      "[Saved] NN1_w21_2022Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.936248, Val Loss: 0.818678\n",
      "    Epoch 5/20, Train Loss: 0.936921, Val Loss: 0.818478\n",
      "Early stopping at epoch 9, Train Loss: 0.936191, Val Loss: 0.818712\n",
      "    Final - Train Loss: 0.936191, Val Loss: 0.818712\n",
      "[Saved] NN2_w21_2022Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.933412, Val Loss: 0.817317\n",
      "    Epoch 5/20, Train Loss: 0.932894, Val Loss: 0.817644\n",
      "    Epoch 10/20, Train Loss: 0.932991, Val Loss: 0.817979\n",
      "Early stopping at epoch 11, Train Loss: 0.933118, Val Loss: 0.817722\n",
      "    Final - Train Loss: 0.933118, Val Loss: 0.817722\n",
      "[Saved] NN3_w21_2022Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.932094, Val Loss: 0.817596\n",
      "    Epoch 5/20, Train Loss: 0.931096, Val Loss: 0.817099\n",
      "    Epoch 10/20, Train Loss: 0.931089, Val Loss: 0.818019\n",
      "Early stopping at epoch 12, Train Loss: 0.930816, Val Loss: 0.817184\n",
      "    Final - Train Loss: 0.930816, Val Loss: 0.817184\n",
      "[Saved] NN4_w21_2022Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.933093, Val Loss: 0.817381\n",
      "    Epoch 5/20, Train Loss: 0.931649, Val Loss: 0.817967\n",
      "Early stopping at epoch 6, Train Loss: 0.931396, Val Loss: 0.820246\n",
      "    Final - Train Loss: 0.931396, Val Loss: 0.820246\n",
      "[Saved] NN5_w21_2022Q3.pth\n",
      "    Training data size: 275937 samples\n",
      "  Quarter 2022Q4: Training models with expanding data\n",
      "    Added 3152 samples from 2022Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.933875, Val Loss: 0.832834\n",
      "    Epoch 5/20, Train Loss: 0.934003, Val Loss: 0.833214\n",
      "Early stopping at epoch 7, Train Loss: 0.934304, Val Loss: 0.833156\n",
      "    Final - Train Loss: 0.934304, Val Loss: 0.833156\n",
      "[Saved] NN1_w21_2022Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.933242, Val Loss: 0.832531\n",
      "    Epoch 5/20, Train Loss: 0.933210, Val Loss: 0.832628\n",
      "    Epoch 10/20, Train Loss: 0.933419, Val Loss: 0.832498\n",
      "Early stopping at epoch 13, Train Loss: 0.933203, Val Loss: 0.832770\n",
      "    Final - Train Loss: 0.933203, Val Loss: 0.832770\n",
      "[Saved] NN2_w21_2022Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.928909, Val Loss: 0.832100\n",
      "    Epoch 5/20, Train Loss: 0.928797, Val Loss: 0.832896\n",
      "Early stopping at epoch 6, Train Loss: 0.929365, Val Loss: 0.832544\n",
      "    Final - Train Loss: 0.929365, Val Loss: 0.832544\n",
      "[Saved] NN3_w21_2022Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.927740, Val Loss: 0.832021\n",
      "    Epoch 5/20, Train Loss: 0.927888, Val Loss: 0.832284\n",
      "Early stopping at epoch 7, Train Loss: 0.926290, Val Loss: 0.831952\n",
      "    Final - Train Loss: 0.926290, Val Loss: 0.831952\n",
      "[Saved] NN4_w21_2022Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.928167, Val Loss: 0.831165\n",
      "    Epoch 5/20, Train Loss: 0.929372, Val Loss: 0.832259\n",
      "Early stopping at epoch 6, Train Loss: 0.928734, Val Loss: 0.831759\n",
      "    Final - Train Loss: 0.928734, Val Loss: 0.831759\n",
      "[Saved] NN5_w21_2022Q4.pth\n",
      "    Training data size: 279089 samples\n",
      "  Quarter 2023Q1: Training models with expanding data\n",
      "    Added 3070 samples from 2022Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.929828, Val Loss: 0.853862\n",
      "    Epoch 5/20, Train Loss: 0.930150, Val Loss: 0.853194\n",
      "Early stopping at epoch 7, Train Loss: 0.930172, Val Loss: 0.853306\n",
      "    Final - Train Loss: 0.930172, Val Loss: 0.853306\n",
      "[Saved] NN1_w21_2023Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.928848, Val Loss: 0.852522\n",
      "    Epoch 5/20, Train Loss: 0.929179, Val Loss: 0.852585\n",
      "    Epoch 10/20, Train Loss: 0.928828, Val Loss: 0.852664\n",
      "    Epoch 15/20, Train Loss: 0.929328, Val Loss: 0.853668\n",
      "    Epoch 20/20, Train Loss: 0.928937, Val Loss: 0.852026\n",
      "    Final - Train Loss: 0.928937, Val Loss: 0.852026\n",
      "[Saved] NN2_w21_2023Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.923867, Val Loss: 0.852365\n",
      "    Epoch 5/20, Train Loss: 0.924916, Val Loss: 0.852497\n",
      "Early stopping at epoch 6, Train Loss: 0.923884, Val Loss: 0.852949\n",
      "    Final - Train Loss: 0.923884, Val Loss: 0.852949\n",
      "[Saved] NN3_w21_2023Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.924305, Val Loss: 0.852481\n",
      "    Epoch 5/20, Train Loss: 0.924096, Val Loss: 0.852804\n",
      "Early stopping at epoch 9, Train Loss: 0.921543, Val Loss: 0.852648\n",
      "    Final - Train Loss: 0.921543, Val Loss: 0.852648\n",
      "[Saved] NN4_w21_2023Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.925451, Val Loss: 0.852207\n",
      "    Epoch 5/20, Train Loss: 0.923431, Val Loss: 0.852220\n",
      "    Epoch 10/20, Train Loss: 0.922753, Val Loss: 0.852344\n",
      "Early stopping at epoch 11, Train Loss: 0.922304, Val Loss: 0.852242\n",
      "    Final - Train Loss: 0.922304, Val Loss: 0.852242\n",
      "[Saved] NN5_w21_2023Q1.pth\n",
      "    Training data size: 282159 samples\n",
      "  Quarter 2023Q2: Training models with expanding data\n",
      "    Added 3064 samples from 2023Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.924724, Val Loss: 0.867844\n",
      "    Epoch 5/20, Train Loss: 0.924486, Val Loss: 0.868125\n",
      "    Epoch 10/20, Train Loss: 0.924941, Val Loss: 0.868029\n",
      "Early stopping at epoch 12, Train Loss: 0.925118, Val Loss: 0.866915\n",
      "    Final - Train Loss: 0.925118, Val Loss: 0.866915\n",
      "[Saved] NN1_w21_2023Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.922216, Val Loss: 0.865522\n",
      "    Epoch 5/20, Train Loss: 0.923639, Val Loss: 0.866033\n",
      "Early stopping at epoch 7, Train Loss: 0.923136, Val Loss: 0.866189\n",
      "    Final - Train Loss: 0.923136, Val Loss: 0.866189\n",
      "[Saved] NN2_w21_2023Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.920449, Val Loss: 0.865450\n",
      "    Epoch 5/20, Train Loss: 0.918669, Val Loss: 0.867046\n",
      "Early stopping at epoch 6, Train Loss: 0.919362, Val Loss: 0.867390\n",
      "    Final - Train Loss: 0.919362, Val Loss: 0.867390\n",
      "[Saved] NN3_w21_2023Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.918429, Val Loss: 0.866937\n",
      "    Epoch 5/20, Train Loss: 0.916808, Val Loss: 0.867131\n",
      "Early stopping at epoch 6, Train Loss: 0.918297, Val Loss: 0.868542\n",
      "    Final - Train Loss: 0.918297, Val Loss: 0.868542\n",
      "[Saved] NN4_w21_2023Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.917931, Val Loss: 0.865180\n",
      "    Epoch 5/20, Train Loss: 0.917171, Val Loss: 0.865332\n",
      "Early stopping at epoch 6, Train Loss: 0.915499, Val Loss: 0.866096\n",
      "    Final - Train Loss: 0.915499, Val Loss: 0.866096\n",
      "[Saved] NN5_w21_2023Q2.pth\n",
      "    Training data size: 285223 samples\n",
      "  Quarter 2023Q3: Training models with expanding data\n",
      "    Added 3069 samples from 2023Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.918897, Val Loss: 0.875775\n",
      "    Epoch 5/20, Train Loss: 0.919009, Val Loss: 0.875475\n",
      "Early stopping at epoch 9, Train Loss: 0.919256, Val Loss: 0.875271\n",
      "    Final - Train Loss: 0.919256, Val Loss: 0.875271\n",
      "[Saved] NN1_w21_2023Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.917570, Val Loss: 0.874589\n",
      "    Epoch 5/20, Train Loss: 0.917390, Val Loss: 0.874420\n",
      "Early stopping at epoch 8, Train Loss: 0.917778, Val Loss: 0.874290\n",
      "    Final - Train Loss: 0.917778, Val Loss: 0.874290\n",
      "[Saved] NN2_w21_2023Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.914057, Val Loss: 0.874864\n",
      "    Epoch 5/20, Train Loss: 0.912790, Val Loss: 0.877755\n",
      "Early stopping at epoch 7, Train Loss: 0.912669, Val Loss: 0.876049\n",
      "    Final - Train Loss: 0.912669, Val Loss: 0.876049\n",
      "[Saved] NN3_w21_2023Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.911932, Val Loss: 0.877300\n",
      "    Epoch 5/20, Train Loss: 0.913296, Val Loss: 0.875995\n",
      "    Epoch 10/20, Train Loss: 0.911140, Val Loss: 0.876462\n",
      "Early stopping at epoch 10, Train Loss: 0.911140, Val Loss: 0.876462\n",
      "    Final - Train Loss: 0.911140, Val Loss: 0.876462\n",
      "[Saved] NN4_w21_2023Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.913242, Val Loss: 0.874250\n",
      "    Epoch 5/20, Train Loss: 0.911416, Val Loss: 0.874853\n",
      "Early stopping at epoch 6, Train Loss: 0.911159, Val Loss: 0.874489\n",
      "    Final - Train Loss: 0.911159, Val Loss: 0.874489\n",
      "[Saved] NN5_w21_2023Q3.pth\n",
      "    Training data size: 288292 samples\n",
      "  Quarter 2023Q4: Training models with expanding data\n",
      "    Added 3121 samples from 2023Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.919578, Val Loss: 0.857204\n",
      "    Epoch 5/20, Train Loss: 0.919589, Val Loss: 0.861238\n",
      "    Epoch 10/20, Train Loss: 0.919895, Val Loss: 0.857964\n",
      "Early stopping at epoch 14, Train Loss: 0.919869, Val Loss: 0.858712\n",
      "    Final - Train Loss: 0.919869, Val Loss: 0.858712\n",
      "[Saved] NN1_w21_2023Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.917912, Val Loss: 0.856881\n",
      "    Epoch 5/20, Train Loss: 0.918677, Val Loss: 0.854937\n",
      "    Epoch 10/20, Train Loss: 0.918101, Val Loss: 0.853521\n",
      "    Epoch 15/20, Train Loss: 0.918295, Val Loss: 0.856078\n",
      "Early stopping at epoch 15, Train Loss: 0.918295, Val Loss: 0.856078\n",
      "    Final - Train Loss: 0.918295, Val Loss: 0.856078\n",
      "[Saved] NN2_w21_2023Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.916024, Val Loss: 0.854488\n",
      "    Epoch 5/20, Train Loss: 0.915031, Val Loss: 0.854584\n",
      "Early stopping at epoch 6, Train Loss: 0.915070, Val Loss: 0.856822\n",
      "    Final - Train Loss: 0.915070, Val Loss: 0.856822\n",
      "[Saved] NN3_w21_2023Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.912144, Val Loss: 0.856784\n",
      "    Epoch 5/20, Train Loss: 0.912743, Val Loss: 0.855891\n",
      "    Epoch 10/20, Train Loss: 0.913010, Val Loss: 0.858534\n",
      "Early stopping at epoch 10, Train Loss: 0.913010, Val Loss: 0.858534\n",
      "    Final - Train Loss: 0.913010, Val Loss: 0.858534\n",
      "[Saved] NN4_w21_2023Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.911440, Val Loss: 0.852330\n",
      "    Epoch 5/20, Train Loss: 0.912600, Val Loss: 0.852794\n",
      "Early stopping at epoch 6, Train Loss: 0.910724, Val Loss: 0.852748\n",
      "    Final - Train Loss: 0.910724, Val Loss: 0.852748\n",
      "[Saved] NN5_w21_2023Q4.pth\n",
      "    Training data size: 291413 samples\n",
      "  Quarter 2024Q1: Training models with expanding data\n",
      "    Added 3113 samples from 2023Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.916412, Val Loss: 0.856275\n",
      "    Epoch 5/20, Train Loss: 0.916355, Val Loss: 0.856241\n",
      "Early stopping at epoch 7, Train Loss: 0.916375, Val Loss: 0.856374\n",
      "    Final - Train Loss: 0.916375, Val Loss: 0.856374\n",
      "[Saved] NN1_w21_2024Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.913820, Val Loss: 0.854856\n",
      "    Epoch 5/20, Train Loss: 0.914433, Val Loss: 0.855221\n",
      "Early stopping at epoch 8, Train Loss: 0.913607, Val Loss: 0.855322\n",
      "    Final - Train Loss: 0.913607, Val Loss: 0.855322\n",
      "[Saved] NN2_w21_2024Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.910562, Val Loss: 0.854888\n",
      "    Epoch 5/20, Train Loss: 0.912164, Val Loss: 0.856512\n",
      "Early stopping at epoch 6, Train Loss: 0.910085, Val Loss: 0.857032\n",
      "    Final - Train Loss: 0.910085, Val Loss: 0.857032\n",
      "[Saved] NN3_w21_2024Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.909080, Val Loss: 0.856456\n",
      "    Epoch 5/20, Train Loss: 0.907067, Val Loss: 0.857990\n",
      "Early stopping at epoch 8, Train Loss: 0.909077, Val Loss: 0.856392\n",
      "    Final - Train Loss: 0.909077, Val Loss: 0.856392\n",
      "[Saved] NN4_w21_2024Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.910585, Val Loss: 0.854905\n",
      "    Epoch 5/20, Train Loss: 0.907606, Val Loss: 0.855064\n",
      "Early stopping at epoch 6, Train Loss: 0.906775, Val Loss: 0.855060\n",
      "    Final - Train Loss: 0.906775, Val Loss: 0.855060\n",
      "[Saved] NN5_w21_2024Q1.pth\n",
      "    Training data size: 294526 samples\n",
      "  Quarter 2024Q2: Training models with expanding data\n",
      "    Added 3026 samples from 2024Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.911139, Val Loss: 0.859925\n",
      "    Epoch 5/20, Train Loss: 0.911295, Val Loss: 0.860064\n",
      "Early stopping at epoch 9, Train Loss: 0.911506, Val Loss: 0.859654\n",
      "    Final - Train Loss: 0.911506, Val Loss: 0.859654\n",
      "[Saved] NN1_w21_2024Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.908907, Val Loss: 0.858563\n",
      "    Epoch 5/20, Train Loss: 0.909669, Val Loss: 0.857896\n",
      "    Epoch 10/20, Train Loss: 0.910310, Val Loss: 0.858330\n",
      "Early stopping at epoch 10, Train Loss: 0.910310, Val Loss: 0.858330\n",
      "    Final - Train Loss: 0.910310, Val Loss: 0.858330\n",
      "[Saved] NN2_w21_2024Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.906590, Val Loss: 0.858983\n",
      "    Epoch 5/20, Train Loss: 0.905871, Val Loss: 0.858406\n",
      "Early stopping at epoch 7, Train Loss: 0.906379, Val Loss: 0.858821\n",
      "    Final - Train Loss: 0.906379, Val Loss: 0.858821\n",
      "[Saved] NN3_w21_2024Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.902670, Val Loss: 0.863865\n",
      "    Epoch 5/20, Train Loss: 0.901726, Val Loss: 0.865979\n",
      "Early stopping at epoch 6, Train Loss: 0.902073, Val Loss: 0.869021\n",
      "    Final - Train Loss: 0.902073, Val Loss: 0.869021\n",
      "[Saved] NN4_w21_2024Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.904000, Val Loss: 0.858343\n",
      "    Epoch 5/20, Train Loss: 0.902869, Val Loss: 0.858620\n",
      "Early stopping at epoch 6, Train Loss: 0.902270, Val Loss: 0.859100\n",
      "    Final - Train Loss: 0.902270, Val Loss: 0.859100\n",
      "[Saved] NN5_w21_2024Q2.pth\n",
      "    Training data size: 297552 samples\n",
      "  Quarter 2024Q3: Training models with expanding data\n",
      "    Added 3112 samples from 2024Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.906967, Val Loss: 0.860231\n",
      "    Epoch 5/20, Train Loss: 0.906900, Val Loss: 0.860127\n",
      "Early stopping at epoch 8, Train Loss: 0.907078, Val Loss: 0.860093\n",
      "    Final - Train Loss: 0.907078, Val Loss: 0.860093\n",
      "[Saved] NN1_w21_2024Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.904906, Val Loss: 0.859479\n",
      "    Epoch 5/20, Train Loss: 0.905611, Val Loss: 0.859520\n",
      "Early stopping at epoch 7, Train Loss: 0.904650, Val Loss: 0.859685\n",
      "    Final - Train Loss: 0.904650, Val Loss: 0.859685\n",
      "[Saved] NN2_w21_2024Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.900660, Val Loss: 0.859332\n",
      "    Epoch 5/20, Train Loss: 0.902146, Val Loss: 0.861359\n",
      "Early stopping at epoch 6, Train Loss: 0.901196, Val Loss: 0.861162\n",
      "    Final - Train Loss: 0.901196, Val Loss: 0.861162\n",
      "[Saved] NN3_w21_2024Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.898390, Val Loss: 0.862925\n",
      "    Epoch 5/20, Train Loss: 0.898621, Val Loss: 0.864282\n",
      "Early stopping at epoch 9, Train Loss: 0.895885, Val Loss: 0.866075\n",
      "    Final - Train Loss: 0.895885, Val Loss: 0.866075\n",
      "[Saved] NN4_w21_2024Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.898506, Val Loss: 0.859395\n",
      "    Epoch 5/20, Train Loss: 0.898056, Val Loss: 0.859083\n",
      "    Epoch 10/20, Train Loss: 0.896072, Val Loss: 0.859486\n",
      "Early stopping at epoch 10, Train Loss: 0.896072, Val Loss: 0.859486\n",
      "    Final - Train Loss: 0.896072, Val Loss: 0.859486\n",
      "[Saved] NN5_w21_2024Q3.pth\n",
      "    Training data size: 300664 samples\n",
      "Processing window size: 252\n",
      "  Quarter 2015Q4: Training models with expanding data\n",
      "    [Re-tuning] NN1 for 2015Q4\n",
      "[Hyper] NN1: best_MSE=0.826772, params={'learning_rate': 0.0014043382187156764, 'batch_size': 128, 'dropout_rate': 0.4432706353811381}\n",
      "    [Structure Change] dropout_rate changed: 0.1 -> 0.4432706353811381\n",
      "    Rebuilding NN1 model due to structure change\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 1.280075, Val Loss: 0.477867\n",
      "    Epoch 5/50, Train Loss: 1.114687, Val Loss: 0.484775\n",
      "Early stopping at epoch 6, Train Loss: 1.113194, Val Loss: 0.485465\n",
      "    Final - Train Loss: 1.113194, Val Loss: 0.485465\n",
      "[Saved] NN1_w252_2015Q4.pth\n",
      "    Creating new NN2 model\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 1.291211, Val Loss: 0.476947\n",
      "    Epoch 5/50, Train Loss: 1.120736, Val Loss: 0.479621\n",
      "Early stopping at epoch 7, Train Loss: 1.115516, Val Loss: 0.480242\n",
      "    Final - Train Loss: 1.115516, Val Loss: 0.480242\n",
      "[Saved] NN2_w252_2015Q4.pth\n",
      "    Creating new NN3 model\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 1.311648, Val Loss: 0.477632\n",
      "    Epoch 5/50, Train Loss: 1.121534, Val Loss: 0.479289\n",
      "Early stopping at epoch 7, Train Loss: 1.115141, Val Loss: 0.480759\n",
      "    Final - Train Loss: 1.115141, Val Loss: 0.480759\n",
      "[Saved] NN3_w252_2015Q4.pth\n",
      "    Creating new NN4 model\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 1.295387, Val Loss: 0.476759\n",
      "    Epoch 5/50, Train Loss: 1.127240, Val Loss: 0.477454\n",
      "Early stopping at epoch 9, Train Loss: 1.107662, Val Loss: 0.480298\n",
      "    Final - Train Loss: 1.107662, Val Loss: 0.480298\n",
      "[Saved] NN4_w252_2015Q4.pth\n",
      "    Creating new NN5 model\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 1.281948, Val Loss: 0.476735\n",
      "    Epoch 5/50, Train Loss: 1.130282, Val Loss: 0.476896\n",
      "Early stopping at epoch 9, Train Loss: 1.110428, Val Loss: 0.481125\n",
      "    Final - Train Loss: 1.110428, Val Loss: 0.481125\n",
      "[Saved] NN5_w252_2015Q4.pth\n",
      "    Training data size: 184570 samples\n",
      "  Quarter 2016Q1: Training models with expanding data\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.125846, Val Loss: 0.480845\n",
      "    Epoch 5/20, Train Loss: 1.108622, Val Loss: 0.487299\n",
      "Early stopping at epoch 6, Train Loss: 1.106715, Val Loss: 0.488352\n",
      "    Final - Train Loss: 1.106715, Val Loss: 0.488352\n",
      "[Saved] NN1_w252_2016Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.129411, Val Loss: 0.477577\n",
      "    Epoch 5/20, Train Loss: 1.112100, Val Loss: 0.481168\n",
      "Early stopping at epoch 6, Train Loss: 1.109954, Val Loss: 0.482011\n",
      "    Final - Train Loss: 1.109954, Val Loss: 0.482011\n",
      "[Saved] NN2_w252_2016Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.129516, Val Loss: 0.478648\n",
      "    Epoch 5/20, Train Loss: 1.111507, Val Loss: 0.480994\n",
      "Early stopping at epoch 6, Train Loss: 1.107890, Val Loss: 0.480986\n",
      "    Final - Train Loss: 1.107890, Val Loss: 0.480986\n",
      "[Saved] NN3_w252_2016Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.127054, Val Loss: 0.478062\n",
      "    Epoch 5/20, Train Loss: 1.106988, Val Loss: 0.481432\n",
      "Early stopping at epoch 6, Train Loss: 1.100757, Val Loss: 0.481546\n",
      "    Final - Train Loss: 1.100757, Val Loss: 0.481546\n",
      "[Saved] NN4_w252_2016Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.130432, Val Loss: 0.477419\n",
      "    Epoch 5/20, Train Loss: 1.105717, Val Loss: 0.483081\n",
      "Early stopping at epoch 6, Train Loss: 1.100288, Val Loss: 0.482853\n",
      "    Final - Train Loss: 1.100288, Val Loss: 0.482853\n",
      "[Saved] NN5_w252_2016Q1.pth\n",
      "    Training data size: 184570 samples\n",
      "  Quarter 2016Q2: Training models with expanding data\n",
      "    Added 2956 samples from 2016Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.106316, Val Loss: 0.509487\n",
      "    Epoch 5/20, Train Loss: 1.095868, Val Loss: 0.513037\n",
      "Early stopping at epoch 6, Train Loss: 1.093973, Val Loss: 0.513689\n",
      "    Final - Train Loss: 1.093973, Val Loss: 0.513689\n",
      "[Saved] NN1_w252_2016Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.110466, Val Loss: 0.503691\n",
      "    Epoch 5/20, Train Loss: 1.099345, Val Loss: 0.506499\n",
      "Early stopping at epoch 6, Train Loss: 1.097438, Val Loss: 0.507974\n",
      "    Final - Train Loss: 1.097438, Val Loss: 0.507974\n",
      "[Saved] NN2_w252_2016Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.110996, Val Loss: 0.503075\n",
      "    Epoch 5/20, Train Loss: 1.097373, Val Loss: 0.506643\n",
      "Early stopping at epoch 6, Train Loss: 1.092848, Val Loss: 0.507985\n",
      "    Final - Train Loss: 1.092848, Val Loss: 0.507985\n",
      "[Saved] NN3_w252_2016Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.109003, Val Loss: 0.503657\n",
      "    Epoch 5/20, Train Loss: 1.090447, Val Loss: 0.505867\n",
      "Early stopping at epoch 6, Train Loss: 1.081880, Val Loss: 0.507422\n",
      "    Final - Train Loss: 1.081880, Val Loss: 0.507422\n",
      "[Saved] NN4_w252_2016Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.111540, Val Loss: 0.502872\n",
      "    Epoch 5/20, Train Loss: 1.089341, Val Loss: 0.505938\n",
      "Early stopping at epoch 6, Train Loss: 1.080972, Val Loss: 0.506422\n",
      "    Final - Train Loss: 1.080972, Val Loss: 0.506422\n",
      "[Saved] NN5_w252_2016Q2.pth\n",
      "    Training data size: 187526 samples\n",
      "  Quarter 2016Q3: Training models with expanding data\n",
      "    Added 3170 samples from 2016Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.093643, Val Loss: 0.510900\n",
      "    Epoch 5/20, Train Loss: 1.083905, Val Loss: 0.514094\n",
      "Early stopping at epoch 6, Train Loss: 1.083051, Val Loss: 0.516221\n",
      "    Final - Train Loss: 1.083051, Val Loss: 0.516221\n",
      "[Saved] NN1_w252_2016Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.097567, Val Loss: 0.506149\n",
      "    Epoch 5/20, Train Loss: 1.087617, Val Loss: 0.507863\n",
      "Early stopping at epoch 7, Train Loss: 1.083855, Val Loss: 0.508970\n",
      "    Final - Train Loss: 1.083855, Val Loss: 0.508970\n",
      "[Saved] NN2_w252_2016Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.097347, Val Loss: 0.505345\n",
      "    Epoch 5/20, Train Loss: 1.084232, Val Loss: 0.506517\n",
      "Early stopping at epoch 7, Train Loss: 1.075725, Val Loss: 0.507483\n",
      "    Final - Train Loss: 1.075725, Val Loss: 0.507483\n",
      "[Saved] NN3_w252_2016Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.095055, Val Loss: 0.505804\n",
      "    Epoch 5/20, Train Loss: 1.075939, Val Loss: 0.507533\n",
      "Early stopping at epoch 6, Train Loss: 1.065399, Val Loss: 0.507719\n",
      "    Final - Train Loss: 1.065399, Val Loss: 0.507719\n",
      "[Saved] NN4_w252_2016Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.096195, Val Loss: 0.505029\n",
      "    Epoch 5/20, Train Loss: 1.072476, Val Loss: 0.508458\n",
      "Early stopping at epoch 6, Train Loss: 1.062469, Val Loss: 0.509156\n",
      "    Final - Train Loss: 1.062469, Val Loss: 0.509156\n",
      "[Saved] NN5_w252_2016Q3.pth\n",
      "    Training data size: 190696 samples\n",
      "  Quarter 2016Q4: Training models with expanding data\n",
      "    Added 3176 samples from 2016Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.079380, Val Loss: 0.506168\n",
      "    Epoch 5/20, Train Loss: 1.072457, Val Loss: 0.510506\n",
      "Early stopping at epoch 6, Train Loss: 1.070964, Val Loss: 0.512431\n",
      "    Final - Train Loss: 1.070964, Val Loss: 0.512431\n",
      "[Saved] NN1_w252_2016Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.080433, Val Loss: 0.502429\n",
      "    Epoch 5/20, Train Loss: 1.072915, Val Loss: 0.503642\n",
      "Early stopping at epoch 7, Train Loss: 1.067784, Val Loss: 0.503785\n",
      "    Final - Train Loss: 1.067784, Val Loss: 0.503785\n",
      "[Saved] NN2_w252_2016Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.079950, Val Loss: 0.501087\n",
      "    Epoch 5/20, Train Loss: 1.065655, Val Loss: 0.502835\n",
      "Early stopping at epoch 6, Train Loss: 1.059994, Val Loss: 0.502663\n",
      "    Final - Train Loss: 1.059994, Val Loss: 0.502663\n",
      "[Saved] NN3_w252_2016Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.079304, Val Loss: 0.501425\n",
      "    Epoch 5/20, Train Loss: 1.056012, Val Loss: 0.502528\n",
      "Early stopping at epoch 6, Train Loss: 1.046644, Val Loss: 0.503899\n",
      "    Final - Train Loss: 1.046644, Val Loss: 0.503899\n",
      "[Saved] NN4_w252_2016Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.080155, Val Loss: 0.501272\n",
      "    Epoch 5/20, Train Loss: 1.053199, Val Loss: 0.503812\n",
      "Early stopping at epoch 6, Train Loss: 1.044960, Val Loss: 0.504169\n",
      "    Final - Train Loss: 1.044960, Val Loss: 0.504169\n",
      "[Saved] NN5_w252_2016Q4.pth\n",
      "    Training data size: 193872 samples\n",
      "  Quarter 2017Q1: Training models with expanding data\n",
      "    Added 3123 samples from 2016Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.067274, Val Loss: 0.512071\n",
      "    Epoch 5/20, Train Loss: 1.060088, Val Loss: 0.518601\n",
      "Early stopping at epoch 6, Train Loss: 1.060028, Val Loss: 0.518775\n",
      "    Final - Train Loss: 1.060028, Val Loss: 0.518775\n",
      "[Saved] NN1_w252_2017Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.066042, Val Loss: 0.507194\n",
      "    Epoch 5/20, Train Loss: 1.057501, Val Loss: 0.509297\n",
      "Early stopping at epoch 6, Train Loss: 1.055012, Val Loss: 0.509067\n",
      "    Final - Train Loss: 1.055012, Val Loss: 0.509067\n",
      "[Saved] NN2_w252_2017Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.065024, Val Loss: 0.506667\n",
      "    Epoch 5/20, Train Loss: 1.050763, Val Loss: 0.508060\n",
      "Early stopping at epoch 6, Train Loss: 1.046514, Val Loss: 0.509011\n",
      "    Final - Train Loss: 1.046514, Val Loss: 0.509011\n",
      "[Saved] NN3_w252_2017Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.063147, Val Loss: 0.506409\n",
      "    Epoch 5/20, Train Loss: 1.038688, Val Loss: 0.508951\n",
      "Early stopping at epoch 6, Train Loss: 1.030373, Val Loss: 0.510002\n",
      "    Final - Train Loss: 1.030373, Val Loss: 0.510002\n",
      "[Saved] NN4_w252_2017Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.063195, Val Loss: 0.507018\n",
      "    Epoch 5/20, Train Loss: 1.033294, Val Loss: 0.507988\n",
      "Early stopping at epoch 6, Train Loss: 1.023805, Val Loss: 0.510260\n",
      "    Final - Train Loss: 1.023805, Val Loss: 0.510260\n",
      "[Saved] NN5_w252_2017Q1.pth\n",
      "    Training data size: 196995 samples\n",
      "  Quarter 2017Q2: Training models with expanding data\n",
      "    Added 3083 samples from 2017Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.056188, Val Loss: 0.506901\n",
      "    Epoch 5/20, Train Loss: 1.050172, Val Loss: 0.511079\n",
      "Early stopping at epoch 6, Train Loss: 1.048060, Val Loss: 0.511656\n",
      "    Final - Train Loss: 1.048060, Val Loss: 0.511656\n",
      "[Saved] NN1_w252_2017Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.054005, Val Loss: 0.501711\n",
      "    Epoch 5/20, Train Loss: 1.044322, Val Loss: 0.502804\n",
      "Early stopping at epoch 6, Train Loss: 1.042470, Val Loss: 0.504016\n",
      "    Final - Train Loss: 1.042470, Val Loss: 0.504016\n",
      "[Saved] NN2_w252_2017Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.052189, Val Loss: 0.500241\n",
      "    Epoch 5/20, Train Loss: 1.035452, Val Loss: 0.501542\n",
      "Early stopping at epoch 6, Train Loss: 1.031226, Val Loss: 0.502136\n",
      "    Final - Train Loss: 1.031226, Val Loss: 0.502136\n",
      "[Saved] NN3_w252_2017Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.048395, Val Loss: 0.500966\n",
      "    Epoch 5/20, Train Loss: 1.021026, Val Loss: 0.502207\n",
      "Early stopping at epoch 6, Train Loss: 1.014892, Val Loss: 0.503641\n",
      "    Final - Train Loss: 1.014892, Val Loss: 0.503641\n",
      "[Saved] NN4_w252_2017Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.045486, Val Loss: 0.500572\n",
      "    Epoch 5/20, Train Loss: 1.017153, Val Loss: 0.504026\n",
      "Early stopping at epoch 6, Train Loss: 1.008300, Val Loss: 0.505926\n",
      "    Final - Train Loss: 1.008300, Val Loss: 0.505926\n",
      "[Saved] NN5_w252_2017Q2.pth\n",
      "    Training data size: 200078 samples\n",
      "  Quarter 2017Q3: Training models with expanding data\n",
      "    Added 3122 samples from 2017Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.045551, Val Loss: 0.499897\n",
      "    Epoch 5/20, Train Loss: 1.039819, Val Loss: 0.504154\n",
      "Early stopping at epoch 6, Train Loss: 1.037659, Val Loss: 0.505264\n",
      "    Final - Train Loss: 1.037659, Val Loss: 0.505264\n",
      "[Saved] NN1_w252_2017Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.041738, Val Loss: 0.496535\n",
      "    Epoch 5/20, Train Loss: 1.036194, Val Loss: 0.498148\n",
      "Early stopping at epoch 7, Train Loss: 1.029338, Val Loss: 0.499496\n",
      "    Final - Train Loss: 1.029338, Val Loss: 0.499496\n",
      "[Saved] NN2_w252_2017Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.039478, Val Loss: 0.495060\n",
      "    Epoch 5/20, Train Loss: 1.023005, Val Loss: 0.497143\n",
      "Early stopping at epoch 6, Train Loss: 1.019955, Val Loss: 0.497014\n",
      "    Final - Train Loss: 1.019955, Val Loss: 0.497014\n",
      "[Saved] NN3_w252_2017Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.030633, Val Loss: 0.495750\n",
      "    Epoch 5/20, Train Loss: 1.007771, Val Loss: 0.497021\n",
      "Early stopping at epoch 6, Train Loss: 0.998956, Val Loss: 0.497432\n",
      "    Final - Train Loss: 0.998956, Val Loss: 0.497432\n",
      "[Saved] NN4_w252_2017Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.030753, Val Loss: 0.495728\n",
      "    Epoch 5/20, Train Loss: 0.997964, Val Loss: 0.497102\n",
      "Early stopping at epoch 7, Train Loss: 0.984313, Val Loss: 0.501346\n",
      "    Final - Train Loss: 0.984313, Val Loss: 0.501346\n",
      "[Saved] NN5_w252_2017Q3.pth\n",
      "    Training data size: 203200 samples\n",
      "  Quarter 2017Q4: Training models with expanding data\n",
      "    Added 3114 samples from 2017Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.034767, Val Loss: 0.503095\n",
      "    Epoch 5/20, Train Loss: 1.029791, Val Loss: 0.506156\n",
      "Early stopping at epoch 7, Train Loss: 1.026399, Val Loss: 0.507807\n",
      "    Final - Train Loss: 1.026399, Val Loss: 0.507807\n",
      "[Saved] NN1_w252_2017Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.031539, Val Loss: 0.498849\n",
      "    Epoch 5/20, Train Loss: 1.020225, Val Loss: 0.499627\n",
      "Early stopping at epoch 7, Train Loss: 1.016053, Val Loss: 0.500922\n",
      "    Final - Train Loss: 1.016053, Val Loss: 0.500922\n",
      "[Saved] NN2_w252_2017Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.026523, Val Loss: 0.497859\n",
      "    Epoch 5/20, Train Loss: 1.010933, Val Loss: 0.499711\n",
      "Early stopping at epoch 6, Train Loss: 1.006792, Val Loss: 0.499879\n",
      "    Final - Train Loss: 1.006792, Val Loss: 0.499879\n",
      "[Saved] NN3_w252_2017Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.014869, Val Loss: 0.497638\n",
      "    Epoch 5/20, Train Loss: 0.991461, Val Loss: 0.499044\n",
      "Early stopping at epoch 6, Train Loss: 0.985479, Val Loss: 0.498896\n",
      "    Final - Train Loss: 0.985479, Val Loss: 0.498896\n",
      "[Saved] NN4_w252_2017Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.004196, Val Loss: 0.497116\n",
      "    Epoch 5/20, Train Loss: 0.978375, Val Loss: 0.500489\n",
      "Early stopping at epoch 6, Train Loss: 0.971747, Val Loss: 0.500321\n",
      "    Final - Train Loss: 0.971747, Val Loss: 0.500321\n",
      "[Saved] NN5_w252_2017Q4.pth\n",
      "    Training data size: 206314 samples\n",
      "  Quarter 2018Q1: Training models with expanding data\n",
      "    Added 3115 samples from 2017Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.021745, Val Loss: 0.513650\n",
      "    Epoch 5/20, Train Loss: 1.017630, Val Loss: 0.518740\n",
      "Early stopping at epoch 6, Train Loss: 1.017304, Val Loss: 0.519172\n",
      "    Final - Train Loss: 1.017304, Val Loss: 0.519172\n",
      "[Saved] NN1_w252_2018Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.016274, Val Loss: 0.507942\n",
      "    Epoch 5/20, Train Loss: 1.007080, Val Loss: 0.509436\n",
      "Early stopping at epoch 6, Train Loss: 1.004752, Val Loss: 0.509443\n",
      "    Final - Train Loss: 1.004752, Val Loss: 0.509443\n",
      "[Saved] NN2_w252_2018Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.011965, Val Loss: 0.507647\n",
      "    Epoch 5/20, Train Loss: 0.999318, Val Loss: 0.508761\n",
      "Early stopping at epoch 7, Train Loss: 0.989325, Val Loss: 0.509289\n",
      "    Final - Train Loss: 0.989325, Val Loss: 0.509289\n",
      "[Saved] NN3_w252_2018Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.997848, Val Loss: 0.508022\n",
      "    Epoch 5/20, Train Loss: 0.978852, Val Loss: 0.509577\n",
      "Early stopping at epoch 7, Train Loss: 0.966732, Val Loss: 0.509747\n",
      "    Final - Train Loss: 0.966732, Val Loss: 0.509747\n",
      "[Saved] NN4_w252_2018Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.987015, Val Loss: 0.509396\n",
      "    Epoch 5/20, Train Loss: 0.960043, Val Loss: 0.510754\n",
      "Early stopping at epoch 6, Train Loss: 0.952445, Val Loss: 0.510520\n",
      "    Final - Train Loss: 0.952445, Val Loss: 0.510520\n",
      "[Saved] NN5_w252_2018Q1.pth\n",
      "    Training data size: 209429 samples\n",
      "  Quarter 2018Q2: Training models with expanding data\n",
      "    Added 2996 samples from 2018Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.013195, Val Loss: 0.534899\n",
      "    Epoch 5/20, Train Loss: 1.008963, Val Loss: 0.538180\n",
      "Early stopping at epoch 6, Train Loss: 1.007653, Val Loss: 0.539547\n",
      "    Final - Train Loss: 1.007653, Val Loss: 0.539547\n",
      "[Saved] NN1_w252_2018Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.004043, Val Loss: 0.529370\n",
      "    Epoch 5/20, Train Loss: 0.997908, Val Loss: 0.530003\n",
      "Early stopping at epoch 8, Train Loss: 0.990526, Val Loss: 0.530944\n",
      "    Final - Train Loss: 0.990526, Val Loss: 0.530944\n",
      "[Saved] NN2_w252_2018Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.996221, Val Loss: 0.529011\n",
      "    Epoch 5/20, Train Loss: 0.982477, Val Loss: 0.529764\n",
      "Early stopping at epoch 8, Train Loss: 0.975204, Val Loss: 0.530676\n",
      "    Final - Train Loss: 0.975204, Val Loss: 0.530676\n",
      "[Saved] NN3_w252_2018Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.976164, Val Loss: 0.528940\n",
      "    Epoch 5/20, Train Loss: 0.959566, Val Loss: 0.530260\n",
      "Early stopping at epoch 6, Train Loss: 0.953875, Val Loss: 0.531353\n",
      "    Final - Train Loss: 0.953875, Val Loss: 0.531353\n",
      "[Saved] NN4_w252_2018Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.969206, Val Loss: 0.531359\n",
      "    Epoch 5/20, Train Loss: 0.943083, Val Loss: 0.534563\n",
      "Early stopping at epoch 7, Train Loss: 0.931420, Val Loss: 0.533431\n",
      "    Final - Train Loss: 0.931420, Val Loss: 0.533431\n",
      "[Saved] NN5_w252_2018Q2.pth\n",
      "    Training data size: 212425 samples\n",
      "  Quarter 2018Q3: Training models with expanding data\n",
      "    Added 3160 samples from 2018Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.002885, Val Loss: 0.545405\n",
      "    Epoch 5/20, Train Loss: 0.998139, Val Loss: 0.549044\n",
      "Early stopping at epoch 6, Train Loss: 0.997688, Val Loss: 0.549848\n",
      "    Final - Train Loss: 0.997688, Val Loss: 0.549848\n",
      "[Saved] NN1_w252_2018Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.990989, Val Loss: 0.538809\n",
      "    Epoch 5/20, Train Loss: 0.980338, Val Loss: 0.539419\n",
      "Early stopping at epoch 7, Train Loss: 0.976174, Val Loss: 0.539463\n",
      "    Final - Train Loss: 0.976174, Val Loss: 0.539463\n",
      "[Saved] NN2_w252_2018Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.975913, Val Loss: 0.538360\n",
      "    Epoch 5/20, Train Loss: 0.965350, Val Loss: 0.540319\n",
      "Early stopping at epoch 6, Train Loss: 0.960354, Val Loss: 0.539504\n",
      "    Final - Train Loss: 0.960354, Val Loss: 0.539504\n",
      "[Saved] NN3_w252_2018Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.964482, Val Loss: 0.538690\n",
      "    Epoch 5/20, Train Loss: 0.948454, Val Loss: 0.540773\n",
      "Early stopping at epoch 6, Train Loss: 0.944215, Val Loss: 0.540607\n",
      "    Final - Train Loss: 0.944215, Val Loss: 0.540607\n",
      "[Saved] NN4_w252_2018Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.949008, Val Loss: 0.538805\n",
      "    Epoch 5/20, Train Loss: 0.926817, Val Loss: 0.542105\n",
      "Early stopping at epoch 6, Train Loss: 0.922362, Val Loss: 0.542345\n",
      "    Final - Train Loss: 0.922362, Val Loss: 0.542345\n",
      "[Saved] NN5_w252_2018Q3.pth\n",
      "    Training data size: 215585 samples\n",
      "  Quarter 2018Q4: Training models with expanding data\n",
      "    Added 3125 samples from 2018Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.995879, Val Loss: 0.542341\n",
      "    Epoch 5/20, Train Loss: 0.991414, Val Loss: 0.545303\n",
      "Early stopping at epoch 6, Train Loss: 0.989468, Val Loss: 0.546579\n",
      "    Final - Train Loss: 0.989468, Val Loss: 0.546579\n",
      "[Saved] NN1_w252_2018Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.978283, Val Loss: 0.535596\n",
      "    Epoch 5/20, Train Loss: 0.970456, Val Loss: 0.535483\n",
      "Early stopping at epoch 9, Train Loss: 0.962374, Val Loss: 0.537033\n",
      "    Final - Train Loss: 0.962374, Val Loss: 0.537033\n",
      "[Saved] NN2_w252_2018Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.964453, Val Loss: 0.534558\n",
      "    Epoch 5/20, Train Loss: 0.954984, Val Loss: 0.536080\n",
      "Early stopping at epoch 6, Train Loss: 0.953014, Val Loss: 0.535714\n",
      "    Final - Train Loss: 0.953014, Val Loss: 0.535714\n",
      "[Saved] NN3_w252_2018Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.949966, Val Loss: 0.535989\n",
      "    Epoch 5/20, Train Loss: 0.936799, Val Loss: 0.536595\n",
      "Early stopping at epoch 7, Train Loss: 0.928064, Val Loss: 0.537611\n",
      "    Final - Train Loss: 0.928064, Val Loss: 0.537611\n",
      "[Saved] NN4_w252_2018Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.935358, Val Loss: 0.538076\n",
      "    Epoch 5/20, Train Loss: 0.916989, Val Loss: 0.537313\n",
      "Early stopping at epoch 8, Train Loss: 0.900540, Val Loss: 0.538304\n",
      "    Final - Train Loss: 0.900540, Val Loss: 0.538304\n",
      "[Saved] NN5_w252_2018Q4.pth\n",
      "    Training data size: 218710 samples\n",
      "  Quarter 2019Q1: Training models with expanding data\n",
      "    Added 3045 samples from 2018Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.985399, Val Loss: 0.589224\n",
      "    Epoch 5/20, Train Loss: 0.981961, Val Loss: 0.592950\n",
      "Early stopping at epoch 6, Train Loss: 0.979603, Val Loss: 0.594308\n",
      "    Final - Train Loss: 0.979603, Val Loss: 0.594308\n",
      "[Saved] NN1_w252_2019Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.958774, Val Loss: 0.577878\n",
      "    Epoch 5/20, Train Loss: 0.955432, Val Loss: 0.579717\n",
      "Early stopping at epoch 6, Train Loss: 0.954577, Val Loss: 0.579790\n",
      "    Final - Train Loss: 0.954577, Val Loss: 0.579790\n",
      "[Saved] NN2_w252_2019Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.952405, Val Loss: 0.578265\n",
      "    Epoch 5/20, Train Loss: 0.944042, Val Loss: 0.579032\n",
      "Early stopping at epoch 6, Train Loss: 0.944506, Val Loss: 0.579773\n",
      "    Final - Train Loss: 0.944506, Val Loss: 0.579773\n",
      "[Saved] NN3_w252_2019Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.932685, Val Loss: 0.578365\n",
      "    Epoch 5/20, Train Loss: 0.923191, Val Loss: 0.579644\n",
      "Early stopping at epoch 6, Train Loss: 0.914477, Val Loss: 0.580880\n",
      "    Final - Train Loss: 0.914477, Val Loss: 0.580880\n",
      "[Saved] NN4_w252_2019Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.911431, Val Loss: 0.577589\n",
      "    Epoch 5/20, Train Loss: 0.890321, Val Loss: 0.579475\n",
      "Early stopping at epoch 6, Train Loss: 0.886863, Val Loss: 0.580281\n",
      "    Final - Train Loss: 0.886863, Val Loss: 0.580281\n",
      "[Saved] NN5_w252_2019Q1.pth\n",
      "    Training data size: 221755 samples\n",
      "  Quarter 2019Q2: Training models with expanding data\n",
      "    Added 3022 samples from 2019Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.977455, Val Loss: 0.594911\n",
      "    Epoch 5/20, Train Loss: 0.972957, Val Loss: 0.598749\n",
      "Early stopping at epoch 6, Train Loss: 0.971034, Val Loss: 0.599876\n",
      "    Final - Train Loss: 0.971034, Val Loss: 0.599876\n",
      "[Saved] NN1_w252_2019Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.950637, Val Loss: 0.585939\n",
      "    Epoch 5/20, Train Loss: 0.946477, Val Loss: 0.587064\n",
      "Early stopping at epoch 6, Train Loss: 0.943041, Val Loss: 0.587034\n",
      "    Final - Train Loss: 0.943041, Val Loss: 0.587034\n",
      "[Saved] NN2_w252_2019Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.940437, Val Loss: 0.585622\n",
      "    Epoch 5/20, Train Loss: 0.934360, Val Loss: 0.588509\n",
      "Early stopping at epoch 7, Train Loss: 0.929744, Val Loss: 0.588492\n",
      "    Final - Train Loss: 0.929744, Val Loss: 0.588492\n",
      "[Saved] NN3_w252_2019Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.922063, Val Loss: 0.587142\n",
      "    Epoch 5/20, Train Loss: 0.910529, Val Loss: 0.590656\n",
      "Early stopping at epoch 8, Train Loss: 0.900750, Val Loss: 0.588486\n",
      "    Final - Train Loss: 0.900750, Val Loss: 0.588486\n",
      "[Saved] NN4_w252_2019Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.898979, Val Loss: 0.587761\n",
      "    Epoch 5/20, Train Loss: 0.880200, Val Loss: 0.589299\n",
      "Early stopping at epoch 7, Train Loss: 0.873157, Val Loss: 0.590757\n",
      "    Final - Train Loss: 0.873157, Val Loss: 0.590757\n",
      "[Saved] NN5_w252_2019Q2.pth\n",
      "    Training data size: 224777 samples\n",
      "  Quarter 2019Q3: Training models with expanding data\n",
      "    Added 3120 samples from 2019Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.974464, Val Loss: 0.571237\n",
      "    Epoch 5/20, Train Loss: 0.970681, Val Loss: 0.575215\n",
      "Early stopping at epoch 6, Train Loss: 0.969232, Val Loss: 0.575588\n",
      "    Final - Train Loss: 0.969232, Val Loss: 0.575588\n",
      "[Saved] NN1_w252_2019Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.947674, Val Loss: 0.565008\n",
      "    Epoch 5/20, Train Loss: 0.942874, Val Loss: 0.565007\n",
      "Early stopping at epoch 9, Train Loss: 0.936944, Val Loss: 0.565211\n",
      "    Final - Train Loss: 0.936944, Val Loss: 0.565211\n",
      "[Saved] NN2_w252_2019Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.935719, Val Loss: 0.565303\n",
      "    Epoch 5/20, Train Loss: 0.931365, Val Loss: 0.566174\n",
      "Early stopping at epoch 6, Train Loss: 0.926419, Val Loss: 0.566595\n",
      "    Final - Train Loss: 0.926419, Val Loss: 0.566595\n",
      "[Saved] NN3_w252_2019Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.910189, Val Loss: 0.567974\n",
      "    Epoch 5/20, Train Loss: 0.897522, Val Loss: 0.568808\n",
      "    Epoch 10/20, Train Loss: 0.883784, Val Loss: 0.570021\n",
      "Early stopping at epoch 12, Train Loss: 0.873059, Val Loss: 0.568300\n",
      "    Final - Train Loss: 0.873059, Val Loss: 0.568300\n",
      "[Saved] NN4_w252_2019Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.886716, Val Loss: 0.567900\n",
      "    Epoch 5/20, Train Loss: 0.870268, Val Loss: 0.568673\n",
      "Early stopping at epoch 7, Train Loss: 0.864275, Val Loss: 0.567585\n",
      "    Final - Train Loss: 0.864275, Val Loss: 0.567585\n",
      "[Saved] NN5_w252_2019Q3.pth\n",
      "    Training data size: 227897 samples\n",
      "  Quarter 2019Q4: Training models with expanding data\n",
      "    Added 3167 samples from 2019Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.968261, Val Loss: 0.580731\n",
      "    Epoch 5/20, Train Loss: 0.965086, Val Loss: 0.583547\n",
      "Early stopping at epoch 6, Train Loss: 0.963815, Val Loss: 0.583259\n",
      "    Final - Train Loss: 0.963815, Val Loss: 0.583259\n",
      "[Saved] NN1_w252_2019Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.936698, Val Loss: 0.573221\n",
      "    Epoch 5/20, Train Loss: 0.931742, Val Loss: 0.573607\n",
      "Early stopping at epoch 7, Train Loss: 0.930869, Val Loss: 0.573572\n",
      "    Final - Train Loss: 0.930869, Val Loss: 0.573572\n",
      "[Saved] NN2_w252_2019Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.928429, Val Loss: 0.574477\n",
      "    Epoch 5/20, Train Loss: 0.921311, Val Loss: 0.575711\n",
      "Early stopping at epoch 6, Train Loss: 0.920036, Val Loss: 0.578960\n",
      "    Final - Train Loss: 0.920036, Val Loss: 0.578960\n",
      "[Saved] NN3_w252_2019Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.879850, Val Loss: 0.580270\n",
      "    Epoch 5/20, Train Loss: 0.871532, Val Loss: 0.578925\n",
      "    Epoch 10/20, Train Loss: 0.856887, Val Loss: 0.583918\n",
      "    Epoch 15/20, Train Loss: 0.847339, Val Loss: 0.575707\n",
      "Early stopping at epoch 16, Train Loss: 0.841155, Val Loss: 0.582932\n",
      "    Final - Train Loss: 0.841155, Val Loss: 0.582932\n",
      "[Saved] NN4_w252_2019Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.873252, Val Loss: 0.576494\n",
      "    Epoch 5/20, Train Loss: 0.860774, Val Loss: 0.576162\n",
      "Early stopping at epoch 7, Train Loss: 0.850104, Val Loss: 0.576004\n",
      "    Final - Train Loss: 0.850104, Val Loss: 0.576004\n",
      "[Saved] NN5_w252_2019Q4.pth\n",
      "    Training data size: 231064 samples\n",
      "  Quarter 2020Q1: Training models with expanding data\n",
      "    Added 3179 samples from 2019Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.965806, Val Loss: 0.558942\n",
      "    Epoch 5/20, Train Loss: 0.962714, Val Loss: 0.560444\n",
      "Early stopping at epoch 6, Train Loss: 0.960941, Val Loss: 0.561647\n",
      "    Final - Train Loss: 0.960941, Val Loss: 0.561647\n",
      "[Saved] NN1_w252_2020Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.932796, Val Loss: 0.553604\n",
      "    Epoch 5/20, Train Loss: 0.925743, Val Loss: 0.554620\n",
      "Early stopping at epoch 6, Train Loss: 0.924582, Val Loss: 0.554729\n",
      "    Final - Train Loss: 0.924582, Val Loss: 0.554729\n",
      "[Saved] NN2_w252_2020Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.924989, Val Loss: 0.554384\n",
      "    Epoch 5/20, Train Loss: 0.918245, Val Loss: 0.555113\n",
      "Early stopping at epoch 6, Train Loss: 0.914495, Val Loss: 0.556322\n",
      "    Final - Train Loss: 0.914495, Val Loss: 0.556322\n",
      "[Saved] NN3_w252_2020Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.853387, Val Loss: 0.556059\n",
      "    Epoch 5/20, Train Loss: 0.843165, Val Loss: 0.555910\n",
      "Early stopping at epoch 9, Train Loss: 0.836818, Val Loss: 0.556939\n",
      "    Final - Train Loss: 0.836818, Val Loss: 0.556939\n",
      "[Saved] NN4_w252_2020Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.864092, Val Loss: 0.559006\n",
      "    Epoch 5/20, Train Loss: 0.850265, Val Loss: 0.564689\n",
      "Early stopping at epoch 7, Train Loss: 0.840695, Val Loss: 0.565512\n",
      "    Final - Train Loss: 0.840695, Val Loss: 0.565512\n",
      "[Saved] NN5_w252_2020Q1.pth\n",
      "    Training data size: 234243 samples\n",
      "  Quarter 2020Q2: Training models with expanding data\n",
      "    Added 2595 samples from 2020Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.958970, Val Loss: 0.624462\n",
      "    Epoch 5/20, Train Loss: 0.955450, Val Loss: 0.628376\n",
      "Early stopping at epoch 6, Train Loss: 0.955871, Val Loss: 0.627565\n",
      "    Final - Train Loss: 0.955871, Val Loss: 0.627565\n",
      "[Saved] NN1_w252_2020Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.926825, Val Loss: 0.618208\n",
      "    Epoch 5/20, Train Loss: 0.921084, Val Loss: 0.617900\n",
      "Early stopping at epoch 9, Train Loss: 0.916096, Val Loss: 0.617683\n",
      "    Final - Train Loss: 0.916096, Val Loss: 0.617683\n",
      "[Saved] NN2_w252_2020Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.916093, Val Loss: 0.618212\n",
      "    Epoch 5/20, Train Loss: 0.912957, Val Loss: 0.620626\n",
      "Early stopping at epoch 6, Train Loss: 0.909497, Val Loss: 0.620268\n",
      "    Final - Train Loss: 0.909497, Val Loss: 0.620268\n",
      "[Saved] NN3_w252_2020Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.839686, Val Loss: 0.617347\n",
      "    Epoch 5/20, Train Loss: 0.835933, Val Loss: 0.619858\n",
      "Early stopping at epoch 6, Train Loss: 0.831135, Val Loss: 0.618753\n",
      "    Final - Train Loss: 0.831135, Val Loss: 0.618753\n",
      "[Saved] NN4_w252_2020Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.850189, Val Loss: 0.627175\n",
      "    Epoch 5/20, Train Loss: 0.838616, Val Loss: 0.635162\n",
      "Early stopping at epoch 9, Train Loss: 0.828005, Val Loss: 0.626819\n",
      "    Final - Train Loss: 0.828005, Val Loss: 0.626819\n",
      "[Saved] NN5_w252_2020Q2.pth\n",
      "    Training data size: 236838 samples\n",
      "  Quarter 2020Q3: Training models with expanding data\n",
      "    Added 2839 samples from 2020Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.952701, Val Loss: 0.694651\n",
      "    Epoch 5/20, Train Loss: 0.951125, Val Loss: 0.696901\n",
      "Early stopping at epoch 6, Train Loss: 0.949103, Val Loss: 0.696565\n",
      "    Final - Train Loss: 0.949103, Val Loss: 0.696565\n",
      "[Saved] NN1_w252_2020Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.914254, Val Loss: 0.689727\n",
      "    Epoch 5/20, Train Loss: 0.909836, Val Loss: 0.689470\n",
      "Early stopping at epoch 8, Train Loss: 0.906948, Val Loss: 0.689837\n",
      "    Final - Train Loss: 0.906948, Val Loss: 0.689837\n",
      "[Saved] NN2_w252_2020Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.906802, Val Loss: 0.690049\n",
      "    Epoch 5/20, Train Loss: 0.903741, Val Loss: 0.690717\n",
      "Early stopping at epoch 7, Train Loss: 0.900844, Val Loss: 0.690709\n",
      "    Final - Train Loss: 0.900844, Val Loss: 0.690709\n",
      "[Saved] NN3_w252_2020Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.834038, Val Loss: 0.689099\n",
      "    Epoch 5/20, Train Loss: 0.829367, Val Loss: 0.689788\n",
      "Early stopping at epoch 7, Train Loss: 0.825255, Val Loss: 0.687865\n",
      "    Final - Train Loss: 0.825255, Val Loss: 0.687865\n",
      "[Saved] NN4_w252_2020Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.829891, Val Loss: 0.696226\n",
      "    Epoch 5/20, Train Loss: 0.817087, Val Loss: 0.704202\n",
      "Early stopping at epoch 7, Train Loss: 0.814038, Val Loss: 0.694961\n",
      "    Final - Train Loss: 0.814038, Val Loss: 0.694961\n",
      "[Saved] NN5_w252_2020Q3.pth\n",
      "    Training data size: 239677 samples\n",
      "  Quarter 2020Q4: Training models with expanding data\n",
      "    Added 3151 samples from 2020Q3 to training set\n",
      "    [Re-tuning] NN1 for 2020Q4\n",
      "[Hyper] NN1: best_MSE=0.800922, params={'learning_rate': 0.006127290841275993, 'batch_size': 64, 'dropout_rate': 0.21432812451238947}\n",
      "    [Structure Change] dropout_rate changed: 0.4432706353811381 -> 0.21432812451238947\n",
      "    Rebuilding NN1 model due to structure change\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 0.991446, Val Loss: 0.729257\n",
      "    Epoch 5/50, Train Loss: 0.963951, Val Loss: 0.731282\n",
      "Early stopping at epoch 7, Train Loss: 0.953493, Val Loss: 0.735844\n",
      "    Final - Train Loss: 0.953493, Val Loss: 0.735844\n",
      "[Saved] NN1_w252_2020Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.947565, Val Loss: 0.726614\n",
      "    Epoch 5/20, Train Loss: 0.960212, Val Loss: 0.726565\n",
      "    Epoch 10/20, Train Loss: 0.943577, Val Loss: 0.727669\n",
      "Early stopping at epoch 10, Train Loss: 0.943577, Val Loss: 0.727669\n",
      "    Final - Train Loss: 0.943577, Val Loss: 0.727669\n",
      "[Saved] NN2_w252_2020Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.951482, Val Loss: 0.727489\n",
      "    Epoch 5/20, Train Loss: 0.957234, Val Loss: 0.727761\n",
      "Early stopping at epoch 8, Train Loss: 0.943675, Val Loss: 0.728278\n",
      "    Final - Train Loss: 0.943675, Val Loss: 0.728278\n",
      "[Saved] NN3_w252_2020Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.932166, Val Loss: 0.727305\n",
      "    Epoch 5/20, Train Loss: 0.947283, Val Loss: 0.727270\n",
      "Early stopping at epoch 7, Train Loss: 0.934610, Val Loss: 0.726884\n",
      "    Final - Train Loss: 0.934610, Val Loss: 0.726884\n",
      "[Saved] NN4_w252_2020Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.935593, Val Loss: 0.726818\n",
      "    Epoch 5/20, Train Loss: 0.950770, Val Loss: 0.728367\n",
      "Early stopping at epoch 6, Train Loss: 0.941643, Val Loss: 0.727450\n",
      "    Final - Train Loss: 0.941643, Val Loss: 0.727450\n",
      "[Saved] NN5_w252_2020Q4.pth\n",
      "    Training data size: 242828 samples\n",
      "  Quarter 2021Q1: Training models with expanding data\n",
      "    Added 3114 samples from 2020Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.966215, Val Loss: 0.757056\n",
      "    Epoch 5/20, Train Loss: 0.949516, Val Loss: 0.765014\n",
      "Early stopping at epoch 6, Train Loss: 0.942836, Val Loss: 0.765509\n",
      "    Final - Train Loss: 0.942836, Val Loss: 0.765509\n",
      "[Saved] NN1_w252_2021Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.944934, Val Loss: 0.756023\n",
      "    Epoch 5/20, Train Loss: 0.939588, Val Loss: 0.756424\n",
      "Early stopping at epoch 7, Train Loss: 0.938386, Val Loss: 0.757487\n",
      "    Final - Train Loss: 0.938386, Val Loss: 0.757487\n",
      "[Saved] NN2_w252_2021Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.948924, Val Loss: 0.755934\n",
      "    Epoch 5/20, Train Loss: 0.938536, Val Loss: 0.757015\n",
      "Early stopping at epoch 6, Train Loss: 0.934410, Val Loss: 0.756437\n",
      "    Final - Train Loss: 0.934410, Val Loss: 0.756437\n",
      "[Saved] NN3_w252_2021Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.941126, Val Loss: 0.755828\n",
      "    Epoch 5/20, Train Loss: 0.931014, Val Loss: 0.756071\n",
      "Early stopping at epoch 7, Train Loss: 0.919907, Val Loss: 0.756641\n",
      "    Final - Train Loss: 0.919907, Val Loss: 0.756641\n",
      "[Saved] NN4_w252_2021Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.944802, Val Loss: 0.756064\n",
      "    Epoch 5/20, Train Loss: 0.936129, Val Loss: 0.755766\n",
      "    Epoch 10/20, Train Loss: 0.904211, Val Loss: 0.756442\n",
      "Early stopping at epoch 10, Train Loss: 0.904211, Val Loss: 0.756442\n",
      "    Final - Train Loss: 0.904211, Val Loss: 0.756442\n",
      "[Saved] NN5_w252_2021Q1.pth\n",
      "    Training data size: 245942 samples\n",
      "  Quarter 2021Q2: Training models with expanding data\n",
      "    Added 2989 samples from 2021Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.954894, Val Loss: 0.800716\n",
      "    Epoch 5/20, Train Loss: 0.935645, Val Loss: 0.817293\n",
      "Early stopping at epoch 6, Train Loss: 0.931505, Val Loss: 0.836520\n",
      "    Final - Train Loss: 0.931505, Val Loss: 0.836520\n",
      "[Saved] NN1_w252_2021Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.932425, Val Loss: 0.797253\n",
      "    Epoch 5/20, Train Loss: 0.927081, Val Loss: 0.798661\n",
      "Early stopping at epoch 6, Train Loss: 0.925499, Val Loss: 0.797567\n",
      "    Final - Train Loss: 0.925499, Val Loss: 0.797567\n",
      "[Saved] NN2_w252_2021Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.935092, Val Loss: 0.797305\n",
      "    Epoch 5/20, Train Loss: 0.928841, Val Loss: 0.797818\n",
      "Early stopping at epoch 7, Train Loss: 0.919894, Val Loss: 0.801161\n",
      "    Final - Train Loss: 0.919894, Val Loss: 0.801161\n",
      "[Saved] NN3_w252_2021Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.927560, Val Loss: 0.797716\n",
      "    Epoch 5/20, Train Loss: 0.915011, Val Loss: 0.798655\n",
      "    Epoch 10/20, Train Loss: 0.894280, Val Loss: 0.798765\n",
      "Early stopping at epoch 13, Train Loss: 0.874835, Val Loss: 0.798063\n",
      "    Final - Train Loss: 0.874835, Val Loss: 0.798063\n",
      "[Saved] NN4_w252_2021Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.912381, Val Loss: 0.797419\n",
      "    Epoch 5/20, Train Loss: 0.903208, Val Loss: 0.797562\n",
      "Early stopping at epoch 8, Train Loss: 0.883527, Val Loss: 0.797843\n",
      "    Final - Train Loss: 0.883527, Val Loss: 0.797843\n",
      "[Saved] NN5_w252_2021Q2.pth\n",
      "    Training data size: 248931 samples\n",
      "  Quarter 2021Q3: Training models with expanding data\n",
      "    Added 3132 samples from 2021Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.943198, Val Loss: 0.817338\n",
      "    Epoch 5/20, Train Loss: 0.925303, Val Loss: 0.854343\n",
      "Early stopping at epoch 6, Train Loss: 0.919291, Val Loss: 0.878078\n",
      "    Final - Train Loss: 0.919291, Val Loss: 0.878078\n",
      "[Saved] NN1_w252_2021Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.920265, Val Loss: 0.809307\n",
      "    Epoch 5/20, Train Loss: 0.918774, Val Loss: 0.811431\n",
      "Early stopping at epoch 6, Train Loss: 0.916240, Val Loss: 0.812755\n",
      "    Final - Train Loss: 0.916240, Val Loss: 0.812755\n",
      "[Saved] NN2_w252_2021Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.922473, Val Loss: 0.809351\n",
      "    Epoch 5/20, Train Loss: 0.914082, Val Loss: 0.809337\n",
      "Early stopping at epoch 8, Train Loss: 0.902314, Val Loss: 0.811291\n",
      "    Final - Train Loss: 0.902314, Val Loss: 0.811291\n",
      "[Saved] NN3_w252_2021Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.880739, Val Loss: 0.809820\n",
      "    Epoch 5/20, Train Loss: 0.870031, Val Loss: 0.810458\n",
      "Early stopping at epoch 7, Train Loss: 0.862728, Val Loss: 0.811459\n",
      "    Final - Train Loss: 0.862728, Val Loss: 0.811459\n",
      "[Saved] NN4_w252_2021Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.891428, Val Loss: 0.809114\n",
      "    Epoch 5/20, Train Loss: 0.877243, Val Loss: 0.809016\n",
      "    Epoch 10/20, Train Loss: 0.854514, Val Loss: 0.809817\n",
      "Early stopping at epoch 10, Train Loss: 0.854514, Val Loss: 0.809817\n",
      "    Final - Train Loss: 0.854514, Val Loss: 0.809817\n",
      "[Saved] NN5_w252_2021Q3.pth\n",
      "    Training data size: 252063 samples\n",
      "  Quarter 2021Q4: Training models with expanding data\n",
      "    Added 3173 samples from 2021Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.932295, Val Loss: 0.837408\n",
      "    Epoch 5/20, Train Loss: 0.919039, Val Loss: 0.886518\n",
      "Early stopping at epoch 6, Train Loss: 0.914830, Val Loss: 0.922418\n",
      "    Final - Train Loss: 0.914830, Val Loss: 0.922418\n",
      "[Saved] NN1_w252_2021Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.910651, Val Loss: 0.819167\n",
      "    Epoch 5/20, Train Loss: 0.912115, Val Loss: 0.820649\n",
      "Early stopping at epoch 8, Train Loss: 0.904148, Val Loss: 0.819553\n",
      "    Final - Train Loss: 0.904148, Val Loss: 0.819553\n",
      "[Saved] NN2_w252_2021Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.903548, Val Loss: 0.818568\n",
      "    Epoch 5/20, Train Loss: 0.895018, Val Loss: 0.821068\n",
      "Early stopping at epoch 6, Train Loss: 0.890655, Val Loss: 0.819495\n",
      "    Final - Train Loss: 0.890655, Val Loss: 0.819495\n",
      "[Saved] NN3_w252_2021Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.869972, Val Loss: 0.819906\n",
      "    Epoch 5/20, Train Loss: 0.859928, Val Loss: 0.820533\n",
      "Early stopping at epoch 7, Train Loss: 0.855211, Val Loss: 0.818880\n",
      "    Final - Train Loss: 0.855211, Val Loss: 0.818880\n",
      "[Saved] NN4_w252_2021Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.864382, Val Loss: 0.818041\n",
      "    Epoch 5/20, Train Loss: 0.846789, Val Loss: 0.818431\n",
      "    Epoch 10/20, Train Loss: 0.824527, Val Loss: 0.818293\n",
      "Early stopping at epoch 14, Train Loss: 0.806311, Val Loss: 0.818235\n",
      "    Final - Train Loss: 0.806311, Val Loss: 0.818235\n",
      "[Saved] NN5_w252_2021Q4.pth\n",
      "    Training data size: 255236 samples\n",
      "  Quarter 2022Q1: Training models with expanding data\n",
      "    Added 3153 samples from 2021Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.922361, Val Loss: 0.849774\n",
      "    Epoch 5/20, Train Loss: 0.911108, Val Loss: 0.912022\n",
      "Early stopping at epoch 6, Train Loss: 0.912954, Val Loss: 0.912902\n",
      "    Final - Train Loss: 0.912954, Val Loss: 0.912902\n",
      "[Saved] NN1_w252_2022Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.902538, Val Loss: 0.830539\n",
      "    Epoch 5/20, Train Loss: 0.902019, Val Loss: 0.831510\n",
      "Early stopping at epoch 6, Train Loss: 0.898386, Val Loss: 0.831016\n",
      "    Final - Train Loss: 0.898386, Val Loss: 0.831016\n",
      "[Saved] NN2_w252_2022Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.891960, Val Loss: 0.831399\n",
      "    Epoch 5/20, Train Loss: 0.886045, Val Loss: 0.831071\n",
      "Early stopping at epoch 7, Train Loss: 0.883059, Val Loss: 0.831746\n",
      "    Final - Train Loss: 0.883059, Val Loss: 0.831746\n",
      "[Saved] NN3_w252_2022Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.853774, Val Loss: 0.830327\n",
      "    Epoch 5/20, Train Loss: 0.848848, Val Loss: 0.830284\n",
      "    Epoch 10/20, Train Loss: 0.832707, Val Loss: 0.830191\n",
      "Early stopping at epoch 13, Train Loss: 0.825241, Val Loss: 0.830820\n",
      "    Final - Train Loss: 0.825241, Val Loss: 0.830820\n",
      "[Saved] NN4_w252_2022Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.812474, Val Loss: 0.830703\n",
      "    Epoch 5/20, Train Loss: 0.806547, Val Loss: 0.830615\n",
      "    Epoch 10/20, Train Loss: 0.788787, Val Loss: 0.831086\n",
      "    Epoch 15/20, Train Loss: 0.761526, Val Loss: 0.831570\n",
      "Early stopping at epoch 17, Train Loss: 0.750219, Val Loss: 0.832365\n",
      "    Final - Train Loss: 0.750219, Val Loss: 0.832365\n",
      "[Saved] NN5_w252_2022Q1.pth\n",
      "    Training data size: 258389 samples\n",
      "  Quarter 2022Q2: Training models with expanding data\n",
      "    Added 3029 samples from 2022Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.915246, Val Loss: 0.900617\n",
      "    Epoch 5/20, Train Loss: 0.905826, Val Loss: 0.969246\n",
      "Early stopping at epoch 6, Train Loss: 0.900519, Val Loss: 0.967324\n",
      "    Final - Train Loss: 0.900519, Val Loss: 0.967324\n",
      "[Saved] NN1_w252_2022Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.894228, Val Loss: 0.866069\n",
      "    Epoch 5/20, Train Loss: 0.892792, Val Loss: 0.865371\n",
      "    Epoch 10/20, Train Loss: 0.889608, Val Loss: 0.866768\n",
      "Early stopping at epoch 10, Train Loss: 0.889608, Val Loss: 0.866768\n",
      "    Final - Train Loss: 0.889608, Val Loss: 0.866768\n",
      "[Saved] NN2_w252_2022Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.881529, Val Loss: 0.867554\n",
      "    Epoch 5/20, Train Loss: 0.877802, Val Loss: 0.866567\n",
      "Early stopping at epoch 8, Train Loss: 0.867100, Val Loss: 0.867886\n",
      "    Final - Train Loss: 0.867100, Val Loss: 0.867886\n",
      "[Saved] NN3_w252_2022Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.831643, Val Loss: 0.865423\n",
      "    Epoch 5/20, Train Loss: 0.822345, Val Loss: 0.865969\n",
      "    Epoch 10/20, Train Loss: 0.810294, Val Loss: 0.866159\n",
      "Early stopping at epoch 12, Train Loss: 0.806777, Val Loss: 0.866429\n",
      "    Final - Train Loss: 0.806777, Val Loss: 0.866429\n",
      "[Saved] NN4_w252_2022Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.774624, Val Loss: 0.869304\n",
      "    Epoch 5/20, Train Loss: 0.768759, Val Loss: 0.868518\n",
      "Early stopping at epoch 8, Train Loss: 0.762504, Val Loss: 0.868819\n",
      "    Final - Train Loss: 0.762504, Val Loss: 0.868819\n",
      "[Saved] NN5_w252_2022Q2.pth\n",
      "    Training data size: 261418 samples\n",
      "  Quarter 2022Q3: Training models with expanding data\n",
      "    Added 2969 samples from 2022Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.909617, Val Loss: 0.962318\n",
      "    Epoch 5/20, Train Loss: 0.899701, Val Loss: 1.008110\n",
      "Early stopping at epoch 7, Train Loss: 0.896484, Val Loss: 1.009176\n",
      "    Final - Train Loss: 0.896484, Val Loss: 1.009176\n",
      "[Saved] NN1_w252_2022Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.888161, Val Loss: 0.908100\n",
      "    Epoch 5/20, Train Loss: 0.883389, Val Loss: 0.908409\n",
      "    Epoch 10/20, Train Loss: 0.879873, Val Loss: 0.907505\n",
      "Early stopping at epoch 12, Train Loss: 0.876975, Val Loss: 0.907944\n",
      "    Final - Train Loss: 0.876975, Val Loss: 0.907944\n",
      "[Saved] NN2_w252_2022Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.870391, Val Loss: 0.909528\n",
      "    Epoch 5/20, Train Loss: 0.865142, Val Loss: 0.909110\n",
      "Early stopping at epoch 8, Train Loss: 0.857929, Val Loss: 0.908371\n",
      "    Final - Train Loss: 0.857929, Val Loss: 0.908371\n",
      "[Saved] NN3_w252_2022Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.809118, Val Loss: 0.906387\n",
      "    Epoch 5/20, Train Loss: 0.809238, Val Loss: 0.907152\n",
      "Early stopping at epoch 6, Train Loss: 0.803668, Val Loss: 0.907621\n",
      "    Final - Train Loss: 0.803668, Val Loss: 0.907621\n",
      "[Saved] NN4_w252_2022Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.769988, Val Loss: 0.908327\n",
      "    Epoch 5/20, Train Loss: 0.764713, Val Loss: 0.908520\n",
      "Early stopping at epoch 7, Train Loss: 0.757414, Val Loss: 0.909708\n",
      "    Final - Train Loss: 0.757414, Val Loss: 0.909708\n",
      "[Saved] NN5_w252_2022Q3.pth\n",
      "    Training data size: 264387 samples\n",
      "  Quarter 2022Q4: Training models with expanding data\n",
      "    Added 3152 samples from 2022Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.899352, Val Loss: 0.971080\n",
      "    Epoch 5/20, Train Loss: 0.894694, Val Loss: 1.023468\n",
      "Early stopping at epoch 6, Train Loss: 0.893056, Val Loss: 1.031960\n",
      "    Final - Train Loss: 0.893056, Val Loss: 1.031960\n",
      "[Saved] NN1_w252_2022Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.872684, Val Loss: 0.926524\n",
      "    Epoch 5/20, Train Loss: 0.870761, Val Loss: 0.926895\n",
      "Early stopping at epoch 7, Train Loss: 0.869168, Val Loss: 0.927373\n",
      "    Final - Train Loss: 0.869168, Val Loss: 0.927373\n",
      "[Saved] NN2_w252_2022Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.859709, Val Loss: 0.929015\n",
      "    Epoch 5/20, Train Loss: 0.855883, Val Loss: 0.929640\n",
      "Early stopping at epoch 7, Train Loss: 0.851607, Val Loss: 0.931229\n",
      "    Final - Train Loss: 0.851607, Val Loss: 0.931229\n",
      "[Saved] NN3_w252_2022Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.805542, Val Loss: 0.926045\n",
      "    Epoch 5/20, Train Loss: 0.800497, Val Loss: 0.926296\n",
      "Early stopping at epoch 6, Train Loss: 0.799740, Val Loss: 0.926340\n",
      "    Final - Train Loss: 0.799740, Val Loss: 0.926340\n",
      "[Saved] NN4_w252_2022Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.758769, Val Loss: 0.926107\n",
      "    Epoch 5/20, Train Loss: 0.755322, Val Loss: 0.926797\n",
      "Early stopping at epoch 6, Train Loss: 0.756851, Val Loss: 0.926858\n",
      "    Final - Train Loss: 0.756851, Val Loss: 0.926858\n",
      "[Saved] NN5_w252_2022Q4.pth\n",
      "    Training data size: 267539 samples\n",
      "  Quarter 2023Q1: Training models with expanding data\n",
      "    Added 3070 samples from 2022Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.890092, Val Loss: 1.004638\n",
      "    Epoch 5/20, Train Loss: 0.888407, Val Loss: 1.031380\n",
      "Early stopping at epoch 6, Train Loss: 0.886656, Val Loss: 1.022536\n",
      "    Final - Train Loss: 0.886656, Val Loss: 1.022536\n",
      "[Saved] NN1_w252_2023Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.866748, Val Loss: 0.960796\n",
      "    Epoch 5/20, Train Loss: 0.869558, Val Loss: 0.961156\n",
      "Early stopping at epoch 7, Train Loss: 0.865440, Val Loss: 0.961384\n",
      "    Final - Train Loss: 0.865440, Val Loss: 0.961384\n",
      "[Saved] NN2_w252_2023Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.848370, Val Loss: 0.961242\n",
      "    Epoch 5/20, Train Loss: 0.846933, Val Loss: 0.960999\n",
      "Early stopping at epoch 7, Train Loss: 0.841597, Val Loss: 0.963345\n",
      "    Final - Train Loss: 0.841597, Val Loss: 0.963345\n",
      "[Saved] NN3_w252_2023Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.799354, Val Loss: 0.960415\n",
      "    Epoch 5/20, Train Loss: 0.796010, Val Loss: 0.959465\n",
      "Early stopping at epoch 8, Train Loss: 0.789322, Val Loss: 0.959795\n",
      "    Final - Train Loss: 0.789322, Val Loss: 0.959795\n",
      "[Saved] NN4_w252_2023Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.752483, Val Loss: 0.962734\n",
      "    Epoch 5/20, Train Loss: 0.751045, Val Loss: 0.963971\n",
      "Early stopping at epoch 7, Train Loss: 0.743172, Val Loss: 0.965820\n",
      "    Final - Train Loss: 0.743172, Val Loss: 0.965820\n",
      "[Saved] NN5_w252_2023Q1.pth\n",
      "    Training data size: 270609 samples\n",
      "  Quarter 2023Q2: Training models with expanding data\n",
      "    Added 3064 samples from 2023Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.886348, Val Loss: 1.047781\n",
      "    Epoch 5/20, Train Loss: 0.882117, Val Loss: 1.103902\n",
      "Early stopping at epoch 6, Train Loss: 0.879968, Val Loss: 1.120938\n",
      "    Final - Train Loss: 0.879968, Val Loss: 1.120938\n",
      "[Saved] NN1_w252_2023Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.859971, Val Loss: 0.974875\n",
      "    Epoch 5/20, Train Loss: 0.862443, Val Loss: 0.975442\n",
      "Early stopping at epoch 6, Train Loss: 0.861163, Val Loss: 0.975370\n",
      "    Final - Train Loss: 0.861163, Val Loss: 0.975370\n",
      "[Saved] NN2_w252_2023Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.838613, Val Loss: 0.975361\n",
      "    Epoch 5/20, Train Loss: 0.835949, Val Loss: 0.975222\n",
      "    Epoch 10/20, Train Loss: 0.829572, Val Loss: 0.976487\n",
      "Early stopping at epoch 13, Train Loss: 0.823420, Val Loss: 0.977244\n",
      "    Final - Train Loss: 0.823420, Val Loss: 0.977244\n",
      "[Saved] NN3_w252_2023Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.788932, Val Loss: 0.973470\n",
      "    Epoch 5/20, Train Loss: 0.792559, Val Loss: 0.973911\n",
      "Early stopping at epoch 9, Train Loss: 0.781457, Val Loss: 0.973584\n",
      "    Final - Train Loss: 0.781457, Val Loss: 0.973584\n",
      "[Saved] NN4_w252_2023Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.747311, Val Loss: 0.976773\n",
      "    Epoch 5/20, Train Loss: 0.739163, Val Loss: 0.976721\n",
      "    Epoch 10/20, Train Loss: 0.733942, Val Loss: 0.976570\n",
      "Early stopping at epoch 12, Train Loss: 0.732821, Val Loss: 0.977586\n",
      "    Final - Train Loss: 0.732821, Val Loss: 0.977586\n",
      "[Saved] NN5_w252_2023Q2.pth\n",
      "    Training data size: 273673 samples\n",
      "  Quarter 2023Q3: Training models with expanding data\n",
      "    Added 3069 samples from 2023Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.885878, Val Loss: 1.020930\n",
      "    Epoch 5/20, Train Loss: 0.883660, Val Loss: 1.054205\n",
      "Early stopping at epoch 7, Train Loss: 0.879914, Val Loss: 1.058837\n",
      "    Final - Train Loss: 0.879914, Val Loss: 1.058837\n",
      "[Saved] NN1_w252_2023Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.857903, Val Loss: 0.959938\n",
      "    Epoch 5/20, Train Loss: 0.861434, Val Loss: 0.960676\n",
      "Early stopping at epoch 7, Train Loss: 0.859291, Val Loss: 0.959648\n",
      "    Final - Train Loss: 0.859291, Val Loss: 0.959648\n",
      "[Saved] NN2_w252_2023Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.827170, Val Loss: 0.960477\n",
      "    Epoch 5/20, Train Loss: 0.824311, Val Loss: 0.962547\n",
      "Early stopping at epoch 7, Train Loss: 0.826468, Val Loss: 0.960735\n",
      "    Final - Train Loss: 0.826468, Val Loss: 0.960735\n",
      "[Saved] NN3_w252_2023Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.786296, Val Loss: 0.960450\n",
      "    Epoch 5/20, Train Loss: 0.782523, Val Loss: 0.960353\n",
      "Early stopping at epoch 7, Train Loss: 0.782345, Val Loss: 0.961020\n",
      "    Final - Train Loss: 0.782345, Val Loss: 0.961020\n",
      "[Saved] NN4_w252_2023Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.734847, Val Loss: 0.959122\n",
      "    Epoch 5/20, Train Loss: 0.734896, Val Loss: 0.960736\n",
      "Early stopping at epoch 9, Train Loss: 0.724456, Val Loss: 0.963371\n",
      "    Final - Train Loss: 0.724456, Val Loss: 0.963371\n",
      "[Saved] NN5_w252_2023Q3.pth\n",
      "    Training data size: 276742 samples\n",
      "  Quarter 2023Q4: Training models with expanding data\n",
      "    Added 3121 samples from 2023Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.881348, Val Loss: 0.998547\n",
      "    Epoch 5/20, Train Loss: 0.878962, Val Loss: 1.018052\n",
      "Early stopping at epoch 6, Train Loss: 0.876513, Val Loss: 1.038390\n",
      "    Final - Train Loss: 0.876513, Val Loss: 1.038390\n",
      "[Saved] NN1_w252_2023Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.855271, Val Loss: 0.948784\n",
      "    Epoch 5/20, Train Loss: 0.859871, Val Loss: 0.948885\n",
      "Early stopping at epoch 7, Train Loss: 0.858192, Val Loss: 0.949506\n",
      "    Final - Train Loss: 0.858192, Val Loss: 0.949506\n",
      "[Saved] NN2_w252_2023Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.821922, Val Loss: 0.948897\n",
      "    Epoch 5/20, Train Loss: 0.822638, Val Loss: 0.948477\n",
      "    Epoch 10/20, Train Loss: 0.813837, Val Loss: 0.950138\n",
      "Early stopping at epoch 12, Train Loss: 0.811131, Val Loss: 0.950572\n",
      "    Final - Train Loss: 0.811131, Val Loss: 0.950572\n",
      "[Saved] NN3_w252_2023Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.783062, Val Loss: 0.948245\n",
      "    Epoch 5/20, Train Loss: 0.781148, Val Loss: 0.948477\n",
      "    Epoch 10/20, Train Loss: 0.774749, Val Loss: 0.948673\n",
      "Early stopping at epoch 12, Train Loss: 0.767703, Val Loss: 0.951526\n",
      "    Final - Train Loss: 0.767703, Val Loss: 0.951526\n",
      "[Saved] NN4_w252_2023Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.733344, Val Loss: 0.949827\n",
      "    Epoch 5/20, Train Loss: 0.726059, Val Loss: 0.950044\n",
      "    Epoch 10/20, Train Loss: 0.717965, Val Loss: 0.952572\n",
      "Early stopping at epoch 11, Train Loss: 0.717706, Val Loss: 0.950609\n",
      "    Final - Train Loss: 0.717706, Val Loss: 0.950609\n",
      "[Saved] NN5_w252_2023Q4.pth\n",
      "    Training data size: 279863 samples\n",
      "  Quarter 2024Q1: Training models with expanding data\n",
      "    Added 3113 samples from 2023Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.874767, Val Loss: 1.018727\n",
      "    Epoch 5/20, Train Loss: 0.873999, Val Loss: 1.049998\n",
      "Early stopping at epoch 7, Train Loss: 0.870396, Val Loss: 1.060484\n",
      "    Final - Train Loss: 0.870396, Val Loss: 1.060484\n",
      "[Saved] NN1_w252_2024Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.852633, Val Loss: 0.960594\n",
      "    Epoch 5/20, Train Loss: 0.854758, Val Loss: 0.960689\n",
      "Early stopping at epoch 8, Train Loss: 0.851180, Val Loss: 0.963087\n",
      "    Final - Train Loss: 0.851180, Val Loss: 0.963087\n",
      "[Saved] NN2_w252_2024Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.807315, Val Loss: 0.963446\n",
      "    Epoch 5/20, Train Loss: 0.803905, Val Loss: 0.961994\n",
      "    Epoch 10/20, Train Loss: 0.798769, Val Loss: 0.963535\n",
      "Early stopping at epoch 13, Train Loss: 0.798966, Val Loss: 0.961990\n",
      "    Final - Train Loss: 0.798966, Val Loss: 0.961990\n",
      "[Saved] NN3_w252_2024Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.767748, Val Loss: 0.959866\n",
      "    Epoch 5/20, Train Loss: 0.765833, Val Loss: 0.959922\n",
      "Early stopping at epoch 9, Train Loss: 0.758161, Val Loss: 0.962207\n",
      "    Final - Train Loss: 0.758161, Val Loss: 0.962207\n",
      "[Saved] NN4_w252_2024Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.718232, Val Loss: 0.960582\n",
      "    Epoch 5/20, Train Loss: 0.714779, Val Loss: 0.961770\n",
      "Early stopping at epoch 6, Train Loss: 0.715578, Val Loss: 0.961811\n",
      "    Final - Train Loss: 0.715578, Val Loss: 0.961811\n",
      "[Saved] NN5_w252_2024Q1.pth\n",
      "    Training data size: 282976 samples\n",
      "  Quarter 2024Q2: Training models with expanding data\n",
      "    Added 3026 samples from 2024Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.869166, Val Loss: 1.036484\n",
      "    Epoch 5/20, Train Loss: 0.868037, Val Loss: 1.086716\n",
      "Early stopping at epoch 6, Train Loss: 0.866561, Val Loss: 1.142720\n",
      "    Final - Train Loss: 0.866561, Val Loss: 1.142720\n",
      "[Saved] NN1_w252_2024Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.846237, Val Loss: 0.962055\n",
      "    Epoch 5/20, Train Loss: 0.847988, Val Loss: 0.962947\n",
      "Early stopping at epoch 6, Train Loss: 0.845937, Val Loss: 0.964119\n",
      "    Final - Train Loss: 0.845937, Val Loss: 0.964119\n",
      "[Saved] NN2_w252_2024Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.794670, Val Loss: 0.965251\n",
      "    Epoch 5/20, Train Loss: 0.790873, Val Loss: 0.963087\n",
      "Early stopping at epoch 8, Train Loss: 0.789686, Val Loss: 0.964948\n",
      "    Final - Train Loss: 0.789686, Val Loss: 0.964948\n",
      "[Saved] NN3_w252_2024Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.760266, Val Loss: 0.962343\n",
      "    Epoch 5/20, Train Loss: 0.756801, Val Loss: 0.961945\n",
      "    Epoch 10/20, Train Loss: 0.752877, Val Loss: 0.962067\n",
      "Early stopping at epoch 12, Train Loss: 0.747122, Val Loss: 0.962464\n",
      "    Final - Train Loss: 0.747122, Val Loss: 0.962464\n",
      "[Saved] NN4_w252_2024Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.715962, Val Loss: 0.963749\n",
      "    Epoch 5/20, Train Loss: 0.718239, Val Loss: 0.967587\n",
      "Early stopping at epoch 7, Train Loss: 0.708871, Val Loss: 0.963539\n",
      "    Final - Train Loss: 0.708871, Val Loss: 0.963539\n",
      "[Saved] NN5_w252_2024Q2.pth\n",
      "    Training data size: 286002 samples\n",
      "  Quarter 2024Q3: Training models with expanding data\n",
      "    Added 3112 samples from 2024Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.866638, Val Loss: 1.014088\n",
      "    Epoch 5/20, Train Loss: 0.867479, Val Loss: 1.028283\n",
      "Early stopping at epoch 7, Train Loss: 0.870129, Val Loss: 1.049983\n",
      "    Final - Train Loss: 0.870129, Val Loss: 1.049983\n",
      "[Saved] NN1_w252_2024Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.846133, Val Loss: 0.948990\n",
      "    Epoch 5/20, Train Loss: 0.848679, Val Loss: 0.950646\n",
      "Early stopping at epoch 9, Train Loss: 0.844154, Val Loss: 0.949113\n",
      "    Final - Train Loss: 0.844154, Val Loss: 0.949113\n",
      "[Saved] NN2_w252_2024Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.789642, Val Loss: 0.949490\n",
      "    Epoch 5/20, Train Loss: 0.788783, Val Loss: 0.953870\n",
      "Early stopping at epoch 9, Train Loss: 0.785041, Val Loss: 0.952413\n",
      "    Final - Train Loss: 0.785041, Val Loss: 0.952413\n",
      "[Saved] NN3_w252_2024Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.748558, Val Loss: 0.948289\n",
      "    Epoch 5/20, Train Loss: 0.750431, Val Loss: 0.948144\n",
      "    Epoch 10/20, Train Loss: 0.745044, Val Loss: 0.948698\n",
      "Early stopping at epoch 10, Train Loss: 0.745044, Val Loss: 0.948698\n",
      "    Final - Train Loss: 0.745044, Val Loss: 0.948698\n",
      "[Saved] NN4_w252_2024Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.713594, Val Loss: 0.948259\n",
      "    Epoch 5/20, Train Loss: 0.715167, Val Loss: 0.948855\n",
      "Early stopping at epoch 7, Train Loss: 0.710246, Val Loss: 0.949532\n",
      "    Final - Train Loss: 0.710246, Val Loss: 0.949532\n",
      "[Saved] NN5_w252_2024Q3.pth\n",
      "    Training data size: 289114 samples\n",
      "Processing window size: 512\n",
      "  Quarter 2015Q4: Training models with expanding data\n",
      "    [Re-tuning] NN1 for 2015Q4\n",
      "[Hyper] NN1: best_MSE=0.916965, params={'learning_rate': 0.007195471200967417, 'batch_size': 256, 'dropout_rate': 0.23356280759810694}\n",
      "    [Structure Change] dropout_rate changed: 0.1 -> 0.23356280759810694\n",
      "    Rebuilding NN1 model due to structure change\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 1.165117, Val Loss: 0.531358\n",
      "    Epoch 5/50, Train Loss: 1.077810, Val Loss: 0.534308\n",
      "Early stopping at epoch 6, Train Loss: 1.065677, Val Loss: 0.534821\n",
      "    Final - Train Loss: 1.065677, Val Loss: 0.534821\n",
      "[Saved] NN1_w512_2015Q4.pth\n",
      "    Creating new NN2 model\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 1.166956, Val Loss: 0.532440\n",
      "    Epoch 5/50, Train Loss: 1.084989, Val Loss: 0.532147\n",
      "Early stopping at epoch 8, Train Loss: 1.052911, Val Loss: 0.533763\n",
      "    Final - Train Loss: 1.052911, Val Loss: 0.533763\n",
      "[Saved] NN2_w512_2015Q4.pth\n",
      "    Creating new NN3 model\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 1.164937, Val Loss: 0.531551\n",
      "    Epoch 5/50, Train Loss: 1.090323, Val Loss: 0.531898\n",
      "Early stopping at epoch 7, Train Loss: 1.066791, Val Loss: 0.533055\n",
      "    Final - Train Loss: 1.066791, Val Loss: 0.533055\n",
      "[Saved] NN3_w512_2015Q4.pth\n",
      "    Creating new NN4 model\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 1.158399, Val Loss: 0.532784\n",
      "    Epoch 5/50, Train Loss: 1.088928, Val Loss: 0.531248\n",
      "Early stopping at epoch 7, Train Loss: 1.062558, Val Loss: 0.532041\n",
      "    Final - Train Loss: 1.062558, Val Loss: 0.532041\n",
      "[Saved] NN4_w512_2015Q4.pth\n",
      "    Creating new NN5 model\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 1.158679, Val Loss: 0.529378\n",
      "    Epoch 5/50, Train Loss: 1.087600, Val Loss: 0.531677\n",
      "Early stopping at epoch 6, Train Loss: 1.076696, Val Loss: 0.531981\n",
      "    Final - Train Loss: 1.076696, Val Loss: 0.531981\n",
      "[Saved] NN5_w512_2015Q4.pth\n",
      "    Training data size: 171570 samples\n",
      "  Quarter 2016Q1: Training models with expanding data\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.111205, Val Loss: 0.532076\n",
      "    Epoch 5/20, Train Loss: 1.062875, Val Loss: 0.534024\n",
      "Early stopping at epoch 7, Train Loss: 1.034363, Val Loss: 0.537224\n",
      "    Final - Train Loss: 1.034363, Val Loss: 0.537224\n",
      "[Saved] NN1_w512_2016Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.100649, Val Loss: 0.530650\n",
      "    Epoch 5/20, Train Loss: 1.054848, Val Loss: 0.531547\n",
      "Early stopping at epoch 6, Train Loss: 1.045002, Val Loss: 0.532562\n",
      "    Final - Train Loss: 1.045002, Val Loss: 0.532562\n",
      "[Saved] NN2_w512_2016Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.113227, Val Loss: 0.529751\n",
      "    Epoch 5/20, Train Loss: 1.069627, Val Loss: 0.534580\n",
      "Early stopping at epoch 6, Train Loss: 1.048971, Val Loss: 0.536715\n",
      "    Final - Train Loss: 1.048971, Val Loss: 0.536715\n",
      "[Saved] NN3_w512_2016Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.113517, Val Loss: 0.529261\n",
      "    Epoch 5/20, Train Loss: 1.069373, Val Loss: 0.531555\n",
      "Early stopping at epoch 6, Train Loss: 1.054216, Val Loss: 0.535434\n",
      "    Final - Train Loss: 1.054216, Val Loss: 0.535434\n",
      "[Saved] NN4_w512_2016Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.119523, Val Loss: 0.529202\n",
      "    Epoch 5/20, Train Loss: 1.084479, Val Loss: 0.530967\n",
      "Early stopping at epoch 6, Train Loss: 1.056257, Val Loss: 0.532038\n",
      "    Final - Train Loss: 1.056257, Val Loss: 0.532038\n",
      "[Saved] NN5_w512_2016Q1.pth\n",
      "    Training data size: 171570 samples\n",
      "  Quarter 2016Q2: Training models with expanding data\n",
      "    Added 2956 samples from 2016Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.078727, Val Loss: 0.548307\n",
      "    Epoch 5/20, Train Loss: 1.025858, Val Loss: 0.550657\n",
      "Early stopping at epoch 6, Train Loss: 1.016389, Val Loss: 0.550811\n",
      "    Final - Train Loss: 1.016389, Val Loss: 0.550811\n",
      "[Saved] NN1_w512_2016Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.079708, Val Loss: 0.545833\n",
      "    Epoch 5/20, Train Loss: 1.034917, Val Loss: 0.549486\n",
      "Early stopping at epoch 6, Train Loss: 1.021266, Val Loss: 0.549837\n",
      "    Final - Train Loss: 1.021266, Val Loss: 0.549837\n",
      "[Saved] NN2_w512_2016Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.093945, Val Loss: 0.544517\n",
      "    Epoch 5/20, Train Loss: 1.047870, Val Loss: 0.549098\n",
      "Early stopping at epoch 6, Train Loss: 1.035732, Val Loss: 0.549135\n",
      "    Final - Train Loss: 1.035732, Val Loss: 0.549135\n",
      "[Saved] NN3_w512_2016Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.092940, Val Loss: 0.545063\n",
      "    Epoch 5/20, Train Loss: 1.027439, Val Loss: 0.547176\n",
      "Early stopping at epoch 6, Train Loss: 1.003281, Val Loss: 0.549836\n",
      "    Final - Train Loss: 1.003281, Val Loss: 0.549836\n",
      "[Saved] NN4_w512_2016Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.102115, Val Loss: 0.544155\n",
      "    Epoch 5/20, Train Loss: 1.036237, Val Loss: 0.549413\n",
      "Early stopping at epoch 6, Train Loss: 1.001221, Val Loss: 0.555553\n",
      "    Final - Train Loss: 1.001221, Val Loss: 0.555553\n",
      "[Saved] NN5_w512_2016Q2.pth\n",
      "    Training data size: 174526 samples\n",
      "  Quarter 2016Q3: Training models with expanding data\n",
      "    Added 3170 samples from 2016Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.052000, Val Loss: 0.558989\n",
      "    Epoch 5/20, Train Loss: 1.009192, Val Loss: 0.562242\n",
      "Early stopping at epoch 6, Train Loss: 0.995048, Val Loss: 0.562189\n",
      "    Final - Train Loss: 0.995048, Val Loss: 0.562189\n",
      "[Saved] NN1_w512_2016Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.057385, Val Loss: 0.557663\n",
      "    Epoch 5/20, Train Loss: 1.011716, Val Loss: 0.559521\n",
      "Early stopping at epoch 6, Train Loss: 1.001072, Val Loss: 0.560825\n",
      "    Final - Train Loss: 1.001072, Val Loss: 0.560825\n",
      "[Saved] NN2_w512_2016Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.069016, Val Loss: 0.556629\n",
      "    Epoch 5/20, Train Loss: 1.022181, Val Loss: 0.559593\n",
      "Early stopping at epoch 6, Train Loss: 1.005485, Val Loss: 0.562923\n",
      "    Final - Train Loss: 1.005485, Val Loss: 0.562923\n",
      "[Saved] NN3_w512_2016Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.067470, Val Loss: 0.556893\n",
      "    Epoch 5/20, Train Loss: 0.995206, Val Loss: 0.560435\n",
      "Early stopping at epoch 7, Train Loss: 0.935055, Val Loss: 0.563169\n",
      "    Final - Train Loss: 0.935055, Val Loss: 0.563169\n",
      "[Saved] NN4_w512_2016Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.073305, Val Loss: 0.556191\n",
      "    Epoch 5/20, Train Loss: 0.983851, Val Loss: 0.564593\n",
      "Early stopping at epoch 6, Train Loss: 0.944762, Val Loss: 0.567507\n",
      "    Final - Train Loss: 0.944762, Val Loss: 0.567507\n",
      "[Saved] NN5_w512_2016Q3.pth\n",
      "    Training data size: 177696 samples\n",
      "  Quarter 2016Q4: Training models with expanding data\n",
      "    Added 3176 samples from 2016Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.029738, Val Loss: 0.553001\n",
      "    Epoch 5/20, Train Loss: 0.989130, Val Loss: 0.555468\n",
      "Early stopping at epoch 6, Train Loss: 0.981209, Val Loss: 0.556534\n",
      "    Final - Train Loss: 0.981209, Val Loss: 0.556534\n",
      "[Saved] NN1_w512_2016Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.032612, Val Loss: 0.549102\n",
      "    Epoch 5/20, Train Loss: 0.989622, Val Loss: 0.552997\n",
      "Early stopping at epoch 6, Train Loss: 0.978678, Val Loss: 0.552980\n",
      "    Final - Train Loss: 0.978678, Val Loss: 0.552980\n",
      "[Saved] NN2_w512_2016Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.045969, Val Loss: 0.550065\n",
      "    Epoch 5/20, Train Loss: 0.986918, Val Loss: 0.553056\n",
      "Early stopping at epoch 7, Train Loss: 0.951538, Val Loss: 0.553787\n",
      "    Final - Train Loss: 0.951538, Val Loss: 0.553787\n",
      "[Saved] NN3_w512_2016Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.022257, Val Loss: 0.550073\n",
      "    Epoch 5/20, Train Loss: 0.926975, Val Loss: 0.554437\n",
      "Early stopping at epoch 7, Train Loss: 0.870721, Val Loss: 0.558115\n",
      "    Final - Train Loss: 0.870721, Val Loss: 0.558115\n",
      "[Saved] NN4_w512_2016Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.045646, Val Loss: 0.550124\n",
      "    Epoch 5/20, Train Loss: 0.942758, Val Loss: 0.561062\n",
      "Early stopping at epoch 6, Train Loss: 0.903085, Val Loss: 0.565925\n",
      "    Final - Train Loss: 0.903085, Val Loss: 0.565925\n",
      "[Saved] NN5_w512_2016Q4.pth\n",
      "    Training data size: 180872 samples\n",
      "  Quarter 2017Q1: Training models with expanding data\n",
      "    Added 3123 samples from 2016Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.006559, Val Loss: 0.563473\n",
      "    Epoch 5/20, Train Loss: 0.973742, Val Loss: 0.566630\n",
      "Early stopping at epoch 6, Train Loss: 0.966104, Val Loss: 0.567719\n",
      "    Final - Train Loss: 0.966104, Val Loss: 0.567719\n",
      "[Saved] NN1_w512_2017Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.007096, Val Loss: 0.558536\n",
      "    Epoch 5/20, Train Loss: 0.973711, Val Loss: 0.559851\n",
      "Early stopping at epoch 6, Train Loss: 0.966273, Val Loss: 0.559847\n",
      "    Final - Train Loss: 0.966273, Val Loss: 0.559847\n",
      "[Saved] NN2_w512_2017Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.010249, Val Loss: 0.558550\n",
      "    Epoch 5/20, Train Loss: 0.944221, Val Loss: 0.561087\n",
      "Early stopping at epoch 6, Train Loss: 0.923061, Val Loss: 0.561589\n",
      "    Final - Train Loss: 0.923061, Val Loss: 0.561589\n",
      "[Saved] NN3_w512_2017Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.956193, Val Loss: 0.559481\n",
      "    Epoch 5/20, Train Loss: 0.866528, Val Loss: 0.566530\n",
      "Early stopping at epoch 6, Train Loss: 0.845560, Val Loss: 0.572497\n",
      "    Final - Train Loss: 0.845560, Val Loss: 0.572497\n",
      "[Saved] NN4_w512_2017Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 1.014356, Val Loss: 0.556772\n",
      "    Epoch 5/20, Train Loss: 0.906059, Val Loss: 0.574281\n",
      "Early stopping at epoch 6, Train Loss: 0.873048, Val Loss: 0.578871\n",
      "    Final - Train Loss: 0.873048, Val Loss: 0.578871\n",
      "[Saved] NN5_w512_2017Q1.pth\n",
      "    Training data size: 183995 samples\n",
      "  Quarter 2017Q2: Training models with expanding data\n",
      "    Added 3083 samples from 2017Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.988142, Val Loss: 0.548270\n",
      "    Epoch 5/20, Train Loss: 0.960887, Val Loss: 0.553289\n",
      "Early stopping at epoch 6, Train Loss: 0.952829, Val Loss: 0.554354\n",
      "    Final - Train Loss: 0.952829, Val Loss: 0.554354\n",
      "[Saved] NN1_w512_2017Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.983939, Val Loss: 0.543566\n",
      "    Epoch 5/20, Train Loss: 0.957449, Val Loss: 0.544500\n",
      "Early stopping at epoch 7, Train Loss: 0.939521, Val Loss: 0.545608\n",
      "    Final - Train Loss: 0.939521, Val Loss: 0.545608\n",
      "[Saved] NN2_w512_2017Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.978957, Val Loss: 0.542638\n",
      "    Epoch 5/20, Train Loss: 0.915629, Val Loss: 0.543553\n",
      "Early stopping at epoch 8, Train Loss: 0.869687, Val Loss: 0.545344\n",
      "    Final - Train Loss: 0.869687, Val Loss: 0.545344\n",
      "[Saved] NN3_w512_2017Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.923885, Val Loss: 0.543278\n",
      "    Epoch 5/20, Train Loss: 0.843467, Val Loss: 0.549405\n",
      "Early stopping at epoch 6, Train Loss: 0.819485, Val Loss: 0.552257\n",
      "    Final - Train Loss: 0.819485, Val Loss: 0.552257\n",
      "[Saved] NN4_w512_2017Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.973551, Val Loss: 0.542030\n",
      "    Epoch 5/20, Train Loss: 0.860421, Val Loss: 0.558702\n",
      "Early stopping at epoch 6, Train Loss: 0.839110, Val Loss: 0.560641\n",
      "    Final - Train Loss: 0.839110, Val Loss: 0.560641\n",
      "[Saved] NN5_w512_2017Q2.pth\n",
      "    Training data size: 187078 samples\n",
      "  Quarter 2017Q3: Training models with expanding data\n",
      "    Added 3122 samples from 2017Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.970481, Val Loss: 0.550311\n",
      "    Epoch 5/20, Train Loss: 0.949179, Val Loss: 0.554954\n",
      "Early stopping at epoch 6, Train Loss: 0.944751, Val Loss: 0.555531\n",
      "    Final - Train Loss: 0.944751, Val Loss: 0.555531\n",
      "[Saved] NN1_w512_2017Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.956200, Val Loss: 0.544089\n",
      "    Epoch 5/20, Train Loss: 0.934119, Val Loss: 0.545121\n",
      "Early stopping at epoch 6, Train Loss: 0.921572, Val Loss: 0.545082\n",
      "    Final - Train Loss: 0.921572, Val Loss: 0.545082\n",
      "[Saved] NN2_w512_2017Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.916051, Val Loss: 0.543881\n",
      "    Epoch 5/20, Train Loss: 0.858795, Val Loss: 0.546178\n",
      "Early stopping at epoch 6, Train Loss: 0.844298, Val Loss: 0.548096\n",
      "    Final - Train Loss: 0.844298, Val Loss: 0.548096\n",
      "[Saved] NN3_w512_2017Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.886906, Val Loss: 0.545835\n",
      "    Epoch 5/20, Train Loss: 0.813494, Val Loss: 0.550634\n",
      "Early stopping at epoch 6, Train Loss: 0.793824, Val Loss: 0.554251\n",
      "    Final - Train Loss: 0.793824, Val Loss: 0.554251\n",
      "[Saved] NN4_w512_2017Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.930407, Val Loss: 0.546129\n",
      "    Epoch 5/20, Train Loss: 0.839969, Val Loss: 0.561897\n",
      "Early stopping at epoch 6, Train Loss: 0.815384, Val Loss: 0.567170\n",
      "    Final - Train Loss: 0.815384, Val Loss: 0.567170\n",
      "[Saved] NN5_w512_2017Q3.pth\n",
      "    Training data size: 190200 samples\n",
      "  Quarter 2017Q4: Training models with expanding data\n",
      "    Added 3114 samples from 2017Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.952953, Val Loss: 0.559522\n",
      "    Epoch 5/20, Train Loss: 0.936891, Val Loss: 0.565761\n",
      "Early stopping at epoch 6, Train Loss: 0.931855, Val Loss: 0.565518\n",
      "    Final - Train Loss: 0.931855, Val Loss: 0.565518\n",
      "[Saved] NN1_w512_2017Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.932552, Val Loss: 0.552283\n",
      "    Epoch 5/20, Train Loss: 0.916874, Val Loss: 0.553548\n",
      "Early stopping at epoch 6, Train Loss: 0.908767, Val Loss: 0.554427\n",
      "    Final - Train Loss: 0.908767, Val Loss: 0.554427\n",
      "[Saved] NN2_w512_2017Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.887722, Val Loss: 0.550997\n",
      "    Epoch 5/20, Train Loss: 0.842521, Val Loss: 0.552422\n",
      "Early stopping at epoch 6, Train Loss: 0.826045, Val Loss: 0.552280\n",
      "    Final - Train Loss: 0.826045, Val Loss: 0.552280\n",
      "[Saved] NN3_w512_2017Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.857445, Val Loss: 0.553313\n",
      "    Epoch 5/20, Train Loss: 0.794989, Val Loss: 0.560024\n",
      "Early stopping at epoch 6, Train Loss: 0.775719, Val Loss: 0.562577\n",
      "    Final - Train Loss: 0.775719, Val Loss: 0.562577\n",
      "[Saved] NN4_w512_2017Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.889555, Val Loss: 0.555748\n",
      "    Epoch 5/20, Train Loss: 0.808896, Val Loss: 0.572122\n",
      "Early stopping at epoch 6, Train Loss: 0.787438, Val Loss: 0.577683\n",
      "    Final - Train Loss: 0.787438, Val Loss: 0.577683\n",
      "[Saved] NN5_w512_2017Q4.pth\n",
      "    Training data size: 193314 samples\n",
      "  Quarter 2018Q1: Training models with expanding data\n",
      "    Added 3115 samples from 2017Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.940433, Val Loss: 0.562992\n",
      "    Epoch 5/20, Train Loss: 0.924598, Val Loss: 0.567311\n",
      "Early stopping at epoch 6, Train Loss: 0.917426, Val Loss: 0.568561\n",
      "    Final - Train Loss: 0.917426, Val Loss: 0.568561\n",
      "[Saved] NN1_w512_2018Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.920656, Val Loss: 0.554225\n",
      "    Epoch 5/20, Train Loss: 0.904143, Val Loss: 0.555741\n",
      "Early stopping at epoch 6, Train Loss: 0.900294, Val Loss: 0.556368\n",
      "    Final - Train Loss: 0.900294, Val Loss: 0.556368\n",
      "[Saved] NN2_w512_2018Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.865391, Val Loss: 0.553337\n",
      "    Epoch 5/20, Train Loss: 0.820189, Val Loss: 0.555484\n",
      "Early stopping at epoch 6, Train Loss: 0.814347, Val Loss: 0.555532\n",
      "    Final - Train Loss: 0.814347, Val Loss: 0.555532\n",
      "[Saved] NN3_w512_2018Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.829155, Val Loss: 0.555509\n",
      "    Epoch 5/20, Train Loss: 0.774210, Val Loss: 0.560557\n",
      "Early stopping at epoch 6, Train Loss: 0.755603, Val Loss: 0.562969\n",
      "    Final - Train Loss: 0.755603, Val Loss: 0.562969\n",
      "[Saved] NN4_w512_2018Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.849670, Val Loss: 0.559441\n",
      "    Epoch 5/20, Train Loss: 0.785222, Val Loss: 0.571460\n",
      "Early stopping at epoch 6, Train Loss: 0.761524, Val Loss: 0.576459\n",
      "    Final - Train Loss: 0.761524, Val Loss: 0.576459\n",
      "[Saved] NN5_w512_2018Q1.pth\n",
      "    Training data size: 196429 samples\n",
      "  Quarter 2018Q2: Training models with expanding data\n",
      "    Added 2996 samples from 2018Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.927836, Val Loss: 0.593141\n",
      "    Epoch 5/20, Train Loss: 0.913356, Val Loss: 0.597964\n",
      "Early stopping at epoch 6, Train Loss: 0.907406, Val Loss: 0.597843\n",
      "    Final - Train Loss: 0.907406, Val Loss: 0.597843\n",
      "[Saved] NN1_w512_2018Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.906116, Val Loss: 0.583864\n",
      "    Epoch 5/20, Train Loss: 0.890446, Val Loss: 0.584721\n",
      "Early stopping at epoch 9, Train Loss: 0.869364, Val Loss: 0.584326\n",
      "    Final - Train Loss: 0.869364, Val Loss: 0.584326\n",
      "[Saved] NN2_w512_2018Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.840125, Val Loss: 0.582424\n",
      "    Epoch 5/20, Train Loss: 0.806288, Val Loss: 0.585282\n",
      "Early stopping at epoch 6, Train Loss: 0.795585, Val Loss: 0.585392\n",
      "    Final - Train Loss: 0.795585, Val Loss: 0.585392\n",
      "[Saved] NN3_w512_2018Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.802360, Val Loss: 0.583865\n",
      "    Epoch 5/20, Train Loss: 0.756274, Val Loss: 0.591059\n",
      "Early stopping at epoch 6, Train Loss: 0.744946, Val Loss: 0.592421\n",
      "    Final - Train Loss: 0.744946, Val Loss: 0.592421\n",
      "[Saved] NN4_w512_2018Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.813856, Val Loss: 0.587409\n",
      "    Epoch 5/20, Train Loss: 0.759757, Val Loss: 0.602572\n",
      "Early stopping at epoch 6, Train Loss: 0.739382, Val Loss: 0.604572\n",
      "    Final - Train Loss: 0.739382, Val Loss: 0.604572\n",
      "[Saved] NN5_w512_2018Q2.pth\n",
      "    Training data size: 199425 samples\n",
      "  Quarter 2018Q3: Training models with expanding data\n",
      "    Added 3160 samples from 2018Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.915097, Val Loss: 0.601123\n",
      "    Epoch 5/20, Train Loss: 0.902059, Val Loss: 0.606524\n",
      "Early stopping at epoch 6, Train Loss: 0.899992, Val Loss: 0.609596\n",
      "    Final - Train Loss: 0.899992, Val Loss: 0.609596\n",
      "[Saved] NN1_w512_2018Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.879965, Val Loss: 0.591720\n",
      "    Epoch 5/20, Train Loss: 0.859508, Val Loss: 0.594806\n",
      "Early stopping at epoch 6, Train Loss: 0.856845, Val Loss: 0.594779\n",
      "    Final - Train Loss: 0.856845, Val Loss: 0.594779\n",
      "[Saved] NN2_w512_2018Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.819953, Val Loss: 0.590069\n",
      "    Epoch 5/20, Train Loss: 0.796974, Val Loss: 0.593592\n",
      "Early stopping at epoch 6, Train Loss: 0.781640, Val Loss: 0.593493\n",
      "    Final - Train Loss: 0.781640, Val Loss: 0.593493\n",
      "[Saved] NN3_w512_2018Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.787768, Val Loss: 0.591340\n",
      "    Epoch 5/20, Train Loss: 0.738745, Val Loss: 0.599399\n",
      "Early stopping at epoch 6, Train Loss: 0.726644, Val Loss: 0.600061\n",
      "    Final - Train Loss: 0.726644, Val Loss: 0.600061\n",
      "[Saved] NN4_w512_2018Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.781573, Val Loss: 0.592474\n",
      "    Epoch 5/20, Train Loss: 0.745396, Val Loss: 0.617821\n",
      "Early stopping at epoch 6, Train Loss: 0.725728, Val Loss: 0.619296\n",
      "    Final - Train Loss: 0.725728, Val Loss: 0.619296\n",
      "[Saved] NN5_w512_2018Q3.pth\n",
      "    Training data size: 202585 samples\n",
      "  Quarter 2018Q4: Training models with expanding data\n",
      "    Added 3125 samples from 2018Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.903060, Val Loss: 0.603005\n",
      "    Epoch 5/20, Train Loss: 0.893782, Val Loss: 0.610727\n",
      "Early stopping at epoch 6, Train Loss: 0.888426, Val Loss: 0.610392\n",
      "    Final - Train Loss: 0.888426, Val Loss: 0.610392\n",
      "[Saved] NN1_w512_2018Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.861888, Val Loss: 0.593158\n",
      "    Epoch 5/20, Train Loss: 0.850521, Val Loss: 0.596350\n",
      "Early stopping at epoch 6, Train Loss: 0.849759, Val Loss: 0.595072\n",
      "    Final - Train Loss: 0.849759, Val Loss: 0.595072\n",
      "[Saved] NN2_w512_2018Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.803317, Val Loss: 0.589720\n",
      "    Epoch 5/20, Train Loss: 0.779267, Val Loss: 0.590948\n",
      "Early stopping at epoch 6, Train Loss: 0.771753, Val Loss: 0.592837\n",
      "    Final - Train Loss: 0.771753, Val Loss: 0.592837\n",
      "[Saved] NN3_w512_2018Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.760525, Val Loss: 0.591565\n",
      "    Epoch 5/20, Train Loss: 0.724226, Val Loss: 0.596282\n",
      "Early stopping at epoch 6, Train Loss: 0.709288, Val Loss: 0.598972\n",
      "    Final - Train Loss: 0.709288, Val Loss: 0.598972\n",
      "[Saved] NN4_w512_2018Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.757658, Val Loss: 0.596317\n",
      "    Epoch 5/20, Train Loss: 0.708839, Val Loss: 0.609488\n",
      "Early stopping at epoch 6, Train Loss: 0.698041, Val Loss: 0.611896\n",
      "    Final - Train Loss: 0.698041, Val Loss: 0.611896\n",
      "[Saved] NN5_w512_2018Q4.pth\n",
      "    Training data size: 205710 samples\n",
      "  Quarter 2019Q1: Training models with expanding data\n",
      "    Added 3045 samples from 2018Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.891928, Val Loss: 0.646229\n",
      "    Epoch 5/20, Train Loss: 0.883693, Val Loss: 0.655814\n",
      "Early stopping at epoch 6, Train Loss: 0.882261, Val Loss: 0.654607\n",
      "    Final - Train Loss: 0.882261, Val Loss: 0.654607\n",
      "[Saved] NN1_w512_2019Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.849731, Val Loss: 0.632796\n",
      "    Epoch 5/20, Train Loss: 0.840663, Val Loss: 0.635258\n",
      "Early stopping at epoch 6, Train Loss: 0.834709, Val Loss: 0.634910\n",
      "    Final - Train Loss: 0.834709, Val Loss: 0.634910\n",
      "[Saved] NN2_w512_2019Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.785334, Val Loss: 0.629061\n",
      "    Epoch 5/20, Train Loss: 0.775690, Val Loss: 0.631288\n",
      "Early stopping at epoch 6, Train Loss: 0.765107, Val Loss: 0.631556\n",
      "    Final - Train Loss: 0.765107, Val Loss: 0.631556\n",
      "[Saved] NN3_w512_2019Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.742597, Val Loss: 0.631011\n",
      "    Epoch 5/20, Train Loss: 0.702650, Val Loss: 0.635763\n",
      "Early stopping at epoch 6, Train Loss: 0.694352, Val Loss: 0.638127\n",
      "    Final - Train Loss: 0.694352, Val Loss: 0.638127\n",
      "[Saved] NN4_w512_2019Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.731915, Val Loss: 0.640363\n",
      "    Epoch 5/20, Train Loss: 0.688237, Val Loss: 0.652666\n",
      "Early stopping at epoch 6, Train Loss: 0.683443, Val Loss: 0.657517\n",
      "    Final - Train Loss: 0.683443, Val Loss: 0.657517\n",
      "[Saved] NN5_w512_2019Q1.pth\n",
      "    Training data size: 208755 samples\n",
      "  Quarter 2019Q2: Training models with expanding data\n",
      "    Added 3022 samples from 2019Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.891331, Val Loss: 0.627425\n",
      "    Epoch 5/20, Train Loss: 0.885993, Val Loss: 0.633255\n",
      "Early stopping at epoch 6, Train Loss: 0.881646, Val Loss: 0.634449\n",
      "    Final - Train Loss: 0.881646, Val Loss: 0.634449\n",
      "[Saved] NN1_w512_2019Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.843998, Val Loss: 0.615048\n",
      "    Epoch 5/20, Train Loss: 0.834786, Val Loss: 0.616131\n",
      "Early stopping at epoch 6, Train Loss: 0.831314, Val Loss: 0.616746\n",
      "    Final - Train Loss: 0.831314, Val Loss: 0.616746\n",
      "[Saved] NN2_w512_2019Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.776360, Val Loss: 0.613397\n",
      "    Epoch 5/20, Train Loss: 0.767833, Val Loss: 0.614682\n",
      "Early stopping at epoch 6, Train Loss: 0.768848, Val Loss: 0.614055\n",
      "    Final - Train Loss: 0.768848, Val Loss: 0.614055\n",
      "[Saved] NN3_w512_2019Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.734215, Val Loss: 0.614537\n",
      "    Epoch 5/20, Train Loss: 0.699184, Val Loss: 0.619423\n",
      "Early stopping at epoch 6, Train Loss: 0.693613, Val Loss: 0.621347\n",
      "    Final - Train Loss: 0.693613, Val Loss: 0.621347\n",
      "[Saved] NN4_w512_2019Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.723189, Val Loss: 0.624914\n",
      "    Epoch 5/20, Train Loss: 0.697692, Val Loss: 0.638556\n",
      "Early stopping at epoch 6, Train Loss: 0.676602, Val Loss: 0.641202\n",
      "    Final - Train Loss: 0.676602, Val Loss: 0.641202\n",
      "[Saved] NN5_w512_2019Q2.pth\n",
      "    Training data size: 211777 samples\n",
      "  Quarter 2019Q3: Training models with expanding data\n",
      "    Added 3120 samples from 2019Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.885936, Val Loss: 0.619325\n",
      "    Epoch 5/20, Train Loss: 0.879807, Val Loss: 0.621742\n",
      "Early stopping at epoch 6, Train Loss: 0.877500, Val Loss: 0.622010\n",
      "    Final - Train Loss: 0.877500, Val Loss: 0.622010\n",
      "[Saved] NN1_w512_2019Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.839608, Val Loss: 0.607717\n",
      "    Epoch 5/20, Train Loss: 0.830787, Val Loss: 0.609066\n",
      "Early stopping at epoch 6, Train Loss: 0.823108, Val Loss: 0.609029\n",
      "    Final - Train Loss: 0.823108, Val Loss: 0.609029\n",
      "[Saved] NN2_w512_2019Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.770888, Val Loss: 0.604513\n",
      "    Epoch 5/20, Train Loss: 0.763335, Val Loss: 0.605560\n",
      "Early stopping at epoch 6, Train Loss: 0.756488, Val Loss: 0.606355\n",
      "    Final - Train Loss: 0.756488, Val Loss: 0.606355\n",
      "[Saved] NN3_w512_2019Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.716470, Val Loss: 0.606734\n",
      "    Epoch 5/20, Train Loss: 0.694227, Val Loss: 0.615454\n",
      "Early stopping at epoch 6, Train Loss: 0.687236, Val Loss: 0.618346\n",
      "    Final - Train Loss: 0.687236, Val Loss: 0.618346\n",
      "[Saved] NN4_w512_2019Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.695293, Val Loss: 0.614623\n",
      "    Epoch 5/20, Train Loss: 0.673026, Val Loss: 0.633621\n",
      "Early stopping at epoch 6, Train Loss: 0.662899, Val Loss: 0.635901\n",
      "    Final - Train Loss: 0.662899, Val Loss: 0.635901\n",
      "[Saved] NN5_w512_2019Q3.pth\n",
      "    Training data size: 214897 samples\n",
      "  Quarter 2019Q4: Training models with expanding data\n",
      "    Added 3167 samples from 2019Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.880759, Val Loss: 0.621001\n",
      "    Epoch 5/20, Train Loss: 0.877304, Val Loss: 0.623706\n",
      "Early stopping at epoch 6, Train Loss: 0.876403, Val Loss: 0.625302\n",
      "    Final - Train Loss: 0.876403, Val Loss: 0.625302\n",
      "[Saved] NN1_w512_2019Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.834041, Val Loss: 0.610807\n",
      "    Epoch 5/20, Train Loss: 0.826271, Val Loss: 0.613235\n",
      "Early stopping at epoch 7, Train Loss: 0.820258, Val Loss: 0.614640\n",
      "    Final - Train Loss: 0.820258, Val Loss: 0.614640\n",
      "[Saved] NN2_w512_2019Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.765892, Val Loss: 0.607245\n",
      "    Epoch 5/20, Train Loss: 0.760509, Val Loss: 0.609136\n",
      "Early stopping at epoch 7, Train Loss: 0.743350, Val Loss: 0.610953\n",
      "    Final - Train Loss: 0.743350, Val Loss: 0.610953\n",
      "[Saved] NN3_w512_2019Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.706732, Val Loss: 0.609740\n",
      "    Epoch 5/20, Train Loss: 0.698083, Val Loss: 0.619318\n",
      "Early stopping at epoch 6, Train Loss: 0.686175, Val Loss: 0.620958\n",
      "    Final - Train Loss: 0.686175, Val Loss: 0.620958\n",
      "[Saved] NN4_w512_2019Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.687113, Val Loss: 0.616727\n",
      "    Epoch 5/20, Train Loss: 0.678130, Val Loss: 0.644695\n",
      "Early stopping at epoch 6, Train Loss: 0.655422, Val Loss: 0.640657\n",
      "    Final - Train Loss: 0.655422, Val Loss: 0.640657\n",
      "[Saved] NN5_w512_2019Q4.pth\n",
      "    Training data size: 218064 samples\n",
      "  Quarter 2020Q1: Training models with expanding data\n",
      "    Added 3179 samples from 2019Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.874241, Val Loss: 0.622151\n",
      "    Epoch 5/20, Train Loss: 0.869824, Val Loss: 0.625948\n",
      "Early stopping at epoch 6, Train Loss: 0.865866, Val Loss: 0.627713\n",
      "    Final - Train Loss: 0.865866, Val Loss: 0.627713\n",
      "[Saved] NN1_w512_2020Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.820556, Val Loss: 0.610411\n",
      "    Epoch 5/20, Train Loss: 0.813105, Val Loss: 0.612492\n",
      "Early stopping at epoch 7, Train Loss: 0.808633, Val Loss: 0.612957\n",
      "    Final - Train Loss: 0.808633, Val Loss: 0.612957\n",
      "[Saved] NN2_w512_2020Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.753073, Val Loss: 0.607613\n",
      "    Epoch 5/20, Train Loss: 0.752599, Val Loss: 0.611627\n",
      "Early stopping at epoch 7, Train Loss: 0.740270, Val Loss: 0.611436\n",
      "    Final - Train Loss: 0.740270, Val Loss: 0.611436\n",
      "[Saved] NN3_w512_2020Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.696168, Val Loss: 0.609237\n",
      "    Epoch 5/20, Train Loss: 0.688148, Val Loss: 0.618193\n",
      "Early stopping at epoch 6, Train Loss: 0.690577, Val Loss: 0.619746\n",
      "    Final - Train Loss: 0.690577, Val Loss: 0.619746\n",
      "[Saved] NN4_w512_2020Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.667673, Val Loss: 0.615558\n",
      "    Epoch 5/20, Train Loss: 0.640820, Val Loss: 0.640024\n",
      "Early stopping at epoch 6, Train Loss: 0.634343, Val Loss: 0.643786\n",
      "    Final - Train Loss: 0.634343, Val Loss: 0.643786\n",
      "[Saved] NN5_w512_2020Q1.pth\n",
      "    Training data size: 221243 samples\n",
      "  Quarter 2020Q2: Training models with expanding data\n",
      "    Added 2595 samples from 2020Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.866391, Val Loss: 0.682787\n",
      "    Epoch 5/20, Train Loss: 0.865369, Val Loss: 0.691233\n",
      "Early stopping at epoch 6, Train Loss: 0.863821, Val Loss: 0.689566\n",
      "    Final - Train Loss: 0.863821, Val Loss: 0.689566\n",
      "[Saved] NN1_w512_2020Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.809489, Val Loss: 0.670973\n",
      "    Epoch 5/20, Train Loss: 0.804718, Val Loss: 0.672716\n",
      "Early stopping at epoch 6, Train Loss: 0.802004, Val Loss: 0.673588\n",
      "    Final - Train Loss: 0.802004, Val Loss: 0.673588\n",
      "[Saved] NN2_w512_2020Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.738638, Val Loss: 0.668648\n",
      "    Epoch 5/20, Train Loss: 0.739025, Val Loss: 0.672623\n",
      "Early stopping at epoch 6, Train Loss: 0.734380, Val Loss: 0.673949\n",
      "    Final - Train Loss: 0.734380, Val Loss: 0.673949\n",
      "[Saved] NN3_w512_2020Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.679078, Val Loss: 0.668651\n",
      "    Epoch 5/20, Train Loss: 0.682527, Val Loss: 0.676792\n",
      "Early stopping at epoch 6, Train Loss: 0.685085, Val Loss: 0.681229\n",
      "    Final - Train Loss: 0.685085, Val Loss: 0.681229\n",
      "[Saved] NN4_w512_2020Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.650324, Val Loss: 0.677159\n",
      "    Epoch 5/20, Train Loss: 0.628643, Val Loss: 0.698467\n",
      "Early stopping at epoch 6, Train Loss: 0.622037, Val Loss: 0.700385\n",
      "    Final - Train Loss: 0.622037, Val Loss: 0.700385\n",
      "[Saved] NN5_w512_2020Q2.pth\n",
      "    Training data size: 223838 samples\n",
      "  Quarter 2020Q3: Training models with expanding data\n",
      "    Added 2839 samples from 2020Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.859569, Val Loss: 0.786993\n",
      "    Epoch 5/20, Train Loss: 0.855636, Val Loss: 0.795508\n",
      "Early stopping at epoch 6, Train Loss: 0.852450, Val Loss: 0.795879\n",
      "    Final - Train Loss: 0.852450, Val Loss: 0.795879\n",
      "[Saved] NN1_w512_2020Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.799382, Val Loss: 0.768705\n",
      "    Epoch 5/20, Train Loss: 0.798805, Val Loss: 0.770627\n",
      "Early stopping at epoch 7, Train Loss: 0.792465, Val Loss: 0.771738\n",
      "    Final - Train Loss: 0.792465, Val Loss: 0.771738\n",
      "[Saved] NN2_w512_2020Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.724958, Val Loss: 0.766475\n",
      "    Epoch 5/20, Train Loss: 0.729949, Val Loss: 0.770588\n",
      "Early stopping at epoch 6, Train Loss: 0.718507, Val Loss: 0.772925\n",
      "    Final - Train Loss: 0.718507, Val Loss: 0.772925\n",
      "[Saved] NN3_w512_2020Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.669500, Val Loss: 0.765910\n",
      "    Epoch 5/20, Train Loss: 0.671307, Val Loss: 0.773201\n",
      "Early stopping at epoch 6, Train Loss: 0.660428, Val Loss: 0.774649\n",
      "    Final - Train Loss: 0.660428, Val Loss: 0.774649\n",
      "[Saved] NN4_w512_2020Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.635149, Val Loss: 0.774024\n",
      "    Epoch 5/20, Train Loss: 0.618505, Val Loss: 0.801787\n",
      "Early stopping at epoch 6, Train Loss: 0.607303, Val Loss: 0.802036\n",
      "    Final - Train Loss: 0.607303, Val Loss: 0.802036\n",
      "[Saved] NN5_w512_2020Q3.pth\n",
      "    Training data size: 226677 samples\n",
      "  Quarter 2020Q4: Training models with expanding data\n",
      "    Added 3151 samples from 2020Q3 to training set\n",
      "    [Re-tuning] NN1 for 2020Q4\n",
      "[Hyper] NN1: best_MSE=0.876514, params={'learning_rate': 0.0011573914700648597, 'batch_size': 64, 'dropout_rate': 0.46351980445719754}\n",
      "    [Structure Change] dropout_rate changed: 0.23356280759810694 -> 0.46351980445719754\n",
      "    Rebuilding NN1 model due to structure change\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 1.067628, Val Loss: 0.802012\n",
      "    Epoch 5/50, Train Loss: 0.957954, Val Loss: 0.804210\n",
      "Early stopping at epoch 6, Train Loss: 0.954162, Val Loss: 0.804205\n",
      "    Final - Train Loss: 0.954162, Val Loss: 0.804205\n",
      "[Saved] NN1_w512_2020Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.872776, Val Loss: 0.811403\n",
      "    Epoch 5/20, Train Loss: 0.826008, Val Loss: 0.815872\n",
      "Early stopping at epoch 6, Train Loss: 0.818153, Val Loss: 0.817714\n",
      "    Final - Train Loss: 0.818153, Val Loss: 0.817714\n",
      "[Saved] NN2_w512_2020Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.835327, Val Loss: 0.805381\n",
      "    Epoch 5/20, Train Loss: 0.758101, Val Loss: 0.808220\n",
      "Early stopping at epoch 6, Train Loss: 0.743975, Val Loss: 0.808066\n",
      "    Final - Train Loss: 0.743975, Val Loss: 0.808066\n",
      "[Saved] NN3_w512_2020Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.801496, Val Loss: 0.803139\n",
      "    Epoch 5/20, Train Loss: 0.710334, Val Loss: 0.807995\n",
      "Early stopping at epoch 6, Train Loss: 0.700599, Val Loss: 0.809778\n",
      "    Final - Train Loss: 0.700599, Val Loss: 0.809778\n",
      "[Saved] NN4_w512_2020Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.779849, Val Loss: 0.811553\n",
      "    Epoch 5/20, Train Loss: 0.642100, Val Loss: 0.819510\n",
      "Early stopping at epoch 6, Train Loss: 0.628715, Val Loss: 0.822490\n",
      "    Final - Train Loss: 0.628715, Val Loss: 0.822490\n",
      "[Saved] NN5_w512_2020Q4.pth\n",
      "    Training data size: 229828 samples\n",
      "  Quarter 2021Q1: Training models with expanding data\n",
      "    Added 3114 samples from 2020Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.961554, Val Loss: 0.847572\n",
      "    Epoch 5/20, Train Loss: 0.944821, Val Loss: 0.850846\n",
      "Early stopping at epoch 6, Train Loss: 0.942779, Val Loss: 0.851668\n",
      "    Final - Train Loss: 0.942779, Val Loss: 0.851668\n",
      "[Saved] NN1_w512_2021Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.852791, Val Loss: 0.859100\n",
      "    Epoch 5/20, Train Loss: 0.811336, Val Loss: 0.867169\n",
      "Early stopping at epoch 6, Train Loss: 0.806183, Val Loss: 0.867310\n",
      "    Final - Train Loss: 0.806183, Val Loss: 0.867310\n",
      "[Saved] NN2_w512_2021Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.804912, Val Loss: 0.849150\n",
      "    Epoch 5/20, Train Loss: 0.740763, Val Loss: 0.852350\n",
      "Early stopping at epoch 6, Train Loss: 0.730682, Val Loss: 0.852577\n",
      "    Final - Train Loss: 0.730682, Val Loss: 0.852577\n",
      "[Saved] NN3_w512_2021Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.757525, Val Loss: 0.849663\n",
      "    Epoch 5/20, Train Loss: 0.697955, Val Loss: 0.852208\n",
      "Early stopping at epoch 6, Train Loss: 0.691632, Val Loss: 0.852863\n",
      "    Final - Train Loss: 0.691632, Val Loss: 0.852863\n",
      "[Saved] NN4_w512_2021Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.725658, Val Loss: 0.854716\n",
      "    Epoch 5/20, Train Loss: 0.630509, Val Loss: 0.866944\n",
      "Early stopping at epoch 6, Train Loss: 0.620580, Val Loss: 0.862472\n",
      "    Final - Train Loss: 0.620580, Val Loss: 0.862472\n",
      "[Saved] NN5_w512_2021Q1.pth\n",
      "    Training data size: 232942 samples\n",
      "  Quarter 2021Q2: Training models with expanding data\n",
      "    Added 2989 samples from 2021Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.948331, Val Loss: 0.896632\n",
      "    Epoch 5/20, Train Loss: 0.937171, Val Loss: 0.900826\n",
      "Early stopping at epoch 6, Train Loss: 0.933874, Val Loss: 0.901529\n",
      "    Final - Train Loss: 0.933874, Val Loss: 0.901529\n",
      "[Saved] NN1_w512_2021Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.832322, Val Loss: 0.907735\n",
      "    Epoch 5/20, Train Loss: 0.802034, Val Loss: 0.909880\n",
      "Early stopping at epoch 6, Train Loss: 0.795031, Val Loss: 0.914070\n",
      "    Final - Train Loss: 0.795031, Val Loss: 0.914070\n",
      "[Saved] NN2_w512_2021Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.785520, Val Loss: 0.897375\n",
      "    Epoch 5/20, Train Loss: 0.727883, Val Loss: 0.898999\n",
      "Early stopping at epoch 6, Train Loss: 0.718665, Val Loss: 0.899934\n",
      "    Final - Train Loss: 0.718665, Val Loss: 0.899934\n",
      "[Saved] NN3_w512_2021Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.733465, Val Loss: 0.896977\n",
      "    Epoch 5/20, Train Loss: 0.689256, Val Loss: 0.899395\n",
      "Early stopping at epoch 6, Train Loss: 0.679542, Val Loss: 0.901711\n",
      "    Final - Train Loss: 0.679542, Val Loss: 0.901711\n",
      "[Saved] NN4_w512_2021Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.687282, Val Loss: 0.908725\n",
      "    Epoch 5/20, Train Loss: 0.617201, Val Loss: 0.908320\n",
      "Early stopping at epoch 8, Train Loss: 0.587345, Val Loss: 0.913339\n",
      "    Final - Train Loss: 0.587345, Val Loss: 0.913339\n",
      "[Saved] NN5_w512_2021Q2.pth\n",
      "    Training data size: 235931 samples\n",
      "  Quarter 2021Q3: Training models with expanding data\n",
      "    Added 3132 samples from 2021Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.939099, Val Loss: 0.903186\n",
      "    Epoch 5/20, Train Loss: 0.928366, Val Loss: 0.906559\n",
      "Early stopping at epoch 6, Train Loss: 0.924885, Val Loss: 0.908128\n",
      "    Final - Train Loss: 0.924885, Val Loss: 0.908128\n",
      "[Saved] NN1_w512_2021Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.822124, Val Loss: 0.913642\n",
      "    Epoch 5/20, Train Loss: 0.790367, Val Loss: 0.915743\n",
      "Early stopping at epoch 7, Train Loss: 0.780050, Val Loss: 0.915332\n",
      "    Final - Train Loss: 0.780050, Val Loss: 0.915332\n",
      "[Saved] NN2_w512_2021Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.764101, Val Loss: 0.903010\n",
      "    Epoch 5/20, Train Loss: 0.715039, Val Loss: 0.904577\n",
      "Early stopping at epoch 6, Train Loss: 0.707680, Val Loss: 0.905105\n",
      "    Final - Train Loss: 0.707680, Val Loss: 0.905105\n",
      "[Saved] NN3_w512_2021Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.717708, Val Loss: 0.903297\n",
      "    Epoch 5/20, Train Loss: 0.675366, Val Loss: 0.904935\n",
      "Early stopping at epoch 7, Train Loss: 0.661987, Val Loss: 0.906424\n",
      "    Final - Train Loss: 0.661987, Val Loss: 0.906424\n",
      "[Saved] NN4_w512_2021Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.624577, Val Loss: 0.916559\n",
      "    Epoch 5/20, Train Loss: 0.587374, Val Loss: 0.911651\n",
      "Early stopping at epoch 8, Train Loss: 0.567603, Val Loss: 0.914009\n",
      "    Final - Train Loss: 0.567603, Val Loss: 0.914009\n",
      "[Saved] NN5_w512_2021Q3.pth\n",
      "    Training data size: 239063 samples\n",
      "  Quarter 2021Q4: Training models with expanding data\n",
      "    Added 3173 samples from 2021Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.930382, Val Loss: 0.911500\n",
      "    Epoch 5/20, Train Loss: 0.920472, Val Loss: 0.916350\n",
      "Early stopping at epoch 6, Train Loss: 0.916631, Val Loss: 0.918944\n",
      "    Final - Train Loss: 0.916631, Val Loss: 0.918944\n",
      "[Saved] NN1_w512_2021Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.805039, Val Loss: 0.921216\n",
      "    Epoch 5/20, Train Loss: 0.776661, Val Loss: 0.926484\n",
      "Early stopping at epoch 6, Train Loss: 0.770522, Val Loss: 0.926210\n",
      "    Final - Train Loss: 0.770522, Val Loss: 0.926210\n",
      "[Saved] NN2_w512_2021Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.747680, Val Loss: 0.912479\n",
      "    Epoch 5/20, Train Loss: 0.707111, Val Loss: 0.912228\n",
      "Early stopping at epoch 7, Train Loss: 0.694146, Val Loss: 0.912423\n",
      "    Final - Train Loss: 0.694146, Val Loss: 0.912423\n",
      "[Saved] NN3_w512_2021Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.692250, Val Loss: 0.911542\n",
      "    Epoch 5/20, Train Loss: 0.659582, Val Loss: 0.915660\n",
      "Early stopping at epoch 6, Train Loss: 0.654130, Val Loss: 0.918004\n",
      "    Final - Train Loss: 0.654130, Val Loss: 0.918004\n",
      "[Saved] NN4_w512_2021Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.598692, Val Loss: 0.924548\n",
      "    Epoch 5/20, Train Loss: 0.568259, Val Loss: 0.921391\n",
      "    Epoch 10/20, Train Loss: 0.538614, Val Loss: 0.927558\n",
      "Early stopping at epoch 10, Train Loss: 0.538614, Val Loss: 0.927558\n",
      "    Final - Train Loss: 0.538614, Val Loss: 0.927558\n",
      "[Saved] NN5_w512_2021Q4.pth\n",
      "    Training data size: 242236 samples\n",
      "  Quarter 2022Q1: Training models with expanding data\n",
      "    Added 3153 samples from 2021Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.924845, Val Loss: 0.922789\n",
      "    Epoch 5/20, Train Loss: 0.915140, Val Loss: 0.928880\n",
      "Early stopping at epoch 6, Train Loss: 0.913083, Val Loss: 0.928849\n",
      "    Final - Train Loss: 0.913083, Val Loss: 0.928849\n",
      "[Saved] NN1_w512_2022Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.794639, Val Loss: 0.934030\n",
      "    Epoch 5/20, Train Loss: 0.769953, Val Loss: 0.939351\n",
      "Early stopping at epoch 6, Train Loss: 0.762452, Val Loss: 0.935774\n",
      "    Final - Train Loss: 0.762452, Val Loss: 0.935774\n",
      "[Saved] NN2_w512_2022Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.722468, Val Loss: 0.924141\n",
      "    Epoch 5/20, Train Loss: 0.688393, Val Loss: 0.924318\n",
      "Early stopping at epoch 8, Train Loss: 0.674604, Val Loss: 0.924637\n",
      "    Final - Train Loss: 0.674604, Val Loss: 0.924637\n",
      "[Saved] NN3_w512_2022Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.682149, Val Loss: 0.924926\n",
      "    Epoch 5/20, Train Loss: 0.654413, Val Loss: 0.929278\n",
      "Early stopping at epoch 6, Train Loss: 0.648808, Val Loss: 0.930310\n",
      "    Final - Train Loss: 0.648808, Val Loss: 0.930310\n",
      "[Saved] NN4_w512_2022Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.564465, Val Loss: 0.937191\n",
      "    Epoch 5/20, Train Loss: 0.540549, Val Loss: 0.935768\n",
      "Early stopping at epoch 8, Train Loss: 0.524220, Val Loss: 0.940055\n",
      "    Final - Train Loss: 0.524220, Val Loss: 0.940055\n",
      "[Saved] NN5_w512_2022Q1.pth\n",
      "    Training data size: 245389 samples\n",
      "  Quarter 2022Q2: Training models with expanding data\n",
      "    Added 3029 samples from 2022Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.920664, Val Loss: 0.956194\n",
      "    Epoch 5/20, Train Loss: 0.909809, Val Loss: 0.967659\n",
      "Early stopping at epoch 6, Train Loss: 0.907336, Val Loss: 0.971345\n",
      "    Final - Train Loss: 0.907336, Val Loss: 0.971345\n",
      "[Saved] NN1_w512_2022Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.787874, Val Loss: 0.959310\n",
      "    Epoch 5/20, Train Loss: 0.765242, Val Loss: 0.962706\n",
      "Early stopping at epoch 6, Train Loss: 0.758315, Val Loss: 0.962024\n",
      "    Final - Train Loss: 0.758315, Val Loss: 0.962024\n",
      "[Saved] NN2_w512_2022Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.698460, Val Loss: 0.952687\n",
      "    Epoch 5/20, Train Loss: 0.672964, Val Loss: 0.953730\n",
      "Early stopping at epoch 6, Train Loss: 0.666182, Val Loss: 0.954468\n",
      "    Final - Train Loss: 0.666182, Val Loss: 0.954468\n",
      "[Saved] NN3_w512_2022Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.675673, Val Loss: 0.956172\n",
      "    Epoch 5/20, Train Loss: 0.651907, Val Loss: 0.964238\n",
      "Early stopping at epoch 6, Train Loss: 0.647180, Val Loss: 0.966257\n",
      "    Final - Train Loss: 0.647180, Val Loss: 0.966257\n",
      "[Saved] NN4_w512_2022Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.553389, Val Loss: 0.959347\n",
      "    Epoch 5/20, Train Loss: 0.529147, Val Loss: 0.969666\n",
      "Early stopping at epoch 6, Train Loss: 0.524084, Val Loss: 0.968427\n",
      "    Final - Train Loss: 0.524084, Val Loss: 0.968427\n",
      "[Saved] NN5_w512_2022Q2.pth\n",
      "    Training data size: 248418 samples\n",
      "  Quarter 2022Q3: Training models with expanding data\n",
      "    Added 2969 samples from 2022Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.915791, Val Loss: 1.007584\n",
      "    Epoch 5/20, Train Loss: 0.905952, Val Loss: 1.016666\n",
      "Early stopping at epoch 6, Train Loss: 0.904520, Val Loss: 1.022056\n",
      "    Final - Train Loss: 0.904520, Val Loss: 1.022056\n",
      "[Saved] NN1_w512_2022Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.783099, Val Loss: 1.011209\n",
      "    Epoch 5/20, Train Loss: 0.758332, Val Loss: 1.016281\n",
      "Early stopping at epoch 8, Train Loss: 0.745699, Val Loss: 1.015273\n",
      "    Final - Train Loss: 0.745699, Val Loss: 1.015273\n",
      "[Saved] NN2_w512_2022Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.695126, Val Loss: 1.006808\n",
      "    Epoch 5/20, Train Loss: 0.672461, Val Loss: 1.008269\n",
      "Early stopping at epoch 7, Train Loss: 0.659084, Val Loss: 1.007416\n",
      "    Final - Train Loss: 0.659084, Val Loss: 1.007416\n",
      "[Saved] NN3_w512_2022Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.670382, Val Loss: 1.006570\n",
      "    Epoch 5/20, Train Loss: 0.647705, Val Loss: 1.011844\n",
      "Early stopping at epoch 6, Train Loss: 0.643388, Val Loss: 1.013613\n",
      "    Final - Train Loss: 0.643388, Val Loss: 1.013613\n",
      "[Saved] NN4_w512_2022Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.551735, Val Loss: 1.010963\n",
      "    Epoch 5/20, Train Loss: 0.529366, Val Loss: 1.020553\n",
      "Early stopping at epoch 6, Train Loss: 0.525763, Val Loss: 1.023903\n",
      "    Final - Train Loss: 0.525763, Val Loss: 1.023903\n",
      "[Saved] NN5_w512_2022Q3.pth\n",
      "    Training data size: 251387 samples\n",
      "  Quarter 2022Q4: Training models with expanding data\n",
      "    Added 3152 samples from 2022Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.907923, Val Loss: 1.051214\n",
      "    Epoch 5/20, Train Loss: 0.899666, Val Loss: 1.059140\n",
      "Early stopping at epoch 6, Train Loss: 0.897200, Val Loss: 1.065115\n",
      "    Final - Train Loss: 0.897200, Val Loss: 1.065115\n",
      "[Saved] NN1_w512_2022Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.762638, Val Loss: 1.048729\n",
      "    Epoch 5/20, Train Loss: 0.742640, Val Loss: 1.047688\n",
      "Early stopping at epoch 7, Train Loss: 0.738071, Val Loss: 1.047323\n",
      "    Final - Train Loss: 0.738071, Val Loss: 1.047323\n",
      "[Saved] NN2_w512_2022Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.680471, Val Loss: 1.040778\n",
      "    Epoch 5/20, Train Loss: 0.659549, Val Loss: 1.039945\n",
      "    Epoch 10/20, Train Loss: 0.649019, Val Loss: 1.044578\n",
      "Early stopping at epoch 10, Train Loss: 0.649019, Val Loss: 1.044578\n",
      "    Final - Train Loss: 0.649019, Val Loss: 1.044578\n",
      "[Saved] NN3_w512_2022Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.664176, Val Loss: 1.045259\n",
      "    Epoch 5/20, Train Loss: 0.643190, Val Loss: 1.052797\n",
      "Early stopping at epoch 7, Train Loss: 0.628800, Val Loss: 1.058907\n",
      "    Final - Train Loss: 0.628800, Val Loss: 1.058907\n",
      "[Saved] NN4_w512_2022Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.541544, Val Loss: 1.046647\n",
      "    Epoch 5/20, Train Loss: 0.524090, Val Loss: 1.051759\n",
      "Early stopping at epoch 7, Train Loss: 0.517725, Val Loss: 1.052986\n",
      "    Final - Train Loss: 0.517725, Val Loss: 1.052986\n",
      "[Saved] NN5_w512_2022Q4.pth\n",
      "    Training data size: 254539 samples\n",
      "  Quarter 2023Q1: Training models with expanding data\n",
      "    Added 3070 samples from 2022Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.901244, Val Loss: 1.087600\n",
      "    Epoch 5/20, Train Loss: 0.892174, Val Loss: 1.099470\n",
      "Early stopping at epoch 6, Train Loss: 0.890004, Val Loss: 1.102907\n",
      "    Final - Train Loss: 0.890004, Val Loss: 1.102907\n",
      "[Saved] NN1_w512_2023Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.745882, Val Loss: 1.080979\n",
      "    Epoch 5/20, Train Loss: 0.734219, Val Loss: 1.085237\n",
      "Early stopping at epoch 6, Train Loss: 0.729423, Val Loss: 1.086656\n",
      "    Final - Train Loss: 0.729423, Val Loss: 1.086656\n",
      "[Saved] NN2_w512_2023Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.658019, Val Loss: 1.075556\n",
      "    Epoch 5/20, Train Loss: 0.647450, Val Loss: 1.078177\n",
      "Early stopping at epoch 6, Train Loss: 0.641509, Val Loss: 1.075819\n",
      "    Final - Train Loss: 0.641509, Val Loss: 1.075819\n",
      "[Saved] NN3_w512_2023Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.650684, Val Loss: 1.079611\n",
      "    Epoch 5/20, Train Loss: 0.631032, Val Loss: 1.086263\n",
      "Early stopping at epoch 6, Train Loss: 0.629709, Val Loss: 1.084664\n",
      "    Final - Train Loss: 0.629709, Val Loss: 1.084664\n",
      "[Saved] NN4_w512_2023Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.535383, Val Loss: 1.084212\n",
      "    Epoch 5/20, Train Loss: 0.512320, Val Loss: 1.086233\n",
      "Early stopping at epoch 7, Train Loss: 0.510030, Val Loss: 1.088218\n",
      "    Final - Train Loss: 0.510030, Val Loss: 1.088218\n",
      "[Saved] NN5_w512_2023Q1.pth\n",
      "    Training data size: 257609 samples\n",
      "  Quarter 2023Q2: Training models with expanding data\n",
      "    Added 3064 samples from 2023Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.901968, Val Loss: 1.065962\n",
      "    Epoch 5/20, Train Loss: 0.893902, Val Loss: 1.074370\n",
      "Early stopping at epoch 6, Train Loss: 0.891175, Val Loss: 1.079196\n",
      "    Final - Train Loss: 0.891175, Val Loss: 1.079196\n",
      "[Saved] NN1_w512_2023Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.752608, Val Loss: 1.063281\n",
      "    Epoch 5/20, Train Loss: 0.731728, Val Loss: 1.064589\n",
      "Early stopping at epoch 9, Train Loss: 0.722554, Val Loss: 1.065706\n",
      "    Final - Train Loss: 0.722554, Val Loss: 1.065706\n",
      "[Saved] NN2_w512_2023Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.659283, Val Loss: 1.056732\n",
      "    Epoch 5/20, Train Loss: 0.645544, Val Loss: 1.059373\n",
      "Early stopping at epoch 6, Train Loss: 0.644681, Val Loss: 1.059202\n",
      "    Final - Train Loss: 0.644681, Val Loss: 1.059202\n",
      "[Saved] NN3_w512_2023Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.651688, Val Loss: 1.057606\n",
      "    Epoch 5/20, Train Loss: 0.635742, Val Loss: 1.060612\n",
      "Early stopping at epoch 8, Train Loss: 0.622187, Val Loss: 1.065838\n",
      "    Final - Train Loss: 0.622187, Val Loss: 1.065838\n",
      "[Saved] NN4_w512_2023Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.531158, Val Loss: 1.079129\n",
      "    Epoch 5/20, Train Loss: 0.514297, Val Loss: 1.102304\n",
      "Early stopping at epoch 6, Train Loss: 0.512534, Val Loss: 1.090728\n",
      "    Final - Train Loss: 0.512534, Val Loss: 1.090728\n",
      "[Saved] NN5_w512_2023Q2.pth\n",
      "    Training data size: 260673 samples\n",
      "  Quarter 2023Q3: Training models with expanding data\n",
      "    Added 3069 samples from 2023Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.897530, Val Loss: 1.070904\n",
      "    Epoch 5/20, Train Loss: 0.890368, Val Loss: 1.081649\n",
      "Early stopping at epoch 6, Train Loss: 0.888899, Val Loss: 1.085669\n",
      "    Final - Train Loss: 0.888899, Val Loss: 1.085669\n",
      "[Saved] NN1_w512_2023Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.736003, Val Loss: 1.063612\n",
      "    Epoch 5/20, Train Loss: 0.720479, Val Loss: 1.064937\n",
      "Early stopping at epoch 9, Train Loss: 0.713521, Val Loss: 1.067332\n",
      "    Final - Train Loss: 0.713521, Val Loss: 1.067332\n",
      "[Saved] NN2_w512_2023Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.654145, Val Loss: 1.060492\n",
      "    Epoch 5/20, Train Loss: 0.644605, Val Loss: 1.063416\n",
      "Early stopping at epoch 6, Train Loss: 0.644189, Val Loss: 1.062655\n",
      "    Final - Train Loss: 0.644189, Val Loss: 1.062655\n",
      "[Saved] NN3_w512_2023Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.640357, Val Loss: 1.062052\n",
      "    Epoch 5/20, Train Loss: 0.621268, Val Loss: 1.066843\n",
      "Early stopping at epoch 6, Train Loss: 0.616862, Val Loss: 1.067534\n",
      "    Final - Train Loss: 0.616862, Val Loss: 1.067534\n",
      "[Saved] NN4_w512_2023Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.530208, Val Loss: 1.069245\n",
      "    Epoch 5/20, Train Loss: 0.515567, Val Loss: 1.074433\n",
      "Early stopping at epoch 6, Train Loss: 0.507573, Val Loss: 1.072483\n",
      "    Final - Train Loss: 0.507573, Val Loss: 1.072483\n",
      "[Saved] NN5_w512_2023Q3.pth\n",
      "    Training data size: 263742 samples\n",
      "  Quarter 2023Q4: Training models with expanding data\n",
      "    Added 3121 samples from 2023Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.891662, Val Loss: 1.069704\n",
      "    Epoch 5/20, Train Loss: 0.884513, Val Loss: 1.083674\n",
      "Early stopping at epoch 6, Train Loss: 0.883319, Val Loss: 1.082639\n",
      "    Final - Train Loss: 0.883319, Val Loss: 1.082639\n",
      "[Saved] NN1_w512_2023Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.723520, Val Loss: 1.069142\n",
      "    Epoch 5/20, Train Loss: 0.712629, Val Loss: 1.068447\n",
      "Early stopping at epoch 8, Train Loss: 0.703984, Val Loss: 1.067345\n",
      "    Final - Train Loss: 0.703984, Val Loss: 1.067345\n",
      "[Saved] NN2_w512_2023Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.649998, Val Loss: 1.062984\n",
      "    Epoch 5/20, Train Loss: 0.643364, Val Loss: 1.065615\n",
      "Early stopping at epoch 8, Train Loss: 0.637200, Val Loss: 1.067691\n",
      "    Final - Train Loss: 0.637200, Val Loss: 1.067691\n",
      "[Saved] NN3_w512_2023Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.632908, Val Loss: 1.060947\n",
      "    Epoch 5/20, Train Loss: 0.615047, Val Loss: 1.065250\n",
      "Early stopping at epoch 6, Train Loss: 0.614621, Val Loss: 1.065230\n",
      "    Final - Train Loss: 0.614621, Val Loss: 1.065230\n",
      "[Saved] NN4_w512_2023Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.527898, Val Loss: 1.069257\n",
      "    Epoch 5/20, Train Loss: 0.511937, Val Loss: 1.073843\n",
      "Early stopping at epoch 6, Train Loss: 0.505483, Val Loss: 1.072849\n",
      "    Final - Train Loss: 0.505483, Val Loss: 1.072849\n",
      "[Saved] NN5_w512_2023Q4.pth\n",
      "    Training data size: 266863 samples\n",
      "  Quarter 2024Q1: Training models with expanding data\n",
      "    Added 3113 samples from 2023Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.886094, Val Loss: 1.080529\n",
      "    Epoch 5/20, Train Loss: 0.880540, Val Loss: 1.091867\n",
      "Early stopping at epoch 6, Train Loss: 0.877828, Val Loss: 1.094947\n",
      "    Final - Train Loss: 0.877828, Val Loss: 1.094947\n",
      "[Saved] NN1_w512_2024Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.711151, Val Loss: 1.077818\n",
      "    Epoch 5/20, Train Loss: 0.702875, Val Loss: 1.079126\n",
      "Early stopping at epoch 6, Train Loss: 0.703366, Val Loss: 1.079990\n",
      "    Final - Train Loss: 0.703366, Val Loss: 1.079990\n",
      "[Saved] NN2_w512_2024Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.645225, Val Loss: 1.071669\n",
      "    Epoch 5/20, Train Loss: 0.637791, Val Loss: 1.072872\n",
      "Early stopping at epoch 7, Train Loss: 0.630851, Val Loss: 1.076760\n",
      "    Final - Train Loss: 0.630851, Val Loss: 1.076760\n",
      "[Saved] NN3_w512_2024Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.626760, Val Loss: 1.073742\n",
      "    Epoch 5/20, Train Loss: 0.612984, Val Loss: 1.078368\n",
      "Early stopping at epoch 6, Train Loss: 0.607658, Val Loss: 1.079349\n",
      "    Final - Train Loss: 0.607658, Val Loss: 1.079349\n",
      "[Saved] NN4_w512_2024Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.526652, Val Loss: 1.080717\n",
      "    Epoch 5/20, Train Loss: 0.506884, Val Loss: 1.082870\n",
      "Early stopping at epoch 7, Train Loss: 0.500655, Val Loss: 1.084734\n",
      "    Final - Train Loss: 0.500655, Val Loss: 1.084734\n",
      "[Saved] NN5_w512_2024Q1.pth\n",
      "    Training data size: 269976 samples\n",
      "  Quarter 2024Q2: Training models with expanding data\n",
      "    Added 3026 samples from 2024Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.885739, Val Loss: 1.064127\n",
      "    Epoch 5/20, Train Loss: 0.878329, Val Loss: 1.077388\n",
      "Early stopping at epoch 6, Train Loss: 0.876657, Val Loss: 1.077556\n",
      "    Final - Train Loss: 0.876657, Val Loss: 1.077556\n",
      "[Saved] NN1_w512_2024Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.712609, Val Loss: 1.059422\n",
      "    Epoch 5/20, Train Loss: 0.705107, Val Loss: 1.060278\n",
      "Early stopping at epoch 7, Train Loss: 0.699828, Val Loss: 1.060011\n",
      "    Final - Train Loss: 0.699828, Val Loss: 1.060011\n",
      "[Saved] NN2_w512_2024Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.641282, Val Loss: 1.056039\n",
      "    Epoch 5/20, Train Loss: 0.633817, Val Loss: 1.058740\n",
      "Early stopping at epoch 7, Train Loss: 0.630266, Val Loss: 1.061770\n",
      "    Final - Train Loss: 0.630266, Val Loss: 1.061770\n",
      "[Saved] NN3_w512_2024Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.628999, Val Loss: 1.054699\n",
      "    Epoch 5/20, Train Loss: 0.611451, Val Loss: 1.062891\n",
      "Early stopping at epoch 6, Train Loss: 0.606229, Val Loss: 1.068394\n",
      "    Final - Train Loss: 0.606229, Val Loss: 1.068394\n",
      "[Saved] NN4_w512_2024Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.525780, Val Loss: 1.070216\n",
      "    Epoch 5/20, Train Loss: 0.501889, Val Loss: 1.071498\n",
      "Early stopping at epoch 8, Train Loss: 0.494455, Val Loss: 1.070384\n",
      "    Final - Train Loss: 0.494455, Val Loss: 1.070384\n",
      "[Saved] NN5_w512_2024Q2.pth\n",
      "    Training data size: 273002 samples\n",
      "  Quarter 2024Q3: Training models with expanding data\n",
      "    Added 3112 samples from 2024Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.879636, Val Loss: 1.071710\n",
      "    Epoch 5/20, Train Loss: 0.873055, Val Loss: 1.082472\n",
      "Early stopping at epoch 6, Train Loss: 0.871404, Val Loss: 1.085944\n",
      "    Final - Train Loss: 0.871404, Val Loss: 1.085944\n",
      "[Saved] NN1_w512_2024Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.708656, Val Loss: 1.063174\n",
      "    Epoch 5/20, Train Loss: 0.698129, Val Loss: 1.068913\n",
      "Early stopping at epoch 6, Train Loss: 0.698217, Val Loss: 1.070506\n",
      "    Final - Train Loss: 0.698217, Val Loss: 1.070506\n",
      "[Saved] NN2_w512_2024Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.635036, Val Loss: 1.061902\n",
      "    Epoch 5/20, Train Loss: 0.630115, Val Loss: 1.063909\n",
      "Early stopping at epoch 6, Train Loss: 0.629027, Val Loss: 1.068428\n",
      "    Final - Train Loss: 0.629027, Val Loss: 1.068428\n",
      "[Saved] NN3_w512_2024Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.622937, Val Loss: 1.062346\n",
      "    Epoch 5/20, Train Loss: 0.607137, Val Loss: 1.073485\n",
      "Early stopping at epoch 6, Train Loss: 0.601619, Val Loss: 1.080688\n",
      "    Final - Train Loss: 0.601619, Val Loss: 1.080688\n",
      "[Saved] NN4_w512_2024Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.513919, Val Loss: 1.068143\n",
      "    Epoch 5/20, Train Loss: 0.496847, Val Loss: 1.069090\n",
      "Early stopping at epoch 9, Train Loss: 0.486857, Val Loss: 1.078245\n",
      "    Final - Train Loss: 0.486857, Val Loss: 1.078245\n",
      "[Saved] NN5_w512_2024Q3.pth\n",
      "    Training data size: 276114 samples\n",
      "MLP Quarterly Expanding Window Training Completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NN1_w5': {'learning_rate': 0.001363296120391928,\n",
       "  'batch_size': 64,\n",
       "  'dropout_rate': 0.31201103153955173},\n",
       " 'NN2_w5': {'learning_rate': 0.001363296120391928,\n",
       "  'batch_size': 64,\n",
       "  'dropout_rate': 0.31201103153955173},\n",
       " 'NN3_w5': {'learning_rate': 0.001363296120391928,\n",
       "  'batch_size': 64,\n",
       "  'dropout_rate': 0.31201103153955173},\n",
       " 'NN4_w5': {'learning_rate': 0.001363296120391928,\n",
       "  'batch_size': 64,\n",
       "  'dropout_rate': 0.31201103153955173},\n",
       " 'NN5_w5': {'learning_rate': 0.001363296120391928,\n",
       "  'batch_size': 64,\n",
       "  'dropout_rate': 0.31201103153955173},\n",
       " 'NN1_w21': {'learning_rate': 0.007962341411229049,\n",
       "  'batch_size': 64,\n",
       "  'dropout_rate': 0.316043700121437},\n",
       " 'NN2_w21': {'learning_rate': 0.007962341411229049,\n",
       "  'batch_size': 64,\n",
       "  'dropout_rate': 0.316043700121437},\n",
       " 'NN3_w21': {'learning_rate': 0.007962341411229049,\n",
       "  'batch_size': 64,\n",
       "  'dropout_rate': 0.316043700121437},\n",
       " 'NN4_w21': {'learning_rate': 0.007962341411229049,\n",
       "  'batch_size': 64,\n",
       "  'dropout_rate': 0.316043700121437},\n",
       " 'NN5_w21': {'learning_rate': 0.007962341411229049,\n",
       "  'batch_size': 64,\n",
       "  'dropout_rate': 0.316043700121437},\n",
       " 'NN1_w252': {'learning_rate': 0.006127290841275993,\n",
       "  'batch_size': 64,\n",
       "  'dropout_rate': 0.21432812451238947},\n",
       " 'NN2_w252': {'learning_rate': 0.006127290841275993,\n",
       "  'batch_size': 64,\n",
       "  'dropout_rate': 0.21432812451238947},\n",
       " 'NN3_w252': {'learning_rate': 0.006127290841275993,\n",
       "  'batch_size': 64,\n",
       "  'dropout_rate': 0.21432812451238947},\n",
       " 'NN4_w252': {'learning_rate': 0.006127290841275993,\n",
       "  'batch_size': 64,\n",
       "  'dropout_rate': 0.21432812451238947},\n",
       " 'NN5_w252': {'learning_rate': 0.006127290841275993,\n",
       "  'batch_size': 64,\n",
       "  'dropout_rate': 0.21432812451238947},\n",
       " 'NN1_w512': {'learning_rate': 0.0011573914700648597,\n",
       "  'batch_size': 64,\n",
       "  'dropout_rate': 0.46351980445719754},\n",
       " 'NN2_w512': {'learning_rate': 0.0011573914700648597,\n",
       "  'batch_size': 64,\n",
       "  'dropout_rate': 0.46351980445719754},\n",
       " 'NN3_w512': {'learning_rate': 0.0011573914700648597,\n",
       "  'batch_size': 64,\n",
       "  'dropout_rate': 0.46351980445719754},\n",
       " 'NN4_w512': {'learning_rate': 0.0011573914700648597,\n",
       "  'batch_size': 64,\n",
       "  'dropout_rate': 0.46351980445719754},\n",
       " 'NN5_w512': {'learning_rate': 0.0011573914700648597,\n",
       "  'batch_size': 64,\n",
       "  'dropout_rate': 0.46351980445719754}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mlp_models_expanding_quarterly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Daily Rebalance Portfolio Backtesting Simulation\n",
      "Loaded y scaler for window 5\n",
      "Loaded y scaler for window 21\n",
      "Loaded y scaler for window 252\n",
      "Loaded y scaler for window 512\n",
      "Processing window size: 5\n",
      "  Model: NN1, Scheme: VW\n",
      "      Applied inverse scaling for window 5 - 3006 samples\n",
      "      Applied inverse scaling for window 5 - 3170 samples\n",
      "      Applied inverse scaling for window 5 - 3176 samples\n",
      "      Applied inverse scaling for window 5 - 3123 samples\n",
      "      Applied inverse scaling for window 5 - 3083 samples\n",
      "      Applied inverse scaling for window 5 - 3122 samples\n",
      "      Applied inverse scaling for window 5 - 3114 samples\n",
      "      Applied inverse scaling for window 5 - 3115 samples\n",
      "      Applied inverse scaling for window 5 - 2996 samples\n",
      "      Applied inverse scaling for window 5 - 3160 samples\n",
      "      Applied inverse scaling for window 5 - 3125 samples\n",
      "      Applied inverse scaling for window 5 - 3045 samples\n",
      "      Applied inverse scaling for window 5 - 3022 samples\n",
      "      Applied inverse scaling for window 5 - 3120 samples\n",
      "      Applied inverse scaling for window 5 - 3167 samples\n",
      "      Applied inverse scaling for window 5 - 3179 samples\n",
      "      Applied inverse scaling for window 5 - 2595 samples\n",
      "      Applied inverse scaling for window 5 - 2839 samples\n",
      "      Applied inverse scaling for window 5 - 3151 samples\n",
      "      Applied inverse scaling for window 5 - 3114 samples\n",
      "      Applied inverse scaling for window 5 - 2989 samples\n",
      "      Applied inverse scaling for window 5 - 3132 samples\n",
      "      Applied inverse scaling for window 5 - 3173 samples\n",
      "      Applied inverse scaling for window 5 - 3153 samples\n",
      "      Applied inverse scaling for window 5 - 3029 samples\n",
      "      Applied inverse scaling for window 5 - 2969 samples\n",
      "      Applied inverse scaling for window 5 - 3152 samples\n",
      "      Applied inverse scaling for window 5 - 3070 samples\n",
      "      Applied inverse scaling for window 5 - 3064 samples\n",
      "      Applied inverse scaling for window 5 - 3069 samples\n",
      "      Applied inverse scaling for window 5 - 3121 samples\n",
      "      Applied inverse scaling for window 5 - 3113 samples\n",
      "      Applied inverse scaling for window 5 - 3026 samples\n",
      "      Applied inverse scaling for window 5 - 3112 samples\n",
      "      Applied inverse scaling for window 5 - 3162 samples\n",
      "      Applied inverse scaling for window 5 - 3094 samples\n",
      "[Update] Metrics updated for NN1 w=5\n",
      "  Model: NN1, Scheme: EW\n",
      "      Applied inverse scaling for window 5 - 3006 samples\n",
      "      Applied inverse scaling for window 5 - 3170 samples\n",
      "      Applied inverse scaling for window 5 - 3176 samples\n",
      "      Applied inverse scaling for window 5 - 3123 samples\n",
      "      Applied inverse scaling for window 5 - 3083 samples\n",
      "      Applied inverse scaling for window 5 - 3122 samples\n",
      "      Applied inverse scaling for window 5 - 3114 samples\n",
      "      Applied inverse scaling for window 5 - 3115 samples\n",
      "      Applied inverse scaling for window 5 - 2996 samples\n",
      "      Applied inverse scaling for window 5 - 3160 samples\n",
      "      Applied inverse scaling for window 5 - 3125 samples\n",
      "      Applied inverse scaling for window 5 - 3045 samples\n",
      "      Applied inverse scaling for window 5 - 3022 samples\n",
      "      Applied inverse scaling for window 5 - 3120 samples\n",
      "      Applied inverse scaling for window 5 - 3167 samples\n",
      "      Applied inverse scaling for window 5 - 3179 samples\n",
      "      Applied inverse scaling for window 5 - 2595 samples\n",
      "      Applied inverse scaling for window 5 - 2839 samples\n",
      "      Applied inverse scaling for window 5 - 3151 samples\n",
      "      Applied inverse scaling for window 5 - 3114 samples\n",
      "      Applied inverse scaling for window 5 - 2989 samples\n",
      "      Applied inverse scaling for window 5 - 3132 samples\n",
      "      Applied inverse scaling for window 5 - 3173 samples\n",
      "      Applied inverse scaling for window 5 - 3153 samples\n",
      "      Applied inverse scaling for window 5 - 3029 samples\n",
      "      Applied inverse scaling for window 5 - 2969 samples\n",
      "      Applied inverse scaling for window 5 - 3152 samples\n",
      "      Applied inverse scaling for window 5 - 3070 samples\n",
      "      Applied inverse scaling for window 5 - 3064 samples\n",
      "      Applied inverse scaling for window 5 - 3069 samples\n",
      "      Applied inverse scaling for window 5 - 3121 samples\n",
      "      Applied inverse scaling for window 5 - 3113 samples\n",
      "      Applied inverse scaling for window 5 - 3026 samples\n",
      "      Applied inverse scaling for window 5 - 3112 samples\n",
      "      Applied inverse scaling for window 5 - 3162 samples\n",
      "      Applied inverse scaling for window 5 - 3094 samples\n",
      "  Model: NN2, Scheme: VW\n",
      "      Applied inverse scaling for window 5 - 3006 samples\n",
      "      Applied inverse scaling for window 5 - 3170 samples\n",
      "      Applied inverse scaling for window 5 - 3176 samples\n",
      "      Applied inverse scaling for window 5 - 3123 samples\n",
      "      Applied inverse scaling for window 5 - 3083 samples\n",
      "      Applied inverse scaling for window 5 - 3122 samples\n",
      "      Applied inverse scaling for window 5 - 3114 samples\n",
      "      Applied inverse scaling for window 5 - 3115 samples\n",
      "      Applied inverse scaling for window 5 - 2996 samples\n",
      "      Applied inverse scaling for window 5 - 3160 samples\n",
      "      Applied inverse scaling for window 5 - 3125 samples\n",
      "      Applied inverse scaling for window 5 - 3045 samples\n",
      "      Applied inverse scaling for window 5 - 3022 samples\n",
      "      Applied inverse scaling for window 5 - 3120 samples\n",
      "      Applied inverse scaling for window 5 - 3167 samples\n",
      "      Applied inverse scaling for window 5 - 3179 samples\n",
      "      Applied inverse scaling for window 5 - 2595 samples\n",
      "      Applied inverse scaling for window 5 - 2839 samples\n",
      "      Applied inverse scaling for window 5 - 3151 samples\n",
      "      Applied inverse scaling for window 5 - 3114 samples\n",
      "      Applied inverse scaling for window 5 - 2989 samples\n",
      "      Applied inverse scaling for window 5 - 3132 samples\n",
      "      Applied inverse scaling for window 5 - 3173 samples\n",
      "      Applied inverse scaling for window 5 - 3153 samples\n",
      "      Applied inverse scaling for window 5 - 3029 samples\n",
      "      Applied inverse scaling for window 5 - 2969 samples\n",
      "      Applied inverse scaling for window 5 - 3152 samples\n",
      "      Applied inverse scaling for window 5 - 3070 samples\n",
      "      Applied inverse scaling for window 5 - 3064 samples\n",
      "      Applied inverse scaling for window 5 - 3069 samples\n",
      "      Applied inverse scaling for window 5 - 3121 samples\n",
      "      Applied inverse scaling for window 5 - 3113 samples\n",
      "      Applied inverse scaling for window 5 - 3026 samples\n",
      "      Applied inverse scaling for window 5 - 3112 samples\n",
      "      Applied inverse scaling for window 5 - 3162 samples\n",
      "      Applied inverse scaling for window 5 - 3094 samples\n",
      "[Update] Metrics updated for NN2 w=5\n",
      "  Model: NN2, Scheme: EW\n",
      "      Applied inverse scaling for window 5 - 3006 samples\n",
      "      Applied inverse scaling for window 5 - 3170 samples\n",
      "      Applied inverse scaling for window 5 - 3176 samples\n",
      "      Applied inverse scaling for window 5 - 3123 samples\n",
      "      Applied inverse scaling for window 5 - 3083 samples\n",
      "      Applied inverse scaling for window 5 - 3122 samples\n",
      "      Applied inverse scaling for window 5 - 3114 samples\n",
      "      Applied inverse scaling for window 5 - 3115 samples\n",
      "      Applied inverse scaling for window 5 - 2996 samples\n",
      "      Applied inverse scaling for window 5 - 3160 samples\n",
      "      Applied inverse scaling for window 5 - 3125 samples\n",
      "      Applied inverse scaling for window 5 - 3045 samples\n",
      "      Applied inverse scaling for window 5 - 3022 samples\n",
      "      Applied inverse scaling for window 5 - 3120 samples\n",
      "      Applied inverse scaling for window 5 - 3167 samples\n",
      "      Applied inverse scaling for window 5 - 3179 samples\n",
      "      Applied inverse scaling for window 5 - 2595 samples\n",
      "      Applied inverse scaling for window 5 - 2839 samples\n",
      "      Applied inverse scaling for window 5 - 3151 samples\n",
      "      Applied inverse scaling for window 5 - 3114 samples\n",
      "      Applied inverse scaling for window 5 - 2989 samples\n",
      "      Applied inverse scaling for window 5 - 3132 samples\n",
      "      Applied inverse scaling for window 5 - 3173 samples\n",
      "      Applied inverse scaling for window 5 - 3153 samples\n",
      "      Applied inverse scaling for window 5 - 3029 samples\n",
      "      Applied inverse scaling for window 5 - 2969 samples\n",
      "      Applied inverse scaling for window 5 - 3152 samples\n",
      "      Applied inverse scaling for window 5 - 3070 samples\n",
      "      Applied inverse scaling for window 5 - 3064 samples\n",
      "      Applied inverse scaling for window 5 - 3069 samples\n",
      "      Applied inverse scaling for window 5 - 3121 samples\n",
      "      Applied inverse scaling for window 5 - 3113 samples\n",
      "      Applied inverse scaling for window 5 - 3026 samples\n",
      "      Applied inverse scaling for window 5 - 3112 samples\n",
      "      Applied inverse scaling for window 5 - 3162 samples\n",
      "      Applied inverse scaling for window 5 - 3094 samples\n",
      "  Model: NN3, Scheme: VW\n",
      "      Applied inverse scaling for window 5 - 3006 samples\n",
      "      Applied inverse scaling for window 5 - 3170 samples\n",
      "      Applied inverse scaling for window 5 - 3176 samples\n",
      "      Applied inverse scaling for window 5 - 3123 samples\n",
      "      Applied inverse scaling for window 5 - 3083 samples\n",
      "      Applied inverse scaling for window 5 - 3122 samples\n",
      "      Applied inverse scaling for window 5 - 3114 samples\n",
      "      Applied inverse scaling for window 5 - 3115 samples\n",
      "      Applied inverse scaling for window 5 - 2996 samples\n",
      "      Applied inverse scaling for window 5 - 3160 samples\n",
      "      Applied inverse scaling for window 5 - 3125 samples\n",
      "      Applied inverse scaling for window 5 - 3045 samples\n",
      "      Applied inverse scaling for window 5 - 3022 samples\n",
      "      Applied inverse scaling for window 5 - 3120 samples\n",
      "      Applied inverse scaling for window 5 - 3167 samples\n",
      "      Applied inverse scaling for window 5 - 3179 samples\n",
      "      Applied inverse scaling for window 5 - 2595 samples\n",
      "      Applied inverse scaling for window 5 - 2839 samples\n",
      "      Applied inverse scaling for window 5 - 3151 samples\n",
      "      Applied inverse scaling for window 5 - 3114 samples\n",
      "      Applied inverse scaling for window 5 - 2989 samples\n",
      "      Applied inverse scaling for window 5 - 3132 samples\n",
      "      Applied inverse scaling for window 5 - 3173 samples\n",
      "      Applied inverse scaling for window 5 - 3153 samples\n",
      "      Applied inverse scaling for window 5 - 3029 samples\n",
      "      Applied inverse scaling for window 5 - 2969 samples\n",
      "      Applied inverse scaling for window 5 - 3152 samples\n",
      "      Applied inverse scaling for window 5 - 3070 samples\n",
      "      Applied inverse scaling for window 5 - 3064 samples\n",
      "      Applied inverse scaling for window 5 - 3069 samples\n",
      "      Applied inverse scaling for window 5 - 3121 samples\n",
      "      Applied inverse scaling for window 5 - 3113 samples\n",
      "      Applied inverse scaling for window 5 - 3026 samples\n",
      "      Applied inverse scaling for window 5 - 3112 samples\n",
      "      Applied inverse scaling for window 5 - 3162 samples\n",
      "      Applied inverse scaling for window 5 - 3094 samples\n",
      "[Update] Metrics updated for NN3 w=5\n",
      "  Model: NN3, Scheme: EW\n",
      "      Applied inverse scaling for window 5 - 3006 samples\n",
      "      Applied inverse scaling for window 5 - 3170 samples\n",
      "      Applied inverse scaling for window 5 - 3176 samples\n",
      "      Applied inverse scaling for window 5 - 3123 samples\n",
      "      Applied inverse scaling for window 5 - 3083 samples\n",
      "      Applied inverse scaling for window 5 - 3122 samples\n",
      "      Applied inverse scaling for window 5 - 3114 samples\n",
      "      Applied inverse scaling for window 5 - 3115 samples\n",
      "      Applied inverse scaling for window 5 - 2996 samples\n",
      "      Applied inverse scaling for window 5 - 3160 samples\n",
      "      Applied inverse scaling for window 5 - 3125 samples\n",
      "      Applied inverse scaling for window 5 - 3045 samples\n",
      "      Applied inverse scaling for window 5 - 3022 samples\n",
      "      Applied inverse scaling for window 5 - 3120 samples\n",
      "      Applied inverse scaling for window 5 - 3167 samples\n",
      "      Applied inverse scaling for window 5 - 3179 samples\n",
      "      Applied inverse scaling for window 5 - 2595 samples\n",
      "      Applied inverse scaling for window 5 - 2839 samples\n",
      "      Applied inverse scaling for window 5 - 3151 samples\n",
      "      Applied inverse scaling for window 5 - 3114 samples\n",
      "      Applied inverse scaling for window 5 - 2989 samples\n",
      "      Applied inverse scaling for window 5 - 3132 samples\n",
      "      Applied inverse scaling for window 5 - 3173 samples\n",
      "      Applied inverse scaling for window 5 - 3153 samples\n",
      "      Applied inverse scaling for window 5 - 3029 samples\n",
      "      Applied inverse scaling for window 5 - 2969 samples\n",
      "      Applied inverse scaling for window 5 - 3152 samples\n",
      "      Applied inverse scaling for window 5 - 3070 samples\n",
      "      Applied inverse scaling for window 5 - 3064 samples\n",
      "      Applied inverse scaling for window 5 - 3069 samples\n",
      "      Applied inverse scaling for window 5 - 3121 samples\n",
      "      Applied inverse scaling for window 5 - 3113 samples\n",
      "      Applied inverse scaling for window 5 - 3026 samples\n",
      "      Applied inverse scaling for window 5 - 3112 samples\n",
      "      Applied inverse scaling for window 5 - 3162 samples\n",
      "      Applied inverse scaling for window 5 - 3094 samples\n",
      "  Model: NN4, Scheme: VW\n",
      "      Applied inverse scaling for window 5 - 3006 samples\n",
      "      Applied inverse scaling for window 5 - 3170 samples\n",
      "      Applied inverse scaling for window 5 - 3176 samples\n",
      "      Applied inverse scaling for window 5 - 3123 samples\n",
      "      Applied inverse scaling for window 5 - 3083 samples\n",
      "      Applied inverse scaling for window 5 - 3122 samples\n",
      "      Applied inverse scaling for window 5 - 3114 samples\n",
      "      Applied inverse scaling for window 5 - 3115 samples\n",
      "      Applied inverse scaling for window 5 - 2996 samples\n",
      "      Applied inverse scaling for window 5 - 3160 samples\n",
      "      Applied inverse scaling for window 5 - 3125 samples\n",
      "      Applied inverse scaling for window 5 - 3045 samples\n",
      "      Applied inverse scaling for window 5 - 3022 samples\n",
      "      Applied inverse scaling for window 5 - 3120 samples\n",
      "      Applied inverse scaling for window 5 - 3167 samples\n",
      "      Applied inverse scaling for window 5 - 3179 samples\n",
      "      Applied inverse scaling for window 5 - 2595 samples\n",
      "      Applied inverse scaling for window 5 - 2839 samples\n",
      "      Applied inverse scaling for window 5 - 3151 samples\n",
      "      Applied inverse scaling for window 5 - 3114 samples\n",
      "      Applied inverse scaling for window 5 - 2989 samples\n",
      "      Applied inverse scaling for window 5 - 3132 samples\n",
      "      Applied inverse scaling for window 5 - 3173 samples\n",
      "      Applied inverse scaling for window 5 - 3153 samples\n",
      "      Applied inverse scaling for window 5 - 3029 samples\n",
      "      Applied inverse scaling for window 5 - 2969 samples\n",
      "      Applied inverse scaling for window 5 - 3152 samples\n",
      "      Applied inverse scaling for window 5 - 3070 samples\n",
      "      Applied inverse scaling for window 5 - 3064 samples\n",
      "      Applied inverse scaling for window 5 - 3069 samples\n",
      "      Applied inverse scaling for window 5 - 3121 samples\n",
      "      Applied inverse scaling for window 5 - 3113 samples\n",
      "      Applied inverse scaling for window 5 - 3026 samples\n",
      "      Applied inverse scaling for window 5 - 3112 samples\n",
      "      Applied inverse scaling for window 5 - 3162 samples\n",
      "      Applied inverse scaling for window 5 - 3094 samples\n",
      "[Update] Metrics updated for NN4 w=5\n",
      "  Model: NN4, Scheme: EW\n",
      "      Applied inverse scaling for window 5 - 3006 samples\n",
      "      Applied inverse scaling for window 5 - 3170 samples\n",
      "      Applied inverse scaling for window 5 - 3176 samples\n",
      "      Applied inverse scaling for window 5 - 3123 samples\n",
      "      Applied inverse scaling for window 5 - 3083 samples\n",
      "      Applied inverse scaling for window 5 - 3122 samples\n",
      "      Applied inverse scaling for window 5 - 3114 samples\n",
      "      Applied inverse scaling for window 5 - 3115 samples\n",
      "      Applied inverse scaling for window 5 - 2996 samples\n",
      "      Applied inverse scaling for window 5 - 3160 samples\n",
      "      Applied inverse scaling for window 5 - 3125 samples\n",
      "      Applied inverse scaling for window 5 - 3045 samples\n",
      "      Applied inverse scaling for window 5 - 3022 samples\n",
      "      Applied inverse scaling for window 5 - 3120 samples\n",
      "      Applied inverse scaling for window 5 - 3167 samples\n",
      "      Applied inverse scaling for window 5 - 3179 samples\n",
      "      Applied inverse scaling for window 5 - 2595 samples\n",
      "      Applied inverse scaling for window 5 - 2839 samples\n",
      "      Applied inverse scaling for window 5 - 3151 samples\n",
      "      Applied inverse scaling for window 5 - 3114 samples\n",
      "      Applied inverse scaling for window 5 - 2989 samples\n",
      "      Applied inverse scaling for window 5 - 3132 samples\n",
      "      Applied inverse scaling for window 5 - 3173 samples\n",
      "      Applied inverse scaling for window 5 - 3153 samples\n",
      "      Applied inverse scaling for window 5 - 3029 samples\n",
      "      Applied inverse scaling for window 5 - 2969 samples\n",
      "      Applied inverse scaling for window 5 - 3152 samples\n",
      "      Applied inverse scaling for window 5 - 3070 samples\n",
      "      Applied inverse scaling for window 5 - 3064 samples\n",
      "      Applied inverse scaling for window 5 - 3069 samples\n",
      "      Applied inverse scaling for window 5 - 3121 samples\n",
      "      Applied inverse scaling for window 5 - 3113 samples\n",
      "      Applied inverse scaling for window 5 - 3026 samples\n",
      "      Applied inverse scaling for window 5 - 3112 samples\n",
      "      Applied inverse scaling for window 5 - 3162 samples\n",
      "      Applied inverse scaling for window 5 - 3094 samples\n",
      "  Model: NN5, Scheme: VW\n",
      "      Applied inverse scaling for window 5 - 3006 samples\n",
      "      Applied inverse scaling for window 5 - 3170 samples\n",
      "      Applied inverse scaling for window 5 - 3176 samples\n",
      "      Applied inverse scaling for window 5 - 3123 samples\n",
      "      Applied inverse scaling for window 5 - 3083 samples\n",
      "      Applied inverse scaling for window 5 - 3122 samples\n",
      "      Applied inverse scaling for window 5 - 3114 samples\n",
      "      Applied inverse scaling for window 5 - 3115 samples\n",
      "      Applied inverse scaling for window 5 - 2996 samples\n",
      "      Applied inverse scaling for window 5 - 3160 samples\n",
      "      Applied inverse scaling for window 5 - 3125 samples\n",
      "      Applied inverse scaling for window 5 - 3045 samples\n",
      "      Applied inverse scaling for window 5 - 3022 samples\n",
      "      Applied inverse scaling for window 5 - 3120 samples\n",
      "      Applied inverse scaling for window 5 - 3167 samples\n",
      "      Applied inverse scaling for window 5 - 3179 samples\n",
      "      Applied inverse scaling for window 5 - 2595 samples\n",
      "      Applied inverse scaling for window 5 - 2839 samples\n",
      "      Applied inverse scaling for window 5 - 3151 samples\n",
      "      Applied inverse scaling for window 5 - 3114 samples\n",
      "      Applied inverse scaling for window 5 - 2989 samples\n",
      "      Applied inverse scaling for window 5 - 3132 samples\n",
      "      Applied inverse scaling for window 5 - 3173 samples\n",
      "      Applied inverse scaling for window 5 - 3153 samples\n",
      "      Applied inverse scaling for window 5 - 3029 samples\n",
      "      Applied inverse scaling for window 5 - 2969 samples\n",
      "      Applied inverse scaling for window 5 - 3152 samples\n",
      "      Applied inverse scaling for window 5 - 3070 samples\n",
      "      Applied inverse scaling for window 5 - 3064 samples\n",
      "      Applied inverse scaling for window 5 - 3069 samples\n",
      "      Applied inverse scaling for window 5 - 3121 samples\n",
      "      Applied inverse scaling for window 5 - 3113 samples\n",
      "      Applied inverse scaling for window 5 - 3026 samples\n",
      "      Applied inverse scaling for window 5 - 3112 samples\n",
      "      Applied inverse scaling for window 5 - 3162 samples\n",
      "      Applied inverse scaling for window 5 - 3094 samples\n",
      "[Update] Metrics updated for NN5 w=5\n",
      "  Model: NN5, Scheme: EW\n",
      "      Applied inverse scaling for window 5 - 3006 samples\n",
      "      Applied inverse scaling for window 5 - 3170 samples\n",
      "      Applied inverse scaling for window 5 - 3176 samples\n",
      "      Applied inverse scaling for window 5 - 3123 samples\n",
      "      Applied inverse scaling for window 5 - 3083 samples\n",
      "      Applied inverse scaling for window 5 - 3122 samples\n",
      "      Applied inverse scaling for window 5 - 3114 samples\n",
      "      Applied inverse scaling for window 5 - 3115 samples\n",
      "      Applied inverse scaling for window 5 - 2996 samples\n",
      "      Applied inverse scaling for window 5 - 3160 samples\n",
      "      Applied inverse scaling for window 5 - 3125 samples\n",
      "      Applied inverse scaling for window 5 - 3045 samples\n",
      "      Applied inverse scaling for window 5 - 3022 samples\n",
      "      Applied inverse scaling for window 5 - 3120 samples\n",
      "      Applied inverse scaling for window 5 - 3167 samples\n",
      "      Applied inverse scaling for window 5 - 3179 samples\n",
      "      Applied inverse scaling for window 5 - 2595 samples\n",
      "      Applied inverse scaling for window 5 - 2839 samples\n",
      "      Applied inverse scaling for window 5 - 3151 samples\n",
      "      Applied inverse scaling for window 5 - 3114 samples\n",
      "      Applied inverse scaling for window 5 - 2989 samples\n",
      "      Applied inverse scaling for window 5 - 3132 samples\n",
      "      Applied inverse scaling for window 5 - 3173 samples\n",
      "      Applied inverse scaling for window 5 - 3153 samples\n",
      "      Applied inverse scaling for window 5 - 3029 samples\n",
      "      Applied inverse scaling for window 5 - 2969 samples\n",
      "      Applied inverse scaling for window 5 - 3152 samples\n",
      "      Applied inverse scaling for window 5 - 3070 samples\n",
      "      Applied inverse scaling for window 5 - 3064 samples\n",
      "      Applied inverse scaling for window 5 - 3069 samples\n",
      "      Applied inverse scaling for window 5 - 3121 samples\n",
      "      Applied inverse scaling for window 5 - 3113 samples\n",
      "      Applied inverse scaling for window 5 - 3026 samples\n",
      "      Applied inverse scaling for window 5 - 3112 samples\n",
      "      Applied inverse scaling for window 5 - 3162 samples\n",
      "      Applied inverse scaling for window 5 - 3094 samples\n",
      "Processing window size: 21\n",
      "  Model: NN1, Scheme: VW\n",
      "      Applied inverse scaling for window 21 - 3006 samples\n",
      "      Applied inverse scaling for window 21 - 3170 samples\n",
      "      Applied inverse scaling for window 21 - 3176 samples\n",
      "      Applied inverse scaling for window 21 - 3123 samples\n",
      "      Applied inverse scaling for window 21 - 3083 samples\n",
      "      Applied inverse scaling for window 21 - 3122 samples\n",
      "      Applied inverse scaling for window 21 - 3114 samples\n",
      "      Applied inverse scaling for window 21 - 3115 samples\n",
      "      Applied inverse scaling for window 21 - 2996 samples\n",
      "      Applied inverse scaling for window 21 - 3160 samples\n",
      "      Applied inverse scaling for window 21 - 3125 samples\n",
      "      Applied inverse scaling for window 21 - 3045 samples\n",
      "      Applied inverse scaling for window 21 - 3022 samples\n",
      "      Applied inverse scaling for window 21 - 3120 samples\n",
      "      Applied inverse scaling for window 21 - 3167 samples\n",
      "      Applied inverse scaling for window 21 - 3179 samples\n",
      "      Applied inverse scaling for window 21 - 2595 samples\n",
      "      Applied inverse scaling for window 21 - 2839 samples\n",
      "      Applied inverse scaling for window 21 - 3151 samples\n",
      "      Applied inverse scaling for window 21 - 3114 samples\n",
      "      Applied inverse scaling for window 21 - 2989 samples\n",
      "      Applied inverse scaling for window 21 - 3132 samples\n",
      "      Applied inverse scaling for window 21 - 3173 samples\n",
      "      Applied inverse scaling for window 21 - 3153 samples\n",
      "      Applied inverse scaling for window 21 - 3029 samples\n",
      "      Applied inverse scaling for window 21 - 2969 samples\n",
      "      Applied inverse scaling for window 21 - 3152 samples\n",
      "      Applied inverse scaling for window 21 - 3070 samples\n",
      "      Applied inverse scaling for window 21 - 3064 samples\n",
      "      Applied inverse scaling for window 21 - 3069 samples\n",
      "      Applied inverse scaling for window 21 - 3121 samples\n",
      "      Applied inverse scaling for window 21 - 3113 samples\n",
      "      Applied inverse scaling for window 21 - 3026 samples\n",
      "      Applied inverse scaling for window 21 - 3112 samples\n",
      "      Applied inverse scaling for window 21 - 3162 samples\n",
      "      Applied inverse scaling for window 21 - 3094 samples\n",
      "[Update] Metrics updated for NN1 w=21\n",
      "  Model: NN1, Scheme: EW\n",
      "      Applied inverse scaling for window 21 - 3006 samples\n",
      "      Applied inverse scaling for window 21 - 3170 samples\n",
      "      Applied inverse scaling for window 21 - 3176 samples\n",
      "      Applied inverse scaling for window 21 - 3123 samples\n",
      "      Applied inverse scaling for window 21 - 3083 samples\n",
      "      Applied inverse scaling for window 21 - 3122 samples\n",
      "      Applied inverse scaling for window 21 - 3114 samples\n",
      "      Applied inverse scaling for window 21 - 3115 samples\n",
      "      Applied inverse scaling for window 21 - 2996 samples\n",
      "      Applied inverse scaling for window 21 - 3160 samples\n",
      "      Applied inverse scaling for window 21 - 3125 samples\n",
      "      Applied inverse scaling for window 21 - 3045 samples\n",
      "      Applied inverse scaling for window 21 - 3022 samples\n",
      "      Applied inverse scaling for window 21 - 3120 samples\n",
      "      Applied inverse scaling for window 21 - 3167 samples\n",
      "      Applied inverse scaling for window 21 - 3179 samples\n",
      "      Applied inverse scaling for window 21 - 2595 samples\n",
      "      Applied inverse scaling for window 21 - 2839 samples\n",
      "      Applied inverse scaling for window 21 - 3151 samples\n",
      "      Applied inverse scaling for window 21 - 3114 samples\n",
      "      Applied inverse scaling for window 21 - 2989 samples\n",
      "      Applied inverse scaling for window 21 - 3132 samples\n",
      "      Applied inverse scaling for window 21 - 3173 samples\n",
      "      Applied inverse scaling for window 21 - 3153 samples\n",
      "      Applied inverse scaling for window 21 - 3029 samples\n",
      "      Applied inverse scaling for window 21 - 2969 samples\n",
      "      Applied inverse scaling for window 21 - 3152 samples\n",
      "      Applied inverse scaling for window 21 - 3070 samples\n",
      "      Applied inverse scaling for window 21 - 3064 samples\n",
      "      Applied inverse scaling for window 21 - 3069 samples\n",
      "      Applied inverse scaling for window 21 - 3121 samples\n",
      "      Applied inverse scaling for window 21 - 3113 samples\n",
      "      Applied inverse scaling for window 21 - 3026 samples\n",
      "      Applied inverse scaling for window 21 - 3112 samples\n",
      "      Applied inverse scaling for window 21 - 3162 samples\n",
      "      Applied inverse scaling for window 21 - 3094 samples\n",
      "  Model: NN2, Scheme: VW\n",
      "      Applied inverse scaling for window 21 - 3006 samples\n",
      "      Applied inverse scaling for window 21 - 3170 samples\n",
      "      Applied inverse scaling for window 21 - 3176 samples\n",
      "      Applied inverse scaling for window 21 - 3123 samples\n",
      "      Applied inverse scaling for window 21 - 3083 samples\n",
      "      Applied inverse scaling for window 21 - 3122 samples\n",
      "      Applied inverse scaling for window 21 - 3114 samples\n",
      "      Applied inverse scaling for window 21 - 3115 samples\n",
      "      Applied inverse scaling for window 21 - 2996 samples\n",
      "      Applied inverse scaling for window 21 - 3160 samples\n",
      "      Applied inverse scaling for window 21 - 3125 samples\n",
      "      Applied inverse scaling for window 21 - 3045 samples\n",
      "      Applied inverse scaling for window 21 - 3022 samples\n",
      "      Applied inverse scaling for window 21 - 3120 samples\n",
      "      Applied inverse scaling for window 21 - 3167 samples\n",
      "      Applied inverse scaling for window 21 - 3179 samples\n",
      "      Applied inverse scaling for window 21 - 2595 samples\n",
      "      Applied inverse scaling for window 21 - 2839 samples\n",
      "      Applied inverse scaling for window 21 - 3151 samples\n",
      "      Applied inverse scaling for window 21 - 3114 samples\n",
      "      Applied inverse scaling for window 21 - 2989 samples\n",
      "      Applied inverse scaling for window 21 - 3132 samples\n",
      "      Applied inverse scaling for window 21 - 3173 samples\n",
      "      Applied inverse scaling for window 21 - 3153 samples\n",
      "      Applied inverse scaling for window 21 - 3029 samples\n",
      "      Applied inverse scaling for window 21 - 2969 samples\n",
      "      Applied inverse scaling for window 21 - 3152 samples\n",
      "      Applied inverse scaling for window 21 - 3070 samples\n",
      "      Applied inverse scaling for window 21 - 3064 samples\n",
      "      Applied inverse scaling for window 21 - 3069 samples\n",
      "      Applied inverse scaling for window 21 - 3121 samples\n",
      "      Applied inverse scaling for window 21 - 3113 samples\n",
      "      Applied inverse scaling for window 21 - 3026 samples\n",
      "      Applied inverse scaling for window 21 - 3112 samples\n",
      "      Applied inverse scaling for window 21 - 3162 samples\n",
      "      Applied inverse scaling for window 21 - 3094 samples\n",
      "[Update] Metrics updated for NN2 w=21\n",
      "  Model: NN2, Scheme: EW\n",
      "      Applied inverse scaling for window 21 - 3006 samples\n",
      "      Applied inverse scaling for window 21 - 3170 samples\n",
      "      Applied inverse scaling for window 21 - 3176 samples\n",
      "      Applied inverse scaling for window 21 - 3123 samples\n",
      "      Applied inverse scaling for window 21 - 3083 samples\n",
      "      Applied inverse scaling for window 21 - 3122 samples\n",
      "      Applied inverse scaling for window 21 - 3114 samples\n",
      "      Applied inverse scaling for window 21 - 3115 samples\n",
      "      Applied inverse scaling for window 21 - 2996 samples\n",
      "      Applied inverse scaling for window 21 - 3160 samples\n",
      "      Applied inverse scaling for window 21 - 3125 samples\n",
      "      Applied inverse scaling for window 21 - 3045 samples\n",
      "      Applied inverse scaling for window 21 - 3022 samples\n",
      "      Applied inverse scaling for window 21 - 3120 samples\n",
      "      Applied inverse scaling for window 21 - 3167 samples\n",
      "      Applied inverse scaling for window 21 - 3179 samples\n",
      "      Applied inverse scaling for window 21 - 2595 samples\n",
      "      Applied inverse scaling for window 21 - 2839 samples\n",
      "      Applied inverse scaling for window 21 - 3151 samples\n",
      "      Applied inverse scaling for window 21 - 3114 samples\n",
      "      Applied inverse scaling for window 21 - 2989 samples\n",
      "      Applied inverse scaling for window 21 - 3132 samples\n",
      "      Applied inverse scaling for window 21 - 3173 samples\n",
      "      Applied inverse scaling for window 21 - 3153 samples\n",
      "      Applied inverse scaling for window 21 - 3029 samples\n",
      "      Applied inverse scaling for window 21 - 2969 samples\n",
      "      Applied inverse scaling for window 21 - 3152 samples\n",
      "      Applied inverse scaling for window 21 - 3070 samples\n",
      "      Applied inverse scaling for window 21 - 3064 samples\n",
      "      Applied inverse scaling for window 21 - 3069 samples\n",
      "      Applied inverse scaling for window 21 - 3121 samples\n",
      "      Applied inverse scaling for window 21 - 3113 samples\n",
      "      Applied inverse scaling for window 21 - 3026 samples\n",
      "      Applied inverse scaling for window 21 - 3112 samples\n",
      "      Applied inverse scaling for window 21 - 3162 samples\n",
      "      Applied inverse scaling for window 21 - 3094 samples\n",
      "  Model: NN3, Scheme: VW\n",
      "      Applied inverse scaling for window 21 - 3006 samples\n",
      "      Applied inverse scaling for window 21 - 3170 samples\n",
      "      Applied inverse scaling for window 21 - 3176 samples\n",
      "      Applied inverse scaling for window 21 - 3123 samples\n",
      "      Applied inverse scaling for window 21 - 3083 samples\n",
      "      Applied inverse scaling for window 21 - 3122 samples\n",
      "      Applied inverse scaling for window 21 - 3114 samples\n",
      "      Applied inverse scaling for window 21 - 3115 samples\n",
      "      Applied inverse scaling for window 21 - 2996 samples\n",
      "      Applied inverse scaling for window 21 - 3160 samples\n",
      "      Applied inverse scaling for window 21 - 3125 samples\n",
      "      Applied inverse scaling for window 21 - 3045 samples\n",
      "      Applied inverse scaling for window 21 - 3022 samples\n",
      "      Applied inverse scaling for window 21 - 3120 samples\n",
      "      Applied inverse scaling for window 21 - 3167 samples\n",
      "      Applied inverse scaling for window 21 - 3179 samples\n",
      "      Applied inverse scaling for window 21 - 2595 samples\n",
      "      Applied inverse scaling for window 21 - 2839 samples\n",
      "      Applied inverse scaling for window 21 - 3151 samples\n",
      "      Applied inverse scaling for window 21 - 3114 samples\n",
      "      Applied inverse scaling for window 21 - 2989 samples\n",
      "      Applied inverse scaling for window 21 - 3132 samples\n",
      "      Applied inverse scaling for window 21 - 3173 samples\n",
      "      Applied inverse scaling for window 21 - 3153 samples\n",
      "      Applied inverse scaling for window 21 - 3029 samples\n",
      "      Applied inverse scaling for window 21 - 2969 samples\n",
      "      Applied inverse scaling for window 21 - 3152 samples\n",
      "      Applied inverse scaling for window 21 - 3070 samples\n",
      "      Applied inverse scaling for window 21 - 3064 samples\n",
      "      Applied inverse scaling for window 21 - 3069 samples\n",
      "      Applied inverse scaling for window 21 - 3121 samples\n",
      "      Applied inverse scaling for window 21 - 3113 samples\n",
      "      Applied inverse scaling for window 21 - 3026 samples\n",
      "      Applied inverse scaling for window 21 - 3112 samples\n",
      "      Applied inverse scaling for window 21 - 3162 samples\n",
      "      Applied inverse scaling for window 21 - 3094 samples\n",
      "[Update] Metrics updated for NN3 w=21\n",
      "  Model: NN3, Scheme: EW\n",
      "      Applied inverse scaling for window 21 - 3006 samples\n",
      "      Applied inverse scaling for window 21 - 3170 samples\n",
      "      Applied inverse scaling for window 21 - 3176 samples\n",
      "      Applied inverse scaling for window 21 - 3123 samples\n",
      "      Applied inverse scaling for window 21 - 3083 samples\n",
      "      Applied inverse scaling for window 21 - 3122 samples\n",
      "      Applied inverse scaling for window 21 - 3114 samples\n",
      "      Applied inverse scaling for window 21 - 3115 samples\n",
      "      Applied inverse scaling for window 21 - 2996 samples\n",
      "      Applied inverse scaling for window 21 - 3160 samples\n",
      "      Applied inverse scaling for window 21 - 3125 samples\n",
      "      Applied inverse scaling for window 21 - 3045 samples\n",
      "      Applied inverse scaling for window 21 - 3022 samples\n",
      "      Applied inverse scaling for window 21 - 3120 samples\n",
      "      Applied inverse scaling for window 21 - 3167 samples\n",
      "      Applied inverse scaling for window 21 - 3179 samples\n",
      "      Applied inverse scaling for window 21 - 2595 samples\n",
      "      Applied inverse scaling for window 21 - 2839 samples\n",
      "      Applied inverse scaling for window 21 - 3151 samples\n",
      "      Applied inverse scaling for window 21 - 3114 samples\n",
      "      Applied inverse scaling for window 21 - 2989 samples\n",
      "      Applied inverse scaling for window 21 - 3132 samples\n",
      "      Applied inverse scaling for window 21 - 3173 samples\n",
      "      Applied inverse scaling for window 21 - 3153 samples\n",
      "      Applied inverse scaling for window 21 - 3029 samples\n",
      "      Applied inverse scaling for window 21 - 2969 samples\n",
      "      Applied inverse scaling for window 21 - 3152 samples\n",
      "      Applied inverse scaling for window 21 - 3070 samples\n",
      "      Applied inverse scaling for window 21 - 3064 samples\n",
      "      Applied inverse scaling for window 21 - 3069 samples\n",
      "      Applied inverse scaling for window 21 - 3121 samples\n",
      "      Applied inverse scaling for window 21 - 3113 samples\n",
      "      Applied inverse scaling for window 21 - 3026 samples\n",
      "      Applied inverse scaling for window 21 - 3112 samples\n",
      "      Applied inverse scaling for window 21 - 3162 samples\n",
      "      Applied inverse scaling for window 21 - 3094 samples\n",
      "  Model: NN4, Scheme: VW\n",
      "      Applied inverse scaling for window 21 - 3006 samples\n",
      "      Applied inverse scaling for window 21 - 3170 samples\n",
      "      Applied inverse scaling for window 21 - 3176 samples\n",
      "      Applied inverse scaling for window 21 - 3123 samples\n",
      "      Applied inverse scaling for window 21 - 3083 samples\n",
      "      Applied inverse scaling for window 21 - 3122 samples\n",
      "      Applied inverse scaling for window 21 - 3114 samples\n",
      "      Applied inverse scaling for window 21 - 3115 samples\n",
      "      Applied inverse scaling for window 21 - 2996 samples\n",
      "      Applied inverse scaling for window 21 - 3160 samples\n",
      "      Applied inverse scaling for window 21 - 3125 samples\n",
      "      Applied inverse scaling for window 21 - 3045 samples\n",
      "      Applied inverse scaling for window 21 - 3022 samples\n",
      "      Applied inverse scaling for window 21 - 3120 samples\n",
      "      Applied inverse scaling for window 21 - 3167 samples\n",
      "      Applied inverse scaling for window 21 - 3179 samples\n",
      "      Applied inverse scaling for window 21 - 2595 samples\n",
      "      Applied inverse scaling for window 21 - 2839 samples\n",
      "      Applied inverse scaling for window 21 - 3151 samples\n",
      "      Applied inverse scaling for window 21 - 3114 samples\n",
      "      Applied inverse scaling for window 21 - 2989 samples\n",
      "      Applied inverse scaling for window 21 - 3132 samples\n",
      "      Applied inverse scaling for window 21 - 3173 samples\n",
      "      Applied inverse scaling for window 21 - 3153 samples\n",
      "      Applied inverse scaling for window 21 - 3029 samples\n",
      "      Applied inverse scaling for window 21 - 2969 samples\n",
      "      Applied inverse scaling for window 21 - 3152 samples\n",
      "      Applied inverse scaling for window 21 - 3070 samples\n",
      "      Applied inverse scaling for window 21 - 3064 samples\n",
      "      Applied inverse scaling for window 21 - 3069 samples\n",
      "      Applied inverse scaling for window 21 - 3121 samples\n",
      "      Applied inverse scaling for window 21 - 3113 samples\n",
      "      Applied inverse scaling for window 21 - 3026 samples\n",
      "      Applied inverse scaling for window 21 - 3112 samples\n",
      "      Applied inverse scaling for window 21 - 3162 samples\n",
      "      Applied inverse scaling for window 21 - 3094 samples\n",
      "[Update] Metrics updated for NN4 w=21\n",
      "  Model: NN4, Scheme: EW\n",
      "      Applied inverse scaling for window 21 - 3006 samples\n",
      "      Applied inverse scaling for window 21 - 3170 samples\n",
      "      Applied inverse scaling for window 21 - 3176 samples\n",
      "      Applied inverse scaling for window 21 - 3123 samples\n",
      "      Applied inverse scaling for window 21 - 3083 samples\n",
      "      Applied inverse scaling for window 21 - 3122 samples\n",
      "      Applied inverse scaling for window 21 - 3114 samples\n",
      "      Applied inverse scaling for window 21 - 3115 samples\n",
      "      Applied inverse scaling for window 21 - 2996 samples\n",
      "      Applied inverse scaling for window 21 - 3160 samples\n",
      "      Applied inverse scaling for window 21 - 3125 samples\n",
      "      Applied inverse scaling for window 21 - 3045 samples\n",
      "      Applied inverse scaling for window 21 - 3022 samples\n",
      "      Applied inverse scaling for window 21 - 3120 samples\n",
      "      Applied inverse scaling for window 21 - 3167 samples\n",
      "      Applied inverse scaling for window 21 - 3179 samples\n",
      "      Applied inverse scaling for window 21 - 2595 samples\n",
      "      Applied inverse scaling for window 21 - 2839 samples\n",
      "      Applied inverse scaling for window 21 - 3151 samples\n",
      "      Applied inverse scaling for window 21 - 3114 samples\n",
      "      Applied inverse scaling for window 21 - 2989 samples\n",
      "      Applied inverse scaling for window 21 - 3132 samples\n",
      "      Applied inverse scaling for window 21 - 3173 samples\n",
      "      Applied inverse scaling for window 21 - 3153 samples\n",
      "      Applied inverse scaling for window 21 - 3029 samples\n",
      "      Applied inverse scaling for window 21 - 2969 samples\n",
      "      Applied inverse scaling for window 21 - 3152 samples\n",
      "      Applied inverse scaling for window 21 - 3070 samples\n",
      "      Applied inverse scaling for window 21 - 3064 samples\n",
      "      Applied inverse scaling for window 21 - 3069 samples\n",
      "      Applied inverse scaling for window 21 - 3121 samples\n",
      "      Applied inverse scaling for window 21 - 3113 samples\n",
      "      Applied inverse scaling for window 21 - 3026 samples\n",
      "      Applied inverse scaling for window 21 - 3112 samples\n",
      "      Applied inverse scaling for window 21 - 3162 samples\n",
      "      Applied inverse scaling for window 21 - 3094 samples\n",
      "  Model: NN5, Scheme: VW\n",
      "      Applied inverse scaling for window 21 - 3006 samples\n",
      "      Applied inverse scaling for window 21 - 3170 samples\n",
      "      Applied inverse scaling for window 21 - 3176 samples\n",
      "      Applied inverse scaling for window 21 - 3123 samples\n",
      "      Applied inverse scaling for window 21 - 3083 samples\n",
      "      Applied inverse scaling for window 21 - 3122 samples\n",
      "      Applied inverse scaling for window 21 - 3114 samples\n",
      "      Applied inverse scaling for window 21 - 3115 samples\n",
      "      Applied inverse scaling for window 21 - 2996 samples\n",
      "      Applied inverse scaling for window 21 - 3160 samples\n",
      "      Applied inverse scaling for window 21 - 3125 samples\n",
      "      Applied inverse scaling for window 21 - 3045 samples\n",
      "      Applied inverse scaling for window 21 - 3022 samples\n",
      "      Applied inverse scaling for window 21 - 3120 samples\n",
      "      Applied inverse scaling for window 21 - 3167 samples\n",
      "      Applied inverse scaling for window 21 - 3179 samples\n",
      "      Applied inverse scaling for window 21 - 2595 samples\n",
      "      Applied inverse scaling for window 21 - 2839 samples\n",
      "      Applied inverse scaling for window 21 - 3151 samples\n",
      "      Applied inverse scaling for window 21 - 3114 samples\n",
      "      Applied inverse scaling for window 21 - 2989 samples\n",
      "      Applied inverse scaling for window 21 - 3132 samples\n",
      "      Applied inverse scaling for window 21 - 3173 samples\n",
      "      Applied inverse scaling for window 21 - 3153 samples\n",
      "      Applied inverse scaling for window 21 - 3029 samples\n",
      "      Applied inverse scaling for window 21 - 2969 samples\n",
      "      Applied inverse scaling for window 21 - 3152 samples\n",
      "      Applied inverse scaling for window 21 - 3070 samples\n",
      "      Applied inverse scaling for window 21 - 3064 samples\n",
      "      Applied inverse scaling for window 21 - 3069 samples\n",
      "      Applied inverse scaling for window 21 - 3121 samples\n",
      "      Applied inverse scaling for window 21 - 3113 samples\n",
      "      Applied inverse scaling for window 21 - 3026 samples\n",
      "      Applied inverse scaling for window 21 - 3112 samples\n",
      "      Applied inverse scaling for window 21 - 3162 samples\n",
      "      Applied inverse scaling for window 21 - 3094 samples\n",
      "[Update] Metrics updated for NN5 w=21\n",
      "  Model: NN5, Scheme: EW\n",
      "      Applied inverse scaling for window 21 - 3006 samples\n",
      "      Applied inverse scaling for window 21 - 3170 samples\n",
      "      Applied inverse scaling for window 21 - 3176 samples\n",
      "      Applied inverse scaling for window 21 - 3123 samples\n",
      "      Applied inverse scaling for window 21 - 3083 samples\n",
      "      Applied inverse scaling for window 21 - 3122 samples\n",
      "      Applied inverse scaling for window 21 - 3114 samples\n",
      "      Applied inverse scaling for window 21 - 3115 samples\n",
      "      Applied inverse scaling for window 21 - 2996 samples\n",
      "      Applied inverse scaling for window 21 - 3160 samples\n",
      "      Applied inverse scaling for window 21 - 3125 samples\n",
      "      Applied inverse scaling for window 21 - 3045 samples\n",
      "      Applied inverse scaling for window 21 - 3022 samples\n",
      "      Applied inverse scaling for window 21 - 3120 samples\n",
      "      Applied inverse scaling for window 21 - 3167 samples\n",
      "      Applied inverse scaling for window 21 - 3179 samples\n",
      "      Applied inverse scaling for window 21 - 2595 samples\n",
      "      Applied inverse scaling for window 21 - 2839 samples\n",
      "      Applied inverse scaling for window 21 - 3151 samples\n",
      "      Applied inverse scaling for window 21 - 3114 samples\n",
      "      Applied inverse scaling for window 21 - 2989 samples\n",
      "      Applied inverse scaling for window 21 - 3132 samples\n",
      "      Applied inverse scaling for window 21 - 3173 samples\n",
      "      Applied inverse scaling for window 21 - 3153 samples\n",
      "      Applied inverse scaling for window 21 - 3029 samples\n",
      "      Applied inverse scaling for window 21 - 2969 samples\n",
      "      Applied inverse scaling for window 21 - 3152 samples\n",
      "      Applied inverse scaling for window 21 - 3070 samples\n",
      "      Applied inverse scaling for window 21 - 3064 samples\n",
      "      Applied inverse scaling for window 21 - 3069 samples\n",
      "      Applied inverse scaling for window 21 - 3121 samples\n",
      "      Applied inverse scaling for window 21 - 3113 samples\n",
      "      Applied inverse scaling for window 21 - 3026 samples\n",
      "      Applied inverse scaling for window 21 - 3112 samples\n",
      "      Applied inverse scaling for window 21 - 3162 samples\n",
      "      Applied inverse scaling for window 21 - 3094 samples\n",
      "Processing window size: 252\n",
      "  Model: NN1, Scheme: VW\n",
      "      Applied inverse scaling for window 252 - 3006 samples\n",
      "      Applied inverse scaling for window 252 - 3170 samples\n",
      "      Applied inverse scaling for window 252 - 3176 samples\n",
      "      Applied inverse scaling for window 252 - 3123 samples\n",
      "      Applied inverse scaling for window 252 - 3083 samples\n",
      "      Applied inverse scaling for window 252 - 3122 samples\n",
      "      Applied inverse scaling for window 252 - 3114 samples\n",
      "      Applied inverse scaling for window 252 - 3115 samples\n",
      "      Applied inverse scaling for window 252 - 2996 samples\n",
      "      Applied inverse scaling for window 252 - 3160 samples\n",
      "      Applied inverse scaling for window 252 - 3125 samples\n",
      "      Applied inverse scaling for window 252 - 3045 samples\n",
      "      Applied inverse scaling for window 252 - 3022 samples\n",
      "      Applied inverse scaling for window 252 - 3120 samples\n",
      "      Applied inverse scaling for window 252 - 3167 samples\n",
      "      Applied inverse scaling for window 252 - 3179 samples\n",
      "      Applied inverse scaling for window 252 - 2595 samples\n",
      "      Applied inverse scaling for window 252 - 2839 samples\n",
      "      Applied inverse scaling for window 252 - 3151 samples\n",
      "      Applied inverse scaling for window 252 - 3114 samples\n",
      "      Applied inverse scaling for window 252 - 2989 samples\n",
      "      Applied inverse scaling for window 252 - 3132 samples\n",
      "      Applied inverse scaling for window 252 - 3173 samples\n",
      "      Applied inverse scaling for window 252 - 3153 samples\n",
      "      Applied inverse scaling for window 252 - 3029 samples\n",
      "      Applied inverse scaling for window 252 - 2969 samples\n",
      "      Applied inverse scaling for window 252 - 3152 samples\n",
      "      Applied inverse scaling for window 252 - 3070 samples\n",
      "      Applied inverse scaling for window 252 - 3064 samples\n",
      "      Applied inverse scaling for window 252 - 3069 samples\n",
      "      Applied inverse scaling for window 252 - 3121 samples\n",
      "      Applied inverse scaling for window 252 - 3113 samples\n",
      "      Applied inverse scaling for window 252 - 3026 samples\n",
      "      Applied inverse scaling for window 252 - 3112 samples\n",
      "      Applied inverse scaling for window 252 - 3162 samples\n",
      "      Applied inverse scaling for window 252 - 3094 samples\n",
      "[Update] Metrics updated for NN1 w=252\n",
      "  Model: NN1, Scheme: EW\n",
      "      Applied inverse scaling for window 252 - 3006 samples\n",
      "      Applied inverse scaling for window 252 - 3170 samples\n",
      "      Applied inverse scaling for window 252 - 3176 samples\n",
      "      Applied inverse scaling for window 252 - 3123 samples\n",
      "      Applied inverse scaling for window 252 - 3083 samples\n",
      "      Applied inverse scaling for window 252 - 3122 samples\n",
      "      Applied inverse scaling for window 252 - 3114 samples\n",
      "      Applied inverse scaling for window 252 - 3115 samples\n",
      "      Applied inverse scaling for window 252 - 2996 samples\n",
      "      Applied inverse scaling for window 252 - 3160 samples\n",
      "      Applied inverse scaling for window 252 - 3125 samples\n",
      "      Applied inverse scaling for window 252 - 3045 samples\n",
      "      Applied inverse scaling for window 252 - 3022 samples\n",
      "      Applied inverse scaling for window 252 - 3120 samples\n",
      "      Applied inverse scaling for window 252 - 3167 samples\n",
      "      Applied inverse scaling for window 252 - 3179 samples\n",
      "      Applied inverse scaling for window 252 - 2595 samples\n",
      "      Applied inverse scaling for window 252 - 2839 samples\n",
      "      Applied inverse scaling for window 252 - 3151 samples\n",
      "      Applied inverse scaling for window 252 - 3114 samples\n",
      "      Applied inverse scaling for window 252 - 2989 samples\n",
      "      Applied inverse scaling for window 252 - 3132 samples\n",
      "      Applied inverse scaling for window 252 - 3173 samples\n",
      "      Applied inverse scaling for window 252 - 3153 samples\n",
      "      Applied inverse scaling for window 252 - 3029 samples\n",
      "      Applied inverse scaling for window 252 - 2969 samples\n",
      "      Applied inverse scaling for window 252 - 3152 samples\n",
      "      Applied inverse scaling for window 252 - 3070 samples\n",
      "      Applied inverse scaling for window 252 - 3064 samples\n",
      "      Applied inverse scaling for window 252 - 3069 samples\n",
      "      Applied inverse scaling for window 252 - 3121 samples\n",
      "      Applied inverse scaling for window 252 - 3113 samples\n",
      "      Applied inverse scaling for window 252 - 3026 samples\n",
      "      Applied inverse scaling for window 252 - 3112 samples\n",
      "      Applied inverse scaling for window 252 - 3162 samples\n",
      "      Applied inverse scaling for window 252 - 3094 samples\n",
      "  Model: NN2, Scheme: VW\n",
      "      Applied inverse scaling for window 252 - 3006 samples\n",
      "      Applied inverse scaling for window 252 - 3170 samples\n",
      "      Applied inverse scaling for window 252 - 3176 samples\n",
      "      Applied inverse scaling for window 252 - 3123 samples\n",
      "      Applied inverse scaling for window 252 - 3083 samples\n",
      "      Applied inverse scaling for window 252 - 3122 samples\n",
      "      Applied inverse scaling for window 252 - 3114 samples\n",
      "      Applied inverse scaling for window 252 - 3115 samples\n",
      "      Applied inverse scaling for window 252 - 2996 samples\n",
      "      Applied inverse scaling for window 252 - 3160 samples\n",
      "      Applied inverse scaling for window 252 - 3125 samples\n",
      "      Applied inverse scaling for window 252 - 3045 samples\n",
      "      Applied inverse scaling for window 252 - 3022 samples\n",
      "      Applied inverse scaling for window 252 - 3120 samples\n",
      "      Applied inverse scaling for window 252 - 3167 samples\n",
      "      Applied inverse scaling for window 252 - 3179 samples\n",
      "      Applied inverse scaling for window 252 - 2595 samples\n",
      "      Applied inverse scaling for window 252 - 2839 samples\n",
      "      Applied inverse scaling for window 252 - 3151 samples\n",
      "      Applied inverse scaling for window 252 - 3114 samples\n",
      "      Applied inverse scaling for window 252 - 2989 samples\n",
      "      Applied inverse scaling for window 252 - 3132 samples\n",
      "      Applied inverse scaling for window 252 - 3173 samples\n",
      "      Applied inverse scaling for window 252 - 3153 samples\n",
      "      Applied inverse scaling for window 252 - 3029 samples\n",
      "      Applied inverse scaling for window 252 - 2969 samples\n",
      "      Applied inverse scaling for window 252 - 3152 samples\n",
      "      Applied inverse scaling for window 252 - 3070 samples\n",
      "      Applied inverse scaling for window 252 - 3064 samples\n",
      "      Applied inverse scaling for window 252 - 3069 samples\n",
      "      Applied inverse scaling for window 252 - 3121 samples\n",
      "      Applied inverse scaling for window 252 - 3113 samples\n",
      "      Applied inverse scaling for window 252 - 3026 samples\n",
      "      Applied inverse scaling for window 252 - 3112 samples\n",
      "      Applied inverse scaling for window 252 - 3162 samples\n",
      "      Applied inverse scaling for window 252 - 3094 samples\n",
      "[Update] Metrics updated for NN2 w=252\n",
      "  Model: NN2, Scheme: EW\n",
      "      Applied inverse scaling for window 252 - 3006 samples\n",
      "      Applied inverse scaling for window 252 - 3170 samples\n",
      "      Applied inverse scaling for window 252 - 3176 samples\n",
      "      Applied inverse scaling for window 252 - 3123 samples\n",
      "      Applied inverse scaling for window 252 - 3083 samples\n",
      "      Applied inverse scaling for window 252 - 3122 samples\n",
      "      Applied inverse scaling for window 252 - 3114 samples\n",
      "      Applied inverse scaling for window 252 - 3115 samples\n",
      "      Applied inverse scaling for window 252 - 2996 samples\n",
      "      Applied inverse scaling for window 252 - 3160 samples\n",
      "      Applied inverse scaling for window 252 - 3125 samples\n",
      "      Applied inverse scaling for window 252 - 3045 samples\n",
      "      Applied inverse scaling for window 252 - 3022 samples\n",
      "      Applied inverse scaling for window 252 - 3120 samples\n",
      "      Applied inverse scaling for window 252 - 3167 samples\n",
      "      Applied inverse scaling for window 252 - 3179 samples\n",
      "      Applied inverse scaling for window 252 - 2595 samples\n",
      "      Applied inverse scaling for window 252 - 2839 samples\n",
      "      Applied inverse scaling for window 252 - 3151 samples\n",
      "      Applied inverse scaling for window 252 - 3114 samples\n",
      "      Applied inverse scaling for window 252 - 2989 samples\n",
      "      Applied inverse scaling for window 252 - 3132 samples\n",
      "      Applied inverse scaling for window 252 - 3173 samples\n",
      "      Applied inverse scaling for window 252 - 3153 samples\n",
      "      Applied inverse scaling for window 252 - 3029 samples\n",
      "      Applied inverse scaling for window 252 - 2969 samples\n",
      "      Applied inverse scaling for window 252 - 3152 samples\n",
      "      Applied inverse scaling for window 252 - 3070 samples\n",
      "      Applied inverse scaling for window 252 - 3064 samples\n",
      "      Applied inverse scaling for window 252 - 3069 samples\n",
      "      Applied inverse scaling for window 252 - 3121 samples\n",
      "      Applied inverse scaling for window 252 - 3113 samples\n",
      "      Applied inverse scaling for window 252 - 3026 samples\n",
      "      Applied inverse scaling for window 252 - 3112 samples\n",
      "      Applied inverse scaling for window 252 - 3162 samples\n",
      "      Applied inverse scaling for window 252 - 3094 samples\n",
      "  Model: NN3, Scheme: VW\n",
      "      Applied inverse scaling for window 252 - 3006 samples\n",
      "      Applied inverse scaling for window 252 - 3170 samples\n",
      "      Applied inverse scaling for window 252 - 3176 samples\n",
      "      Applied inverse scaling for window 252 - 3123 samples\n",
      "      Applied inverse scaling for window 252 - 3083 samples\n",
      "      Applied inverse scaling for window 252 - 3122 samples\n",
      "      Applied inverse scaling for window 252 - 3114 samples\n",
      "      Applied inverse scaling for window 252 - 3115 samples\n",
      "      Applied inverse scaling for window 252 - 2996 samples\n",
      "      Applied inverse scaling for window 252 - 3160 samples\n",
      "      Applied inverse scaling for window 252 - 3125 samples\n",
      "      Applied inverse scaling for window 252 - 3045 samples\n",
      "      Applied inverse scaling for window 252 - 3022 samples\n",
      "      Applied inverse scaling for window 252 - 3120 samples\n",
      "      Applied inverse scaling for window 252 - 3167 samples\n",
      "      Applied inverse scaling for window 252 - 3179 samples\n",
      "      Applied inverse scaling for window 252 - 2595 samples\n",
      "      Applied inverse scaling for window 252 - 2839 samples\n",
      "      Applied inverse scaling for window 252 - 3151 samples\n",
      "      Applied inverse scaling for window 252 - 3114 samples\n",
      "      Applied inverse scaling for window 252 - 2989 samples\n",
      "      Applied inverse scaling for window 252 - 3132 samples\n",
      "      Applied inverse scaling for window 252 - 3173 samples\n",
      "      Applied inverse scaling for window 252 - 3153 samples\n",
      "      Applied inverse scaling for window 252 - 3029 samples\n",
      "      Applied inverse scaling for window 252 - 2969 samples\n",
      "      Applied inverse scaling for window 252 - 3152 samples\n",
      "      Applied inverse scaling for window 252 - 3070 samples\n",
      "      Applied inverse scaling for window 252 - 3064 samples\n",
      "      Applied inverse scaling for window 252 - 3069 samples\n",
      "      Applied inverse scaling for window 252 - 3121 samples\n",
      "      Applied inverse scaling for window 252 - 3113 samples\n",
      "      Applied inverse scaling for window 252 - 3026 samples\n",
      "      Applied inverse scaling for window 252 - 3112 samples\n",
      "      Applied inverse scaling for window 252 - 3162 samples\n",
      "      Applied inverse scaling for window 252 - 3094 samples\n",
      "[Update] Metrics updated for NN3 w=252\n",
      "  Model: NN3, Scheme: EW\n",
      "      Applied inverse scaling for window 252 - 3006 samples\n",
      "      Applied inverse scaling for window 252 - 3170 samples\n",
      "      Applied inverse scaling for window 252 - 3176 samples\n",
      "      Applied inverse scaling for window 252 - 3123 samples\n",
      "      Applied inverse scaling for window 252 - 3083 samples\n",
      "      Applied inverse scaling for window 252 - 3122 samples\n",
      "      Applied inverse scaling for window 252 - 3114 samples\n",
      "      Applied inverse scaling for window 252 - 3115 samples\n",
      "      Applied inverse scaling for window 252 - 2996 samples\n",
      "      Applied inverse scaling for window 252 - 3160 samples\n",
      "      Applied inverse scaling for window 252 - 3125 samples\n",
      "      Applied inverse scaling for window 252 - 3045 samples\n",
      "      Applied inverse scaling for window 252 - 3022 samples\n",
      "      Applied inverse scaling for window 252 - 3120 samples\n",
      "      Applied inverse scaling for window 252 - 3167 samples\n",
      "      Applied inverse scaling for window 252 - 3179 samples\n",
      "      Applied inverse scaling for window 252 - 2595 samples\n",
      "      Applied inverse scaling for window 252 - 2839 samples\n",
      "      Applied inverse scaling for window 252 - 3151 samples\n",
      "      Applied inverse scaling for window 252 - 3114 samples\n",
      "      Applied inverse scaling for window 252 - 2989 samples\n",
      "      Applied inverse scaling for window 252 - 3132 samples\n",
      "      Applied inverse scaling for window 252 - 3173 samples\n",
      "      Applied inverse scaling for window 252 - 3153 samples\n",
      "      Applied inverse scaling for window 252 - 3029 samples\n",
      "      Applied inverse scaling for window 252 - 2969 samples\n",
      "      Applied inverse scaling for window 252 - 3152 samples\n",
      "      Applied inverse scaling for window 252 - 3070 samples\n",
      "      Applied inverse scaling for window 252 - 3064 samples\n",
      "      Applied inverse scaling for window 252 - 3069 samples\n",
      "      Applied inverse scaling for window 252 - 3121 samples\n",
      "      Applied inverse scaling for window 252 - 3113 samples\n",
      "      Applied inverse scaling for window 252 - 3026 samples\n",
      "      Applied inverse scaling for window 252 - 3112 samples\n",
      "      Applied inverse scaling for window 252 - 3162 samples\n",
      "      Applied inverse scaling for window 252 - 3094 samples\n",
      "  Model: NN4, Scheme: VW\n",
      "      Applied inverse scaling for window 252 - 3006 samples\n",
      "      Applied inverse scaling for window 252 - 3170 samples\n",
      "      Applied inverse scaling for window 252 - 3176 samples\n",
      "      Applied inverse scaling for window 252 - 3123 samples\n",
      "      Applied inverse scaling for window 252 - 3083 samples\n",
      "      Applied inverse scaling for window 252 - 3122 samples\n",
      "      Applied inverse scaling for window 252 - 3114 samples\n",
      "      Applied inverse scaling for window 252 - 3115 samples\n",
      "      Applied inverse scaling for window 252 - 2996 samples\n",
      "      Applied inverse scaling for window 252 - 3160 samples\n",
      "      Applied inverse scaling for window 252 - 3125 samples\n",
      "      Applied inverse scaling for window 252 - 3045 samples\n",
      "      Applied inverse scaling for window 252 - 3022 samples\n",
      "      Applied inverse scaling for window 252 - 3120 samples\n",
      "      Applied inverse scaling for window 252 - 3167 samples\n",
      "      Applied inverse scaling for window 252 - 3179 samples\n",
      "      Applied inverse scaling for window 252 - 2595 samples\n",
      "      Applied inverse scaling for window 252 - 2839 samples\n",
      "      Applied inverse scaling for window 252 - 3151 samples\n",
      "      Applied inverse scaling for window 252 - 3114 samples\n",
      "      Applied inverse scaling for window 252 - 2989 samples\n",
      "      Applied inverse scaling for window 252 - 3132 samples\n",
      "      Applied inverse scaling for window 252 - 3173 samples\n",
      "      Applied inverse scaling for window 252 - 3153 samples\n",
      "      Applied inverse scaling for window 252 - 3029 samples\n",
      "      Applied inverse scaling for window 252 - 2969 samples\n",
      "      Applied inverse scaling for window 252 - 3152 samples\n",
      "      Applied inverse scaling for window 252 - 3070 samples\n",
      "      Applied inverse scaling for window 252 - 3064 samples\n",
      "      Applied inverse scaling for window 252 - 3069 samples\n",
      "      Applied inverse scaling for window 252 - 3121 samples\n",
      "      Applied inverse scaling for window 252 - 3113 samples\n",
      "      Applied inverse scaling for window 252 - 3026 samples\n",
      "      Applied inverse scaling for window 252 - 3112 samples\n",
      "      Applied inverse scaling for window 252 - 3162 samples\n",
      "      Applied inverse scaling for window 252 - 3094 samples\n",
      "[Update] Metrics updated for NN4 w=252\n",
      "  Model: NN4, Scheme: EW\n",
      "      Applied inverse scaling for window 252 - 3006 samples\n",
      "      Applied inverse scaling for window 252 - 3170 samples\n",
      "      Applied inverse scaling for window 252 - 3176 samples\n",
      "      Applied inverse scaling for window 252 - 3123 samples\n",
      "      Applied inverse scaling for window 252 - 3083 samples\n",
      "      Applied inverse scaling for window 252 - 3122 samples\n",
      "      Applied inverse scaling for window 252 - 3114 samples\n",
      "      Applied inverse scaling for window 252 - 3115 samples\n",
      "      Applied inverse scaling for window 252 - 2996 samples\n",
      "      Applied inverse scaling for window 252 - 3160 samples\n",
      "      Applied inverse scaling for window 252 - 3125 samples\n",
      "      Applied inverse scaling for window 252 - 3045 samples\n",
      "      Applied inverse scaling for window 252 - 3022 samples\n",
      "      Applied inverse scaling for window 252 - 3120 samples\n",
      "      Applied inverse scaling for window 252 - 3167 samples\n",
      "      Applied inverse scaling for window 252 - 3179 samples\n",
      "      Applied inverse scaling for window 252 - 2595 samples\n",
      "      Applied inverse scaling for window 252 - 2839 samples\n",
      "      Applied inverse scaling for window 252 - 3151 samples\n",
      "      Applied inverse scaling for window 252 - 3114 samples\n",
      "      Applied inverse scaling for window 252 - 2989 samples\n",
      "      Applied inverse scaling for window 252 - 3132 samples\n",
      "      Applied inverse scaling for window 252 - 3173 samples\n",
      "      Applied inverse scaling for window 252 - 3153 samples\n",
      "      Applied inverse scaling for window 252 - 3029 samples\n",
      "      Applied inverse scaling for window 252 - 2969 samples\n",
      "      Applied inverse scaling for window 252 - 3152 samples\n",
      "      Applied inverse scaling for window 252 - 3070 samples\n",
      "      Applied inverse scaling for window 252 - 3064 samples\n",
      "      Applied inverse scaling for window 252 - 3069 samples\n",
      "      Applied inverse scaling for window 252 - 3121 samples\n",
      "      Applied inverse scaling for window 252 - 3113 samples\n",
      "      Applied inverse scaling for window 252 - 3026 samples\n",
      "      Applied inverse scaling for window 252 - 3112 samples\n",
      "      Applied inverse scaling for window 252 - 3162 samples\n",
      "      Applied inverse scaling for window 252 - 3094 samples\n",
      "  Model: NN5, Scheme: VW\n",
      "      Applied inverse scaling for window 252 - 3006 samples\n",
      "      Applied inverse scaling for window 252 - 3170 samples\n",
      "      Applied inverse scaling for window 252 - 3176 samples\n",
      "      Applied inverse scaling for window 252 - 3123 samples\n",
      "      Applied inverse scaling for window 252 - 3083 samples\n",
      "      Applied inverse scaling for window 252 - 3122 samples\n",
      "      Applied inverse scaling for window 252 - 3114 samples\n",
      "      Applied inverse scaling for window 252 - 3115 samples\n",
      "      Applied inverse scaling for window 252 - 2996 samples\n",
      "      Applied inverse scaling for window 252 - 3160 samples\n",
      "      Applied inverse scaling for window 252 - 3125 samples\n",
      "      Applied inverse scaling for window 252 - 3045 samples\n",
      "      Applied inverse scaling for window 252 - 3022 samples\n",
      "      Applied inverse scaling for window 252 - 3120 samples\n",
      "      Applied inverse scaling for window 252 - 3167 samples\n",
      "      Applied inverse scaling for window 252 - 3179 samples\n",
      "      Applied inverse scaling for window 252 - 2595 samples\n",
      "      Applied inverse scaling for window 252 - 2839 samples\n",
      "      Applied inverse scaling for window 252 - 3151 samples\n",
      "      Applied inverse scaling for window 252 - 3114 samples\n",
      "      Applied inverse scaling for window 252 - 2989 samples\n",
      "      Applied inverse scaling for window 252 - 3132 samples\n",
      "      Applied inverse scaling for window 252 - 3173 samples\n",
      "      Applied inverse scaling for window 252 - 3153 samples\n",
      "      Applied inverse scaling for window 252 - 3029 samples\n",
      "      Applied inverse scaling for window 252 - 2969 samples\n",
      "      Applied inverse scaling for window 252 - 3152 samples\n",
      "      Applied inverse scaling for window 252 - 3070 samples\n",
      "      Applied inverse scaling for window 252 - 3064 samples\n",
      "      Applied inverse scaling for window 252 - 3069 samples\n",
      "      Applied inverse scaling for window 252 - 3121 samples\n",
      "      Applied inverse scaling for window 252 - 3113 samples\n",
      "      Applied inverse scaling for window 252 - 3026 samples\n",
      "      Applied inverse scaling for window 252 - 3112 samples\n",
      "      Applied inverse scaling for window 252 - 3162 samples\n",
      "      Applied inverse scaling for window 252 - 3094 samples\n",
      "[Update] Metrics updated for NN5 w=252\n",
      "  Model: NN5, Scheme: EW\n",
      "      Applied inverse scaling for window 252 - 3006 samples\n",
      "      Applied inverse scaling for window 252 - 3170 samples\n",
      "      Applied inverse scaling for window 252 - 3176 samples\n",
      "      Applied inverse scaling for window 252 - 3123 samples\n",
      "      Applied inverse scaling for window 252 - 3083 samples\n",
      "      Applied inverse scaling for window 252 - 3122 samples\n",
      "      Applied inverse scaling for window 252 - 3114 samples\n",
      "      Applied inverse scaling for window 252 - 3115 samples\n",
      "      Applied inverse scaling for window 252 - 2996 samples\n",
      "      Applied inverse scaling for window 252 - 3160 samples\n",
      "      Applied inverse scaling for window 252 - 3125 samples\n",
      "      Applied inverse scaling for window 252 - 3045 samples\n",
      "      Applied inverse scaling for window 252 - 3022 samples\n",
      "      Applied inverse scaling for window 252 - 3120 samples\n",
      "      Applied inverse scaling for window 252 - 3167 samples\n",
      "      Applied inverse scaling for window 252 - 3179 samples\n",
      "      Applied inverse scaling for window 252 - 2595 samples\n",
      "      Applied inverse scaling for window 252 - 2839 samples\n",
      "      Applied inverse scaling for window 252 - 3151 samples\n",
      "      Applied inverse scaling for window 252 - 3114 samples\n",
      "      Applied inverse scaling for window 252 - 2989 samples\n",
      "      Applied inverse scaling for window 252 - 3132 samples\n",
      "      Applied inverse scaling for window 252 - 3173 samples\n",
      "      Applied inverse scaling for window 252 - 3153 samples\n",
      "      Applied inverse scaling for window 252 - 3029 samples\n",
      "      Applied inverse scaling for window 252 - 2969 samples\n",
      "      Applied inverse scaling for window 252 - 3152 samples\n",
      "      Applied inverse scaling for window 252 - 3070 samples\n",
      "      Applied inverse scaling for window 252 - 3064 samples\n",
      "      Applied inverse scaling for window 252 - 3069 samples\n",
      "      Applied inverse scaling for window 252 - 3121 samples\n",
      "      Applied inverse scaling for window 252 - 3113 samples\n",
      "      Applied inverse scaling for window 252 - 3026 samples\n",
      "      Applied inverse scaling for window 252 - 3112 samples\n",
      "      Applied inverse scaling for window 252 - 3162 samples\n",
      "      Applied inverse scaling for window 252 - 3094 samples\n",
      "Processing window size: 512\n",
      "  Model: NN1, Scheme: VW\n",
      "      Applied inverse scaling for window 512 - 3006 samples\n",
      "      Applied inverse scaling for window 512 - 3170 samples\n",
      "      Applied inverse scaling for window 512 - 3176 samples\n",
      "      Applied inverse scaling for window 512 - 3123 samples\n",
      "      Applied inverse scaling for window 512 - 3083 samples\n",
      "      Applied inverse scaling for window 512 - 3122 samples\n",
      "      Applied inverse scaling for window 512 - 3114 samples\n",
      "      Applied inverse scaling for window 512 - 3115 samples\n",
      "      Applied inverse scaling for window 512 - 2996 samples\n",
      "      Applied inverse scaling for window 512 - 3160 samples\n",
      "      Applied inverse scaling for window 512 - 3125 samples\n",
      "      Applied inverse scaling for window 512 - 3045 samples\n",
      "      Applied inverse scaling for window 512 - 3022 samples\n",
      "      Applied inverse scaling for window 512 - 3120 samples\n",
      "      Applied inverse scaling for window 512 - 3167 samples\n",
      "      Applied inverse scaling for window 512 - 3179 samples\n",
      "      Applied inverse scaling for window 512 - 2595 samples\n",
      "      Applied inverse scaling for window 512 - 2839 samples\n",
      "      Applied inverse scaling for window 512 - 3151 samples\n",
      "      Applied inverse scaling for window 512 - 3114 samples\n",
      "      Applied inverse scaling for window 512 - 2989 samples\n",
      "      Applied inverse scaling for window 512 - 3132 samples\n",
      "      Applied inverse scaling for window 512 - 3173 samples\n",
      "      Applied inverse scaling for window 512 - 3153 samples\n",
      "      Applied inverse scaling for window 512 - 3029 samples\n",
      "      Applied inverse scaling for window 512 - 2969 samples\n",
      "      Applied inverse scaling for window 512 - 3152 samples\n",
      "      Applied inverse scaling for window 512 - 3070 samples\n",
      "      Applied inverse scaling for window 512 - 3064 samples\n",
      "      Applied inverse scaling for window 512 - 3069 samples\n",
      "      Applied inverse scaling for window 512 - 3121 samples\n",
      "      Applied inverse scaling for window 512 - 3113 samples\n",
      "      Applied inverse scaling for window 512 - 3026 samples\n",
      "      Applied inverse scaling for window 512 - 3112 samples\n",
      "      Applied inverse scaling for window 512 - 3162 samples\n",
      "      Applied inverse scaling for window 512 - 3094 samples\n",
      "[Update] Metrics updated for NN1 w=512\n",
      "  Model: NN1, Scheme: EW\n",
      "      Applied inverse scaling for window 512 - 3006 samples\n",
      "      Applied inverse scaling for window 512 - 3170 samples\n",
      "      Applied inverse scaling for window 512 - 3176 samples\n",
      "      Applied inverse scaling for window 512 - 3123 samples\n",
      "      Applied inverse scaling for window 512 - 3083 samples\n",
      "      Applied inverse scaling for window 512 - 3122 samples\n",
      "      Applied inverse scaling for window 512 - 3114 samples\n",
      "      Applied inverse scaling for window 512 - 3115 samples\n",
      "      Applied inverse scaling for window 512 - 2996 samples\n",
      "      Applied inverse scaling for window 512 - 3160 samples\n",
      "      Applied inverse scaling for window 512 - 3125 samples\n",
      "      Applied inverse scaling for window 512 - 3045 samples\n",
      "      Applied inverse scaling for window 512 - 3022 samples\n",
      "      Applied inverse scaling for window 512 - 3120 samples\n",
      "      Applied inverse scaling for window 512 - 3167 samples\n",
      "      Applied inverse scaling for window 512 - 3179 samples\n",
      "      Applied inverse scaling for window 512 - 2595 samples\n",
      "      Applied inverse scaling for window 512 - 2839 samples\n",
      "      Applied inverse scaling for window 512 - 3151 samples\n",
      "      Applied inverse scaling for window 512 - 3114 samples\n",
      "      Applied inverse scaling for window 512 - 2989 samples\n",
      "      Applied inverse scaling for window 512 - 3132 samples\n",
      "      Applied inverse scaling for window 512 - 3173 samples\n",
      "      Applied inverse scaling for window 512 - 3153 samples\n",
      "      Applied inverse scaling for window 512 - 3029 samples\n",
      "      Applied inverse scaling for window 512 - 2969 samples\n",
      "      Applied inverse scaling for window 512 - 3152 samples\n",
      "      Applied inverse scaling for window 512 - 3070 samples\n",
      "      Applied inverse scaling for window 512 - 3064 samples\n",
      "      Applied inverse scaling for window 512 - 3069 samples\n",
      "      Applied inverse scaling for window 512 - 3121 samples\n",
      "      Applied inverse scaling for window 512 - 3113 samples\n",
      "      Applied inverse scaling for window 512 - 3026 samples\n",
      "      Applied inverse scaling for window 512 - 3112 samples\n",
      "      Applied inverse scaling for window 512 - 3162 samples\n",
      "      Applied inverse scaling for window 512 - 3094 samples\n",
      "  Model: NN2, Scheme: VW\n",
      "      Applied inverse scaling for window 512 - 3006 samples\n",
      "      Applied inverse scaling for window 512 - 3170 samples\n",
      "      Applied inverse scaling for window 512 - 3176 samples\n",
      "      Applied inverse scaling for window 512 - 3123 samples\n",
      "      Applied inverse scaling for window 512 - 3083 samples\n",
      "      Applied inverse scaling for window 512 - 3122 samples\n",
      "      Applied inverse scaling for window 512 - 3114 samples\n",
      "      Applied inverse scaling for window 512 - 3115 samples\n",
      "      Applied inverse scaling for window 512 - 2996 samples\n",
      "      Applied inverse scaling for window 512 - 3160 samples\n",
      "      Applied inverse scaling for window 512 - 3125 samples\n",
      "      Applied inverse scaling for window 512 - 3045 samples\n",
      "      Applied inverse scaling for window 512 - 3022 samples\n",
      "      Applied inverse scaling for window 512 - 3120 samples\n",
      "      Applied inverse scaling for window 512 - 3167 samples\n",
      "      Applied inverse scaling for window 512 - 3179 samples\n",
      "      Applied inverse scaling for window 512 - 2595 samples\n",
      "      Applied inverse scaling for window 512 - 2839 samples\n",
      "      Applied inverse scaling for window 512 - 3151 samples\n",
      "      Applied inverse scaling for window 512 - 3114 samples\n",
      "      Applied inverse scaling for window 512 - 2989 samples\n",
      "      Applied inverse scaling for window 512 - 3132 samples\n",
      "      Applied inverse scaling for window 512 - 3173 samples\n",
      "      Applied inverse scaling for window 512 - 3153 samples\n",
      "      Applied inverse scaling for window 512 - 3029 samples\n",
      "      Applied inverse scaling for window 512 - 2969 samples\n",
      "      Applied inverse scaling for window 512 - 3152 samples\n",
      "      Applied inverse scaling for window 512 - 3070 samples\n",
      "      Applied inverse scaling for window 512 - 3064 samples\n",
      "      Applied inverse scaling for window 512 - 3069 samples\n",
      "      Applied inverse scaling for window 512 - 3121 samples\n",
      "      Applied inverse scaling for window 512 - 3113 samples\n",
      "      Applied inverse scaling for window 512 - 3026 samples\n",
      "      Applied inverse scaling for window 512 - 3112 samples\n",
      "      Applied inverse scaling for window 512 - 3162 samples\n",
      "      Applied inverse scaling for window 512 - 3094 samples\n",
      "[Update] Metrics updated for NN2 w=512\n",
      "  Model: NN2, Scheme: EW\n",
      "      Applied inverse scaling for window 512 - 3006 samples\n",
      "      Applied inverse scaling for window 512 - 3170 samples\n",
      "      Applied inverse scaling for window 512 - 3176 samples\n",
      "      Applied inverse scaling for window 512 - 3123 samples\n",
      "      Applied inverse scaling for window 512 - 3083 samples\n",
      "      Applied inverse scaling for window 512 - 3122 samples\n",
      "      Applied inverse scaling for window 512 - 3114 samples\n",
      "      Applied inverse scaling for window 512 - 3115 samples\n",
      "      Applied inverse scaling for window 512 - 2996 samples\n",
      "      Applied inverse scaling for window 512 - 3160 samples\n",
      "      Applied inverse scaling for window 512 - 3125 samples\n",
      "      Applied inverse scaling for window 512 - 3045 samples\n",
      "      Applied inverse scaling for window 512 - 3022 samples\n",
      "      Applied inverse scaling for window 512 - 3120 samples\n",
      "      Applied inverse scaling for window 512 - 3167 samples\n",
      "      Applied inverse scaling for window 512 - 3179 samples\n",
      "      Applied inverse scaling for window 512 - 2595 samples\n",
      "      Applied inverse scaling for window 512 - 2839 samples\n",
      "      Applied inverse scaling for window 512 - 3151 samples\n",
      "      Applied inverse scaling for window 512 - 3114 samples\n",
      "      Applied inverse scaling for window 512 - 2989 samples\n",
      "      Applied inverse scaling for window 512 - 3132 samples\n",
      "      Applied inverse scaling for window 512 - 3173 samples\n",
      "      Applied inverse scaling for window 512 - 3153 samples\n",
      "      Applied inverse scaling for window 512 - 3029 samples\n",
      "      Applied inverse scaling for window 512 - 2969 samples\n",
      "      Applied inverse scaling for window 512 - 3152 samples\n",
      "      Applied inverse scaling for window 512 - 3070 samples\n",
      "      Applied inverse scaling for window 512 - 3064 samples\n",
      "      Applied inverse scaling for window 512 - 3069 samples\n",
      "      Applied inverse scaling for window 512 - 3121 samples\n",
      "      Applied inverse scaling for window 512 - 3113 samples\n",
      "      Applied inverse scaling for window 512 - 3026 samples\n",
      "      Applied inverse scaling for window 512 - 3112 samples\n",
      "      Applied inverse scaling for window 512 - 3162 samples\n",
      "      Applied inverse scaling for window 512 - 3094 samples\n",
      "  Model: NN3, Scheme: VW\n",
      "      Applied inverse scaling for window 512 - 3006 samples\n",
      "      Applied inverse scaling for window 512 - 3170 samples\n",
      "      Applied inverse scaling for window 512 - 3176 samples\n",
      "      Applied inverse scaling for window 512 - 3123 samples\n",
      "      Applied inverse scaling for window 512 - 3083 samples\n",
      "      Applied inverse scaling for window 512 - 3122 samples\n",
      "      Applied inverse scaling for window 512 - 3114 samples\n",
      "      Applied inverse scaling for window 512 - 3115 samples\n",
      "      Applied inverse scaling for window 512 - 2996 samples\n",
      "      Applied inverse scaling for window 512 - 3160 samples\n",
      "      Applied inverse scaling for window 512 - 3125 samples\n",
      "      Applied inverse scaling for window 512 - 3045 samples\n",
      "      Applied inverse scaling for window 512 - 3022 samples\n",
      "      Applied inverse scaling for window 512 - 3120 samples\n",
      "      Applied inverse scaling for window 512 - 3167 samples\n",
      "      Applied inverse scaling for window 512 - 3179 samples\n",
      "      Applied inverse scaling for window 512 - 2595 samples\n",
      "      Applied inverse scaling for window 512 - 2839 samples\n",
      "      Applied inverse scaling for window 512 - 3151 samples\n",
      "      Applied inverse scaling for window 512 - 3114 samples\n",
      "      Applied inverse scaling for window 512 - 2989 samples\n",
      "      Applied inverse scaling for window 512 - 3132 samples\n",
      "      Applied inverse scaling for window 512 - 3173 samples\n",
      "      Applied inverse scaling for window 512 - 3153 samples\n",
      "      Applied inverse scaling for window 512 - 3029 samples\n",
      "      Applied inverse scaling for window 512 - 2969 samples\n",
      "      Applied inverse scaling for window 512 - 3152 samples\n",
      "      Applied inverse scaling for window 512 - 3070 samples\n",
      "      Applied inverse scaling for window 512 - 3064 samples\n",
      "      Applied inverse scaling for window 512 - 3069 samples\n",
      "      Applied inverse scaling for window 512 - 3121 samples\n",
      "      Applied inverse scaling for window 512 - 3113 samples\n",
      "      Applied inverse scaling for window 512 - 3026 samples\n",
      "      Applied inverse scaling for window 512 - 3112 samples\n",
      "      Applied inverse scaling for window 512 - 3162 samples\n",
      "      Applied inverse scaling for window 512 - 3094 samples\n",
      "[Update] Metrics updated for NN3 w=512\n",
      "  Model: NN3, Scheme: EW\n",
      "      Applied inverse scaling for window 512 - 3006 samples\n",
      "      Applied inverse scaling for window 512 - 3170 samples\n",
      "      Applied inverse scaling for window 512 - 3176 samples\n",
      "      Applied inverse scaling for window 512 - 3123 samples\n",
      "      Applied inverse scaling for window 512 - 3083 samples\n",
      "      Applied inverse scaling for window 512 - 3122 samples\n",
      "      Applied inverse scaling for window 512 - 3114 samples\n",
      "      Applied inverse scaling for window 512 - 3115 samples\n",
      "      Applied inverse scaling for window 512 - 2996 samples\n",
      "      Applied inverse scaling for window 512 - 3160 samples\n",
      "      Applied inverse scaling for window 512 - 3125 samples\n",
      "      Applied inverse scaling for window 512 - 3045 samples\n",
      "      Applied inverse scaling for window 512 - 3022 samples\n",
      "      Applied inverse scaling for window 512 - 3120 samples\n",
      "      Applied inverse scaling for window 512 - 3167 samples\n",
      "      Applied inverse scaling for window 512 - 3179 samples\n",
      "      Applied inverse scaling for window 512 - 2595 samples\n",
      "      Applied inverse scaling for window 512 - 2839 samples\n",
      "      Applied inverse scaling for window 512 - 3151 samples\n",
      "      Applied inverse scaling for window 512 - 3114 samples\n",
      "      Applied inverse scaling for window 512 - 2989 samples\n",
      "      Applied inverse scaling for window 512 - 3132 samples\n",
      "      Applied inverse scaling for window 512 - 3173 samples\n",
      "      Applied inverse scaling for window 512 - 3153 samples\n",
      "      Applied inverse scaling for window 512 - 3029 samples\n",
      "      Applied inverse scaling for window 512 - 2969 samples\n",
      "      Applied inverse scaling for window 512 - 3152 samples\n",
      "      Applied inverse scaling for window 512 - 3070 samples\n",
      "      Applied inverse scaling for window 512 - 3064 samples\n",
      "      Applied inverse scaling for window 512 - 3069 samples\n",
      "      Applied inverse scaling for window 512 - 3121 samples\n",
      "      Applied inverse scaling for window 512 - 3113 samples\n",
      "      Applied inverse scaling for window 512 - 3026 samples\n",
      "      Applied inverse scaling for window 512 - 3112 samples\n",
      "      Applied inverse scaling for window 512 - 3162 samples\n",
      "      Applied inverse scaling for window 512 - 3094 samples\n",
      "  Model: NN4, Scheme: VW\n",
      "      Applied inverse scaling for window 512 - 3006 samples\n",
      "      Applied inverse scaling for window 512 - 3170 samples\n",
      "      Applied inverse scaling for window 512 - 3176 samples\n",
      "      Applied inverse scaling for window 512 - 3123 samples\n",
      "      Applied inverse scaling for window 512 - 3083 samples\n",
      "      Applied inverse scaling for window 512 - 3122 samples\n",
      "      Applied inverse scaling for window 512 - 3114 samples\n",
      "      Applied inverse scaling for window 512 - 3115 samples\n",
      "      Applied inverse scaling for window 512 - 2996 samples\n",
      "      Applied inverse scaling for window 512 - 3160 samples\n",
      "      Applied inverse scaling for window 512 - 3125 samples\n",
      "      Applied inverse scaling for window 512 - 3045 samples\n",
      "      Applied inverse scaling for window 512 - 3022 samples\n",
      "      Applied inverse scaling for window 512 - 3120 samples\n",
      "      Applied inverse scaling for window 512 - 3167 samples\n",
      "      Applied inverse scaling for window 512 - 3179 samples\n",
      "      Applied inverse scaling for window 512 - 2595 samples\n",
      "      Applied inverse scaling for window 512 - 2839 samples\n",
      "      Applied inverse scaling for window 512 - 3151 samples\n",
      "      Applied inverse scaling for window 512 - 3114 samples\n",
      "      Applied inverse scaling for window 512 - 2989 samples\n",
      "      Applied inverse scaling for window 512 - 3132 samples\n",
      "      Applied inverse scaling for window 512 - 3173 samples\n",
      "      Applied inverse scaling for window 512 - 3153 samples\n",
      "      Applied inverse scaling for window 512 - 3029 samples\n",
      "      Applied inverse scaling for window 512 - 2969 samples\n",
      "      Applied inverse scaling for window 512 - 3152 samples\n",
      "      Applied inverse scaling for window 512 - 3070 samples\n",
      "      Applied inverse scaling for window 512 - 3064 samples\n",
      "      Applied inverse scaling for window 512 - 3069 samples\n",
      "      Applied inverse scaling for window 512 - 3121 samples\n",
      "      Applied inverse scaling for window 512 - 3113 samples\n",
      "      Applied inverse scaling for window 512 - 3026 samples\n",
      "      Applied inverse scaling for window 512 - 3112 samples\n",
      "      Applied inverse scaling for window 512 - 3162 samples\n",
      "      Applied inverse scaling for window 512 - 3094 samples\n",
      "[Update] Metrics updated for NN4 w=512\n",
      "  Model: NN4, Scheme: EW\n",
      "      Applied inverse scaling for window 512 - 3006 samples\n",
      "      Applied inverse scaling for window 512 - 3170 samples\n",
      "      Applied inverse scaling for window 512 - 3176 samples\n",
      "      Applied inverse scaling for window 512 - 3123 samples\n",
      "      Applied inverse scaling for window 512 - 3083 samples\n",
      "      Applied inverse scaling for window 512 - 3122 samples\n",
      "      Applied inverse scaling for window 512 - 3114 samples\n",
      "      Applied inverse scaling for window 512 - 3115 samples\n",
      "      Applied inverse scaling for window 512 - 2996 samples\n",
      "      Applied inverse scaling for window 512 - 3160 samples\n",
      "      Applied inverse scaling for window 512 - 3125 samples\n",
      "      Applied inverse scaling for window 512 - 3045 samples\n",
      "      Applied inverse scaling for window 512 - 3022 samples\n",
      "      Applied inverse scaling for window 512 - 3120 samples\n",
      "      Applied inverse scaling for window 512 - 3167 samples\n",
      "      Applied inverse scaling for window 512 - 3179 samples\n",
      "      Applied inverse scaling for window 512 - 2595 samples\n",
      "      Applied inverse scaling for window 512 - 2839 samples\n",
      "      Applied inverse scaling for window 512 - 3151 samples\n",
      "      Applied inverse scaling for window 512 - 3114 samples\n",
      "      Applied inverse scaling for window 512 - 2989 samples\n",
      "      Applied inverse scaling for window 512 - 3132 samples\n",
      "      Applied inverse scaling for window 512 - 3173 samples\n",
      "      Applied inverse scaling for window 512 - 3153 samples\n",
      "      Applied inverse scaling for window 512 - 3029 samples\n",
      "      Applied inverse scaling for window 512 - 2969 samples\n",
      "      Applied inverse scaling for window 512 - 3152 samples\n",
      "      Applied inverse scaling for window 512 - 3070 samples\n",
      "      Applied inverse scaling for window 512 - 3064 samples\n",
      "      Applied inverse scaling for window 512 - 3069 samples\n",
      "      Applied inverse scaling for window 512 - 3121 samples\n",
      "      Applied inverse scaling for window 512 - 3113 samples\n",
      "      Applied inverse scaling for window 512 - 3026 samples\n",
      "      Applied inverse scaling for window 512 - 3112 samples\n",
      "      Applied inverse scaling for window 512 - 3162 samples\n",
      "      Applied inverse scaling for window 512 - 3094 samples\n",
      "  Model: NN5, Scheme: VW\n",
      "      Applied inverse scaling for window 512 - 3006 samples\n",
      "      Applied inverse scaling for window 512 - 3170 samples\n",
      "      Applied inverse scaling for window 512 - 3176 samples\n",
      "      Applied inverse scaling for window 512 - 3123 samples\n",
      "      Applied inverse scaling for window 512 - 3083 samples\n",
      "      Applied inverse scaling for window 512 - 3122 samples\n",
      "      Applied inverse scaling for window 512 - 3114 samples\n",
      "      Applied inverse scaling for window 512 - 3115 samples\n",
      "      Applied inverse scaling for window 512 - 2996 samples\n",
      "      Applied inverse scaling for window 512 - 3160 samples\n",
      "      Applied inverse scaling for window 512 - 3125 samples\n",
      "      Applied inverse scaling for window 512 - 3045 samples\n",
      "      Applied inverse scaling for window 512 - 3022 samples\n",
      "      Applied inverse scaling for window 512 - 3120 samples\n",
      "      Applied inverse scaling for window 512 - 3167 samples\n",
      "      Applied inverse scaling for window 512 - 3179 samples\n",
      "      Applied inverse scaling for window 512 - 2595 samples\n",
      "      Applied inverse scaling for window 512 - 2839 samples\n",
      "      Applied inverse scaling for window 512 - 3151 samples\n",
      "      Applied inverse scaling for window 512 - 3114 samples\n",
      "      Applied inverse scaling for window 512 - 2989 samples\n",
      "      Applied inverse scaling for window 512 - 3132 samples\n",
      "      Applied inverse scaling for window 512 - 3173 samples\n",
      "      Applied inverse scaling for window 512 - 3153 samples\n",
      "      Applied inverse scaling for window 512 - 3029 samples\n",
      "      Applied inverse scaling for window 512 - 2969 samples\n",
      "      Applied inverse scaling for window 512 - 3152 samples\n",
      "      Applied inverse scaling for window 512 - 3070 samples\n",
      "      Applied inverse scaling for window 512 - 3064 samples\n",
      "      Applied inverse scaling for window 512 - 3069 samples\n",
      "      Applied inverse scaling for window 512 - 3121 samples\n",
      "      Applied inverse scaling for window 512 - 3113 samples\n",
      "      Applied inverse scaling for window 512 - 3026 samples\n",
      "      Applied inverse scaling for window 512 - 3112 samples\n",
      "      Applied inverse scaling for window 512 - 3162 samples\n",
      "      Applied inverse scaling for window 512 - 3094 samples\n",
      "[Update] Metrics updated for NN5 w=512\n",
      "  Model: NN5, Scheme: EW\n",
      "      Applied inverse scaling for window 512 - 3006 samples\n",
      "      Applied inverse scaling for window 512 - 3170 samples\n",
      "      Applied inverse scaling for window 512 - 3176 samples\n",
      "      Applied inverse scaling for window 512 - 3123 samples\n",
      "      Applied inverse scaling for window 512 - 3083 samples\n",
      "      Applied inverse scaling for window 512 - 3122 samples\n",
      "      Applied inverse scaling for window 512 - 3114 samples\n",
      "      Applied inverse scaling for window 512 - 3115 samples\n",
      "      Applied inverse scaling for window 512 - 2996 samples\n",
      "      Applied inverse scaling for window 512 - 3160 samples\n",
      "      Applied inverse scaling for window 512 - 3125 samples\n",
      "      Applied inverse scaling for window 512 - 3045 samples\n",
      "      Applied inverse scaling for window 512 - 3022 samples\n",
      "      Applied inverse scaling for window 512 - 3120 samples\n",
      "      Applied inverse scaling for window 512 - 3167 samples\n",
      "      Applied inverse scaling for window 512 - 3179 samples\n",
      "      Applied inverse scaling for window 512 - 2595 samples\n",
      "      Applied inverse scaling for window 512 - 2839 samples\n",
      "      Applied inverse scaling for window 512 - 3151 samples\n",
      "      Applied inverse scaling for window 512 - 3114 samples\n",
      "      Applied inverse scaling for window 512 - 2989 samples\n",
      "      Applied inverse scaling for window 512 - 3132 samples\n",
      "      Applied inverse scaling for window 512 - 3173 samples\n",
      "      Applied inverse scaling for window 512 - 3153 samples\n",
      "      Applied inverse scaling for window 512 - 3029 samples\n",
      "      Applied inverse scaling for window 512 - 2969 samples\n",
      "      Applied inverse scaling for window 512 - 3152 samples\n",
      "      Applied inverse scaling for window 512 - 3070 samples\n",
      "      Applied inverse scaling for window 512 - 3064 samples\n",
      "      Applied inverse scaling for window 512 - 3069 samples\n",
      "      Applied inverse scaling for window 512 - 3121 samples\n",
      "      Applied inverse scaling for window 512 - 3113 samples\n",
      "      Applied inverse scaling for window 512 - 3026 samples\n",
      "      Applied inverse scaling for window 512 - 3112 samples\n",
      "      Applied inverse scaling for window 512 - 3162 samples\n",
      "      Applied inverse scaling for window 512 - 3094 samples\n",
      "VW results saved to portfolio_results_daily_rebalance_VW.csv\n",
      "EW results saved to portfolio_results_daily_rebalance_EW.csv\n",
      "VW results saved to portfolio_daily_series_VW.csv\n",
      "EW results saved to portfolio_daily_series_EW.csv\n",
      "Saved 2217000 prediction rows to predictions_daily.csv\n",
      "Generated 120 portfolio summary records\n",
      "Generated 258360 daily series records\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(    scheme model  window portfolio_type  annual_return  annual_vol    sharpe  \\\n",
       " 0       VW   NN1       5      long_only       0.197085    0.208352  0.945925   \n",
       " 1       VW   NN1       5     short_only      -0.101290    0.196682 -0.514992   \n",
       " 2       VW   NN1       5     long_short       0.095795    0.222062  0.431391   \n",
       " 3       EW   NN1       5      long_only       0.214612    0.217917  0.984835   \n",
       " 4       EW   NN1       5     short_only      -0.129870    0.199672 -0.650418   \n",
       " ..     ...   ...     ...            ...            ...         ...       ...   \n",
       " 115     VW   NN5     512     short_only      -0.187741    0.184709 -1.016416   \n",
       " 116     VW   NN5     512     long_short       0.056417    0.187892  0.300265   \n",
       " 117     EW   NN5     512      long_only       0.195396    0.215912  0.904980   \n",
       " 118     EW   NN5     512     short_only      -0.097603    0.182493 -0.534832   \n",
       " 119     EW   NN5     512     long_short       0.097793    0.166178  0.588483   \n",
       " \n",
       "      max_drawdown  max_1d_loss  avg_turnover  ...  tc20_sharpe  \\\n",
       " 0        0.286114    -0.075310      1.680262  ...    -3.107575   \n",
       " 1        0.802471    -0.073426      1.674214  ...    -4.790918   \n",
       " 2        0.502715    -0.074861      3.353926  ...    -7.142109   \n",
       " 3        0.295842    -0.073513      1.535132  ...    -2.558091   \n",
       " 4        0.775598    -0.073426      1.536925  ...    -4.520637   \n",
       " ..            ...          ...           ...  ...          ...   \n",
       " 115      0.847937    -0.050409      1.798033  ...    -5.913273   \n",
       " 116      0.231957    -0.070209      3.579465  ...    -9.265554   \n",
       " 117      0.229229    -0.057721      1.642238  ...    -2.921694   \n",
       " 118      0.677424    -0.035894      1.716587  ...    -5.262923   \n",
       " 119      0.225255    -0.049266      3.358414  ...    -9.521301   \n",
       " \n",
       "      tc20_max_drawdown  tc30_annual_return  tc30_annual_vol  tc30_sharpe  \\\n",
       " 0            -0.996778           -1.073193         0.209808    -5.115122   \n",
       " 1            -0.999758           -1.366996         0.197936    -6.906241   \n",
       " 2            -0.999999           -2.439772         0.224550   -10.865175   \n",
       " 3            -0.993082           -0.945948         0.219144    -4.316554   \n",
       " 4            -0.999664           -1.291785         0.200537    -6.441617   \n",
       " ..                 ...                 ...              ...          ...   \n",
       " 115          -0.999931           -1.547054         0.185435    -8.342859   \n",
       " 116          -1.000000           -2.649658         0.189587   -13.975972   \n",
       " 117          -0.996372           -1.046136         0.216850    -4.824232   \n",
       " 118          -0.999783           -1.395343         0.183343    -7.610552   \n",
       " 119          -0.999999           -2.441168         0.168637   -14.475864   \n",
       " \n",
       "      tc30_max_drawdown  tc40_annual_return  tc40_annual_vol  tc40_sharpe  \\\n",
       " 0            -0.999915           -1.496619         0.210753    -7.101299   \n",
       " 1            -0.999993           -1.788898         0.198852    -8.996116   \n",
       " 2            -1.000000           -3.284962         0.226280   -14.517216   \n",
       " 3            -0.999749           -1.332801         0.219903    -6.060850   \n",
       " 4            -0.999988           -1.679090         0.201166    -8.346782   \n",
       " ..                 ...                 ...              ...          ...   \n",
       " 115          -0.999999           -2.000158         0.186062   -10.749953   \n",
       " 116          -1.000000           -3.551683         0.190956   -18.599533   \n",
       " 117          -0.999896           -1.459980         0.217411    -6.715292   \n",
       " 118          -0.999995           -1.827922         0.183879    -9.940909   \n",
       " 119          -1.000000           -3.287489         0.170077   -19.329364   \n",
       " \n",
       "      tc40_max_drawdown  \n",
       " 0            -0.999998  \n",
       " 1            -1.000000  \n",
       " 2            -1.000000  \n",
       " 3            -0.999991  \n",
       " 4            -1.000000  \n",
       " ..                 ...  \n",
       " 115          -1.000000  \n",
       " 116          -1.000000  \n",
       " 117          -0.999997  \n",
       " 118          -1.000000  \n",
       " 119          -1.000000  \n",
       " \n",
       " [120 rows x 32 columns],\n",
       "        scheme model  window portfolio_type                 date    return  \\\n",
       " 0          VW   NN1       5      long_only  2016-01-05 00:00:00 -0.006821   \n",
       " 1          VW   NN1       5      long_only  2016-01-06 00:00:00 -0.008792   \n",
       " 2          VW   NN1       5      long_only  2016-01-07 00:00:00 -0.017267   \n",
       " 3          VW   NN1       5      long_only  2016-01-08 00:00:00  0.000360   \n",
       " 4          VW   NN1       5      long_only  2016-01-11 00:00:00  0.010806   \n",
       " ...       ...   ...     ...            ...                  ...       ...   \n",
       " 258355     EW   NN5     512     long_short  2024-12-20 00:00:00  0.002447   \n",
       " 258356     EW   NN5     512     long_short  2024-12-23 00:00:00  0.015531   \n",
       " 258357     EW   NN5     512     long_short  2024-12-24 00:00:00  0.000426   \n",
       " 258358     EW   NN5     512     long_short  2024-12-27 00:00:00 -0.004154   \n",
       " 258359     EW   NN5     512     long_short  2024-12-30 00:00:00  0.015602   \n",
       " \n",
       "         turnover  cumulative  tc5_return  tc5_cumulative  tc10_return  \\\n",
       " 0       1.000000   -0.006844   -0.007321       -0.007348    -0.007821   \n",
       " 1       1.243946   -0.015675   -0.009414       -0.016807    -0.010036   \n",
       " 2       2.000000   -0.033093   -0.018267       -0.035243    -0.019267   \n",
       " 3       1.976712   -0.032733   -0.000628       -0.035871    -0.001616   \n",
       " 4       1.966980   -0.021985    0.009822       -0.026096     0.008839   \n",
       " ...          ...         ...         ...             ...          ...   \n",
       " 258355  3.186404    0.690482    0.000854       -2.920407    -0.000740   \n",
       " 258356  3.613494    0.705894    0.013724       -2.906777     0.011918   \n",
       " 258357  2.011931    0.706320   -0.000580       -2.907357    -0.001586   \n",
       " 258358  4.012684    0.702157   -0.006161       -2.913536    -0.008167   \n",
       " 258359  3.981067    0.717638    0.013611       -2.900017     0.011621   \n",
       " \n",
       "         tc10_cumulative  tc20_return  tc20_cumulative  tc30_return  \\\n",
       " 0             -0.007852    -0.008821        -0.008860    -0.009821   \n",
       " 1             -0.017939    -0.011280        -0.020204    -0.012524   \n",
       " 2             -0.037394    -0.021267        -0.041701    -0.023267   \n",
       " 3             -0.039011    -0.003593        -0.045300    -0.005570   \n",
       " 4             -0.030211     0.006872        -0.038452     0.004905   \n",
       " ...                 ...          ...              ...          ...   \n",
       " 258355        -6.537490    -0.003926       -13.790321    -0.007112   \n",
       " 258356        -6.525643     0.008304       -13.782051     0.004691   \n",
       " 258357        -6.527230    -0.003598       -13.785656    -0.005610   \n",
       " 258358        -6.535431    -0.012180       -13.797910    -0.016192   \n",
       " 258359        -6.523877     0.007640       -13.790299     0.003659   \n",
       " \n",
       "         tc30_cumulative  tc40_return  tc40_cumulative  \n",
       " 0             -0.009869    -0.010821        -0.010880  \n",
       " 1             -0.022473    -0.013768        -0.024744  \n",
       " 2             -0.046015    -0.025267        -0.050335  \n",
       " 3             -0.051600    -0.007547        -0.057911  \n",
       " 4             -0.046707     0.002938        -0.054977  \n",
       " ...                 ...          ...              ...  \n",
       " 258355       -21.068184    -0.010299       -28.371257  \n",
       " 258356       -21.063505     0.001077       -28.370180  \n",
       " 258357       -21.069130    -0.007622       -28.377831  \n",
       " 258358       -21.085455    -0.020205       -28.398243  \n",
       " 258359       -21.081803    -0.000322       -28.398565  \n",
       " \n",
       " [258360 rows x 18 columns],\n",
       " <__main__.PortfolioBacktester at 0x410bdd940>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_portfolio_simulation_daily_rebalance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] 5_factor_analysis_VW_gross.csv \n",
      "[Saved] 5_factor_analysis_VW_net.csv \n",
      "[Saved] 5_factor_analysis_EW_gross.csv \n",
      "[Saved] 5_factor_analysis_EW_net.csv \n"
     ]
    }
   ],
   "source": [
    "def run_factor_regression(port_ret, factors, use_excess=True):\n",
    "    df = pd.concat([port_ret, factors], axis=1, join='inner').dropna()\n",
    "    df.columns = ['ret'] + list(factors.columns)\n",
    "    \n",
    "    if use_excess:\n",
    "        y = df['ret'].values\n",
    "    else:\n",
    "        y = df['ret'].values - df['rf'].values\n",
    "    \n",
    "    X = df[['mktrf','smb','hml','rmw','cma','umd']].values\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    model = sm.OLS(y, X)\n",
    "    res = model.fit()\n",
    "    alpha = res.params[0]\n",
    "    resid_std = res.resid.std(ddof=1)\n",
    "\n",
    "    ir_daily = alpha / resid_std\n",
    "    ir_annual = ir_daily * np.sqrt(252)\n",
    "\n",
    "    y_hat = np.asarray(res.fittedvalues)\n",
    "    \n",
    "    out = {\n",
    "        'N_obs'            : len(y),\n",
    "        'alpha_daily'      : alpha,\n",
    "        'alpha_annual'     : alpha*252,      \n",
    "        't_alpha'          : res.tvalues[0],\n",
    "        'IR_daily'         : ir_daily,\n",
    "        'IR_annual'        : ir_annual,\n",
    "        'R2_zero'          : r2_zero(y, y_hat),\n",
    "    }\n",
    "    \n",
    "    factor_names = ['MKT','SMB','HML','RMW','CMA','UMD']\n",
    "    for i, fac in enumerate(factor_names, start=1):\n",
    "        out[f'beta_{fac}'] = res.params[i]\n",
    "        out[f't_{fac}']    = res.tvalues[i]\n",
    "    \n",
    "    return out\n",
    "\n",
    "def batch_factor_analysis(\n",
    "    daily_df: pd.DataFrame,\n",
    "    factors_path: str,\n",
    "    scheme: str,\n",
    "    tc_levels=(0, 5, 10, 20, 40),\n",
    "    portfolio_types=('long_only','short_only','long_short'),\n",
    "    model_filter=None,\n",
    "    window_filter=None,\n",
    "    gross_only=False,\n",
    "    out_dir='factor_IR_results',\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a CSV containing IR results.\n",
    "    gross_only=True  → only tc=0; False → all tc_levels.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    fac = (pd.read_csv(factors_path, parse_dates=['date'])\n",
    "             .set_index('date')\n",
    "             .sort_index())\n",
    "\n",
    "    sub = daily_df[daily_df['scheme'] == scheme].copy()\n",
    "    if model_filter is not None:\n",
    "        sub = sub[sub['model'].isin(model_filter)]\n",
    "    if window_filter is not None:\n",
    "        sub = sub[sub['window'].isin(window_filter)]\n",
    "\n",
    "    tc_iter = (0,) if gross_only else tc_levels\n",
    "    results = []\n",
    "\n",
    "    for (model, win, ptype), g in sub.groupby(['model','window','portfolio_type']):\n",
    "        g = g.sort_values('date').set_index(pd.to_datetime(g['date']))\n",
    "\n",
    "        for tc in tc_iter:\n",
    "            col = 'return' if tc == 0 else f'tc{tc}_return'\n",
    "            if col not in g.columns:\n",
    "                continue\n",
    "            port_ret = g[col]\n",
    "            stats = run_factor_regression(port_ret, fac, use_excess=True)\n",
    "            stats.update({\n",
    "                'scheme'        : scheme,\n",
    "                'model'         : model,\n",
    "                'window'        : win,\n",
    "                'portfolio_type': ptype,\n",
    "                'tc_bps'        : tc,\n",
    "            })\n",
    "            results.append(stats)\n",
    "\n",
    "    df_out = pd.DataFrame(results)[[\n",
    "        'scheme','model','window','portfolio_type','tc_bps','N_obs',\n",
    "        'alpha_daily','alpha_annual','t_alpha',\n",
    "        'IR_daily','IR_annual','R2_zero',\n",
    "        'beta_MKT','t_MKT','beta_SMB','t_SMB',\n",
    "        'beta_HML','t_HML','beta_RMW','t_RMW',\n",
    "        'beta_CMA','t_CMA','beta_UMD','t_UMD'\n",
    "    ]]\n",
    "\n",
    "    tag = 'gross' if gross_only else 'net'\n",
    "    fname = f'5_factor_analysis_{scheme}_{tag}.csv'\n",
    "    df_out.to_csv(os.path.join(out_dir, fname), index=False)\n",
    "    print(f'[Saved] {fname}')\n",
    "    return df_out\n",
    "\n",
    "\n",
    "\n",
    "def run_all_factor_tests(vw_csv=\"portfolio_daily_series_VW.csv\",\n",
    "                         ew_csv=\"portfolio_daily_series_EW.csv\",\n",
    "                         factor_csv=\"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/5_Factors_Plus_Momentum.csv\",\n",
    "                         save_dir=\"results\",\n",
    "                         y_is_excess=True,\n",
    "                         hac_lags=5,\n",
    "                         save_txt=True):\n",
    "    vw_df = pd.read_csv(vw_csv)\n",
    "    ew_df = pd.read_csv(ew_csv)\n",
    "\n",
    "    vw_gross = batch_factor_analysis(\n",
    "        vw_df, factor_csv, scheme='VW', gross_only=True)\n",
    "    vw_net   = batch_factor_analysis(\n",
    "        vw_df, factor_csv, scheme='VW', gross_only=False)\n",
    "\n",
    "    ew_gross = batch_factor_analysis(\n",
    "        ew_df, factor_csv, scheme='EW', gross_only=True)\n",
    "    ew_net   = batch_factor_analysis(\n",
    "        ew_df, factor_csv, scheme='EW', gross_only=False)\n",
    "\n",
    "    return vw_gross, vw_net, ew_gross, ew_net\n",
    "    \n",
    "\n",
    "vw_gross, vw_net, ew_gross, ew_net = run_all_factor_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish: portfolio_daily_series_VW_with_rf.csv\n",
      "Finish: portfolio_daily_series_EW_with_rf.csv\n"
     ]
    }
   ],
   "source": [
    "# === File paths ===\n",
    "rf_file = \"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/CRSP_2016_2024_top50_with_exret.csv\"\n",
    "vw_file = \"portfolio_daily_series_VW.csv\"\n",
    "ew_file = \"portfolio_daily_series_EW.csv\"\n",
    "\n",
    "# === Load risk-free rate (rf) data ===\n",
    "\n",
    "rf_df = pd.read_csv(rf_file, usecols=[\"date\", \"rf\"])\n",
    "rf_df[\"date\"] = pd.to_datetime(rf_df[\"date\"])\n",
    "rf_dict = dict(zip(rf_df[\"date\"], rf_df[\"rf\"]))\n",
    "\n",
    "\n",
    "def adjust_returns_with_rf_grouped(file_path, output_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Parse date with flexible format\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], format='mixed', dayfirst=True)\n",
    "\n",
    "    # Find all return columns (exclude cumulative columns)\n",
    "    return_cols = [col for col in df.columns if \"return\" in col and \"cumul\" not in col]\n",
    "\n",
    "    # Set portfolio_type order to avoid groupby sorting issues\n",
    "    order = [\"long_only\", \"short_only\", \"long_short\"]\n",
    "    df[\"portfolio_type\"] = pd.Categorical(df[\"portfolio_type\"], categories=order, ordered=True)\n",
    "\n",
    "    df_list = []\n",
    "    for _, group in df.groupby([\"scheme\", \"model\", \"window\", \"portfolio_type\"], sort=False):\n",
    "        group = group.sort_values(\"date\").copy()\n",
    "        for col in return_cols:\n",
    "            # Add rf to daily return\n",
    "            group[col] = group.apply(lambda row: row[col] + rf_dict.get(row[\"date\"], 0), axis=1)\n",
    "\n",
    "            # Update corresponding cumulative column\n",
    "            cum_col = col.replace(\"return\", \"cumulative\")\n",
    "            group[cum_col] = np.log1p(group[col]).cumsum()\n",
    "        df_list.append(group)\n",
    "\n",
    "    df_new = pd.concat(df_list).sort_values([\"scheme\", \"model\", \"window\", \"portfolio_type\", \"date\"])\n",
    "    df_new.to_csv(output_path, index=False)\n",
    "    print(f\"Finished: {output_path}\")\n",
    "\n",
    "adjust_returns_with_rf_grouped(vw_file, \"portfolio_daily_series_VW_with_rf.csv\")\n",
    "adjust_returns_with_rf_grouped(ew_file, \"portfolio_daily_series_EW_with_rf.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All figures have been generated and saved to: Baseline_Portfolio/\n"
     ]
    }
   ],
   "source": [
    "# ======== Download S&P500 (2016-2024) ========\n",
    "sp500 = yf.download(\"^GSPC\", start=\"2016-01-01\", end=\"2024-12-31\")\n",
    "price_col = \"Adj Close\" if \"Adj Close\" in sp500.columns else \"Close\"\n",
    "sp500[\"daily_return\"] = sp500[price_col].pct_change().fillna(0)\n",
    "# Cumulative log return (as in the paper)\n",
    "sp500[\"cum_return\"] = np.cumsum(np.log1p(sp500[\"daily_return\"]))\n",
    "sp500 = sp500[[\"cum_return\"]]\n",
    "sp500.index = pd.to_datetime(sp500.index)\n",
    "\n",
    "# ======== Configuration ========\n",
    "files = [\n",
    "    (\"VW\", \"portfolio_daily_series_VW_with_rf.csv\"),\n",
    "    (\"EW\", \"portfolio_daily_series_EW_with_rf.csv\")\n",
    "]\n",
    "tc_levels = [0, 5, 10, 20, 40]      # Transaction cost (bps)\n",
    "windows = [5, 21, 252, 512]         # Window sizes\n",
    "strategies = [\"long_only\", \"short_only\", \"long_short\"]\n",
    "\n",
    "output_dir = \"Baseline_Portfolio\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Major economic event periods (for shading)\n",
    "crisis_periods = [\n",
    "    (datetime(2018, 6, 1), datetime(2019, 1, 1), \"US-China Trade War\"),\n",
    "    (datetime(2020, 2, 1), datetime(2020, 7, 1), \"COVID-19\"),\n",
    "    (datetime(2022, 2, 1), datetime(2022, 6, 1), \"Russia-Ukraine War\"),\n",
    "    (datetime(2023, 1, 1), datetime(2023, 4, 1), \"US Bank Crisis\"),\n",
    "]\n",
    "\n",
    "def plot_comparison_styled(df, scheme, tc, window):\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    model_names = df[\"model\"].unique()\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(model_names)))\n",
    "\n",
    "    offset_step = 0.02\n",
    "\n",
    "    for i, strat in enumerate(strategies, 1):\n",
    "        ax = plt.subplot(3, 1, i)\n",
    "\n",
    "        plt.plot(sp500.index, sp500[\"cum_return\"],\n",
    "                 color=\"black\", lw=2.5, label=\"S&P500 (Total Return)\", zorder=10)\n",
    "\n",
    "        for idx, model_name in enumerate(model_names):\n",
    "            sub = df[(df[\"window\"] == window) &\n",
    "                     (df[\"portfolio_type\"] == strat) &\n",
    "                     (df[\"model\"] == model_name)].sort_values(\"date\")\n",
    "            if sub.empty:\n",
    "                continue\n",
    "\n",
    "            if tc == 0:\n",
    "                ret_col = \"return\"          # Raw excess return\n",
    "            else:\n",
    "                ret_col = f\"tc{tc}_return\"  # Return with transaction cost\n",
    "\n",
    "            if ret_col not in sub.columns:\n",
    "                continue\n",
    "\n",
    "            log_cum = np.cumsum(np.log1p(sub[ret_col].values))\n",
    "\n",
    "            y_shift = idx * offset_step\n",
    "            plt.plot(sub[\"date\"], log_cum + y_shift,\n",
    "                     label=f\"{model_name} ({strat.replace('_',' ').title()})\",\n",
    "                     lw=2, color=colors[idx], alpha=0.9)\n",
    "\n",
    "        for start, end, label in crisis_periods:\n",
    "            ax.axvspan(start, end, color='grey', alpha=0.3)\n",
    "            ax.text(start + pd.Timedelta(days=10),\n",
    "                    ax.get_ylim()[1]*0.92, label, fontsize=8, color='grey')\n",
    "        ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "        ax.set_ylabel(\"Cumulative log return (start = 0)\")\n",
    "        ax.set_title(f\"{scheme} | Window={window} | Strategy={strat} | TC={tc} bps\")\n",
    "        ax.grid(alpha=0.3)\n",
    "        plt.xticks(rotation=30)\n",
    "        plt.legend(bbox_to_anchor=(1.04, 1), loc='upper left', fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fname = f\"{scheme}_window{window}_TC{tc}_logreturn_offset.png\"\n",
    "    plt.savefig(os.path.join(output_dir, fname), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ======== Main loop to generate all figures ========\n",
    "for scheme, file_path in files:\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    for tc in tc_levels:\n",
    "        for window in windows:\n",
    "            plot_comparison_styled(df, scheme, tc, window)\n",
    "\n",
    "print(f\"All figures have been generated and saved to: {output_dir}/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Update] ΔSharpe has been written to portfolio_results_daily_rebalance_VW.csv\n",
      "[Update] ΔSharpe has been written to portfolio_results_daily_rebalance_EW.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load R²_zero from portfolio_metrics.csv\n",
    "metrics_df = pd.read_csv(\"portfolio_metrics.csv\")[[\"Model\", \"Window\", \"R²_zero\"]]\n",
    "metrics_df.rename(columns={\"Model\": \"model\", \"Window\": \"window\"}, inplace=True)\n",
    "\n",
    "# Process VW/EW files\n",
    "for fname in [\"portfolio_results_daily_rebalance_VW.csv\", \"portfolio_results_daily_rebalance_EW.csv\"]:\n",
    "    df = pd.read_csv(fname)\n",
    "\n",
    "    # Merge R²_zero by model and window\n",
    "    df = df.merge(metrics_df, on=[\"model\", \"window\"], how=\"left\")\n",
    "\n",
    "    rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        r2 = float(row[\"R²_zero\"]) if not pd.isna(row[\"R²_zero\"]) else 0.0\n",
    "        if row[\"portfolio_type\"] == \"long_only\":\n",
    "            d_sr, sr_star = delta_sharpe(r2, SR_MKT_EX)\n",
    "            row[\"ΔSharpe\"]  = d_sr\n",
    "            row[\"Sharpe*\"]  = sr_star\n",
    "            row[\"baseline\"] = f\"SPX_excess ({SR_MKT_EX:.2f})\"\n",
    "        else:\n",
    "            d_sr, sr_star = delta_sharpe(r2, 0)\n",
    "            row[\"ΔSharpe\"]  = d_sr\n",
    "            row[\"Sharpe*\"]  = sr_star\n",
    "            row[\"baseline\"] = \"cash (0)\"\n",
    "        rows.append(row)\n",
    "\n",
    "    pd.DataFrame(rows).to_csv(fname, index=False)\n",
    "    print(f\"[Update] ΔSharpe has been written to {fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "PRED_PATH = \"predictions_daily.csv\"\n",
    "METRICS_PATH = \"portfolio_metrics.csv\"\n",
    "TREAT_CONSTANT_DAY_AS_ZERO = False\n",
    "MIN_DAYS_FOR_STATS = 1\n",
    "\n",
    "def _day_ic(g):\n",
    "    if g[\"y_pred\"].nunique(dropna=True) <= 1 or g[\"y_true\"].nunique(dropna=True) <= 1:\n",
    "        return 0.0 if TREAT_CONSTANT_DAY_AS_ZERO else np.nan\n",
    "    return g[\"y_pred\"].corr(g[\"y_true\"], method=\"spearman\")\n",
    "\n",
    "def rankic_stats(df_group):\n",
    "    ics = (df_group.groupby(\"signal_date\", observed=True).apply(_day_ic).dropna())\n",
    "    n = int(ics.shape[0])\n",
    "    if n < MIN_DAYS_FOR_STATS:\n",
    "        return pd.Series({\"RankIC_mean\": np.nan, \"RankIC_t\": np.nan, \"RankIC_pos%\": np.nan, \"N_days\": n})\n",
    "    mean_ic = float(ics.mean())\n",
    "    std_ic  = float(ics.std(ddof=1))\n",
    "    t_ic    = mean_ic / (std_ic / np.sqrt(n)) if std_ic > 0 else np.nan\n",
    "    pos_pct = float((ics > 0).mean())\n",
    "    return pd.Series({\"RankIC_mean\": mean_ic, \"RankIC_t\": t_ic, \"RankIC_pos%\": pos_pct, \"N_days\": n})\n",
    "\n",
    "# Read data and calculate RankIC\n",
    "pred = pd.read_csv(PRED_PATH)\n",
    "pred[\"signal_date\"] = pd.to_datetime(pred[\"signal_date\"], errors=\"coerce\")\n",
    "pred = pred.dropna(subset=[\"signal_date\", \"y_true\", \"y_pred\", \"model\", \"window\"])\n",
    "pred[\"window\"] = pd.to_numeric(pred[\"window\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "rankic_df = (pred.groupby([\"model\", \"window\"], dropna=False)\n",
    "                .apply(rankic_stats)\n",
    "                .reset_index()\n",
    "                .rename(columns={\"model\":\"Model\",\"window\":\"Window\"}))\n",
    "\n",
    "# Merge: keep new RankIC columns, add _old suffix to original metrics columns\n",
    "metrics = pd.read_csv(METRICS_PATH)\n",
    "metrics[\"Window\"] = pd.to_numeric(metrics[\"Window\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "merged = metrics.merge(rankic_df, on=[\"Model\",\"Window\"], how=\"left\", suffixes=(\"_old\",\"\"))\n",
    "\n",
    "# Drop old columns with _old suffix\n",
    "to_drop = [c for c in merged.columns if c.endswith(\"_old\")]\n",
    "merged = merged.drop(columns=to_drop)\n",
    "\n",
    "# Save and overwrite\n",
    "merged.to_csv(METRICS_PATH, index=False)\n",
    "print(\"[OK] Overwrote portfolio_metrics.csv with new RankIC\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-mac)",
   "language": "python",
   "name": "tf-mac"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
