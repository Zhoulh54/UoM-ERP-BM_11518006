{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Basic Libraries ==========\n",
    "import os\n",
    "import gc\n",
    "import joblib\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ========== Visualization ==========\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# ========== Machine Learning ==========\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# ========== Financial/Statistical Tools ==========\n",
    "from pandas.tseries.offsets import BDay\n",
    "from scipy.stats import f as f_dist\n",
    "import statsmodels.api as sm\n",
    "import yfinance as yf\n",
    "\n",
    "# ========== Hyperparameter Optimization ==========\n",
    "import optuna\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] S&P500 Excess Sharpe (2016–24) = 0.652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ========== Core Function Definitions ==========\n",
    "\n",
    "def load_datasets(npz_path):\n",
    "    \"\"\"Load dataset from npz file\"\"\"\n",
    "    data = np.load(npz_path, allow_pickle=True) \n",
    "    datasets = {}\n",
    "    for key in data.files:\n",
    "        datasets[key] = data[key]\n",
    "    return datasets\n",
    "\n",
    "def find_coef_step(model):\n",
    "    \"\"\"\n",
    "    Get the coefficient step from a model, handling both Pipeline and single estimator.\n",
    "    \"\"\"\n",
    "    if hasattr(model, 'named_steps'):\n",
    "        for name, est in model.named_steps.items():\n",
    "            if hasattr(est, 'coef_'):\n",
    "                return name, est\n",
    "            if isinstance(est, Pipeline):\n",
    "                for subname, subest in est.named_steps.items():\n",
    "                    if hasattr(subest, 'coef_'):\n",
    "                        return f\"{name}__{subname}\", subest\n",
    "    else:\n",
    "        if hasattr(model, 'coef_'):\n",
    "            return 'model', model\n",
    "    \n",
    "    raise ValueError(\"No estimator with coef_ found in model\")\n",
    "\n",
    "\n",
    "def annual_sharpe(rets, freq=252):\n",
    "    mu = float(np.mean(rets)) * freq\n",
    "    sd = float(np.std(rets, ddof=1)) * np.sqrt(freq)\n",
    "    return mu / sd if sd > 0 else 0\n",
    "\n",
    "rf_file = \"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/CRSP_2016_2024_top50_with_exret.csv\"\n",
    "rf_df = pd.read_csv(rf_file, usecols=[\"date\", \"rf\"])\n",
    "rf_df[\"date\"] = pd.to_datetime(rf_df[\"date\"])\n",
    "rf_df = rf_df.drop_duplicates(\"date\").set_index(\"date\").sort_index()\n",
    "rf_series = rf_df[\"rf\"].astype(float)\n",
    "\n",
    "px = yf.download(\"^GSPC\", start=\"2016-01-01\", end=\"2024-12-31\")[\"Close\"]\n",
    "sp_ret = px.pct_change().dropna()\n",
    "rf_align = rf_series.reindex(sp_ret.index).fillna(method=\"ffill\")\n",
    "sp_excess = sp_ret.values - rf_align.values\n",
    "\n",
    "SR_MKT_EX = annual_sharpe(sp_excess)\n",
    "print(f\"[INFO] S&P500 Excess Sharpe (2016–24) = {SR_MKT_EX:.3f}\")\n",
    "\n",
    "def delta_sharpe(r2_zero, sr_base):\n",
    "    sr_star = np.sqrt(sr_base**2 + r2_zero) / np.sqrt(1 - r2_zero)\n",
    "    return sr_star - sr_base, sr_star\n",
    "\n",
    "def r2_zero(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate zero-based R² (baseline is zero).\n",
    "    y_true: true values (N,)\n",
    "    y_pred: predicted values (N,)\n",
    "    \"\"\"\n",
    "    rss = np.sum((y_true - y_pred)**2)  \n",
    "    tss = np.sum(y_true**2)            \n",
    "    return 1 - rss / tss\n",
    "\n",
    "def calc_ic_daily(df, method='spearman'):\n",
    "    \"\"\"\n",
    "    Calculate daily cross-sectional RankIC.\n",
    "    df: must contain ['signal_date','y_true','y_pred']\n",
    "    \"\"\"\n",
    "    ics = (df.groupby('signal_date')\n",
    "             .apply(lambda g: g['y_pred'].corr(g['y_true'], method=method))\n",
    "             .dropna())\n",
    "    mean_ic = ics.mean()\n",
    "    std_ic  = ics.std(ddof=1)\n",
    "    t_ic    = mean_ic / (std_ic / np.sqrt(len(ics))) if std_ic > 0 else np.nan\n",
    "    pos_ratio = (ics > 0).mean()\n",
    "    return mean_ic, t_ic, pos_ratio, ics\n",
    "\n",
    "def calc_directional_metrics(y_true, y_pred, permnos=None):\n",
    "    \"\"\"\n",
    "    Improved version:\n",
    "    - Sample-level sign prediction\n",
    "    - If grouped by stock, calculate Overall, Up, Down for each stock and then average\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    if permnos is None:\n",
    "        s_true = np.sign(y_true)\n",
    "        s_pred = np.sign(y_pred)\n",
    "        mask = s_true != 0\n",
    "        s_true = s_true[mask]\n",
    "        s_pred = s_pred[mask]\n",
    "\n",
    "        overall_acc = np.mean(s_true == s_pred)\n",
    "\n",
    "        up_mask = s_true > 0\n",
    "        down_mask = s_true < 0\n",
    "        up_acc = np.mean(s_true[up_mask] == s_pred[up_mask]) if np.any(up_mask) else 0\n",
    "        down_acc = np.mean(s_true[down_mask] == s_pred[down_mask]) if np.any(down_mask) else 0\n",
    "\n",
    "    else:\n",
    "        df = pd.DataFrame({\"permno\": permnos, \"yt\": y_true, \"yp\": y_pred})\n",
    "        overall_accs = []\n",
    "        up_accs = []\n",
    "        down_accs = []\n",
    "\n",
    "        for _, g in df.groupby(\"permno\"):\n",
    "            s_true = np.sign(g[\"yt\"].values)\n",
    "            s_pred = np.sign(g[\"yp\"].values)\n",
    "            mask = s_true != 0\n",
    "            s_true = s_true[mask]\n",
    "            s_pred = s_pred[mask]\n",
    "            if len(s_true) == 0:\n",
    "                continue\n",
    "            overall_accs.append(np.mean(s_true == s_pred))\n",
    "\n",
    "            up_mask = s_true > 0\n",
    "            down_mask = s_true < 0\n",
    "            up_accs.append(np.mean(s_true[up_mask] == s_pred[up_mask]) if np.any(up_mask) else np.nan)\n",
    "            down_accs.append(np.mean(s_true[down_mask] == s_pred[down_mask]) if np.any(down_mask) else np.nan)\n",
    "\n",
    "        overall_acc = np.nanmean(overall_accs)\n",
    "        up_acc = np.nanmean(up_accs)\n",
    "        down_acc = np.nanmean(down_accs)\n",
    "\n",
    "    return overall_acc, up_acc, down_acc\n",
    "\n",
    "def regression_metrics(y_true, y_pred, k, meta=None, permnos=None):\n",
    "    \"\"\"\n",
    "    Combined regression metrics:\n",
    "    - Regression metrics\n",
    "    - Pointwise directional accuracy\n",
    "    - Market cap group metrics\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    n = len(y_true)\n",
    "\n",
    "    r2 = r2_zero(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "\n",
    "    dir_acc, up_acc, down_acc = calc_directional_metrics(y_true, y_pred, permnos)\n",
    "\n",
    "    metrics = {\n",
    "        \"R²_zero\": r2,\n",
    "        \"RMSE\": rmse,\n",
    "        \"MAE\": mae,\n",
    "        \"MSE\": mse,\n",
    "        \"Directional Accuracy\": dir_acc,\n",
    "        \"Up_Directional_Acc\": up_acc,\n",
    "        \"Down_Directional_Acc\": down_acc\n",
    "    }\n",
    "\n",
    "    if meta is not None and \"MKTCAP_PERCENTILE\" in meta:\n",
    "        top_mask = meta[\"MKTCAP_PERCENTILE\"] >= 0.75\n",
    "        bottom_mask = meta[\"MKTCAP_PERCENTILE\"] <= 0.25\n",
    "\n",
    "        if np.any(top_mask):\n",
    "            yt_top = y_true[top_mask]\n",
    "            yp_top = y_pred[top_mask]\n",
    "            perm_top = permnos[top_mask] if permnos is not None else None\n",
    "            r2_top = r2_zero(yt_top, yp_top)\n",
    "            rmse_top = np.sqrt(mean_squared_error(yt_top, yp_top))\n",
    "            mae_top = mean_absolute_error(yt_top, yp_top)\n",
    "            mse_top = mean_squared_error(yt_top, yp_top)\n",
    "            dir_top, up_top, down_top = calc_directional_metrics(yt_top, yp_top, perm_top)\n",
    "            metrics.update({\n",
    "                \"Top25_R2_zero\": r2_top,\n",
    "                \"Top25_RMSE\": rmse_top,\n",
    "                \"Top25_MAE\": mae_top,\n",
    "                \"Top25_MSE\": mse_top,\n",
    "                \"Top25_Dir_Acc\": dir_top,\n",
    "                \"Top25_Up_Acc\": up_top,\n",
    "                \"Top25_Down_Acc\": down_top\n",
    "            })\n",
    "\n",
    "        if np.any(bottom_mask):\n",
    "            yt_bot = y_true[bottom_mask]\n",
    "            yp_bot = y_pred[bottom_mask]\n",
    "            perm_bot = permnos[bottom_mask] if permnos is not None else None\n",
    "            r2_bot = r2_zero(yt_bot, yp_bot)\n",
    "            rmse_bot = np.sqrt(mean_squared_error(yt_bot, yp_bot))\n",
    "            mae_bot = mean_absolute_error(yt_bot, yp_bot)\n",
    "            mse_bot = mean_squared_error(yt_bot, yp_bot)\n",
    "            dir_bot, up_bot, down_bot = calc_directional_metrics(yt_bot, yp_bot, perm_bot)\n",
    "            metrics.update({\n",
    "                \"Bottom25_R2_zero\": r2_bot,\n",
    "                \"Bottom25_RMSE\": rmse_bot,\n",
    "                \"Bottom25_MAE\": mae_bot,\n",
    "                \"Bottom25_MSE\": mse_bot,\n",
    "                \"Bottom25_Dir_Acc\": dir_bot,\n",
    "                \"Bottom25_Up_Acc\": up_bot,\n",
    "                \"Bottom25_Down_Acc\": down_bot\n",
    "            })\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def f_statistic(y_true, y_pred, k):\n",
    "    \"\"\"Return F statistic and corresponding p-value\"\"\"\n",
    "    n   = len(y_true)\n",
    "    rss = np.sum((y_true - y_pred) ** 2)\n",
    "    tss = np.sum(y_true ** 2)\n",
    "    r2  = 1 - rss / tss\n",
    "    if (r2 <= 0) or (n <= k):\n",
    "        return 0.0, 1.0\n",
    "    F = (r2 / k) / ((1 - r2) / (n - k))\n",
    "    p = f_dist.sf(F, k, n - k)\n",
    "    return F, p\n",
    "\n",
    "def overall_interval_metrics_method1(y_all, yhat_all, k, permnos_all=None, meta_all=None):\n",
    "    \"\"\"\n",
    "    Method 1: Calculate metrics for the entire interval (2016-2024, all samples concatenated).\n",
    "    Returns: a dict, can be directly passed to save_metrics()\n",
    "    \"\"\"\n",
    "    base = regression_metrics(\n",
    "        y_true=y_all, \n",
    "        y_pred=yhat_all, \n",
    "        k=k, \n",
    "        meta=meta_all, \n",
    "        permnos=permnos_all\n",
    "    )\n",
    "    F, p = f_statistic(y_all, yhat_all, k)\n",
    "    base[\"F_stat\"]     = F\n",
    "    base[\"F_pvalue\"]   = p\n",
    "    base[\"N_obs\"] = len(y_all)\n",
    "    \n",
    "    delta_cash, sr_star_cash = delta_sharpe(base[\"R²_zero\"], sr_base=0)\n",
    "    base[\"ΔSharpe_cash\"]      = delta_cash\n",
    "    base[\"Sharpe*_cash\"]      = sr_star_cash\n",
    "\n",
    "    delta_mkt , sr_star_mkt  = delta_sharpe(base[\"R²_zero\"], sr_base=SR_MKT_EX)\n",
    "    base[\"ΔSharpe_mkt\"]       = delta_mkt\n",
    "    base[\"Sharpe*_mkt\"]       = sr_star_mkt\n",
    "    \n",
    "    return base\n",
    "\n",
    "def sortino_ratio(rets, freq=252):\n",
    "    \"\"\"Calculate Sortino Ratio\"\"\"\n",
    "    downside = rets[rets < 0]\n",
    "    if len(downside) == 0:\n",
    "        return np.inf\n",
    "    mu = rets.mean() * freq\n",
    "    sigma = np.sqrt((downside ** 2).mean()) * np.sqrt(freq)\n",
    "    return mu / sigma\n",
    "\n",
    "def cvar(rets, alpha=0.95):\n",
    "    \"\"\"Calculate CVaR\"\"\"\n",
    "    q = np.quantile(rets, 1 - alpha)\n",
    "    return rets[rets <= q].mean()\n",
    "\n",
    "def save_predictions(model_name, window_size, y_true, y_pred, permnos, path=\"predictions/\"):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        \"PERMNO\": permnos,\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred\n",
    "    })\n",
    "\n",
    "    filename = f\"{model_name}_w{window_size}.csv\"\n",
    "    df.to_csv(os.path.join(path, filename), index=False)\n",
    "    print(f\"[Save] {filename}\")\n",
    "\n",
    "def save_metrics(metrics_dict, name, window, path=\"results.csv\"):\n",
    "    \"\"\"Save evaluation metrics\"\"\"\n",
    "    row = pd.DataFrame([metrics_dict])\n",
    "    row.insert(0, \"Model\", name)\n",
    "    row.insert(1, \"Window\", window)\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path)\n",
    "        df = df[~((df[\"Model\"] == name) & (df[\"Window\"] == window))]\n",
    "        df = pd.concat([df, row], ignore_index=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        print(f\"[Update] Metrics updated for {name} w={window}\")\n",
    "    else:\n",
    "        row.to_csv(path, index=False)\n",
    "        print(f\"[Create] New metrics file created with {name} w={window}\")\n",
    "\n",
    "def get_quarter_periods(start_year=2015, end_year=2024):\n",
    "    \"\"\"Generate list of quarters\"\"\"\n",
    "    quarters = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for q in range(1, 5):\n",
    "            quarters.append((year, q))\n",
    "    return quarters\n",
    "\n",
    "def save_model_with_quarter(model, name, window, year, quarter, path=\"models/\"):\n",
    "    \"\"\"Save model with quarter info\"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    filename = f\"{name}_w{window}_{year}Q{quarter}.joblib\"\n",
    "    joblib.dump(model, os.path.join(path, filename))\n",
    "    print(f\"Model saved: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNED_MODELS = {\"RF\", \"XGB\"}\n",
    "\n",
    "def tune_model_with_optuna(model_name, X, y, permnos=None, n_trials=50):\n",
    "    \"\"\"\n",
    "    Use Optuna to tune hyperparameters for RF / XGB.\n",
    "    ----------\n",
    "    Returns:\n",
    "        best_model  -> if tuning is successful\n",
    "        None        -> if skipped or failed\n",
    "    \"\"\"\n",
    "    if model_name not in TUNED_MODELS:\n",
    "        print(f\"Skip {model_name} - not tunable\")\n",
    "        return None\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "    def objective(trial):\n",
    "        try:\n",
    "            if model_name == \"RF\":\n",
    "                params = {\n",
    "                    \"n_estimators\"     : trial.suggest_int (\"n_estimators\"    , 100, 300),\n",
    "                    \"max_depth\"        : trial.suggest_int (\"max_depth\"       ,   4, 10),\n",
    "                    \"min_samples_split\": trial.suggest_int (\"min_samples_split\",  2,  8),\n",
    "                    \"min_samples_leaf\" : trial.suggest_int (\"min_samples_leaf\" ,  1,  4),\n",
    "                    \"max_features\"     : trial.suggest_categorical(\"max_features\", [\"sqrt\",\"log2\"]),\n",
    "                }\n",
    "                base_model = RandomForestRegressor(**params, random_state=42, n_jobs=-1)\n",
    "\n",
    "            else:\n",
    "                params = {\n",
    "                    \"n_estimators\"    : trial.suggest_int  (\"n_estimators\"   , 100, 300),\n",
    "                    \"max_depth\"       : trial.suggest_int  (\"max_depth\"      ,   3,   8),\n",
    "                    \"learning_rate\"   : trial.suggest_float(\"learning_rate\"  , 0.01, 0.2),\n",
    "                    \"subsample\"       : trial.suggest_float(\"subsample\"      , 0.7 , 1.0),\n",
    "                    \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\",0.7 , 1.0),\n",
    "                    \"min_child_weight\": trial.suggest_int  (\"min_child_weight\", 1 ,  6),\n",
    "                    \"gamma\"           : trial.suggest_float(\"gamma\"          , 0   , 2.0),\n",
    "                    \"reg_alpha\"       : trial.suggest_float(\"reg_alpha\"      , 1e-4, 0.1, log=True),\n",
    "                    \"reg_lambda\"      : trial.suggest_float(\"reg_lambda\"     , 1e-4, 0.1, log=True),\n",
    "                    \"tree_method\"     : \"hist\",\n",
    "                }\n",
    "                base_model = XGBRegressor(**params, random_state=42, n_jobs=-1, verbosity=0)\n",
    "\n",
    "            scores = []\n",
    "            for tr_idx, val_idx in tscv.split(X):\n",
    "                X_tr, X_val = X[tr_idx], X[val_idx]\n",
    "                y_tr, y_val = y[tr_idx], y[val_idx]\n",
    "\n",
    "                model = deepcopy(base_model)\n",
    "                model.fit(X_tr, y_tr)\n",
    "                preds = model.predict(X_val)\n",
    "                scores.append(mean_squared_error(y_val, preds))\n",
    "\n",
    "            return np.mean(scores)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Optuna] trial failed: {e}\")\n",
    "            return float(\"inf\")\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        sampler=optuna.samplers.TPESampler(seed=42),\n",
    "        pruner =optuna.pruners.MedianPruner(),\n",
    "    )\n",
    "    study.optimize(objective, n_trials=n_trials, n_jobs=-1)\n",
    "\n",
    "    if study.best_trial is None:\n",
    "        print(f\"Optuna failed for {model_name}.\")\n",
    "        return None\n",
    "\n",
    "    best_params = study.best_params\n",
    "    print(f\"[Optuna] {model_name} best MSE = {study.best_value:.6f}\")\n",
    "    print(f\"[Optuna] best params = {best_params}\")\n",
    "\n",
    "    if model_name == \"RF\":\n",
    "        return RandomForestRegressor(**best_params, random_state=42, n_jobs=-1)\n",
    "\n",
    "    return XGBRegressor(**best_params, random_state=42, n_jobs=-1, verbosity=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tree_models_expanding_quarterly(\n",
    "    start_year: int = 2015,\n",
    "    end_year: int = 2024,\n",
    "    window_sizes: list[int] | None = None,\n",
    "    model_names: list[str] | None = None,\n",
    "    npz_path: str = \"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/all_window_datasets.npz\",\n",
    "):\n",
    "    \"\"\"Quarterly expanding window training for RF & XGB (no target z‑score).\"\"\"\n",
    "\n",
    "    if window_sizes is None:\n",
    "        window_sizes = [5, 21, 252, 512]\n",
    "    if model_names is None:\n",
    "        model_names = [\"RF\", \"XGB\"]\n",
    "\n",
    "    print(f\"Starting Tree-Model Quarterly Expanding Window Training ({start_year}-{end_year})\")\n",
    "\n",
    "    datasets = load_datasets(npz_path)\n",
    "    anchor_quarters = {(2015, 4), (2020, 4)}\n",
    "    best_templates: dict[str, object] = {}\n",
    "\n",
    "    for window in window_sizes:\n",
    "        print(f\"\\n Window = {window}\")\n",
    "        X_train_init = datasets[f\"X_train_{window}\"]\n",
    "        y_train_init = datasets[f\"y_train_{window}\"]\n",
    "        X_test_full  = datasets[f\"X_test_{window}\"]\n",
    "        y_test_full  = datasets[f\"y_test_{window}\"]\n",
    "\n",
    "        meta_test = pd.DataFrame.from_dict(datasets[f\"meta_test_{window}\"].item())\n",
    "        meta_test[\"ret_date\"] = pd.to_datetime(meta_test[\"ret_date\"])\n",
    "\n",
    "        X_expanding = deepcopy(X_train_init)\n",
    "        y_expanding = deepcopy(y_train_init)\n",
    "\n",
    "        for yr, qt in [\n",
    "            q for q in get_quarter_periods(start_year, end_year)\n",
    "            if not (q[0] == start_year and q[1] < 4)\n",
    "            and not (q[0] == end_year   and q[1] > 3)\n",
    "        ]:\n",
    "            print(f\"  {yr}-Q{qt}: train\")\n",
    "\n",
    "            for m in model_names:\n",
    "                cache_key = f\"{m}_w{window}\"\n",
    "                if (yr, qt) in anchor_quarters and m in TUNED_MODELS:\n",
    "                    print(f\"      [Optuna] tuning {m} for {yr}-Q{qt}\")\n",
    "                    tuned = tune_model_with_optuna(m, X_expanding, y_expanding, n_trials=25)\n",
    "                    best_templates[cache_key] = tuned if tuned is not None else get_tree_model(m)\n",
    "                elif cache_key not in best_templates:\n",
    "                    best_templates[cache_key] = get_tree_model(m)\n",
    "\n",
    "            if not (yr == start_year and qt == 4):\n",
    "                prev_y, prev_q = ((yr-1, 4) if qt == 1 else (yr, qt-1))\n",
    "                mask_prev = (\n",
    "                    (meta_test[\"ret_date\"].dt.year == prev_y) &\n",
    "                    (meta_test[\"ret_date\"].dt.quarter == prev_q)\n",
    "                )\n",
    "                if mask_prev.any():\n",
    "                    X_expanding = np.vstack([X_expanding, X_test_full[mask_prev]])\n",
    "                    y_expanding = np.hstack([y_expanding, y_test_full[mask_prev]])\n",
    "                    print(f\"      +{mask_prev.sum()} obs from {prev_y}-Q{prev_q}\")\n",
    "\n",
    "            for m in model_names:\n",
    "                cache_key = f\"{m}_w{window}\"\n",
    "                model = clone(best_templates[cache_key])\n",
    "\n",
    "                if m == \"XGB\":\n",
    "                    idx = int(len(X_expanding) * 0.8)\n",
    "                    model.fit(\n",
    "                        X_expanding[:idx], y_expanding[:idx],\n",
    "                        eval_set=[(X_expanding[idx:], y_expanding[idx:])],\n",
    "                        early_stopping_rounds=20,\n",
    "                        verbose=False,\n",
    "                    )\n",
    "                else:\n",
    "                    model.fit(X_expanding, y_expanding)\n",
    "\n",
    "                if hasattr(model, \"feature_importances_\"):\n",
    "                    top3 = np.round(np.sort(model.feature_importances_)[-3:][::-1], 4)\n",
    "                    print(f\"        {m}: top-3 FI = {top3}\")\n",
    "\n",
    "                save_model_with_quarter(model, m, window, yr, qt)\n",
    "                del model\n",
    "                gc.collect()\n",
    "\n",
    "            print(f\"      dataset size = {len(X_expanding):,}\")\n",
    "\n",
    "    print(\"Tree-Model Quarterly Expanding Training done\")\n",
    "    return best_templates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Portfolio Core Class =====\n",
    "# ===== Transaction Cost Settings =====\n",
    "TC_GRID = [0.0005, 0.001, 0.002, 0.003, 0.004]  # 5, 10, 20, 30, 40 bps\n",
    "TC_TAG  = {\n",
    "    0.0005: \"tc5\",\n",
    "    0.001:  \"tc10\", \n",
    "    0.002:  \"tc20\",\n",
    "    0.003:  \"tc30\",\n",
    "    0.004:  \"tc40\"\n",
    "}\n",
    "\n",
    "class PortfolioBacktester:\n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "        \n",
    "    def calc_turnover(self, w_t, r_t, w_tp1):\n",
    "        \"\"\"Calculate turnover using the standard formula provided by the user\"\"\"\n",
    "        if w_t is None:\n",
    "            return np.sum(np.abs(w_tp1))\n",
    "        \n",
    "        gross_ret = np.sum(w_t * r_t)\n",
    "        if abs(1 + gross_ret) < 1e-8:\n",
    "            return np.sum(np.abs(w_tp1))\n",
    "        \n",
    "        passive_weight = w_t * (1 + r_t) / (1 + gross_ret)\n",
    "        turnover = np.sum(np.abs(w_tp1 - passive_weight))\n",
    "        return turnover\n",
    "    \n",
    "    def create_portfolios_with_permno_tracking(self, signals, market_caps, permnos, top_pct=0.1, bottom_pct=0.1, weight_scheme=\"VW\"):\n",
    "        \"\"\"\n",
    "        Create portfolio weights based on signals, strictly tracking permno alignment.\n",
    "        weight_scheme: 'VW' for value-weighted, 'EW' for equal-weighted.\n",
    "        \"\"\"\n",
    "        n_stocks = len(signals)\n",
    "        top_n    = max(1, int(round(n_stocks * top_pct)))\n",
    "        bottom_n = max(1, int(round(n_stocks * bottom_pct)))\n",
    "        \n",
    "        sorted_idx = np.argsort(signals)[::-1]\n",
    "        \n",
    "        top_idx = sorted_idx[:top_n]\n",
    "        bottom_idx = sorted_idx[-bottom_n:]\n",
    "        \n",
    "        portfolio_data = {}\n",
    "        \n",
    "        long_weights = np.zeros(n_stocks)\n",
    "        if len(top_idx) > 0:\n",
    "            if weight_scheme == \"VW\":\n",
    "                top_market_caps = market_caps[top_idx]\n",
    "                if np.sum(top_market_caps) > 0:\n",
    "                    long_weights[top_idx] = top_market_caps / np.sum(top_market_caps)\n",
    "            else:\n",
    "                long_weights[top_idx] = 1.0 / len(top_idx)\n",
    "        \n",
    "        portfolio_data['long_only'] = {\n",
    "            'weights': long_weights,\n",
    "            'permnos': permnos.copy(),\n",
    "            'selected_permnos': permnos[top_idx] if len(top_idx) > 0 else np.array([])\n",
    "        }\n",
    "        \n",
    "        short_weights = np.zeros(n_stocks)\n",
    "        if len(bottom_idx) > 0:\n",
    "            if weight_scheme == \"VW\":\n",
    "                bottom_market_caps = market_caps[bottom_idx]\n",
    "                if np.sum(bottom_market_caps) > 0:\n",
    "                    short_weights[bottom_idx] = -bottom_market_caps / np.sum(bottom_market_caps)\n",
    "            else:\n",
    "                short_weights[bottom_idx] = -1.0 / len(bottom_idx)\n",
    "        \n",
    "        portfolio_data['short_only'] = {\n",
    "            'weights': short_weights,\n",
    "            'permnos': permnos.copy(),\n",
    "            'selected_permnos': permnos[bottom_idx] if len(bottom_idx) > 0 else np.array([])\n",
    "        }\n",
    "        \n",
    "        # Long-Short portfolio (Top long + Bottom short)\n",
    "        ls_raw = long_weights + short_weights\n",
    "\n",
    "        gross_target = 2.0\n",
    "        current_gross = np.sum(np.abs(long_weights)) + np.sum(np.abs(short_weights))\n",
    "        scale = gross_target / current_gross if current_gross > 1e-8 else 0.0\n",
    "        ls_weights = scale * ls_raw\n",
    "\n",
    "        ls_selected_permnos = np.concatenate([\n",
    "            permnos[top_idx] if len(top_idx) > 0 else np.array([]),\n",
    "            permnos[bottom_idx] if len(bottom_idx) > 0 else np.array([])\n",
    "        ])\n",
    "\n",
    "        portfolio_data['long_short'] = {\n",
    "            'weights': ls_weights,\n",
    "            'permnos': permnos.copy(),\n",
    "            'selected_permnos': ls_selected_permnos\n",
    "        }\n",
    "\n",
    "        return portfolio_data\n",
    "    \n",
    "    def calculate_aligned_portfolio_return(self, portfolio_weights, portfolio_permnos, actual_returns, actual_permnos):\n",
    "        \"\"\"Calculate portfolio return strictly aligned by permno\"\"\"\n",
    "        aligned_returns = np.zeros(len(portfolio_permnos))\n",
    "        \n",
    "        return_dict = dict(zip(actual_permnos, actual_returns))\n",
    "        \n",
    "        for i, permno in enumerate(portfolio_permnos):\n",
    "            if permno in return_dict:\n",
    "                aligned_returns[i] = return_dict[permno]\n",
    "        \n",
    "        portfolio_return = np.sum(portfolio_weights * aligned_returns)\n",
    "        return portfolio_return, aligned_returns\n",
    "\n",
    "    def calculate_metrics(self, returns, turnover_series=None):\n",
    "        \"\"\"Calculate portfolio metrics - only returns summary metrics, not long series\"\"\"\n",
    "        returns = np.array(returns)\n",
    "        \n",
    "        annual_return = np.mean(returns) * 252\n",
    "        annual_vol = np.std(returns, ddof=1) * np.sqrt(252)\n",
    "        sharpe = annual_return / annual_vol if annual_vol > 0 else 0\n",
    "        \n",
    "        log_cum = np.cumsum(np.log1p(returns))\n",
    "        peak_log = np.maximum.accumulate(log_cum)\n",
    "        dd_log = peak_log - log_cum\n",
    "        max_drawdown = 1 - np.exp(-dd_log.max()) \n",
    "        max_1d_loss = np.min(returns) \n",
    "        \n",
    "        avg_turnover = np.mean(turnover_series) if turnover_series is not None else 0\n",
    "        \n",
    "        sortino = sortino_ratio(returns)\n",
    "        cvar95  = cvar(returns, alpha=0.95)\n",
    "\n",
    "        result = {\n",
    "            'annual_return': annual_return,\n",
    "            'annual_vol': annual_vol,\n",
    "            'sharpe': sharpe,\n",
    "            'max_drawdown': max_drawdown,\n",
    "            'max_1d_loss': max_1d_loss,\n",
    "            'avg_turnover': avg_turnover,\n",
    "            'sortino': sortino,\n",
    "            'cvar95': cvar95\n",
    "        }\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Main function for daily prediction and next-day rebalancing portfolio simulation ==========\n",
    "\n",
    "def run_portfolio_simulation_daily_rebalance(start_year=2016, end_year=2024, window_sizes=None, model_names=None,\n",
    "                                           npz_path=\"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/all_window_datasets.npz\"):\n",
    "    \"\"\"\n",
    "Portfolio simulation (daily prediction, next-day rebalancing):\n",
    "    1. Load quarterly models (trained with quarterly expanding window)\n",
    "    2. Daily prediction to daily signals\n",
    "    3. Daily portfolio construction (T+1 rebalancing, strict permno alignment)\n",
    "    4. Separate summary metrics and time series data\n",
    "    \"\"\"\n",
    "    if window_sizes is None:\n",
    "        window_sizes = [5, 21, 252, 512]\n",
    "    if model_names is None:\n",
    "        model_names = [\"RF\", \"XGB\"]\n",
    "    \n",
    "    print(\"Starting Daily Rebalance Portfolio Backtesting Simulation\")\n",
    "    \n",
    "    backtester = PortfolioBacktester()\n",
    "    datasets = load_datasets(npz_path)\n",
    "    \n",
    "    summary_results = []\n",
    "    daily_series_data = []\n",
    "    pred_rows = []\n",
    "    \n",
    "    WEIGHT_SCHEMES = [\"VW\", \"EW\"]\n",
    "    \n",
    "    for window in window_sizes:\n",
    "        print(f\"Processing window size: {window}\")\n",
    "        \n",
    "        X_test = datasets[f\"X_test_{window}\"]\n",
    "        y_test = datasets[f\"y_test_{window}\"]\n",
    "        meta_test_dict = datasets[f\"meta_test_{window}\"].item()\n",
    "        meta_test = pd.DataFrame.from_dict(meta_test_dict)\n",
    "        \n",
    "        permnos_test = meta_test[\"PERMNO\"].values\n",
    "        meta_test[\"signal_date\"]  = pd.to_datetime(meta_test[\"date\"])\n",
    "        meta_test[\"ret_date\"]     = pd.to_datetime(meta_test[\"ret_date\"])\n",
    "        market_caps = meta_test.get(\"MKTCAP\", np.ones(len(permnos_test)))\n",
    "        \n",
    "        meta_test['date'] = pd.to_datetime(meta_test[\"date\"])\n",
    "        dates_test = meta_test['signal_date']\n",
    "        \n",
    "        for model_name in model_names:\n",
    "            for scheme in WEIGHT_SCHEMES:\n",
    "                all_y_true   = []\n",
    "                all_y_pred   = []\n",
    "                all_permnos  = []\n",
    "                all_meta     = []\n",
    "                print(f\"  Model: {model_name}, Scheme: {scheme}\")\n",
    "                \n",
    "                portfolio_daily_data = {\n",
    "                    'long_only': {'returns': [], 'turnovers': [], 'dates': []},\n",
    "                    'short_only': {'returns': [], 'turnovers': [], 'dates': []},\n",
    "                    'long_short': {'returns': [], 'turnovers': [], 'dates': []}\n",
    "                }\n",
    "                \n",
    "                prev_portfolio_data = {'long_only': None, 'short_only': None, 'long_short': None}\n",
    "                \n",
    "                signals_buf = {}\n",
    "                \n",
    "                for year in range(start_year, min(end_year + 1, 2025)):\n",
    "                    for quarter in range(1, 5):\n",
    "                        # Determine model file year and quarter (T+1 logic: use previous quarter's model to predict current quarter)\n",
    "                        if quarter == 1:\n",
    "                            model_file_year, model_file_quarter = year - 1, 4\n",
    "                        else:\n",
    "                            model_file_year, model_file_quarter = year, quarter - 1\n",
    "                            \n",
    "                        model_path = f\"models/{model_name}_w{window}_{model_file_year}Q{model_file_quarter}.joblib\"\n",
    "                        \n",
    "                        if not os.path.exists(model_path):\n",
    "                            print(f\"      Skip: Model file not found {model_path}\")\n",
    "                            continue\n",
    "                        \n",
    "                        model = joblib.load(model_path)\n",
    "                        \n",
    "                        quarter_mask = (\n",
    "                            (dates_test.dt.year == year) & \n",
    "                            (dates_test.dt.quarter == quarter)\n",
    "                        )\n",
    "                        if not np.any(quarter_mask):\n",
    "                            continue\n",
    "                        \n",
    "                        X_quarter = X_test[quarter_mask]\n",
    "                        y_quarter = y_test[quarter_mask]\n",
    "                        permnos_quarter = permnos_test[quarter_mask]\n",
    "                        market_caps_quarter = market_caps[quarter_mask]\n",
    "                        dates_quarter = dates_test[quarter_mask]\n",
    "                        ret_dates_quarter = meta_test.loc[quarter_mask, 'ret_date'].values\n",
    "                        \n",
    "                        df_quarter = pd.DataFrame({\n",
    "                            'signal_date': dates_quarter,\n",
    "                            'ret_date': ret_dates_quarter,\n",
    "                            'permno': permnos_quarter,\n",
    "                            'market_cap': market_caps_quarter,\n",
    "                            'actual_return': y_quarter,                 \n",
    "                            'prediction': model.predict(X_quarter)   \n",
    "                        })\n",
    "                        \n",
    "                        if scheme == 'VW':\n",
    "                            df_q_save = df_quarter[['signal_date','ret_date','permno',\n",
    "                                                    'actual_return','prediction','market_cap']].copy()\n",
    "                            df_q_save.rename(columns={'actual_return':'y_true',\n",
    "                                                      'prediction':'y_pred'}, inplace=True)\n",
    "                            df_q_save['model']  = model_name\n",
    "                            df_q_save['window'] = window\n",
    "                            pred_rows.append(df_q_save)\n",
    "                        \n",
    "                        all_y_true.append(df_quarter['actual_return'].values)\n",
    "                        all_y_pred.append(df_quarter['prediction'].values)\n",
    "                        all_permnos.append(df_quarter['permno'].values)\n",
    "                        all_meta.append(meta_test.loc[quarter_mask, :])   \n",
    "\n",
    "                        for signal_date, sig_grp in df_quarter.groupby('signal_date'):\n",
    "                            # (1) Calculate today's signals and store in buffer, do not rebalance yet\n",
    "                            daily_signals = (\n",
    "                                sig_grp.groupby('permno')['prediction'].mean()\n",
    "                                      .to_frame('prediction')\n",
    "                                      .join(sig_grp.groupby('permno')['market_cap'].mean())\n",
    "                            )\n",
    "                            signals_buf[signal_date] = daily_signals\n",
    "\n",
    "                            # (2) Only use yesterday's signals to rebalance today\n",
    "                            prev_date = signal_date - pd.tseries.offsets.BDay(1)\n",
    "                            if prev_date not in signals_buf:\n",
    "                                continue\n",
    "\n",
    "                            sigs = signals_buf.pop(prev_date)\n",
    "\n",
    "                            # (3) Use today's realized returns for settlement (ret_date == signal_date)\n",
    "                            ret_grp = df_quarter[df_quarter['ret_date'] == signal_date]\n",
    "                            if len(ret_grp) == 0:\n",
    "                                continue\n",
    "\n",
    "                            daily_actual_returns = (\n",
    "                                ret_grp.groupby('permno')['actual_return']\n",
    "                                       .mean()\n",
    "                                       .reindex(sigs.index, fill_value=0)\n",
    "                                       .values\n",
    "                            )\n",
    "                            daily_permnos = sigs.index.values\n",
    "\n",
    "                            # (4) Generate 3 sets of weights\n",
    "                            portfolios_data = backtester.create_portfolios_with_permno_tracking(\n",
    "                                signals      = sigs['prediction'].values,\n",
    "                                market_caps  = sigs['market_cap'].values,\n",
    "                                permnos      = daily_permnos,\n",
    "                                weight_scheme= scheme\n",
    "                            )\n",
    "                            \n",
    "                            for portfolio_type in ['long_only', 'short_only', 'long_short']:\n",
    "                                portfolio_info = portfolios_data[portfolio_type]\n",
    "                                \n",
    "                                portfolio_return, aligned_returns = backtester.calculate_aligned_portfolio_return(\n",
    "                                    portfolio_weights=portfolio_info['weights'],\n",
    "                                    portfolio_permnos=portfolio_info['permnos'],\n",
    "                                    actual_returns=daily_actual_returns,\n",
    "                                    actual_permnos=daily_permnos\n",
    "                                )\n",
    "                                \n",
    "                                if prev_portfolio_data[portfolio_type] is not None:\n",
    "                                    prev_w_ser = pd.Series(\n",
    "                                        prev_portfolio_data[portfolio_type]['weights'],\n",
    "                                        index=prev_portfolio_data[portfolio_type]['permnos']\n",
    "                                    )\n",
    "                                    cur_w_ser = pd.Series(\n",
    "                                        portfolio_info['weights'],\n",
    "                                        index=portfolio_info['permnos']\n",
    "                                    )\n",
    "\n",
    "                                    prev_r_ser = pd.Series(\n",
    "                                        prev_portfolio_data[portfolio_type]['aligned_returns'],\n",
    "                                        index=prev_portfolio_data[portfolio_type]['permnos']\n",
    "                                    )\n",
    "\n",
    "                                    aligned_prev_w = prev_w_ser.reindex(cur_w_ser.index, fill_value=0).values\n",
    "                                    aligned_prev_r = prev_r_ser.reindex(cur_w_ser.index, fill_value=0).values\n",
    "\n",
    "                                    aligned_cur_w = cur_w_ser.values\n",
    "\n",
    "                                    turnover = backtester.calc_turnover(\n",
    "                                        w_t  = aligned_prev_w,\n",
    "                                        r_t  = aligned_prev_r,\n",
    "                                        w_tp1= aligned_cur_w\n",
    "                                    )\n",
    "                                else:\n",
    "                                    turnover = np.sum(np.abs(portfolio_info['weights']))\n",
    "                                \n",
    "                                portfolio_daily_data[portfolio_type]['returns'].append(portfolio_return)\n",
    "                                portfolio_daily_data[portfolio_type]['turnovers'].append(turnover)\n",
    "                                portfolio_daily_data[portfolio_type]['dates'].append(signal_date)\n",
    "                                \n",
    "                                prev_portfolio_data[portfolio_type] = {\n",
    "                                    'weights'        : portfolio_info['weights'],\n",
    "                                    'permnos'        : portfolio_info['permnos'],\n",
    "                                    'aligned_returns': aligned_returns      \n",
    "                                }\n",
    "                \n",
    "                for portfolio_type in ['long_only', 'short_only', 'long_short']:\n",
    "                    portfolio_data = portfolio_daily_data[portfolio_type]\n",
    "                    \n",
    "                    if len(portfolio_data['returns']) > 0:\n",
    "                        metrics = backtester.calculate_metrics(\n",
    "                            returns=portfolio_data['returns'],\n",
    "                            turnover_series=portfolio_data['turnovers']\n",
    "                        )\n",
    "                        \n",
    "                        rets = np.array(portfolio_data['returns'])\n",
    "                        tovs = np.array(portfolio_data['turnovers'])\n",
    "\n",
    "                        for tc in TC_GRID:\n",
    "                            tag = TC_TAG[tc]\n",
    "                            adj = rets - tovs * tc\n",
    "\n",
    "                            ann_ret = adj.mean() * 252\n",
    "                            ann_vol = adj.std(ddof=1) * np.sqrt(252)\n",
    "                            sharpe  = ann_ret / ann_vol if ann_vol > 0 else 0\n",
    "\n",
    "                            cum_adj = np.cumprod(1 + adj)\n",
    "                            mdd = ((cum_adj - np.maximum.accumulate(cum_adj)) /\n",
    "                                   np.maximum.accumulate(cum_adj)).min()\n",
    "\n",
    "                            metrics[f'{tag}_annual_return'] = ann_ret\n",
    "                            metrics[f'{tag}_annual_vol']    = ann_vol\n",
    "                            metrics[f'{tag}_sharpe']        = sharpe\n",
    "                            metrics[f'{tag}_max_drawdown']  = mdd\n",
    "                        \n",
    "                        summary_results.append({\n",
    "                            'scheme': scheme,\n",
    "                            'model': model_name,\n",
    "                            'window': window,\n",
    "                            'portfolio_type': portfolio_type,\n",
    "                            **metrics\n",
    "                        })\n",
    "                        \n",
    "                        rets_arr = np.array(portfolio_data['returns'])\n",
    "                        tovs_arr = np.array(portfolio_data['turnovers'])\n",
    "                        cum_no_tc = np.log1p(rets_arr).cumsum()\n",
    "\n",
    "                        tc_ret_dict = {}\n",
    "                        tc_cum_dict = {}\n",
    "                        for tc in TC_GRID:\n",
    "                            tag = TC_TAG[tc]\n",
    "                            r = rets_arr - tovs_arr * tc\n",
    "                            tc_ret_dict[tag] = r\n",
    "                            tc_cum_dict[tag] = np.log1p(r).cumsum()\n",
    "\n",
    "                        for i, date in enumerate(portfolio_data['dates']):\n",
    "                            row = {\n",
    "                                'scheme'        : scheme,\n",
    "                                'model'         : model_name,\n",
    "                                'window'        : window,\n",
    "                                'portfolio_type': portfolio_type,\n",
    "                                'date'          : str(date),\n",
    "                                'return'        : rets_arr[i],\n",
    "                                'turnover'      : tovs_arr[i],\n",
    "                                'cumulative'    : cum_no_tc[i],\n",
    "                            }\n",
    "                            for tag in TC_TAG.values():\n",
    "                                row[f'{tag}_return']     = tc_ret_dict[tag][i]\n",
    "                                row[f'{tag}_cumulative'] = tc_cum_dict[tag][i]\n",
    "\n",
    "                            daily_series_data.append(row)\n",
    "\n",
    "                if scheme == \"VW\" and len(all_y_true) > 0:\n",
    "                    y_all    = np.concatenate(all_y_true)\n",
    "                    yhat_all = np.concatenate(all_y_pred)\n",
    "                    perm_all = np.concatenate(all_permnos)\n",
    "                    meta_all = pd.concat(all_meta, ignore_index=True)\n",
    "\n",
    "                    k = X_test.shape[1]\n",
    "\n",
    "                    m1_metrics = overall_interval_metrics_method1(\n",
    "                        y_all, yhat_all, k,\n",
    "                        permnos_all=perm_all,\n",
    "                        meta_all=meta_all\n",
    "                    )\n",
    "\n",
    "                    full_pred_df = pd.concat(pred_rows, ignore_index=True)\n",
    "                    mean_ic, t_ic, pos_ic, _ = calc_ic_daily(full_pred_df, method='spearman')\n",
    "                    m1_metrics['RankIC_mean']  = mean_ic\n",
    "                    m1_metrics['RankIC_t']     = t_ic\n",
    "                    m1_metrics['RankIC_pos%']  = pos_ic\n",
    "\n",
    "                    save_metrics(m1_metrics, name=model_name, window=window,\n",
    "                        path=\"portfolio_metrics.csv\")\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_results)\n",
    "    daily_df = pd.DataFrame(daily_series_data) if daily_series_data else pd.DataFrame()\n",
    "    \n",
    "    tc_columns = [c for c in summary_df.columns if c.startswith('tc')]\n",
    "    summary_df[tc_columns] = summary_df[tc_columns].fillna(0.0)\n",
    "    \n",
    "    def save_split_by_scheme(df, base_filename):\n",
    "        \"\"\"Helper function to save files split by scheme\"\"\"\n",
    "        if df.empty:\n",
    "            print(f\"Warning: DataFrame is empty, skipping save for {base_filename}\")\n",
    "            return None, None\n",
    "            \n",
    "        vw_df = df[df['scheme'] == 'VW']\n",
    "        ew_df = df[df['scheme'] == 'EW']\n",
    "        \n",
    "        vw_filename = f\"{base_filename}_VW.csv\"\n",
    "        ew_filename = f\"{base_filename}_EW.csv\"\n",
    "        \n",
    "        vw_df.to_csv(vw_filename, index=False)\n",
    "        ew_df.to_csv(ew_filename, index=False)\n",
    "        \n",
    "        print(f\"VW results saved to {vw_filename}\")\n",
    "        print(f\"EW results saved to {ew_filename}\")\n",
    "        \n",
    "        return vw_filename, ew_filename\n",
    "    \n",
    "    save_split_by_scheme(summary_df, \"portfolio_results_daily_rebalance\")\n",
    "    \n",
    "    if not daily_df.empty:\n",
    "        save_split_by_scheme(daily_df, \"portfolio_daily_series\")\n",
    "    \n",
    "    if pred_rows:\n",
    "        pred_df = pd.concat(pred_rows, ignore_index=True)\n",
    "        pred_df.to_csv(\"predictions_daily.csv\", index=False)\n",
    "        print(f\"Saved {len(pred_df)} prediction rows to predictions_daily.csv\")\n",
    "    \n",
    "    print(f\"Generated {len(summary_results)} portfolio summary records\")\n",
    "    print(f\"Generated {len(daily_series_data)} daily series records\")\n",
    "    \n",
    "    return summary_df, daily_df, backtester\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Tree‑Model Quarterly Expanding Window Training (2015‑2024)\n",
      "\n",
      " Window = 5\n",
      "  ▸ 2015‑Q4: train\n",
      "      [Optuna] tuning RF for 2015‑Q4\n",
      "[Optuna] RF best MSE = 0.000282\n",
      "[Optuna] best params        = {'n_estimators': 210, 'max_depth': 5, 'min_samples_split': 6, 'min_samples_leaf': 3, 'max_features': 'sqrt'}\n",
      "      [Optuna] tuning XGB for 2015‑Q4\n",
      "[Optuna] XGB best MSE = 0.000282\n",
      "[Optuna] best params        = {'n_estimators': 251, 'max_depth': 4, 'learning_rate': 0.10997486256194458, 'subsample': 0.725500847406671, 'colsample_bytree': 0.8021779123463336, 'min_child_weight': 1, 'gamma': 0.022258486304219227, 'reg_alpha': 0.039039934108524146, 'reg_lambda': 0.0006803346319462583}\n",
      "        RF: top‑3 FI = [0.2312 0.2133 0.2025]\n",
      "Model saved: RF_w5_2015Q4.joblib\n",
      "        XGB: top‑3 FI = [0.2099 0.2041 0.2016]\n",
      "Model saved: XGB_w5_2015Q4.joblib\n",
      "      dataset size = 196,920\n",
      "  ▸ 2016‑Q1: train\n",
      "        RF: top‑3 FI = [0.2312 0.2133 0.2025]\n",
      "Model saved: RF_w5_2016Q1.joblib\n",
      "        XGB: top‑3 FI = [0.2099 0.2041 0.2016]\n",
      "Model saved: XGB_w5_2016Q1.joblib\n",
      "      dataset size = 196,920\n",
      "  ▸ 2016‑Q2: train\n",
      "      +2956 obs from 2016‑Q1\n",
      "        RF: top‑3 FI = [0.2287 0.2108 0.2068]\n",
      "Model saved: RF_w5_2016Q2.joblib\n",
      "        XGB: top‑3 FI = [0.244  0.2011 0.1898]\n",
      "Model saved: XGB_w5_2016Q2.joblib\n",
      "      dataset size = 199,876\n",
      "  ▸ 2016‑Q3: train\n",
      "      +3170 obs from 2016‑Q2\n",
      "        RF: top‑3 FI = [0.2287 0.2102 0.2081]\n",
      "Model saved: RF_w5_2016Q3.joblib\n",
      "        XGB: top‑3 FI = [0.2137 0.2042 0.1979]\n",
      "Model saved: XGB_w5_2016Q3.joblib\n",
      "      dataset size = 203,046\n",
      "  ▸ 2016‑Q4: train\n",
      "      +3176 obs from 2016‑Q3\n",
      "        RF: top‑3 FI = [0.2263 0.2118 0.2107]\n",
      "Model saved: RF_w5_2016Q4.joblib\n",
      "        XGB: top‑3 FI = [0.2209 0.2143 0.1969]\n",
      "Model saved: XGB_w5_2016Q4.joblib\n",
      "      dataset size = 206,222\n",
      "  ▸ 2017‑Q1: train\n",
      "      +3123 obs from 2016‑Q4\n",
      "        RF: top‑3 FI = [0.2264 0.2125 0.2072]\n",
      "Model saved: RF_w5_2017Q1.joblib\n",
      "        XGB: top‑3 FI = [0.2401 0.1981 0.196 ]\n",
      "Model saved: XGB_w5_2017Q1.joblib\n",
      "      dataset size = 209,345\n",
      "  ▸ 2017‑Q2: train\n",
      "      +3083 obs from 2017‑Q1\n",
      "        RF: top‑3 FI = [0.2293 0.2136 0.206 ]\n",
      "Model saved: RF_w5_2017Q2.joblib\n",
      "        XGB: top‑3 FI = [0.223  0.2021 0.1934]\n",
      "Model saved: XGB_w5_2017Q2.joblib\n",
      "      dataset size = 212,428\n",
      "  ▸ 2017‑Q3: train\n",
      "      +3122 obs from 2017‑Q2\n",
      "        RF: top‑3 FI = [0.2286 0.2116 0.2065]\n",
      "Model saved: RF_w5_2017Q3.joblib\n",
      "        XGB: top‑3 FI = [0.2073 0.2035 0.2001]\n",
      "Model saved: XGB_w5_2017Q3.joblib\n",
      "      dataset size = 215,550\n",
      "  ▸ 2017‑Q4: train\n",
      "      +3114 obs from 2017‑Q3\n",
      "        RF: top‑3 FI = [0.2289 0.2094 0.2055]\n",
      "Model saved: RF_w5_2017Q4.joblib\n",
      "        XGB: top‑3 FI = [0.2438 0.1926 0.1914]\n",
      "Model saved: XGB_w5_2017Q4.joblib\n",
      "      dataset size = 218,664\n",
      "  ▸ 2018‑Q1: train\n",
      "      +3115 obs from 2017‑Q4\n",
      "        RF: top‑3 FI = [0.2303 0.2109 0.2053]\n",
      "Model saved: RF_w5_2018Q1.joblib\n",
      "        XGB: top‑3 FI = [0.2261 0.2129 0.199 ]\n",
      "Model saved: XGB_w5_2018Q1.joblib\n",
      "      dataset size = 221,779\n",
      "  ▸ 2018‑Q2: train\n",
      "      +2996 obs from 2018‑Q1\n",
      "        RF: top‑3 FI = [0.2317 0.2094 0.2034]\n",
      "Model saved: RF_w5_2018Q2.joblib\n",
      "        XGB: top‑3 FI = [0.2275 0.2208 0.1975]\n",
      "Model saved: XGB_w5_2018Q2.joblib\n",
      "      dataset size = 224,775\n",
      "  ▸ 2018‑Q3: train\n",
      "      +3160 obs from 2018‑Q2\n",
      "        RF: top‑3 FI = [0.2382 0.2055 0.2038]\n",
      "Model saved: RF_w5_2018Q3.joblib\n",
      "        XGB: top‑3 FI = [0.2501 0.2005 0.1919]\n",
      "Model saved: XGB_w5_2018Q3.joblib\n",
      "      dataset size = 227,935\n",
      "  ▸ 2018‑Q4: train\n",
      "      +3125 obs from 2018‑Q3\n",
      "        RF: top‑3 FI = [0.2327 0.2079 0.2053]\n",
      "Model saved: RF_w5_2018Q4.joblib\n",
      "        XGB: top‑3 FI = [0.2554 0.1973 0.1934]\n",
      "Model saved: XGB_w5_2018Q4.joblib\n",
      "      dataset size = 231,060\n",
      "  ▸ 2019‑Q1: train\n",
      "      +3045 obs from 2018‑Q4\n",
      "        RF: top‑3 FI = [0.2283 0.2122 0.2041]\n",
      "Model saved: RF_w5_2019Q1.joblib\n",
      "        XGB: top‑3 FI = [0.2092 0.2041 0.1978]\n",
      "Model saved: XGB_w5_2019Q1.joblib\n",
      "      dataset size = 234,105\n",
      "  ▸ 2019‑Q2: train\n",
      "      +3022 obs from 2019‑Q1\n",
      "        RF: top‑3 FI = [0.2281 0.2105 0.2044]\n",
      "Model saved: RF_w5_2019Q2.joblib\n",
      "        XGB: top‑3 FI = [0.2228 0.204  0.1996]\n",
      "Model saved: XGB_w5_2019Q2.joblib\n",
      "      dataset size = 237,127\n",
      "  ▸ 2019‑Q3: train\n",
      "      +3120 obs from 2019‑Q2\n",
      "        RF: top‑3 FI = [0.2278 0.21   0.2074]\n",
      "Model saved: RF_w5_2019Q3.joblib\n",
      "        XGB: top‑3 FI = [0.2152 0.2093 0.209 ]\n",
      "Model saved: XGB_w5_2019Q3.joblib\n",
      "      dataset size = 240,247\n",
      "  ▸ 2019‑Q4: train\n",
      "      +3167 obs from 2019‑Q3\n",
      "        RF: top‑3 FI = [0.229  0.2074 0.204 ]\n",
      "Model saved: RF_w5_2019Q4.joblib\n",
      "        XGB: top‑3 FI = [0.222  0.203  0.1949]\n",
      "Model saved: XGB_w5_2019Q4.joblib\n",
      "      dataset size = 243,414\n",
      "  ▸ 2020‑Q1: train\n",
      "      +3179 obs from 2019‑Q4\n",
      "        RF: top‑3 FI = [0.233  0.2075 0.2027]\n",
      "Model saved: RF_w5_2020Q1.joblib\n",
      "        XGB: top‑3 FI = [0.2368 0.2037 0.2018]\n",
      "Model saved: XGB_w5_2020Q1.joblib\n",
      "      dataset size = 246,593\n",
      "  ▸ 2020‑Q2: train\n",
      "      +2595 obs from 2020‑Q1\n",
      "        RF: top‑3 FI = [0.2222 0.2106 0.2066]\n",
      "Model saved: RF_w5_2020Q2.joblib\n",
      "        XGB: top‑3 FI = [0.2142 0.2025 0.2002]\n",
      "Model saved: XGB_w5_2020Q2.joblib\n",
      "      dataset size = 249,188\n",
      "  ▸ 2020‑Q3: train\n",
      "      +2839 obs from 2020‑Q2\n",
      "        RF: top‑3 FI = [0.2234 0.2121 0.2031]\n",
      "Model saved: RF_w5_2020Q3.joblib\n",
      "        XGB: top‑3 FI = [0.2188 0.2108 0.208 ]\n",
      "Model saved: XGB_w5_2020Q3.joblib\n",
      "      dataset size = 252,027\n",
      "  ▸ 2020‑Q4: train\n",
      "      [Optuna] tuning RF for 2020‑Q4\n",
      "[Optuna] RF best MSE = 0.000269\n",
      "[Optuna] best params        = {'n_estimators': 257, 'max_depth': 5, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "      [Optuna] tuning XGB for 2020‑Q4\n",
      "[Optuna] XGB best MSE = 0.000269\n",
      "[Optuna] best params        = {'n_estimators': 253, 'max_depth': 8, 'learning_rate': 0.030664409621621085, 'subsample': 0.8645779773830219, 'colsample_bytree': 0.9374208602490834, 'min_child_weight': 5, 'gamma': 1.0599363956758652, 'reg_alpha': 0.048107987190867285, 'reg_lambda': 0.008292823095372234}\n",
      "      +3151 obs from 2020‑Q3\n",
      "        RF: top‑3 FI = [0.2235 0.2025 0.2024]\n",
      "Model saved: RF_w5_2020Q4.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w5_2020Q4.joblib\n",
      "      dataset size = 255,178\n",
      "  ▸ 2021‑Q1: train\n",
      "      +3114 obs from 2020‑Q4\n",
      "        RF: top‑3 FI = [0.2254 0.2052 0.1961]\n",
      "Model saved: RF_w5_2021Q1.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w5_2021Q1.joblib\n",
      "      dataset size = 258,292\n",
      "  ▸ 2021‑Q2: train\n",
      "      +2989 obs from 2021‑Q1\n",
      "        RF: top‑3 FI = [0.2261 0.2069 0.2016]\n",
      "Model saved: RF_w5_2021Q2.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w5_2021Q2.joblib\n",
      "      dataset size = 261,281\n",
      "  ▸ 2021‑Q3: train\n",
      "      +3132 obs from 2021‑Q2\n",
      "        RF: top‑3 FI = [0.2234 0.2099 0.1971]\n",
      "Model saved: RF_w5_2021Q3.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w5_2021Q3.joblib\n",
      "      dataset size = 264,413\n",
      "  ▸ 2021‑Q4: train\n",
      "      +3173 obs from 2021‑Q3\n",
      "        RF: top‑3 FI = [0.2237 0.2111 0.1971]\n",
      "Model saved: RF_w5_2021Q4.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w5_2021Q4.joblib\n",
      "      dataset size = 267,586\n",
      "  ▸ 2022‑Q1: train\n",
      "      +3153 obs from 2021‑Q4\n",
      "        RF: top‑3 FI = [0.2244 0.21   0.1948]\n",
      "Model saved: RF_w5_2022Q1.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w5_2022Q1.joblib\n",
      "      dataset size = 270,739\n",
      "  ▸ 2022‑Q2: train\n",
      "      +3029 obs from 2022‑Q1\n",
      "        RF: top‑3 FI = [0.2314 0.2085 0.1939]\n",
      "Model saved: RF_w5_2022Q2.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w5_2022Q2.joblib\n",
      "      dataset size = 273,768\n",
      "  ▸ 2022‑Q3: train\n",
      "      +2969 obs from 2022‑Q2\n",
      "        RF: top‑3 FI = [0.2332 0.21   0.1923]\n",
      "Model saved: RF_w5_2022Q3.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w5_2022Q3.joblib\n",
      "      dataset size = 276,737\n",
      "  ▸ 2022‑Q4: train\n",
      "      +3152 obs from 2022‑Q3\n",
      "        RF: top‑3 FI = [0.2304 0.2137 0.1901]\n",
      "Model saved: RF_w5_2022Q4.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w5_2022Q4.joblib\n",
      "      dataset size = 279,889\n",
      "  ▸ 2023‑Q1: train\n",
      "      +3070 obs from 2022‑Q4\n",
      "        RF: top‑3 FI = [0.2316 0.212  0.1922]\n",
      "Model saved: RF_w5_2023Q1.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w5_2023Q1.joblib\n",
      "      dataset size = 282,959\n",
      "  ▸ 2023‑Q2: train\n",
      "      +3064 obs from 2023‑Q1\n",
      "        RF: top‑3 FI = [0.2315 0.2117 0.1906]\n",
      "Model saved: RF_w5_2023Q2.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w5_2023Q2.joblib\n",
      "      dataset size = 286,023\n",
      "  ▸ 2023‑Q3: train\n",
      "      +3069 obs from 2023‑Q2\n",
      "        RF: top‑3 FI = [0.2319 0.214  0.192 ]\n",
      "Model saved: RF_w5_2023Q3.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w5_2023Q3.joblib\n",
      "      dataset size = 289,092\n",
      "  ▸ 2023‑Q4: train\n",
      "      +3121 obs from 2023‑Q3\n",
      "        RF: top‑3 FI = [0.2321 0.2105 0.1972]\n",
      "Model saved: RF_w5_2023Q4.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w5_2023Q4.joblib\n",
      "      dataset size = 292,213\n",
      "  ▸ 2024‑Q1: train\n",
      "      +3113 obs from 2023‑Q4\n",
      "        RF: top‑3 FI = [0.2323 0.2135 0.1942]\n",
      "Model saved: RF_w5_2024Q1.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w5_2024Q1.joblib\n",
      "      dataset size = 295,326\n",
      "  ▸ 2024‑Q2: train\n",
      "      +3026 obs from 2024‑Q1\n",
      "        RF: top‑3 FI = [0.23   0.2081 0.1995]\n",
      "Model saved: RF_w5_2024Q2.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w5_2024Q2.joblib\n",
      "      dataset size = 298,352\n",
      "  ▸ 2024‑Q3: train\n",
      "      +3112 obs from 2024‑Q2\n",
      "        RF: top‑3 FI = [0.2318 0.2082 0.2008]\n",
      "Model saved: RF_w5_2024Q3.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w5_2024Q3.joblib\n",
      "      dataset size = 301,464\n",
      "\n",
      " Window = 21\n",
      "  ▸ 2015‑Q4: train\n",
      "      [Optuna] tuning RF for 2015‑Q4\n",
      "[Optuna] RF best MSE = 0.000282\n",
      "[Optuna] best params        = {'n_estimators': 223, 'max_depth': 10, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': 'log2'}\n",
      "      [Optuna] tuning XGB for 2015‑Q4\n",
      "[Optuna] XGB best MSE = 0.000282\n",
      "[Optuna] best params        = {'n_estimators': 113, 'max_depth': 8, 'learning_rate': 0.07947448212969156, 'subsample': 0.8533293223662309, 'colsample_bytree': 0.8678451702005304, 'min_child_weight': 3, 'gamma': 1.4817197240841231, 'reg_alpha': 0.059571414517194894, 'reg_lambda': 0.03529893273168915}\n",
      "        RF: top‑3 FI = [0.069  0.0604 0.0571]\n",
      "Model saved: RF_w21_2015Q4.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w21_2015Q4.joblib\n",
      "      dataset size = 196,120\n",
      "  ▸ 2016‑Q1: train\n",
      "        RF: top‑3 FI = [0.069  0.0604 0.0571]\n",
      "Model saved: RF_w21_2016Q1.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w21_2016Q1.joblib\n",
      "      dataset size = 196,120\n",
      "  ▸ 2016‑Q2: train\n",
      "      +2956 obs from 2016‑Q1\n",
      "        RF: top‑3 FI = [0.0685 0.0596 0.0562]\n",
      "Model saved: RF_w21_2016Q2.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w21_2016Q2.joblib\n",
      "      dataset size = 199,076\n",
      "  ▸ 2016‑Q3: train\n",
      "      +3170 obs from 2016‑Q2\n",
      "        RF: top‑3 FI = [0.0683 0.0617 0.0578]\n",
      "Model saved: RF_w21_2016Q3.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w21_2016Q3.joblib\n",
      "      dataset size = 202,246\n",
      "  ▸ 2016‑Q4: train\n",
      "      +3176 obs from 2016‑Q3\n",
      "        RF: top‑3 FI = [0.0703 0.0619 0.0562]\n",
      "Model saved: RF_w21_2016Q4.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w21_2016Q4.joblib\n",
      "      dataset size = 205,422\n",
      "  ▸ 2017‑Q1: train\n",
      "      +3123 obs from 2016‑Q4\n",
      "        RF: top‑3 FI = [0.0686 0.0615 0.0569]\n",
      "Model saved: RF_w21_2017Q1.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w21_2017Q1.joblib\n",
      "      dataset size = 208,545\n",
      "  ▸ 2017‑Q2: train\n",
      "      +3083 obs from 2017‑Q1\n",
      "        RF: top‑3 FI = [0.069  0.061  0.0572]\n",
      "Model saved: RF_w21_2017Q2.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w21_2017Q2.joblib\n",
      "      dataset size = 211,628\n",
      "  ▸ 2017‑Q3: train\n",
      "      +3122 obs from 2017‑Q2\n",
      "        RF: top‑3 FI = [0.071  0.0611 0.0576]\n",
      "Model saved: RF_w21_2017Q3.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w21_2017Q3.joblib\n",
      "      dataset size = 214,750\n",
      "  ▸ 2017‑Q4: train\n",
      "      +3114 obs from 2017‑Q3\n",
      "        RF: top‑3 FI = [0.0698 0.0618 0.0574]\n",
      "Model saved: RF_w21_2017Q4.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w21_2017Q4.joblib\n",
      "      dataset size = 217,864\n",
      "  ▸ 2018‑Q1: train\n",
      "      +3115 obs from 2017‑Q4\n",
      "        RF: top‑3 FI = [0.0697 0.0624 0.0563]\n",
      "Model saved: RF_w21_2018Q1.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w21_2018Q1.joblib\n",
      "      dataset size = 220,979\n",
      "  ▸ 2018‑Q2: train\n",
      "      +2996 obs from 2018‑Q1\n",
      "        RF: top‑3 FI = [0.0726 0.0627 0.0578]\n",
      "Model saved: RF_w21_2018Q2.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w21_2018Q2.joblib\n",
      "      dataset size = 223,975\n",
      "  ▸ 2018‑Q3: train\n",
      "      +3160 obs from 2018‑Q2\n",
      "        RF: top‑3 FI = [0.0734 0.0604 0.0583]\n",
      "Model saved: RF_w21_2018Q3.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w21_2018Q3.joblib\n",
      "      dataset size = 227,135\n",
      "  ▸ 2018‑Q4: train\n",
      "      +3125 obs from 2018‑Q3\n",
      "        RF: top‑3 FI = [0.074  0.0606 0.0576]\n",
      "Model saved: RF_w21_2018Q4.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w21_2018Q4.joblib\n",
      "      dataset size = 230,260\n",
      "  ▸ 2019‑Q1: train\n",
      "      +3045 obs from 2018‑Q4\n",
      "        RF: top‑3 FI = [0.0714 0.0626 0.0569]\n",
      "Model saved: RF_w21_2019Q1.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w21_2019Q1.joblib\n",
      "      dataset size = 233,305\n",
      "  ▸ 2019‑Q2: train\n",
      "      +3022 obs from 2019‑Q1\n",
      "        RF: top‑3 FI = [0.0716 0.0626 0.0579]\n",
      "Model saved: RF_w21_2019Q2.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w21_2019Q2.joblib\n",
      "      dataset size = 236,327\n",
      "  ▸ 2019‑Q3: train\n",
      "      +3120 obs from 2019‑Q2\n",
      "        RF: top‑3 FI = [0.0703 0.0623 0.0577]\n",
      "Model saved: RF_w21_2019Q3.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w21_2019Q3.joblib\n",
      "      dataset size = 239,447\n",
      "  ▸ 2019‑Q4: train\n",
      "      +3167 obs from 2019‑Q3\n",
      "        RF: top‑3 FI = [0.0707 0.063  0.0589]\n",
      "Model saved: RF_w21_2019Q4.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w21_2019Q4.joblib\n",
      "      dataset size = 242,614\n",
      "  ▸ 2020‑Q1: train\n",
      "      +3179 obs from 2019‑Q4\n",
      "        RF: top‑3 FI = [0.0686 0.0607 0.0588]\n",
      "Model saved: RF_w21_2020Q1.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w21_2020Q1.joblib\n",
      "      dataset size = 245,793\n",
      "  ▸ 2020‑Q2: train\n",
      "      +2595 obs from 2020‑Q1\n",
      "        RF: top‑3 FI = [0.0675 0.0642 0.0582]\n",
      "Model saved: RF_w21_2020Q2.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w21_2020Q2.joblib\n",
      "      dataset size = 248,388\n",
      "  ▸ 2020‑Q3: train\n",
      "      +2839 obs from 2020‑Q2\n",
      "        RF: top‑3 FI = [0.0627 0.0616 0.0594]\n",
      "Model saved: RF_w21_2020Q3.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w21_2020Q3.joblib\n",
      "      dataset size = 251,227\n",
      "  ▸ 2020‑Q4: train\n",
      "      [Optuna] tuning RF for 2020‑Q4\n",
      "[Optuna] RF best MSE = 0.000270\n",
      "[Optuna] best params        = {'n_estimators': 294, 'max_depth': 4, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "      [Optuna] tuning XGB for 2020‑Q4\n",
      "[Optuna] XGB best MSE = 0.000270\n",
      "[Optuna] best params        = {'n_estimators': 184, 'max_depth': 3, 'learning_rate': 0.04470800704285301, 'subsample': 0.9764733798433542, 'colsample_bytree': 0.9575925787471935, 'min_child_weight': 4, 'gamma': 1.6313131905499283, 'reg_alpha': 0.000834273121421961, 'reg_lambda': 0.004298320290354616}\n",
      "      +3151 obs from 2020‑Q3\n",
      "        RF: top‑3 FI = [0.0856 0.0716 0.0687]\n",
      "Model saved: RF_w21_2020Q4.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w21_2020Q4.joblib\n",
      "      dataset size = 254,378\n",
      "  ▸ 2021‑Q1: train\n",
      "      +3114 obs from 2020‑Q4\n",
      "        RF: top‑3 FI = [0.0818 0.0749 0.0684]\n",
      "Model saved: RF_w21_2021Q1.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w21_2021Q1.joblib\n",
      "      dataset size = 257,492\n",
      "  ▸ 2021‑Q2: train\n",
      "      +2989 obs from 2021‑Q1\n",
      "        RF: top‑3 FI = [0.0809 0.0744 0.0691]\n",
      "Model saved: RF_w21_2021Q2.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w21_2021Q2.joblib\n",
      "      dataset size = 260,481\n",
      "  ▸ 2021‑Q3: train\n",
      "      +3132 obs from 2021‑Q2\n",
      "        RF: top‑3 FI = [0.0888 0.0809 0.0661]\n",
      "Model saved: RF_w21_2021Q3.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w21_2021Q3.joblib\n",
      "      dataset size = 263,613\n",
      "  ▸ 2021‑Q4: train\n",
      "      +3173 obs from 2021‑Q3\n",
      "        RF: top‑3 FI = [0.088  0.0813 0.0663]\n",
      "Model saved: RF_w21_2021Q4.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w21_2021Q4.joblib\n",
      "      dataset size = 266,786\n",
      "  ▸ 2022‑Q1: train\n",
      "      +3153 obs from 2021‑Q4\n",
      "        RF: top‑3 FI = [0.0854 0.08   0.0659]\n",
      "Model saved: RF_w21_2022Q1.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w21_2022Q1.joblib\n",
      "      dataset size = 269,939\n",
      "  ▸ 2022‑Q2: train\n",
      "      +3029 obs from 2022‑Q1\n",
      "        RF: top‑3 FI = [0.0899 0.0813 0.0652]\n",
      "Model saved: RF_w21_2022Q2.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w21_2022Q2.joblib\n",
      "      dataset size = 272,968\n",
      "  ▸ 2022‑Q3: train\n",
      "      +2969 obs from 2022‑Q2\n",
      "        RF: top‑3 FI = [0.0957 0.0767 0.0635]\n",
      "Model saved: RF_w21_2022Q3.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w21_2022Q3.joblib\n",
      "      dataset size = 275,937\n",
      "  ▸ 2022‑Q4: train\n",
      "      +3152 obs from 2022‑Q3\n",
      "        RF: top‑3 FI = [0.091  0.0774 0.068 ]\n",
      "Model saved: RF_w21_2022Q4.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w21_2022Q4.joblib\n",
      "      dataset size = 279,089\n",
      "  ▸ 2023‑Q1: train\n",
      "      +3070 obs from 2022‑Q4\n",
      "        RF: top‑3 FI = [0.0915 0.078  0.0674]\n",
      "Model saved: RF_w21_2023Q1.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w21_2023Q1.joblib\n",
      "      dataset size = 282,159\n",
      "  ▸ 2023‑Q2: train\n",
      "      +3064 obs from 2023‑Q1\n",
      "        RF: top‑3 FI = [0.0872 0.0737 0.0669]\n",
      "Model saved: RF_w21_2023Q2.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w21_2023Q2.joblib\n",
      "      dataset size = 285,223\n",
      "  ▸ 2023‑Q3: train\n",
      "      +3069 obs from 2023‑Q2\n",
      "        RF: top‑3 FI = [0.0908 0.0745 0.0668]\n",
      "Model saved: RF_w21_2023Q3.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w21_2023Q3.joblib\n",
      "      dataset size = 288,292\n",
      "  ▸ 2023‑Q4: train\n",
      "      +3121 obs from 2023‑Q3\n",
      "        RF: top‑3 FI = [0.0875 0.0745 0.0697]\n",
      "Model saved: RF_w21_2023Q4.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w21_2023Q4.joblib\n",
      "      dataset size = 291,413\n",
      "  ▸ 2024‑Q1: train\n",
      "      +3113 obs from 2023‑Q4\n",
      "        RF: top‑3 FI = [0.09   0.075  0.0672]\n",
      "Model saved: RF_w21_2024Q1.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w21_2024Q1.joblib\n",
      "      dataset size = 294,526\n",
      "  ▸ 2024‑Q2: train\n",
      "      +3026 obs from 2024‑Q1\n",
      "        RF: top‑3 FI = [0.0895 0.0761 0.0661]\n",
      "Model saved: RF_w21_2024Q2.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w21_2024Q2.joblib\n",
      "      dataset size = 297,552\n",
      "  ▸ 2024‑Q3: train\n",
      "      +3112 obs from 2024‑Q2\n",
      "        RF: top‑3 FI = [0.088  0.0754 0.0661]\n",
      "Model saved: RF_w21_2024Q3.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w21_2024Q3.joblib\n",
      "      dataset size = 300,664\n",
      "\n",
      " Window = 252\n",
      "  ▸ 2015‑Q4: train\n",
      "      [Optuna] tuning RF for 2015‑Q4\n",
      "[Optuna] RF best MSE = 0.000283\n",
      "[Optuna] best params        = {'n_estimators': 128, 'max_depth': 6, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "      [Optuna] tuning XGB for 2015‑Q4\n",
      "[Optuna] XGB best MSE = 0.000283\n",
      "[Optuna] best params        = {'n_estimators': 179, 'max_depth': 8, 'learning_rate': 0.046135774784495376, 'subsample': 0.7078978944999137, 'colsample_bytree': 0.722449732848288, 'min_child_weight': 2, 'gamma': 1.42581879832451, 'reg_alpha': 0.0035135612499128327, 'reg_lambda': 0.00010738663582919832}\n",
      "        RF: top‑3 FI = [0.0211 0.0129 0.0109]\n",
      "Model saved: RF_w252_2015Q4.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w252_2015Q4.joblib\n",
      "      dataset size = 184,570\n",
      "  ▸ 2016‑Q1: train\n",
      "        RF: top‑3 FI = [0.0211 0.0129 0.0109]\n",
      "Model saved: RF_w252_2016Q1.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w252_2016Q1.joblib\n",
      "      dataset size = 184,570\n",
      "  ▸ 2016‑Q2: train\n",
      "      +2956 obs from 2016‑Q1\n",
      "        RF: top‑3 FI = [0.0211 0.0129 0.0106]\n",
      "Model saved: RF_w252_2016Q2.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w252_2016Q2.joblib\n",
      "      dataset size = 187,526\n",
      "  ▸ 2016‑Q3: train\n",
      "      +3170 obs from 2016‑Q2\n",
      "        RF: top‑3 FI = [0.0219 0.013  0.0108]\n",
      "Model saved: RF_w252_2016Q3.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w252_2016Q3.joblib\n",
      "      dataset size = 190,696\n",
      "  ▸ 2016‑Q4: train\n",
      "      +3176 obs from 2016‑Q3\n",
      "        RF: top‑3 FI = [0.0236 0.0129 0.011 ]\n",
      "Model saved: RF_w252_2016Q4.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w252_2016Q4.joblib\n",
      "      dataset size = 193,872\n",
      "  ▸ 2017‑Q1: train\n",
      "      +3123 obs from 2016‑Q4\n",
      "        RF: top‑3 FI = [0.0211 0.0132 0.0117]\n",
      "Model saved: RF_w252_2017Q1.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w252_2017Q1.joblib\n",
      "      dataset size = 196,995\n",
      "  ▸ 2017‑Q2: train\n",
      "      +3083 obs from 2017‑Q1\n",
      "        RF: top‑3 FI = [0.0215 0.0131 0.0122]\n",
      "Model saved: RF_w252_2017Q2.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w252_2017Q2.joblib\n",
      "      dataset size = 200,078\n",
      "  ▸ 2017‑Q3: train\n",
      "      +3122 obs from 2017‑Q2\n",
      "        RF: top‑3 FI = [0.0214 0.0131 0.0129]\n",
      "Model saved: RF_w252_2017Q3.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w252_2017Q3.joblib\n",
      "      dataset size = 203,200\n",
      "  ▸ 2017‑Q4: train\n",
      "      +3114 obs from 2017‑Q3\n",
      "        RF: top‑3 FI = [0.022  0.0136 0.0127]\n",
      "Model saved: RF_w252_2017Q4.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w252_2017Q4.joblib\n",
      "      dataset size = 206,314\n",
      "  ▸ 2018‑Q1: train\n",
      "      +3115 obs from 2017‑Q4\n",
      "        RF: top‑3 FI = [0.0216 0.0136 0.0127]\n",
      "Model saved: RF_w252_2018Q1.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w252_2018Q1.joblib\n",
      "      dataset size = 209,429\n",
      "  ▸ 2018‑Q2: train\n",
      "      +2996 obs from 2018‑Q1\n",
      "        RF: top‑3 FI = [0.0216 0.0145 0.0114]\n",
      "Model saved: RF_w252_2018Q2.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w252_2018Q2.joblib\n",
      "      dataset size = 212,425\n",
      "  ▸ 2018‑Q3: train\n",
      "      +3160 obs from 2018‑Q2\n",
      "        RF: top‑3 FI = [0.0216 0.0143 0.0114]\n",
      "Model saved: RF_w252_2018Q3.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w252_2018Q3.joblib\n",
      "      dataset size = 215,585\n",
      "  ▸ 2018‑Q4: train\n",
      "      +3125 obs from 2018‑Q3\n",
      "        RF: top‑3 FI = [0.0217 0.0143 0.0115]\n",
      "Model saved: RF_w252_2018Q4.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w252_2018Q4.joblib\n",
      "      dataset size = 218,710\n",
      "  ▸ 2019‑Q1: train\n",
      "      +3045 obs from 2018‑Q4\n",
      "        RF: top‑3 FI = [0.0217 0.0143 0.0115]\n",
      "Model saved: RF_w252_2019Q1.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w252_2019Q1.joblib\n",
      "      dataset size = 221,755\n",
      "  ▸ 2019‑Q2: train\n",
      "      +3022 obs from 2019‑Q1\n",
      "        RF: top‑3 FI = [0.0203 0.0127 0.0112]\n",
      "Model saved: RF_w252_2019Q2.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w252_2019Q2.joblib\n",
      "      dataset size = 224,777\n",
      "  ▸ 2019‑Q3: train\n",
      "      +3120 obs from 2019‑Q2\n",
      "        RF: top‑3 FI = [0.0229 0.0119 0.0115]\n",
      "Model saved: RF_w252_2019Q3.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w252_2019Q3.joblib\n",
      "      dataset size = 227,897\n",
      "  ▸ 2019‑Q4: train\n",
      "      +3167 obs from 2019‑Q3\n",
      "        RF: top‑3 FI = [0.0206 0.0132 0.0111]\n",
      "Model saved: RF_w252_2019Q4.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w252_2019Q4.joblib\n",
      "      dataset size = 231,064\n",
      "  ▸ 2020‑Q1: train\n",
      "      +3179 obs from 2019‑Q4\n",
      "        RF: top‑3 FI = [0.0216 0.0114 0.0113]\n",
      "Model saved: RF_w252_2020Q1.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w252_2020Q1.joblib\n",
      "      dataset size = 234,243\n",
      "  ▸ 2020‑Q2: train\n",
      "      +2595 obs from 2020‑Q1\n",
      "        RF: top‑3 FI = [0.0207 0.0114 0.0111]\n",
      "Model saved: RF_w252_2020Q2.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w252_2020Q2.joblib\n",
      "      dataset size = 236,838\n",
      "  ▸ 2020‑Q3: train\n",
      "      +2839 obs from 2020‑Q2\n",
      "        RF: top‑3 FI = [0.0168 0.0106 0.0098]\n",
      "Model saved: RF_w252_2020Q3.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w252_2020Q3.joblib\n",
      "      dataset size = 239,677\n",
      "  ▸ 2020‑Q4: train\n",
      "      [Optuna] tuning RF for 2020‑Q4\n",
      "[Optuna] RF best MSE = 0.000272\n",
      "[Optuna] best params        = {'n_estimators': 290, 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2'}\n",
      "      [Optuna] tuning XGB for 2020‑Q4\n",
      "[Optuna] XGB best MSE = 0.000272\n",
      "[Optuna] best params        = {'n_estimators': 241, 'max_depth': 3, 'learning_rate': 0.1967490136647509, 'subsample': 0.706870580364408, 'colsample_bytree': 0.9940849943144303, 'min_child_weight': 1, 'gamma': 1.9846341700615882, 'reg_alpha': 0.02051094027415192, 'reg_lambda': 0.08356671211046274}\n",
      "      +3151 obs from 2020‑Q3\n",
      "        RF: top‑3 FI = [0.0124 0.0105 0.01  ]\n",
      "Model saved: RF_w252_2020Q4.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w252_2020Q4.joblib\n",
      "      dataset size = 242,828\n",
      "  ▸ 2021‑Q1: train\n",
      "      +3114 obs from 2020‑Q4\n",
      "        RF: top‑3 FI = [0.012  0.011  0.0106]\n",
      "Model saved: RF_w252_2021Q1.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w252_2021Q1.joblib\n",
      "      dataset size = 245,942\n",
      "  ▸ 2021‑Q2: train\n",
      "      +2989 obs from 2021‑Q1\n",
      "        RF: top‑3 FI = [0.012  0.0107 0.0103]\n",
      "Model saved: RF_w252_2021Q2.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w252_2021Q2.joblib\n",
      "      dataset size = 248,931\n",
      "  ▸ 2021‑Q3: train\n",
      "      +3132 obs from 2021‑Q2\n",
      "        RF: top‑3 FI = [0.0113 0.0109 0.0103]\n",
      "Model saved: RF_w252_2021Q3.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w252_2021Q3.joblib\n",
      "      dataset size = 252,063\n",
      "  ▸ 2021‑Q4: train\n",
      "      +3173 obs from 2021‑Q3\n",
      "        RF: top‑3 FI = [0.0116 0.0116 0.0108]\n",
      "Model saved: RF_w252_2021Q4.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w252_2021Q4.joblib\n",
      "      dataset size = 255,236\n",
      "  ▸ 2022‑Q1: train\n",
      "      +3153 obs from 2021‑Q4\n",
      "        RF: top‑3 FI = [0.0113 0.0111 0.0101]\n",
      "Model saved: RF_w252_2022Q1.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w252_2022Q1.joblib\n",
      "      dataset size = 258,389\n",
      "  ▸ 2022‑Q2: train\n",
      "      +3029 obs from 2022‑Q1\n",
      "        RF: top‑3 FI = [0.0119 0.0113 0.011 ]\n",
      "Model saved: RF_w252_2022Q2.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w252_2022Q2.joblib\n",
      "      dataset size = 261,418\n",
      "  ▸ 2022‑Q3: train\n",
      "      +2969 obs from 2022‑Q2\n",
      "        RF: top‑3 FI = [0.0147 0.0112 0.0099]\n",
      "Model saved: RF_w252_2022Q3.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w252_2022Q3.joblib\n",
      "      dataset size = 264,387\n",
      "  ▸ 2022‑Q4: train\n",
      "      +3152 obs from 2022‑Q3\n",
      "        RF: top‑3 FI = [0.0135 0.011  0.0101]\n",
      "Model saved: RF_w252_2022Q4.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w252_2022Q4.joblib\n",
      "      dataset size = 267,539\n",
      "  ▸ 2023‑Q1: train\n",
      "      +3070 obs from 2022‑Q4\n",
      "        RF: top‑3 FI = [0.013  0.0104 0.0101]\n",
      "Model saved: RF_w252_2023Q1.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w252_2023Q1.joblib\n",
      "      dataset size = 270,609\n",
      "  ▸ 2023‑Q2: train\n",
      "      +3064 obs from 2023‑Q1\n",
      "        RF: top‑3 FI = [0.0127 0.0114 0.0099]\n",
      "Model saved: RF_w252_2023Q2.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w252_2023Q2.joblib\n",
      "      dataset size = 273,673\n",
      "  ▸ 2023‑Q3: train\n",
      "      +3069 obs from 2023‑Q2\n",
      "        RF: top‑3 FI = [0.0123 0.0099 0.0099]\n",
      "Model saved: RF_w252_2023Q3.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w252_2023Q3.joblib\n",
      "      dataset size = 276,742\n",
      "  ▸ 2023‑Q4: train\n",
      "      +3121 obs from 2023‑Q3\n",
      "        RF: top‑3 FI = [0.0129 0.0105 0.01  ]\n",
      "Model saved: RF_w252_2023Q4.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w252_2023Q4.joblib\n",
      "      dataset size = 279,863\n",
      "  ▸ 2024‑Q1: train\n",
      "      +3113 obs from 2023‑Q4\n",
      "        RF: top‑3 FI = [0.0136 0.011  0.0096]\n",
      "Model saved: RF_w252_2024Q1.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w252_2024Q1.joblib\n",
      "      dataset size = 282,976\n",
      "  ▸ 2024‑Q2: train\n",
      "      +3026 obs from 2024‑Q1\n",
      "        RF: top‑3 FI = [0.0132 0.0099 0.0098]\n",
      "Model saved: RF_w252_2024Q2.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w252_2024Q2.joblib\n",
      "      dataset size = 286,002\n",
      "  ▸ 2024‑Q3: train\n",
      "      +3112 obs from 2024‑Q2\n",
      "        RF: top‑3 FI = [0.0138 0.0102 0.01  ]\n",
      "Model saved: RF_w252_2024Q3.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w252_2024Q3.joblib\n",
      "      dataset size = 289,114\n",
      "\n",
      " Window = 512\n",
      "  ▸ 2015‑Q4: train\n",
      "      [Optuna] tuning RF for 2015‑Q4\n",
      "[Optuna] RF best MSE = 0.000288\n",
      "[Optuna] best params        = {'n_estimators': 132, 'max_depth': 4, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "      [Optuna] tuning XGB for 2015‑Q4\n",
      "[Optuna] XGB best MSE = 0.000288\n",
      "[Optuna] best params        = {'n_estimators': 116, 'max_depth': 6, 'learning_rate': 0.06965127278119697, 'subsample': 0.9269598231882821, 'colsample_bytree': 0.7499624200630078, 'min_child_weight': 4, 'gamma': 1.5976886838026647, 'reg_alpha': 0.00010573062090842218, 'reg_lambda': 0.012536107343422477}\n",
      "        RF: top‑3 FI = [0.0218 0.0172 0.0137]\n",
      "Model saved: RF_w512_2015Q4.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w512_2015Q4.joblib\n",
      "      dataset size = 171,570\n",
      "  ▸ 2016‑Q1: train\n",
      "        RF: top‑3 FI = [0.0218 0.0172 0.0137]\n",
      "Model saved: RF_w512_2016Q1.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w512_2016Q1.joblib\n",
      "      dataset size = 171,570\n",
      "  ▸ 2016‑Q2: train\n",
      "      +2956 obs from 2016‑Q1\n",
      "        RF: top‑3 FI = [0.0215 0.0172 0.0137]\n",
      "Model saved: RF_w512_2016Q2.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w512_2016Q2.joblib\n",
      "      dataset size = 174,526\n",
      "  ▸ 2016‑Q3: train\n",
      "      +3170 obs from 2016‑Q2\n",
      "        RF: top‑3 FI = [0.0215 0.0172 0.0137]\n",
      "Model saved: RF_w512_2016Q3.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w512_2016Q3.joblib\n",
      "      dataset size = 177,696\n",
      "  ▸ 2016‑Q4: train\n",
      "      +3176 obs from 2016‑Q3\n",
      "        RF: top‑3 FI = [0.0215 0.0172 0.0137]\n",
      "Model saved: RF_w512_2016Q4.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w512_2016Q4.joblib\n",
      "      dataset size = 180,872\n",
      "  ▸ 2017‑Q1: train\n",
      "      +3123 obs from 2016‑Q4\n",
      "        RF: top‑3 FI = [0.0215 0.0172 0.0137]\n",
      "Model saved: RF_w512_2017Q1.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w512_2017Q1.joblib\n",
      "      dataset size = 183,995\n",
      "  ▸ 2017‑Q2: train\n",
      "      +3083 obs from 2017‑Q1\n",
      "        RF: top‑3 FI = [0.0218 0.0172 0.0137]\n",
      "Model saved: RF_w512_2017Q2.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w512_2017Q2.joblib\n",
      "      dataset size = 187,078\n",
      "  ▸ 2017‑Q3: train\n",
      "      +3122 obs from 2017‑Q2\n",
      "        RF: top‑3 FI = [0.0223 0.0172 0.0137]\n",
      "Model saved: RF_w512_2017Q3.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w512_2017Q3.joblib\n",
      "      dataset size = 190,200\n",
      "  ▸ 2017‑Q4: train\n",
      "      +3114 obs from 2017‑Q3\n",
      "        RF: top‑3 FI = [0.0223 0.0172 0.0137]\n",
      "Model saved: RF_w512_2017Q4.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w512_2017Q4.joblib\n",
      "      dataset size = 193,314\n",
      "  ▸ 2018‑Q1: train\n",
      "      +3115 obs from 2017‑Q4\n",
      "        RF: top‑3 FI = [0.0221 0.0172 0.0137]\n",
      "Model saved: RF_w512_2018Q1.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w512_2018Q1.joblib\n",
      "      dataset size = 196,429\n",
      "  ▸ 2018‑Q2: train\n",
      "      +2996 obs from 2018‑Q1\n",
      "        RF: top‑3 FI = [0.0221 0.0172 0.0137]\n",
      "Model saved: RF_w512_2018Q2.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w512_2018Q2.joblib\n",
      "      dataset size = 199,425\n",
      "  ▸ 2018‑Q3: train\n",
      "      +3160 obs from 2018‑Q2\n",
      "        RF: top‑3 FI = [0.0221 0.0172 0.0137]\n",
      "Model saved: RF_w512_2018Q3.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w512_2018Q3.joblib\n",
      "      dataset size = 202,585\n",
      "  ▸ 2018‑Q4: train\n",
      "      +3125 obs from 2018‑Q3\n",
      "        RF: top‑3 FI = [0.0219 0.0172 0.0137]\n",
      "Model saved: RF_w512_2018Q4.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w512_2018Q4.joblib\n",
      "      dataset size = 205,710\n",
      "  ▸ 2019‑Q1: train\n",
      "      +3045 obs from 2018‑Q4\n",
      "        RF: top‑3 FI = [0.0219 0.0172 0.0138]\n",
      "Model saved: RF_w512_2019Q1.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w512_2019Q1.joblib\n",
      "      dataset size = 208,755\n",
      "  ▸ 2019‑Q2: train\n",
      "      +3022 obs from 2019‑Q1\n",
      "        RF: top‑3 FI = [0.0219 0.0172 0.0138]\n",
      "Model saved: RF_w512_2019Q2.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w512_2019Q2.joblib\n",
      "      dataset size = 211,777\n",
      "  ▸ 2019‑Q3: train\n",
      "      +3120 obs from 2019‑Q2\n",
      "        RF: top‑3 FI = [0.0219 0.0172 0.0143]\n",
      "Model saved: RF_w512_2019Q3.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w512_2019Q3.joblib\n",
      "      dataset size = 214,897\n",
      "  ▸ 2019‑Q4: train\n",
      "      +3167 obs from 2019‑Q3\n",
      "        RF: top‑3 FI = [0.0221 0.0172 0.0143]\n",
      "Model saved: RF_w512_2019Q4.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w512_2019Q4.joblib\n",
      "      dataset size = 218,064\n",
      "  ▸ 2020‑Q1: train\n",
      "      +3179 obs from 2019‑Q4\n",
      "        RF: top‑3 FI = [0.0221 0.0172 0.0142]\n",
      "Model saved: RF_w512_2020Q1.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w512_2020Q1.joblib\n",
      "      dataset size = 221,243\n",
      "  ▸ 2020‑Q2: train\n",
      "      +2595 obs from 2020‑Q1\n",
      "        RF: top‑3 FI = [0.0221 0.0172 0.0136]\n",
      "Model saved: RF_w512_2020Q2.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w512_2020Q2.joblib\n",
      "      dataset size = 223,838\n",
      "  ▸ 2020‑Q3: train\n",
      "      +2839 obs from 2020‑Q2\n",
      "        RF: top‑3 FI = [0.0221 0.0169 0.0137]\n",
      "Model saved: RF_w512_2020Q3.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w512_2020Q3.joblib\n",
      "      dataset size = 226,677\n",
      "  ▸ 2020‑Q4: train\n",
      "      [Optuna] tuning RF for 2020‑Q4\n",
      "[Optuna] RF best MSE = 0.000274\n",
      "[Optuna] best params        = {'n_estimators': 280, 'max_depth': 10, 'min_samples_split': 6, 'min_samples_leaf': 3, 'max_features': 'log2'}\n",
      "      [Optuna] tuning XGB for 2020‑Q4\n",
      "[Optuna] XGB best MSE = 0.000274\n",
      "[Optuna] best params        = {'n_estimators': 122, 'max_depth': 5, 'learning_rate': 0.07381325213560934, 'subsample': 0.7781985269804682, 'colsample_bytree': 0.8981113441812851, 'min_child_weight': 2, 'gamma': 0.7174211817262341, 'reg_alpha': 0.0008761422729774257, 'reg_lambda': 0.08952520819029627}\n",
      "      +3151 obs from 2020‑Q3\n",
      "        RF: top‑3 FI = [0.008  0.0065 0.0063]\n",
      "Model saved: RF_w512_2020Q4.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w512_2020Q4.joblib\n",
      "      dataset size = 229,828\n",
      "  ▸ 2021‑Q1: train\n",
      "      +3114 obs from 2020‑Q4\n",
      "        RF: top‑3 FI = [0.0068 0.0067 0.0058]\n",
      "Model saved: RF_w512_2021Q1.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w512_2021Q1.joblib\n",
      "      dataset size = 232,942\n",
      "  ▸ 2021‑Q2: train\n",
      "      +2989 obs from 2021‑Q1\n",
      "        RF: top‑3 FI = [0.0075 0.0061 0.0058]\n",
      "Model saved: RF_w512_2021Q2.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w512_2021Q2.joblib\n",
      "      dataset size = 235,931\n",
      "  ▸ 2021‑Q3: train\n",
      "      +3132 obs from 2021‑Q2\n",
      "        RF: top‑3 FI = [0.0066 0.0058 0.0056]\n",
      "Model saved: RF_w512_2021Q3.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w512_2021Q3.joblib\n",
      "      dataset size = 239,063\n",
      "  ▸ 2021‑Q4: train\n",
      "      +3173 obs from 2021‑Q3\n",
      "        RF: top‑3 FI = [0.0076 0.0066 0.0057]\n",
      "Model saved: RF_w512_2021Q4.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w512_2021Q4.joblib\n",
      "      dataset size = 242,236\n",
      "  ▸ 2022‑Q1: train\n",
      "      +3153 obs from 2021‑Q4\n",
      "        RF: top‑3 FI = [0.0087 0.0066 0.006 ]\n",
      "Model saved: RF_w512_2022Q1.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w512_2022Q1.joblib\n",
      "      dataset size = 245,389\n",
      "  ▸ 2022‑Q2: train\n",
      "      +3029 obs from 2022‑Q1\n",
      "        RF: top‑3 FI = [0.0079 0.0067 0.0063]\n",
      "Model saved: RF_w512_2022Q2.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w512_2022Q2.joblib\n",
      "      dataset size = 248,418\n",
      "  ▸ 2022‑Q3: train\n",
      "      +2969 obs from 2022‑Q2\n",
      "        RF: top‑3 FI = [0.009  0.007  0.0057]\n",
      "Model saved: RF_w512_2022Q3.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w512_2022Q3.joblib\n",
      "      dataset size = 251,387\n",
      "  ▸ 2022‑Q4: train\n",
      "      +3152 obs from 2022‑Q3\n",
      "        RF: top‑3 FI = [0.0092 0.0061 0.0053]\n",
      "Model saved: RF_w512_2022Q4.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w512_2022Q4.joblib\n",
      "      dataset size = 254,539\n",
      "  ▸ 2023‑Q1: train\n",
      "      +3070 obs from 2022‑Q4\n",
      "        RF: top‑3 FI = [0.0089 0.0067 0.0057]\n",
      "Model saved: RF_w512_2023Q1.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w512_2023Q1.joblib\n",
      "      dataset size = 257,609\n",
      "  ▸ 2023‑Q2: train\n",
      "      +3064 obs from 2023‑Q1\n",
      "        RF: top‑3 FI = [0.0079 0.007  0.0054]\n",
      "Model saved: RF_w512_2023Q2.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w512_2023Q2.joblib\n",
      "      dataset size = 260,673\n",
      "  ▸ 2023‑Q3: train\n",
      "      +3069 obs from 2023‑Q2\n",
      "        RF: top‑3 FI = [0.0076 0.0072 0.0065]\n",
      "Model saved: RF_w512_2023Q3.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w512_2023Q3.joblib\n",
      "      dataset size = 263,742\n",
      "  ▸ 2023‑Q4: train\n",
      "      +3121 obs from 2023‑Q3\n",
      "        RF: top‑3 FI = [0.0075 0.0068 0.0065]\n",
      "Model saved: RF_w512_2023Q4.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w512_2023Q4.joblib\n",
      "      dataset size = 266,863\n",
      "  ▸ 2024‑Q1: train\n",
      "      +3113 obs from 2023‑Q4\n",
      "        RF: top‑3 FI = [0.0078 0.0063 0.0062]\n",
      "Model saved: RF_w512_2024Q1.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w512_2024Q1.joblib\n",
      "      dataset size = 269,976\n",
      "  ▸ 2024‑Q2: train\n",
      "      +3026 obs from 2024‑Q1\n",
      "        RF: top‑3 FI = [0.0077 0.0068 0.006 ]\n",
      "Model saved: RF_w512_2024Q2.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w512_2024Q2.joblib\n",
      "      dataset size = 273,002\n",
      "  ▸ 2024‑Q3: train\n",
      "      +3112 obs from 2024‑Q2\n",
      "        RF: top‑3 FI = [0.0082 0.0066 0.0062]\n",
      "Model saved: RF_w512_2024Q3.joblib\n",
      "        XGB: top‑3 FI = [0. 0. 0.]\n",
      "Model saved: XGB_w512_2024Q3.joblib\n",
      "      dataset size = 276,114\n",
      "Tree‑Model Quarterly Expanding Training ✓ done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'RF_w5': RandomForestRegressor(max_depth=5, max_features='log2', min_samples_split=3,\n",
       "                       n_estimators=257, n_jobs=-1, random_state=42),\n",
       " 'XGB_w5': XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.9374208602490834, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=1.0599363956758652, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.030664409621621085, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=8, max_leaves=None,\n",
       "              min_child_weight=5, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=253, n_jobs=-1, num_parallel_tree=None,\n",
       "              predictor=None, random_state=42, ...),\n",
       " 'RF_w21': RandomForestRegressor(max_depth=4, max_features='log2', min_samples_split=8,\n",
       "                       n_estimators=294, n_jobs=-1, random_state=42),\n",
       " 'XGB_w21': XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.9575925787471935, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=1.6313131905499283, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.04470800704285301, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "              min_child_weight=4, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=184, n_jobs=-1, num_parallel_tree=None,\n",
       "              predictor=None, random_state=42, ...),\n",
       " 'RF_w252': RandomForestRegressor(max_depth=8, max_features='log2', min_samples_leaf=4,\n",
       "                       min_samples_split=5, n_estimators=290, n_jobs=-1,\n",
       "                       random_state=42),\n",
       " 'XGB_w252': XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.9940849943144303, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=1.9846341700615882, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.1967490136647509, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=241, n_jobs=-1, num_parallel_tree=None,\n",
       "              predictor=None, random_state=42, ...),\n",
       " 'RF_w512': RandomForestRegressor(max_depth=10, max_features='log2', min_samples_leaf=3,\n",
       "                       min_samples_split=6, n_estimators=280, n_jobs=-1,\n",
       "                       random_state=42),\n",
       " 'XGB_w512': XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.8981113441812851, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=0.7174211817262341, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.07381325213560934, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=5, max_leaves=None,\n",
       "              min_child_weight=2, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=122, n_jobs=-1, num_parallel_tree=None,\n",
       "              predictor=None, random_state=42, ...)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tree_models_expanding_quarterly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Daily Rebalance Portfolio Backtesting Simulation\n",
      "Processing window size: 5\n",
      "  Model: RF, Scheme: VW\n",
      "[Create] New metrics file created with RF w=5\n",
      "  Model: RF, Scheme: EW\n",
      "  Model: XGB, Scheme: VW\n",
      "[Update] Metrics updated for XGB w=5\n",
      "  Model: XGB, Scheme: EW\n",
      "Processing window size: 21\n",
      "  Model: RF, Scheme: VW\n",
      "[Update] Metrics updated for RF w=21\n",
      "  Model: RF, Scheme: EW\n",
      "  Model: XGB, Scheme: VW\n",
      "[Update] Metrics updated for XGB w=21\n",
      "  Model: XGB, Scheme: EW\n",
      "Processing window size: 252\n",
      "  Model: RF, Scheme: VW\n",
      "[Update] Metrics updated for RF w=252\n",
      "  Model: RF, Scheme: EW\n",
      "  Model: XGB, Scheme: VW\n",
      "[Update] Metrics updated for XGB w=252\n",
      "  Model: XGB, Scheme: EW\n",
      "Processing window size: 512\n",
      "  Model: RF, Scheme: VW\n",
      "[Update] Metrics updated for RF w=512\n",
      "  Model: RF, Scheme: EW\n",
      "  Model: XGB, Scheme: VW\n",
      "[Update] Metrics updated for XGB w=512\n",
      "  Model: XGB, Scheme: EW\n",
      "VW results saved to portfolio_results_daily_rebalance_VW.csv\n",
      "EW results saved to portfolio_results_daily_rebalance_EW.csv\n",
      "VW results saved to portfolio_daily_series_VW.csv\n",
      "EW results saved to portfolio_daily_series_EW.csv\n",
      "Saved 886800 prediction rows to predictions_daily.csv\n",
      "Generated 48 portfolio summary records\n",
      "Generated 103344 daily series records\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   scheme model  window portfolio_type  annual_return  annual_vol    sharpe  \\\n",
       " 0      VW    RF       5      long_only       0.247857    0.206442  1.200614   \n",
       " 1      VW    RF       5     short_only      -0.074448    0.210248 -0.354096   \n",
       " 2      VW    RF       5     long_short       0.173409    0.227111  0.763543   \n",
       " 3      EW    RF       5      long_only       0.178005    0.217935  0.816782   \n",
       " 4      EW    RF       5     short_only      -0.106996    0.217212 -0.492586   \n",
       " 5      EW    RF       5     long_short       0.071010    0.217094  0.327092   \n",
       " 6      VW   XGB       5      long_only       0.177143    0.195925  0.904136   \n",
       " 7      VW   XGB       5     short_only      -0.102648    0.191989 -0.534657   \n",
       " 8      VW   XGB       5     long_short       0.074494    0.198834  0.374656   \n",
       " 9      EW   XGB       5      long_only       0.151336    0.216868  0.697827   \n",
       " 10     EW   XGB       5     short_only      -0.087669    0.212638 -0.412291   \n",
       " 11     EW   XGB       5     long_short       0.063668    0.183539  0.346889   \n",
       " 12     VW    RF      21      long_only       0.224002    0.206045  1.087153   \n",
       " 13     VW    RF      21     short_only      -0.087692    0.213140 -0.411427   \n",
       " 14     VW    RF      21     long_short       0.136310    0.227263  0.599792   \n",
       " 15     EW    RF      21      long_only       0.159291    0.213086  0.747542   \n",
       " 16     EW    RF      21     short_only      -0.037211    0.230386 -0.161514   \n",
       " 17     EW    RF      21     long_short       0.122081    0.213624  0.571473   \n",
       " 18     VW   XGB      21      long_only       0.140111    0.193958  0.722379   \n",
       " 19     VW   XGB      21     short_only      -0.142911    0.172289 -0.829481   \n",
       " 20     VW   XGB      21     long_short      -0.002800    0.171626 -0.016312   \n",
       " 21     EW   XGB      21      long_only       0.131443    0.226938  0.579202   \n",
       " 22     EW   XGB      21     short_only      -0.109326    0.211775 -0.516237   \n",
       " 23     EW   XGB      21     long_short       0.022117    0.156603  0.141229   \n",
       " 24     VW    RF     252      long_only       0.219259    0.211677  1.035823   \n",
       " 25     VW    RF     252     short_only      -0.061629    0.209053 -0.294799   \n",
       " 26     VW    RF     252     long_short       0.157631    0.223777  0.704411   \n",
       " 27     EW    RF     252      long_only       0.128052    0.224946  0.569258   \n",
       " 28     EW    RF     252     short_only      -0.107694    0.236881 -0.454633   \n",
       " 29     EW    RF     252     long_short       0.020358    0.205500  0.099065   \n",
       " 30     VW   XGB     252      long_only       0.140111    0.193958  0.722379   \n",
       " 31     VW   XGB     252     short_only      -0.142911    0.172289 -0.829481   \n",
       " 32     VW   XGB     252     long_short      -0.002800    0.171626 -0.016312   \n",
       " 33     EW   XGB     252      long_only       0.131443    0.226938  0.579202   \n",
       " 34     EW   XGB     252     short_only      -0.109326    0.211775 -0.516237   \n",
       " 35     EW   XGB     252     long_short       0.022117    0.156603  0.141229   \n",
       " 36     VW    RF     512      long_only       0.197408    0.218976  0.901507   \n",
       " 37     VW    RF     512     short_only      -0.025558    0.188911 -0.135294   \n",
       " 38     VW    RF     512     long_short       0.171850    0.213663  0.804304   \n",
       " 39     EW    RF     512      long_only       0.167785    0.235590  0.712190   \n",
       " 40     EW    RF     512     short_only      -0.053612    0.211033 -0.254043   \n",
       " 41     EW    RF     512     long_short       0.114173    0.197572  0.577882   \n",
       " 42     VW   XGB     512      long_only       0.140111    0.193958  0.722379   \n",
       " 43     VW   XGB     512     short_only      -0.142911    0.172289 -0.829481   \n",
       " 44     VW   XGB     512     long_short      -0.002800    0.171626 -0.016312   \n",
       " 45     EW   XGB     512      long_only       0.131443    0.226938  0.579202   \n",
       " 46     EW   XGB     512     short_only      -0.109326    0.211775 -0.516237   \n",
       " 47     EW   XGB     512     long_short       0.022117    0.156603  0.141229   \n",
       " \n",
       "     max_drawdown  max_1d_loss  avg_turnover  ...  tc20_sharpe  \\\n",
       " 0       0.276637    -0.059510      1.608266  ...    -2.720263   \n",
       " 1       0.677908    -0.073426      1.637366  ...    -4.266920   \n",
       " 2       0.242154    -0.073044      3.245240  ...    -6.391626   \n",
       " 3       0.271484    -0.062402      1.448559  ...    -2.524946   \n",
       " 4       0.730125    -0.073426      1.472585  ...    -3.902806   \n",
       " 5       0.390792    -0.066217      2.921313  ...    -6.405119   \n",
       " 6       0.230324    -0.057613      1.198409  ...    -2.170284   \n",
       " 7       0.719091    -0.073426      1.064713  ...    -3.294316   \n",
       " 8       0.384755    -0.073359      2.263408  ...    -5.237577   \n",
       " 9       0.250258    -0.053895      1.012157  ...    -1.648540   \n",
       " 10      0.666774    -0.073426      0.966038  ...    -2.681123   \n",
       " 11      0.327869    -0.073359      1.977386  ...    -4.950195   \n",
       " 12      0.238671    -0.057125      1.658514  ...    -2.963295   \n",
       " 13      0.730348    -0.073426      1.699439  ...    -4.424038   \n",
       " 14      0.385207    -0.057406      3.357465  ...    -6.805984   \n",
       " 15      0.279375    -0.054692      1.519819  ...    -2.841752   \n",
       " 16      0.589947    -0.076252      1.500572  ...    -3.436527   \n",
       " 17      0.340514    -0.066429      3.020089  ...    -6.521327   \n",
       " 18      0.230324    -0.043794      0.599632  ...    -0.831848   \n",
       " 19      0.771144    -0.037127      0.229434  ...    -1.495765   \n",
       " 20      0.384755    -0.043004      0.830339  ...    -2.416968   \n",
       " 21      0.250258    -0.053895      0.399551  ...    -0.307916   \n",
       " 22      0.716595    -0.042220      0.197867  ...    -0.986912   \n",
       " 23      0.339214    -0.042480      0.595857  ...    -1.768297   \n",
       " 24      0.389795    -0.054726      1.737988  ...    -3.095111   \n",
       " 25      0.651930    -0.085485      1.733123  ...    -4.472262   \n",
       " 26      0.496720    -0.075998      3.470878  ...    -7.072630   \n",
       " 27      0.464290    -0.054611      1.578610  ...    -2.963729   \n",
       " 28      0.722668    -0.086162      1.524739  ...    -3.704023   \n",
       " 29      0.573678    -0.077965      3.103189  ...    -7.510552   \n",
       " 30      0.230324    -0.043794      0.599632  ...    -0.831848   \n",
       " 31      0.771144    -0.037127      0.229434  ...    -1.495765   \n",
       " 32      0.384755    -0.043004      0.830339  ...    -2.416968   \n",
       " 33      0.250258    -0.053895      0.399551  ...    -0.307916   \n",
       " 34      0.716595    -0.042220      0.197867  ...    -0.986912   \n",
       " 35      0.339214    -0.042480      0.595857  ...    -1.768297   \n",
       " 36      0.410143    -0.059440      1.765989  ...    -3.163855   \n",
       " 37      0.530045    -0.054622      1.611511  ...    -4.409167   \n",
       " 38      0.478133    -0.062201      3.377196  ...    -7.133904   \n",
       " 39      0.418423    -0.060463      1.544714  ...    -2.589985   \n",
       " 40      0.566245    -0.064356      1.528960  ...    -3.897908   \n",
       " 41      0.402418    -0.061943      3.073448  ...    -7.249629   \n",
       " 42      0.230324    -0.043794      0.599632  ...    -0.831848   \n",
       " 43      0.771144    -0.037127      0.229434  ...    -1.495765   \n",
       " 44      0.384755    -0.043004      0.830339  ...    -2.416968   \n",
       " 45      0.250258    -0.053895      0.399551  ...    -0.307916   \n",
       " 46      0.716595    -0.042220      0.197867  ...    -0.986912   \n",
       " 47      0.339214    -0.042480      0.595857  ...    -1.768297   \n",
       " \n",
       "     tc20_max_drawdown  tc30_annual_return  tc30_annual_vol  tc30_sharpe  \\\n",
       " 0           -0.993393           -0.967992         0.207503    -4.664945   \n",
       " 1           -0.999633           -1.312296         0.211569    -6.202695   \n",
       " 2           -0.999997           -2.279992         0.230401    -9.895759   \n",
       " 3           -0.992653           -0.917105         0.219301    -4.181955   \n",
       " 4           -0.999448           -1.220270         0.218041    -5.596518   \n",
       " 5           -0.999995           -2.137503         0.220238    -9.705413   \n",
       " 6           -0.978299           -0.728855         0.198074    -3.679702   \n",
       " 7           -0.996616           -0.907571         0.196400    -4.621029   \n",
       " 8           -0.999910           -1.636642         0.209609    -7.808077   \n",
       " 9           -0.962506           -0.613855         0.218793    -2.805640   \n",
       " 10          -0.994436           -0.817993         0.216123    -3.784854   \n",
       " 11          -0.999714           -1.431236         0.194472    -7.359619   \n",
       " 12          -0.995585           -1.029835         0.207096    -4.972730   \n",
       " 13          -0.999753           -1.372467         0.213915    -6.415957   \n",
       " 14          -0.999999           -2.401933         0.229977   -10.444247   \n",
       " 15          -0.995408           -0.989692         0.213987    -4.625002   \n",
       " 16          -0.999143           -1.171643         0.231408    -5.063109   \n",
       " 17          -0.999995           -2.161106         0.215810   -10.013948   \n",
       " 18          -0.800532           -0.313211         0.196059    -1.597533   \n",
       " 19          -0.913522           -0.316363         0.173422    -1.824241   \n",
       " 20          -0.976595           -0.630536         0.177199    -3.558349   \n",
       " 21          -0.609950           -0.170617         0.227463    -0.750089   \n",
       " 22          -0.873605           -0.258914         0.211995    -1.221320   \n",
       " 23          -0.920619           -0.428351         0.158665    -2.699717   \n",
       " 24          -0.997048           -1.094660         0.212705    -5.146384   \n",
       " 25          -0.999727           -1.371870         0.209429    -6.550522   \n",
       " 26          -0.999999           -2.466353         0.226278   -10.899661   \n",
       " 27          -0.997347           -1.065377         0.225624    -4.721918   \n",
       " 28          -0.999577           -1.260397         0.236614    -5.326814   \n",
       " 29          -0.999999           -2.325653         0.206134   -11.282214   \n",
       " 30          -0.800532           -0.313211         0.196059    -1.597533   \n",
       " 31          -0.913522           -0.316363         0.173422    -1.824241   \n",
       " 32          -0.976595           -0.630536         0.177199    -3.558349   \n",
       " 33          -0.609950           -0.170617         0.227463    -0.750089   \n",
       " 34          -0.873605           -0.258914         0.211995    -1.221320   \n",
       " 35          -0.920619           -0.428351         0.158665    -2.699717   \n",
       " 36          -0.997855           -1.137680         0.219136    -5.191651   \n",
       " 37          -0.999346           -1.243861         0.191080    -6.509652   \n",
       " 38          -0.999998           -2.381310         0.215631   -11.043430   \n",
       " 39          -0.995801           -1.000019         0.236174    -4.234244   \n",
       " 40          -0.999313           -1.209505         0.211937    -5.706898   \n",
       " 41          -0.999996           -2.209353         0.198706   -11.118691   \n",
       " 42          -0.800532           -0.313211         0.196059    -1.597533   \n",
       " 43          -0.913522           -0.316363         0.173422    -1.824241   \n",
       " 44          -0.976595           -0.630536         0.177199    -3.558349   \n",
       " 45          -0.609950           -0.170617         0.227463    -0.750089   \n",
       " 46          -0.873605           -0.258914         0.211995    -1.221320   \n",
       " 47          -0.920619           -0.428351         0.158665    -2.699717   \n",
       " \n",
       "     tc30_max_drawdown  tc40_annual_return  tc40_annual_vol  tc40_sharpe  \\\n",
       " 0           -0.999789           -1.373275         0.208437    -6.588449   \n",
       " 1           -0.999989           -1.724913         0.212562    -8.114853   \n",
       " 2           -1.000000           -3.097793         0.232559   -13.320463   \n",
       " 3           -0.999678           -1.282142         0.220153    -5.823862   \n",
       " 4           -0.999977           -1.591362         0.218682    -7.277055   \n",
       " 5           -1.000000           -2.873673         0.222090   -12.939208   \n",
       " 6           -0.998360           -1.030854         0.200126    -5.151012   \n",
       " 7           -0.999659           -1.175878         0.199592    -5.891418   \n",
       " 8           -0.999999           -2.207020         0.217834   -10.131681   \n",
       " 9           -0.995743           -0.868918         0.220445    -3.941661   \n",
       " 10          -0.999306           -1.061434         0.218596    -4.855691   \n",
       " 11          -0.999996           -1.929538         0.202589    -9.524384   \n",
       " 12          -0.999876           -1.447780         0.207957    -6.961916   \n",
       " 13          -0.999994           -1.800726         0.214630    -8.389897   \n",
       " 14          -1.000000           -3.248014         0.231814   -14.011298   \n",
       " 15          -0.999828           -1.372686         0.214672    -6.394350   \n",
       " 16          -0.999966           -1.549787         0.232079    -6.677850   \n",
       " 17          -1.000000           -2.922169         0.217319   -13.446450   \n",
       " 18          -0.941850           -0.464319         0.197723    -2.348325   \n",
       " 19          -0.946933           -0.374181         0.174182    -2.148216   \n",
       " 20          -0.996060           -0.839782         0.181068    -4.637927   \n",
       " 21          -0.824958           -0.271304         0.227994    -1.189963   \n",
       " 22          -0.917179           -0.308776         0.212264    -1.454680   \n",
       " 23          -0.977523           -0.578507         0.160642    -3.601228   \n",
       " 24          -0.999930           -1.532633         0.213431    -7.180913   \n",
       " 25          -0.999993           -1.808617         0.209973    -8.613573   \n",
       " 26          -1.000000           -3.341014         0.227893   -14.660424   \n",
       " 27          -0.999912           -1.463187         0.226153    -6.469899   \n",
       " 28          -0.999984           -1.644631         0.236843    -6.943969   \n",
       " 29          -1.000000           -3.107657         0.207127   -15.003661   \n",
       " 30          -0.941850           -0.464319         0.197723    -2.348325   \n",
       " 31          -0.946933           -0.374181         0.174182    -2.148216   \n",
       " 32          -0.996060           -0.839782         0.181068    -4.637927   \n",
       " 33          -0.824958           -0.271304         0.227994    -1.189963   \n",
       " 34          -0.917179           -0.308776         0.212264    -1.454680   \n",
       " 35          -0.977523           -0.578507         0.160642    -3.601228   \n",
       " 36          -0.999953           -1.582709         0.219503    -7.210420   \n",
       " 37          -0.999980           -1.649962         0.192500    -8.571226   \n",
       " 38          -1.000000           -3.232364         0.217220   -14.880632   \n",
       " 39          -0.999851           -1.389286         0.236702    -5.869358   \n",
       " 40          -0.999974           -1.594803         0.212612    -7.500997   \n",
       " 41          -1.000000           -2.983862         0.199896   -14.927107   \n",
       " 42          -0.941850           -0.464319         0.197723    -2.348325   \n",
       " 43          -0.946933           -0.374181         0.174182    -2.148216   \n",
       " 44          -0.996060           -0.839782         0.181068    -4.637927   \n",
       " 45          -0.824958           -0.271304         0.227994    -1.189963   \n",
       " 46          -0.917179           -0.308776         0.212264    -1.454680   \n",
       " 47          -0.977523           -0.578507         0.160642    -3.601228   \n",
       " \n",
       "     tc40_max_drawdown  \n",
       " 0           -0.999993  \n",
       " 1           -1.000000  \n",
       " 2           -1.000000  \n",
       " 3           -0.999986  \n",
       " 4           -0.999999  \n",
       " 5           -1.000000  \n",
       " 6           -0.999877  \n",
       " 7           -0.999966  \n",
       " 8           -1.000000  \n",
       " 9           -0.999522  \n",
       " 10          -0.999914  \n",
       " 11          -1.000000  \n",
       " 12          -0.999997  \n",
       " 13          -1.000000  \n",
       " 14          -1.000000  \n",
       " 15          -0.999994  \n",
       " 16          -0.999999  \n",
       " 17          -1.000000  \n",
       " 18          -0.984031  \n",
       " 19          -0.967448  \n",
       " 20          -0.999347  \n",
       " 21          -0.922320  \n",
       " 22          -0.945746  \n",
       " 23          -0.993789  \n",
       " 24          -0.999998  \n",
       " 25          -1.000000  \n",
       " 26          -1.000000  \n",
       " 27          -0.999997  \n",
       " 28          -0.999999  \n",
       " 29          -1.000000  \n",
       " 30          -0.984031  \n",
       " 31          -0.967448  \n",
       " 32          -0.999347  \n",
       " 33          -0.922320  \n",
       " 34          -0.945746  \n",
       " 35          -0.993789  \n",
       " 36          -0.999999  \n",
       " 37          -0.999999  \n",
       " 38          -1.000000  \n",
       " 39          -0.999995  \n",
       " 40          -0.999999  \n",
       " 41          -1.000000  \n",
       " 42          -0.984031  \n",
       " 43          -0.967448  \n",
       " 44          -0.999347  \n",
       " 45          -0.922320  \n",
       " 46          -0.945746  \n",
       " 47          -0.993789  \n",
       " \n",
       " [48 rows x 32 columns],\n",
       "        scheme model  window portfolio_type                 date    return  \\\n",
       " 0          VW    RF       5      long_only  2016-01-05 00:00:00 -0.006821   \n",
       " 1          VW    RF       5      long_only  2016-01-06 00:00:00 -0.008792   \n",
       " 2          VW    RF       5      long_only  2016-01-07 00:00:00  0.006322   \n",
       " 3          VW    RF       5      long_only  2016-01-08 00:00:00  0.001463   \n",
       " 4          VW    RF       5      long_only  2016-01-11 00:00:00  0.014026   \n",
       " ...       ...   ...     ...            ...                  ...       ...   \n",
       " 103339     EW   XGB     512     long_short  2024-12-20 00:00:00  0.001550   \n",
       " 103340     EW   XGB     512     long_short  2024-12-23 00:00:00 -0.009516   \n",
       " 103341     EW   XGB     512     long_short  2024-12-24 00:00:00 -0.007011   \n",
       " 103342     EW   XGB     512     long_short  2024-12-27 00:00:00  0.011346   \n",
       " 103343     EW   XGB     512     long_short  2024-12-30 00:00:00  0.017739   \n",
       " \n",
       "         turnover  cumulative  tc5_return  tc5_cumulative  tc10_return  \\\n",
       " 0       1.000000   -0.006844   -0.007321       -0.007348    -0.007821   \n",
       " 1       1.243946   -0.015675   -0.009414       -0.016807    -0.010036   \n",
       " 2       0.397396   -0.009374    0.006123       -0.010702     0.005924   \n",
       " 3       2.000000   -0.007912    0.000463       -0.010239    -0.000537   \n",
       " 4       2.000000    0.006017    0.013026        0.002702     0.012026   \n",
       " ...          ...         ...         ...             ...          ...   \n",
       " 103339  1.605550    0.072063    0.000747       -0.568747    -0.000055   \n",
       " 103340  0.024361    0.062501   -0.009529       -0.578321    -0.009541   \n",
       " 103341  1.218881    0.055465   -0.007620       -0.585971    -0.008230   \n",
       " 103342  0.035560    0.066747    0.011328       -0.574707     0.011310   \n",
       " 103343  0.041384    0.084331    0.017719       -0.557143     0.017698   \n",
       " \n",
       "         tc10_cumulative  tc20_return  tc20_cumulative  tc30_return  \\\n",
       " 0             -0.007852    -0.008821        -0.008860    -0.009821   \n",
       " 1             -0.017939    -0.011280        -0.020204    -0.012524   \n",
       " 2             -0.012032     0.005527        -0.014693     0.005129   \n",
       " 3             -0.012569    -0.002537        -0.017233    -0.004537   \n",
       " 4             -0.000615     0.010026        -0.007257     0.008026   \n",
       " ...                 ...          ...              ...          ...   \n",
       " 103339        -1.209969    -0.001661        -2.493653    -0.003266   \n",
       " 103340        -1.219556    -0.009565        -2.503265    -0.009589   \n",
       " 103341        -1.227820    -0.009449        -2.512758    -0.010667   \n",
       " 103342        -1.216573     0.011274        -2.501547     0.011239   \n",
       " 103343        -1.199030     0.017656        -2.484044     0.017615   \n",
       " \n",
       "         tc30_cumulative  tc40_return  tc40_cumulative  \n",
       " 0             -0.009869    -0.010821        -0.010880  \n",
       " 1             -0.022473    -0.013768        -0.024744  \n",
       " 2             -0.017356     0.004732        -0.020023  \n",
       " 3             -0.021903    -0.006537        -0.026581  \n",
       " 4             -0.013910     0.006026        -0.020574  \n",
       " ...                 ...          ...              ...  \n",
       " 103339        -3.778995    -0.004872        -5.066000  \n",
       " 103340        -3.788631    -0.009614        -5.075660  \n",
       " 103341        -3.799356    -0.011886        -5.087617  \n",
       " 103342        -3.788179     0.011203        -5.076476  \n",
       " 103343        -3.770718     0.017574        -5.059055  \n",
       " \n",
       " [103344 rows x 18 columns],\n",
       " <__main__.PortfolioBacktester at 0x3db956180>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_portfolio_simulation_daily_rebalance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] 5_factor_analysis_VW_gross.csv \n",
      "[Saved] 5_factor_analysis_VW_net.csv \n",
      "[Saved] 5_factor_analysis_EW_gross.csv \n",
      "[Saved] 5_factor_analysis_EW_net.csv \n"
     ]
    }
   ],
   "source": [
    "def run_factor_regression(port_ret, factors, use_excess=True):\n",
    "    df = pd.concat([port_ret, factors], axis=1, join='inner').dropna()\n",
    "    df.columns = ['ret'] + list(factors.columns)\n",
    "    \n",
    "    if use_excess:\n",
    "        y = df['ret'].values\n",
    "    else:\n",
    "        y = df['ret'].values - df['rf'].values\n",
    "    \n",
    "    X = df[['mktrf','smb','hml','rmw','cma','umd']].values\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    model = sm.OLS(y, X)\n",
    "    res = model.fit()\n",
    "    alpha = res.params[0]          \n",
    "    resid_std = res.resid.std(ddof=1)\n",
    "\n",
    "    ir_daily = alpha / resid_std          \n",
    "    ir_annual = ir_daily * np.sqrt(252)   \n",
    "\n",
    "    y_hat = np.asarray(res.fittedvalues)\n",
    "    \n",
    "    out = {\n",
    "        'N_obs'            : len(y),\n",
    "        'alpha_daily'      : alpha,\n",
    "        'alpha_annual'     : alpha*252,      \n",
    "        't_alpha'          : res.tvalues[0],\n",
    "        'IR_daily'         : ir_daily,\n",
    "        'IR_annual'        : ir_annual,\n",
    "        'R2_zero'          : r2_zero(y, y_hat),\n",
    "    }\n",
    "    \n",
    "    factor_names = ['MKT','SMB','HML','RMW','CMA','UMD']\n",
    "    for i, fac in enumerate(factor_names, start=1):\n",
    "        out[f'beta_{fac}'] = res.params[i]\n",
    "        out[f't_{fac}']    = res.tvalues[i]\n",
    "    \n",
    "    return out\n",
    "\n",
    "def batch_factor_analysis(\n",
    "    daily_df: pd.DataFrame,\n",
    "    factors_path: str,\n",
    "    scheme: str,\n",
    "    tc_levels=(0, 5, 10, 20, 40),\n",
    "    portfolio_types=('long_only','short_only','long_short'),\n",
    "    model_filter=None,\n",
    "    window_filter=None,\n",
    "    gross_only=False,            \n",
    "    out_dir='factor_IR_results',\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a CSV file containing IR results.\n",
    "    gross_only=True  → only tc=0; False → all tc_levels.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    fac = (pd.read_csv(factors_path, parse_dates=['date'])\n",
    "             .set_index('date')\n",
    "             .sort_index())\n",
    "\n",
    "    sub = daily_df[daily_df['scheme'] == scheme].copy()\n",
    "    if model_filter is not None:\n",
    "        sub = sub[sub['model'].isin(model_filter)]\n",
    "    if window_filter is not None:\n",
    "        sub = sub[sub['window'].isin(window_filter)]\n",
    "\n",
    "    tc_iter = (0,) if gross_only else tc_levels\n",
    "    results = []\n",
    "\n",
    "    for (model, win, ptype), g in sub.groupby(['model','window','portfolio_type']):\n",
    "        g = g.sort_values('date').set_index(pd.to_datetime(g['date']))\n",
    "\n",
    "        for tc in tc_iter:\n",
    "            col = 'return' if tc == 0 else f'tc{tc}_return'\n",
    "            if col not in g.columns:\n",
    "                continue\n",
    "            port_ret = g[col]\n",
    "            stats = run_factor_regression(port_ret, fac, use_excess=True)\n",
    "            stats.update({\n",
    "                'scheme'        : scheme,\n",
    "                'model'         : model,\n",
    "                'window'        : win,\n",
    "                'portfolio_type': ptype,\n",
    "                'tc_bps'        : tc,\n",
    "            })\n",
    "            results.append(stats)\n",
    "\n",
    "    df_out = pd.DataFrame(results)[[\n",
    "        'scheme','model','window','portfolio_type','tc_bps','N_obs',\n",
    "        'alpha_daily','alpha_annual','t_alpha',\n",
    "        'IR_daily','IR_annual','R2_zero',\n",
    "        'beta_MKT','t_MKT','beta_SMB','t_SMB',\n",
    "        'beta_HML','t_HML','beta_RMW','t_RMW',\n",
    "        'beta_CMA','t_CMA','beta_UMD','t_UMD'\n",
    "    ]]\n",
    "\n",
    "    tag = 'gross' if gross_only else 'net'\n",
    "    fname = f'5_factor_analysis_{scheme}_{tag}.csv'\n",
    "    df_out.to_csv(os.path.join(out_dir, fname), index=False)\n",
    "    print(f'[Saved] {fname}')\n",
    "    return df_out\n",
    "\n",
    "\n",
    "\n",
    "def run_all_factor_tests(vw_csv=\"portfolio_daily_series_VW.csv\",\n",
    "                         ew_csv=\"portfolio_daily_series_EW.csv\",\n",
    "                         factor_csv=\"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/5_Factors_Plus_Momentum.csv\",\n",
    "                         save_dir=\"results\",\n",
    "                         y_is_excess=True,\n",
    "                         hac_lags=5,\n",
    "                         save_txt=True):\n",
    "    vw_df = pd.read_csv(vw_csv)\n",
    "    ew_df = pd.read_csv(ew_csv)\n",
    "\n",
    "    vw_gross = batch_factor_analysis(\n",
    "        vw_df, factor_csv, scheme='VW', gross_only=True)\n",
    "    vw_net   = batch_factor_analysis(\n",
    "        vw_df, factor_csv, scheme='VW', gross_only=False)\n",
    "\n",
    "    ew_gross = batch_factor_analysis(\n",
    "        ew_df, factor_csv, scheme='EW', gross_only=True)\n",
    "    ew_net   = batch_factor_analysis(\n",
    "        ew_df, factor_csv, scheme='EW', gross_only=False)\n",
    "\n",
    "    return vw_gross, vw_net, ew_gross, ew_net\n",
    "    \n",
    "\n",
    "vw_gross, vw_net, ew_gross, ew_net = run_all_factor_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish: portfolio_daily_series_VW_with_rf.csv\n",
      "Finish: portfolio_daily_series_EW_with_rf.csv\n"
     ]
    }
   ],
   "source": [
    "# === File paths ===\n",
    "rf_file = \"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/CRSP_2016_2024_top50_with_exret.csv\"\n",
    "vw_file = \"portfolio_daily_series_VW.csv\"\n",
    "ew_file = \"portfolio_daily_series_EW.csv\"\n",
    "\n",
    "# === Load rf (risk-free rate) data ===\n",
    "\n",
    "rf_df = pd.read_csv(rf_file, usecols=[\"date\", \"rf\"])\n",
    "rf_df[\"date\"] = pd.to_datetime(rf_df[\"date\"])\n",
    "rf_dict = dict(zip(rf_df[\"date\"], rf_df[\"rf\"]))\n",
    "\n",
    "\n",
    "def adjust_returns_with_rf_grouped(file_path, output_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Use format='mixed' or dayfirst=True to handle different date formats\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], format='mixed', dayfirst=True)\n",
    "\n",
    "    # Find all return columns (including tc5_return, tc10_return, etc.)\n",
    "    return_cols = [col for col in df.columns if \"return\" in col and \"cumul\" not in col]\n",
    "\n",
    "    # Force portfolio_type order to avoid groupby sorting issues\n",
    "    order = [\"long_only\", \"short_only\", \"long_short\"]\n",
    "    df[\"portfolio_type\"] = pd.Categorical(df[\"portfolio_type\"], categories=order, ordered=True)\n",
    "\n",
    "    df_list = []\n",
    "    # Group by scheme/model/window/portfolio_type, add rf and recalculate cumulative\n",
    "    for _, group in df.groupby([\"scheme\", \"model\", \"window\", \"portfolio_type\"], sort=False):\n",
    "        group = group.sort_values(\"date\").copy()\n",
    "        for col in return_cols:\n",
    "            # Add rf to daily return\n",
    "            group[col] = group.apply(lambda row: row[col] + rf_dict.get(row[\"date\"], 0), axis=1)\n",
    "\n",
    "            # Find the corresponding cumulative column (keep naming consistent with original table)\n",
    "            cum_col = col.replace(\"return\", \"cumulative\")\n",
    "            group[cum_col] = np.log1p(group[col]).cumsum()\n",
    "        df_list.append(group)\n",
    "\n",
    "    # Concatenate results and output in order\n",
    "    df_new = pd.concat(df_list).sort_values([\"scheme\", \"model\", \"window\", \"portfolio_type\", \"date\"])\n",
    "    df_new.to_csv(output_path, index=False)\n",
    "    print(f\"Finished: {output_path}\")\n",
    "\n",
    "# === Process VW and EW files separately ===\n",
    "adjust_returns_with_rf_grouped(vw_file, \"portfolio_daily_series_VW_with_rf.csv\")\n",
    "adjust_returns_with_rf_grouped(ew_file, \"portfolio_daily_series_EW_with_rf.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All figures have been generated and saved to: Baseline_Portfolio/\n"
     ]
    }
   ],
   "source": [
    "# ======== Download S&P500 (2016-2024) ========\n",
    "sp500 = yf.download(\"^GSPC\", start=\"2016-01-01\", end=\"2024-12-31\")\n",
    "price_col = \"Adj Close\" if \"Adj Close\" in sp500.columns else \"Close\"\n",
    "sp500[\"daily_return\"] = sp500[price_col].pct_change().fillna(0)\n",
    "# Cumulative log return (as in the paper)\n",
    "sp500[\"cum_return\"] = np.cumsum(np.log1p(sp500[\"daily_return\"]))\n",
    "sp500 = sp500[[\"cum_return\"]]\n",
    "sp500.index = pd.to_datetime(sp500.index)\n",
    "\n",
    "# ======== Configuration ========\n",
    "files = [\n",
    "    (\"VW\", \"portfolio_daily_series_VW_with_rf.csv\"),\n",
    "    (\"EW\", \"portfolio_daily_series_EW_with_rf.csv\")\n",
    "]\n",
    "tc_levels = [0, 5, 10, 20, 40]      # Transaction cost (bps)\n",
    "windows = [5, 21, 252, 512]         # Window sizes\n",
    "strategies = [\"long_only\", \"short_only\", \"long_short\"]\n",
    "\n",
    "output_dir = \"Baseline_Portfolio\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Economic crisis periods (for shading)\n",
    "crisis_periods = [\n",
    "    (datetime(2018, 6, 1), datetime(2019, 1, 1), \"US-China Trade War\"),\n",
    "    (datetime(2020, 2, 1), datetime(2020, 7, 1), \"COVID-19\"),\n",
    "    (datetime(2022, 2, 1), datetime(2022, 6, 1), \"Russia-Ukraine War\"),\n",
    "    (datetime(2023, 1, 1), datetime(2023, 4, 1), \"US Bank Crisis\"),\n",
    "]\n",
    "\n",
    "def plot_comparison_styled(df, scheme, tc, window):\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    model_names = df[\"model\"].unique()\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(model_names)))\n",
    "\n",
    "    offset_step = 0.02\n",
    "\n",
    "    for i, strat in enumerate(strategies, 1):\n",
    "        ax = plt.subplot(3, 1, i)\n",
    "\n",
    "        # Plot S&P500 baseline\n",
    "        plt.plot(sp500.index, sp500[\"cum_return\"],\n",
    "                 color=\"black\", lw=2.5, label=\"S&P500 (Total Return)\", zorder=10)\n",
    "\n",
    "        for idx, model_name in enumerate(model_names):\n",
    "            sub = df[(df[\"window\"] == window) &\n",
    "                     (df[\"portfolio_type\"] == strat) &\n",
    "                     (df[\"model\"] == model_name)].sort_values(\"date\")\n",
    "            if sub.empty:\n",
    "                continue\n",
    "\n",
    "            if tc == 0:\n",
    "                ret_col = \"return\"          # Raw excess return\n",
    "            else:\n",
    "                ret_col = f\"tc{tc}_return\"  # Return with transaction cost\n",
    "\n",
    "            if ret_col not in sub.columns:\n",
    "                continue\n",
    "\n",
    "            log_cum = np.cumsum(np.log1p(sub[ret_col].values))\n",
    "\n",
    "            y_shift = idx * offset_step\n",
    "            plt.plot(sub[\"date\"], log_cum + y_shift,\n",
    "                     label=f\"{model_name} ({strat.replace('_',' ').title()})\",\n",
    "                     lw=2, color=colors[idx], alpha=0.9)\n",
    "\n",
    "        # Shade crisis periods\n",
    "        for start, end, label in crisis_periods:\n",
    "            ax.axvspan(start, end, color='grey', alpha=0.3)\n",
    "            ax.text(start + pd.Timedelta(days=10),\n",
    "                    ax.get_ylim()[1]*0.92, label, fontsize=8, color='grey')\n",
    "        ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "        ax.set_ylabel(\"Cumulative log return (start = 0)\")\n",
    "        ax.set_title(f\"{scheme} | Window={window} | Strategy={strat} | TC={tc} bps\")\n",
    "        ax.grid(alpha=0.3)\n",
    "        plt.xticks(rotation=30)\n",
    "        plt.legend(bbox_to_anchor=(1.04, 1), loc='upper left', fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fname = f\"{scheme}_window{window}_TC{tc}_logreturn_offset.png\"\n",
    "    plt.savefig(os.path.join(output_dir, fname), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# ======== Main loop: generate all figures ========\n",
    "for scheme, file_path in files:\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    for tc in tc_levels:\n",
    "        for window in windows:\n",
    "            plot_comparison_styled(df, scheme, tc, window)\n",
    "\n",
    "print(f\"All figures have been generated and saved to: {output_dir}/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Update] ΔSharpe has been written to portfolio_results_daily_rebalance_VW.csv\n",
      "[Update] ΔSharpe has been written to portfolio_results_daily_rebalance_EW.csv\n"
     ]
    }
   ],
   "source": [
    "# Load R²_zero from portfolio_metrics.csv\n",
    "metrics_df = pd.read_csv(\"portfolio_metrics.csv\")[[\"Model\", \"Window\", \"R²_zero\"]]\n",
    "metrics_df.rename(columns={\"Model\": \"model\", \"Window\": \"window\"}, inplace=True)\n",
    "\n",
    "# Process VW/EW files\n",
    "for fname in [\"portfolio_results_daily_rebalance_VW.csv\", \"portfolio_results_daily_rebalance_EW.csv\"]:\n",
    "    df = pd.read_csv(fname)\n",
    "\n",
    "    # Merge R²_zero by model and window\n",
    "    df = df.merge(metrics_df, on=[\"model\", \"window\"], how=\"left\")\n",
    "\n",
    "    rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        r2 = float(row[\"R²_zero\"]) if not pd.isna(row[\"R²_zero\"]) else 0.0\n",
    "        if row[\"portfolio_type\"] == \"long_only\":\n",
    "            d_sr, sr_star = delta_sharpe(r2, SR_MKT_EX)\n",
    "            row[\"ΔSharpe\"]  = d_sr\n",
    "            row[\"Sharpe*\"]  = sr_star\n",
    "            row[\"baseline\"] = f\"SPX_excess ({SR_MKT_EX:.2f})\"\n",
    "        else:\n",
    "            d_sr, sr_star = delta_sharpe(r2, 0)\n",
    "            row[\"ΔSharpe\"]  = d_sr\n",
    "            row[\"Sharpe*\"]  = sr_star\n",
    "            row[\"baseline\"] = \"cash (0)\"\n",
    "        rows.append(row)\n",
    "\n",
    "    pd.DataFrame(rows).to_csv(fname, index=False)\n",
    "    print(f\"[Update] Delta Sharpe has been written to {fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wj/7nbyftys3g77fcp37r3t0_lm0000gn/T/ipykernel_61572/1104344638.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ics = (df_group.groupby(\"signal_date\", observed=True).apply(_day_ic).dropna())\n",
      "/var/folders/wj/7nbyftys3g77fcp37r3t0_lm0000gn/T/ipykernel_61572/1104344638.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ics = (df_group.groupby(\"signal_date\", observed=True).apply(_day_ic).dropna())\n",
      "/var/folders/wj/7nbyftys3g77fcp37r3t0_lm0000gn/T/ipykernel_61572/1104344638.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ics = (df_group.groupby(\"signal_date\", observed=True).apply(_day_ic).dropna())\n",
      "/var/folders/wj/7nbyftys3g77fcp37r3t0_lm0000gn/T/ipykernel_61572/1104344638.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ics = (df_group.groupby(\"signal_date\", observed=True).apply(_day_ic).dropna())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Overwrote portfolio_metrics.csv with new RankIC )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wj/7nbyftys3g77fcp37r3t0_lm0000gn/T/ipykernel_61572/1104344638.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ics = (df_group.groupby(\"signal_date\", observed=True).apply(_day_ic).dropna())\n",
      "/var/folders/wj/7nbyftys3g77fcp37r3t0_lm0000gn/T/ipykernel_61572/1104344638.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ics = (df_group.groupby(\"signal_date\", observed=True).apply(_day_ic).dropna())\n",
      "/var/folders/wj/7nbyftys3g77fcp37r3t0_lm0000gn/T/ipykernel_61572/1104344638.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ics = (df_group.groupby(\"signal_date\", observed=True).apply(_day_ic).dropna())\n",
      "/var/folders/wj/7nbyftys3g77fcp37r3t0_lm0000gn/T/ipykernel_61572/1104344638.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ics = (df_group.groupby(\"signal_date\", observed=True).apply(_day_ic).dropna())\n",
      "/var/folders/wj/7nbyftys3g77fcp37r3t0_lm0000gn/T/ipykernel_61572/1104344638.py:33: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(rankic_stats)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "PRED_PATH = \"predictions_daily.csv\"\n",
    "METRICS_PATH = \"portfolio_metrics.csv\"\n",
    "TREAT_CONSTANT_DAY_AS_ZERO = False\n",
    "MIN_DAYS_FOR_STATS = 1\n",
    "\n",
    "def _day_ic(g):\n",
    "    if g[\"y_pred\"].nunique(dropna=True) <= 1 or g[\"y_true\"].nunique(dropna=True) <= 1:\n",
    "        return 0.0 if TREAT_CONSTANT_DAY_AS_ZERO else np.nan\n",
    "    return g[\"y_pred\"].corr(g[\"y_true\"], method=\"spearman\")\n",
    "\n",
    "def rankic_stats(df_group):\n",
    "    ics = (df_group.groupby(\"signal_date\", observed=True).apply(_day_ic).dropna())\n",
    "    n = int(ics.shape[0])\n",
    "    if n < MIN_DAYS_FOR_STATS:\n",
    "        return pd.Series({\"RankIC_mean\": np.nan, \"RankIC_t\": np.nan, \"RankIC_pos%\": np.nan, \"N_days\": n})\n",
    "    mean_ic = float(ics.mean())\n",
    "    std_ic  = float(ics.std(ddof=1))\n",
    "    t_ic    = mean_ic / (std_ic / np.sqrt(n)) if std_ic > 0 else np.nan\n",
    "    pos_pct = float((ics > 0).mean())\n",
    "    return pd.Series({\"RankIC_mean\": mean_ic, \"RankIC_t\": t_ic, \"RankIC_pos%\": pos_pct, \"N_days\": n})\n",
    "\n",
    "# Read data and calculate RankIC\n",
    "pred = pd.read_csv(PRED_PATH)\n",
    "pred[\"signal_date\"] = pd.to_datetime(pred[\"signal_date\"], errors=\"coerce\")\n",
    "pred = pred.dropna(subset=[\"signal_date\", \"y_true\", \"y_pred\", \"model\", \"window\"])\n",
    "pred[\"window\"] = pd.to_numeric(pred[\"window\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "rankic_df = (pred.groupby([\"model\", \"window\"], dropna=False)\n",
    "                .apply(rankic_stats)\n",
    "                .reset_index()\n",
    "                .rename(columns={\"model\":\"Model\",\"window\":\"Window\"}))\n",
    "\n",
    "# Merge: keep new RankIC columns, add _old suffix to old metrics columns\n",
    "metrics = pd.read_csv(METRICS_PATH)\n",
    "metrics[\"Window\"] = pd.to_numeric(metrics[\"Window\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "merged = metrics.merge(rankic_df, on=[\"Model\",\"Window\"], how=\"left\", suffixes=(\"_old\",\"\"))\n",
    "\n",
    "# Drop old columns with _old suffix\n",
    "to_drop = [c for c in merged.columns if c.endswith(\"_old\")]\n",
    "merged = merged.drop(columns=to_drop)\n",
    "\n",
    "# Save the updated file\n",
    "merged.to_csv(METRICS_PATH, index=False)\n",
    "print(\"[OK] Overwrote portfolio_metrics.csv with new RankIC\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-mac)",
   "language": "python",
   "name": "tf-mac"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
