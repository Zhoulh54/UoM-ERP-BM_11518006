{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CYHKV4dTfBT",
        "outputId": "0978dff6-8cf5-433a-b67c-95e6ec50602f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/chronos_t5_mini_project_portfolio\n",
            "Cloning into 'chronos-forecasting'...\n",
            "remote: Enumerating objects: 493, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 493 (delta 17), reused 2 (delta 2), pack-reused 470 (from 2)\u001b[K\n",
            "Receiving objects: 100% (493/493), 1.03 MiB | 12.83 MiB/s, done.\n",
            "Resolving deltas: 100% (212/212), done.\n",
            "/content/drive/MyDrive/chronos_t5_mini_project_portfolio/chronos-forecasting\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Using device: cuda\n",
            "Device name: Tesla T4\n",
            "Total memory: 14.7 GB\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Chronos T5 Mini - Google Colab 投资组合回测\n",
        "# ============================================================================\n",
        "\n",
        "# 环境设置和依赖安装\n",
        "from google.colab import drive\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import yfinance as yf\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from scipy.stats import f as f_dist\n",
        "import statsmodels.api as sm\n",
        "from datetime import datetime\n",
        "import matplotlib.dates as mdates\n",
        "import joblib\n",
        "\n",
        "# 挂载 Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 创建项目目录并切换\n",
        "!mkdir -p /content/drive/MyDrive/chronos_t5_mini_project_portfolio\n",
        "%cd /content/drive/MyDrive/chronos_t5_mini_project_portfolio\n",
        "\n",
        "# 克隆源码库（如果不存在）\n",
        "!git clone https://github.com/amazon-science/chronos-forecasting\n",
        "%cd chronos-forecasting\n",
        "\n",
        "# 安装依赖包\n",
        "%pip install torch transformers datasets accelerate scikit-learn tqdm joblib\n",
        "\n",
        "# 添加模块路径并导入Chronos\n",
        "sys.path.append(\"src\")\n",
        "from chronos import BaseChronosPipeline\n",
        "\n",
        "# ========== 设置随机种子确保可复现性 ==========\n",
        "def set_seed(seed=42):\n",
        "    \"\"\"设置所有随机种子以确保可复现性（CUDA版本）\"\"\"\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    # 确保确定性行为\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "# 设置随机种子\n",
        "set_seed(42)\n",
        "\n",
        "# CUDA设备检查\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Total memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUy9Z_IDTyd0",
        "outputId": "05d90864-7e91-4c2c-d873-3b8583dc1e08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully loaded Chronos T5 Mini model on cuda\n",
            "Model device: cuda:0\n",
            "CUDA cache cleared\n"
          ]
        }
      ],
      "source": [
        "# 加载Chronos T5-Mini预训练模型\n",
        "try:\n",
        "    pipeline = BaseChronosPipeline.from_pretrained(\n",
        "        \"amazon/chronos-t5-mini\",\n",
        "        device_map=\"auto\" if torch.cuda.is_available() else \"cpu\",\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32  # 使用FP16节省显存\n",
        "    )\n",
        "    print(f\"Successfully loaded Chronos T5 Mini model on {device}\")\n",
        "\n",
        "    # 检查模型设备\n",
        "    if hasattr(pipeline.model, 'device'):\n",
        "        print(f\"Model device: {pipeline.model.device}\")\n",
        "\n",
        "    # 清理CUDA缓存\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\"CUDA cache cleared\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load model: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhkaBvhNU3xb",
        "outputId": "aaa6f573-ed71-4b39-e849-be5b42a3d627"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded successfully! Keys: ['X_train_5', 'y_train_5', 'meta_train_5', 'market_caps_train_5', 'X_test_5', 'y_test_5', 'meta_test_5', 'market_caps_test_5', 'X_train_21', 'y_train_21', 'meta_train_21', 'market_caps_train_21', 'X_test_21', 'y_test_21', 'meta_test_21', 'market_caps_test_21', 'X_train_252', 'y_train_252', 'meta_train_252', 'market_caps_train_252', 'X_test_252', 'y_test_252', 'meta_test_252', 'market_caps_test_252', 'X_train_512', 'y_train_512', 'meta_train_512', 'market_caps_train_512', 'X_test_512', 'y_test_512', 'meta_test_512', 'market_caps_test_512']\n",
            "Available datasets: 32 files\n",
            "GPU优化批处理配置（T5-Mini）:\n",
            "  窗口 5: 批次大小 = 1024\n",
            "  窗口 21: 批次大小 = 256\n",
            "  窗口 252: 批次大小 = 32\n",
            "  窗口 512: 批次大小 = 16\n",
            "配置完成！\n"
          ]
        }
      ],
      "source": [
        "# 数据加载和常量定义\n",
        "data_path = \"/content/drive/MyDrive/ERP Data/all_window_datasets_unscaled.npz\"\n",
        "\n",
        "# 数据加载函数\n",
        "def load_datasets(data_path):\n",
        "    \"\"\"加载数据集\"\"\"\n",
        "    try:\n",
        "        data = np.load(data_path, allow_pickle=True)\n",
        "        print(f\"Data loaded successfully! Keys: {list(data.keys())}\")\n",
        "\n",
        "        # 创建datasets字典\n",
        "        datasets = {}\n",
        "        for key in data.files:\n",
        "            datasets[key] = data[key]\n",
        "        print(f\"Available datasets: {len(datasets)} files\")\n",
        "        return datasets\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Data file not found: {data_path}\")\n",
        "        print(\"Please ensure all_window_datasets.npz is uploaded to Google Drive 'ERP Data' folder\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        raise\n",
        "\n",
        "# 加载数据\n",
        "datasets = load_datasets(data_path)\n",
        "\n",
        "# 全局常量定义\n",
        "WINDOW_SIZES = [5, 21, 252, 512]\n",
        "START_YEAR = 2016\n",
        "END_YEAR = 2024\n",
        "TRANSACTION_COST = 0.0015\n",
        "\n",
        "# GPU优化的批处理大小配置（针对T5-Mini模型）\n",
        "def get_batch_size(window_size):\n",
        "    \"\"\"根据窗口大小返回适合GPU的批处理大小\"\"\"\n",
        "    if window_size <= 5:\n",
        "        return 1024  # 短序列可以使用较大批次\n",
        "    elif window_size <= 21:\n",
        "        return 256   # 中等序列\n",
        "    elif window_size <= 252:\n",
        "        return 32    # 长序列需要小批次\n",
        "    elif window_size <= 512:\n",
        "        return 16    # 超长序列\n",
        "    else:\n",
        "        return 8\n",
        "\n",
        "print(\"GPU优化批处理配置（T5-Mini）:\")\n",
        "for ws in WINDOW_SIZES:\n",
        "    batch_size = get_batch_size(ws)\n",
        "    print(f\"  窗口 {ws}: 批次大小 = {batch_size}\")\n",
        "\n",
        "print(\"配置完成！\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdt36X1a-BrK",
        "outputId": "639f6e6f-6a07-44c6-dad9-9e9ef8df18ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-486884232.py:15: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  px = yf.download(\"^GSPC\", start=\"2016-01-01\", end=\"2024-12-31\")[\"Close\"]\n",
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] S&P500 Excess Sharpe (2016–24) = 0.652\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "/tmp/ipython-input-486884232.py:17: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  rf_align = rf_series.reindex(sp_ret.index).fillna(method=\"ffill\")\n"
          ]
        }
      ],
      "source": [
        "def annual_sharpe(rets, freq=252):\n",
        "    mu = float(np.mean(rets)) * freq\n",
        "    sd = float(np.std(rets, ddof=1)) * np.sqrt(freq)\n",
        "    return mu / sd if sd > 0 else 0\n",
        "\n",
        "# ===  加载无风险利率 & 计算 S&P500 Excess Sharpe ===\n",
        "\n",
        "rf_file = \"/content/drive/MyDrive/ERP Data/CRSP_2016_2024_top50_with_exret.csv\"\n",
        "try:\n",
        "    rf_df = pd.read_csv(rf_file, usecols=[\"date\", \"rf\"])\n",
        "    rf_df[\"date\"] = pd.to_datetime(rf_df[\"date\"])\n",
        "    rf_df = rf_df.drop_duplicates(\"date\").set_index(\"date\").sort_index()\n",
        "    rf_series = rf_df[\"rf\"].astype(float)\n",
        "\n",
        "    px = yf.download(\"^GSPC\", start=\"2016-01-01\", end=\"2024-12-31\")[\"Close\"]\n",
        "    sp_ret = px.pct_change().dropna()\n",
        "    rf_align = rf_series.reindex(sp_ret.index).fillna(method=\"ffill\")\n",
        "    sp_excess = sp_ret.values - rf_align.values\n",
        "\n",
        "    SR_MKT_EX = annual_sharpe(sp_excess)\n",
        "    print(f\"[INFO] S&P500 Excess Sharpe (2016–24) = {SR_MKT_EX:.3f}\")\n",
        "except Exception as e:\n",
        "    print(f\"Warning: Could not load risk-free rate data: {e}\")\n",
        "    SR_MKT_EX = 0.5  # 使用默认值\n",
        "# === 2. ΔSharpe 计算函数 ===\n",
        "def delta_sharpe(r2_zero: float, sr_base: float):\n",
        "    \"\"\"\n",
        "    若 r2_zero <= 0   → ΔSharpe = 0，Sharpe* = sr_base\n",
        "    若 r2_zero >= 1   → ΔSharpe = 0，Sharpe* = sr_base（极端情况兜底）\n",
        "    其余区间按原公式计算\n",
        "    \"\"\"\n",
        "    if (r2_zero <= 0) or (r2_zero >= 1):\n",
        "        return 0.0, sr_base\n",
        "    sr_star = np.sqrt(sr_base ** 2 + r2_zero) / np.sqrt(1 - r2_zero)\n",
        "    return sr_star - sr_base, sr_star\n",
        "\n",
        "# === Zero-based R² ===\n",
        "def r2_zero(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    计算老师要求的 R² (zero-based, 基准为0)\n",
        "    y_true: 实际值数组 (N,)\n",
        "    y_pred: 预测值数组 (N,)\n",
        "    \"\"\"\n",
        "    rss = np.sum((y_true - y_pred)**2)\n",
        "    tss = np.sum(y_true**2)\n",
        "    return 1 - rss / tss\n",
        "\n",
        "def calc_ic_daily(df, method='spearman'):\n",
        "    \"\"\"\n",
        "    计算每日横截面 RankIC\n",
        "    df: 至少包含 ['signal_date','y_true','y_pred']\n",
        "    \"\"\"\n",
        "    ics = (df.groupby('signal_date')\n",
        "             .apply(lambda g: g['y_pred'].corr(g['y_true'], method=method))\n",
        "             .dropna())\n",
        "    mean_ic = ics.mean()\n",
        "    std_ic  = ics.std(ddof=1)\n",
        "    t_ic    = mean_ic / (std_ic / np.sqrt(len(ics))) if std_ic > 0 else np.nan\n",
        "    pos_ratio = (ics > 0).mean()  # 正相关天数比例\n",
        "    return mean_ic, t_ic, pos_ratio, ics\n",
        "\n",
        "def calc_directional_metrics(y_true, y_pred, permnos=None):\n",
        "    \"\"\"\n",
        "    改进版：\n",
        "    - 样本级符号预测\n",
        "    - 分股票则每支股票分别算 Overall、Up、Down，再平均\n",
        "    \"\"\"\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "\n",
        "    if permnos is None:\n",
        "        # 不分组\n",
        "        s_true = np.sign(y_true)\n",
        "        s_pred = np.sign(y_pred)\n",
        "        mask = s_true != 0  # 避免零值干扰\n",
        "        s_true = s_true[mask]\n",
        "        s_pred = s_pred[mask]\n",
        "\n",
        "        overall_acc = np.mean(s_true == s_pred)\n",
        "\n",
        "        up_mask = s_true > 0\n",
        "        down_mask = s_true < 0\n",
        "        up_acc = np.mean(s_true[up_mask] == s_pred[up_mask]) if np.any(up_mask) else 0\n",
        "        down_acc = np.mean(s_true[down_mask] == s_pred[down_mask]) if np.any(down_mask) else 0\n",
        "\n",
        "    else:\n",
        "        # 分 PERMNO\n",
        "        df = pd.DataFrame({\"permno\": permnos, \"yt\": y_true, \"yp\": y_pred})\n",
        "        overall_accs = []\n",
        "        up_accs = []\n",
        "        down_accs = []\n",
        "\n",
        "        for _, g in df.groupby(\"permno\"):\n",
        "            s_true = np.sign(g[\"yt\"].values)\n",
        "            s_pred = np.sign(g[\"yp\"].values)\n",
        "            mask = s_true != 0\n",
        "            s_true = s_true[mask]\n",
        "            s_pred = s_pred[mask]\n",
        "            if len(s_true) == 0:\n",
        "                continue\n",
        "            overall_accs.append(np.mean(s_true == s_pred))\n",
        "\n",
        "            up_mask = s_true > 0\n",
        "            down_mask = s_true < 0\n",
        "            up_accs.append(np.mean(s_true[up_mask] == s_pred[up_mask]) if np.any(up_mask) else np.nan)\n",
        "            down_accs.append(np.mean(s_true[down_mask] == s_pred[down_mask]) if np.any(down_mask) else np.nan)\n",
        "\n",
        "        overall_acc = np.nanmean(overall_accs)\n",
        "        up_acc = np.nanmean(up_accs)\n",
        "        down_acc = np.nanmean(down_accs)\n",
        "\n",
        "    return overall_acc, up_acc, down_acc\n",
        "\n",
        "\n",
        "# === 组合版回归指标 ===\n",
        "def regression_metrics(y_true, y_pred, k, meta=None, permnos=None):\n",
        "    \"\"\"\n",
        "    包含：\n",
        "    - 回归指标\n",
        "    - 单点方向准确率\n",
        "    - 市值分组指标\n",
        "    \"\"\"\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    n = len(y_true)\n",
        "\n",
        "    r2 = r2_zero(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "\n",
        "    # 新版 Directional Metrics\n",
        "    dir_acc, up_acc, down_acc = calc_directional_metrics(y_true, y_pred, permnos)\n",
        "\n",
        "    metrics = {\n",
        "        \"R²_zero\": r2,\n",
        "        \"RMSE\": rmse,\n",
        "        \"MAE\": mae,\n",
        "        \"MSE\": mse,\n",
        "        \"Directional Accuracy\": dir_acc,\n",
        "        \"Up_Directional_Acc\": up_acc,\n",
        "        \"Down_Directional_Acc\": down_acc\n",
        "    }\n",
        "\n",
        "    # === 市值组 ===\n",
        "    if meta is not None and \"MKTCAP_PERCENTILE\" in meta:\n",
        "        top_mask = meta[\"MKTCAP_PERCENTILE\"] >= 0.75\n",
        "        bottom_mask = meta[\"MKTCAP_PERCENTILE\"] <= 0.25\n",
        "\n",
        "        if np.any(top_mask):\n",
        "            yt_top = y_true[top_mask]\n",
        "            yp_top = y_pred[top_mask]\n",
        "            perm_top = permnos[top_mask] if permnos is not None else None\n",
        "            r2_top = r2_zero(yt_top, yp_top)  # 用r2_zero替换\n",
        "            rmse_top = np.sqrt(mean_squared_error(yt_top, yp_top))\n",
        "            mae_top = mean_absolute_error(yt_top, yp_top)\n",
        "            mse_top = mean_squared_error(yt_top, yp_top)\n",
        "            dir_top, up_top, down_top = calc_directional_metrics(yt_top, yp_top, perm_top)\n",
        "            metrics.update({\n",
        "                \"Top25_R2_zero\": r2_top,\n",
        "                \"Top25_RMSE\": rmse_top,\n",
        "                \"Top25_MAE\": mae_top,\n",
        "                \"Top25_MSE\": mse_top,\n",
        "                \"Top25_Dir_Acc\": dir_top,\n",
        "                \"Top25_Up_Acc\": up_top,\n",
        "                \"Top25_Down_Acc\": down_top\n",
        "            })\n",
        "\n",
        "        if np.any(bottom_mask):\n",
        "            yt_bot = y_true[bottom_mask]\n",
        "            yp_bot = y_pred[bottom_mask]\n",
        "            perm_bot = permnos[bottom_mask] if permnos is not None else None\n",
        "            r2_bot = r2_zero(yt_bot, yp_bot)  # 用r2_zero替换\n",
        "            rmse_bot = np.sqrt(mean_squared_error(yt_bot, yp_bot))\n",
        "            mae_bot = mean_absolute_error(yt_bot, yp_bot)\n",
        "            mse_bot = mean_squared_error(yt_bot, yp_bot)\n",
        "            dir_bot, up_bot, down_bot = calc_directional_metrics(yt_bot, yp_bot, perm_bot)\n",
        "            metrics.update({\n",
        "                \"Bottom25_R2_zero\": r2_bot,\n",
        "                \"Bottom25_RMSE\": rmse_bot,\n",
        "                \"Bottom25_MAE\": mae_bot,\n",
        "                \"Bottom25_MSE\": mse_bot,\n",
        "                \"Bottom25_Dir_Acc\": dir_bot,\n",
        "                \"Bottom25_Up_Acc\": up_bot,\n",
        "                \"Bottom25_Down_Acc\": down_bot\n",
        "            })\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def f_statistic(y_true, y_pred, k):\n",
        "    \"\"\"返回 F 统计量和对应的 p 值\"\"\"\n",
        "    n   = len(y_true)\n",
        "    rss = np.sum((y_true - y_pred) ** 2)\n",
        "    tss = np.sum(y_true ** 2)\n",
        "    r2  = 1 - rss / tss\n",
        "    if (r2 <= 0) or (n <= k):          # 无法解释或自由度不足\n",
        "        return 0.0, 1.0\n",
        "    F = (r2 / k) / ((1 - r2) / (n - k))\n",
        "    p = f_dist.sf(F, k, n - k)         # 上尾概率\n",
        "    return F, p\n",
        "\n",
        "def overall_interval_metrics_method1(y_all, yhat_all, k, permnos_all=None, meta_all=None):\n",
        "    \"\"\"\n",
        "    方式1：整体区间一次性计算指标（2016-2024 全部样本拼接）\n",
        "    返回：一个 dict，可直接喂给 save_metrics()\n",
        "    \"\"\"\n",
        "    # 基本回归 + 方向指标（已写好的组合函数）\n",
        "    base = regression_metrics(\n",
        "        y_true=y_all,\n",
        "        y_pred=yhat_all,\n",
        "        k=k,\n",
        "        meta=meta_all,\n",
        "        permnos=permnos_all\n",
        "    )\n",
        "    F, p = f_statistic(y_all, yhat_all, k)\n",
        "    base[\"F_stat\"]     = F\n",
        "    base[\"F_pvalue\"]   = p\n",
        "    # 也可以顺手补样本数\n",
        "    base[\"N_obs\"] = len(y_all)\n",
        "    # === ΔSharpe：两种基准 ================================================\n",
        "\n",
        "    delta_cash, sr_star_cash = delta_sharpe(base[\"R²_zero\"], sr_base=0)\n",
        "    base[\"ΔSharpe_cash\"]      = delta_cash      # 给 long_short / short_only\n",
        "    base[\"Sharpe*_cash\"]      = sr_star_cash\n",
        "\n",
        "    delta_mkt , sr_star_mkt  = delta_sharpe(base[\"R²_zero\"], sr_base=SR_MKT_EX)\n",
        "    base[\"ΔSharpe_mkt\"]       = delta_mkt       # 给 long_only\n",
        "    base[\"Sharpe*_mkt\"]       = sr_star_mkt\n",
        "\n",
        "    return base\n",
        "\n",
        "def sortino_ratio(rets, freq=252):\n",
        "    \"\"\"计算Sortino Ratio\"\"\"\n",
        "    downside = rets[rets < 0]\n",
        "    if len(downside) == 0:\n",
        "        return np.inf\n",
        "    mu = rets.mean() * freq\n",
        "    sigma = np.sqrt((downside ** 2).mean()) * np.sqrt(freq)\n",
        "    return mu / sigma\n",
        "\n",
        "def cvar(rets, alpha=0.95):\n",
        "    \"\"\"计算CVaR\"\"\"\n",
        "    q = np.quantile(rets, 1 - alpha)\n",
        "    return rets[rets <= q].mean()\n",
        "\n",
        "# 添加缺失的save_metrics函数\n",
        "def save_metrics(metrics_dict, name, window, path=\"portfolio_metrics.csv\"):\n",
        "    \"\"\"保存指标到CSV文件\"\"\"\n",
        "    row = {'Model': name, 'Window': window}\n",
        "    row.update(metrics_dict)\n",
        "\n",
        "    # 检查文件是否存在\n",
        "    if os.path.exists(path):\n",
        "        df = pd.read_csv(path)\n",
        "        df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
        "    else:\n",
        "        df = pd.DataFrame([row])\n",
        "\n",
        "    df.to_csv(path, index=False)\n",
        "    print(f\"Metrics saved for {name}_w{window} to {path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1gfyONtoY9MW"
      },
      "outputs": [],
      "source": [
        "## ========== Portfolio 构建核心类 ==========\n",
        "# ========== 交易成本设置 ==========\n",
        "TC_GRID = [0.0005, 0.001, 0.002, 0.003, 0.004]  # 5, 10, 20, 30, 40 bps\n",
        "TC_TAG  = {\n",
        "    0.0005: \"tc5\",\n",
        "    0.001:  \"tc10\",\n",
        "    0.002:  \"tc20\",\n",
        "    0.003:  \"tc30\",\n",
        "    0.004:  \"tc40\"\n",
        "}\n",
        "\n",
        "class PortfolioBacktester:\n",
        "    def __init__(self):\n",
        "        self.results = {}\n",
        "\n",
        "    def calc_turnover(self, w_t, r_t, w_tp1):\n",
        "        \"\"\"计算换手率 - 使用用户提供的标准公式\"\"\"\n",
        "        if w_t is None:\n",
        "            return np.sum(np.abs(w_tp1))\n",
        "\n",
        "        gross_ret = np.sum(w_t * r_t)\n",
        "        if abs(1 + gross_ret) < 1e-8:  # 避免除零\n",
        "            return np.sum(np.abs(w_tp1))\n",
        "\n",
        "        passive_weight = w_t * (1 + r_t) / (1 + gross_ret)\n",
        "        turnover = np.sum(np.abs(w_tp1 - passive_weight))\n",
        "        return turnover\n",
        "\n",
        "    def create_portfolios_with_permno_tracking(self, signals, market_caps, permnos, top_pct=0.1, bottom_pct=0.1, weight_scheme=\"VW\"):\n",
        "        \"\"\"\n",
        "        根据信号创建投资组合权重，并严格跟踪permno对齐\n",
        "        weight_scheme: 'VW' 市值加权, 'EW' 等权重\n",
        "        \"\"\"\n",
        "        n_stocks = len(signals)\n",
        "        # ---① 至少留 1 支 ---\n",
        "        top_n    = max(1, int(round(n_stocks * top_pct)))\n",
        "        bottom_n = max(1, int(round(n_stocks * bottom_pct)))\n",
        "\n",
        "        # 按预测值排序，记录原始索引\n",
        "        sorted_idx = np.argsort(signals)[::-1]  # 降序\n",
        "\n",
        "        # 选择top和bottom股票的索引\n",
        "        top_idx = sorted_idx[:top_n]\n",
        "        bottom_idx = sorted_idx[-bottom_n:]  # bottom_n ≥ 1 保证非空\n",
        "\n",
        "        # 为每个组合类型创建单独的权重和permno跟踪\n",
        "        portfolio_data = {}\n",
        "\n",
        "        # Long-only portfolio (Top 10%)\n",
        "        long_weights = np.zeros(n_stocks)\n",
        "        if len(top_idx) > 0:\n",
        "            if weight_scheme == \"VW\":\n",
        "                top_market_caps = market_caps[top_idx]\n",
        "                if np.sum(top_market_caps) > 0:\n",
        "                    long_weights[top_idx] = top_market_caps / np.sum(top_market_caps)\n",
        "            else:  # 'EW'\n",
        "                # ---② EW 分配用实际长度，避免再除 0 ---\n",
        "                long_weights[top_idx] = 1.0 / len(top_idx)\n",
        "\n",
        "        portfolio_data['long_only'] = {\n",
        "            'weights': long_weights,\n",
        "            'permnos': permnos.copy(),\n",
        "            'selected_permnos': permnos[top_idx] if len(top_idx) > 0 else np.array([])\n",
        "        }\n",
        "\n",
        "        # Short-only portfolio (Bottom 10%)\n",
        "        short_weights = np.zeros(n_stocks)\n",
        "        if len(bottom_idx) > 0:\n",
        "            if weight_scheme == \"VW\":\n",
        "                bottom_market_caps = market_caps[bottom_idx]\n",
        "                if np.sum(bottom_market_caps) > 0:\n",
        "                    short_weights[bottom_idx] = -bottom_market_caps / np.sum(bottom_market_caps)\n",
        "            else:  # 'EW'\n",
        "                # ---② EW 分配用实际长度，避免再除 0 ---\n",
        "                short_weights[bottom_idx] = -1.0 / len(bottom_idx)\n",
        "\n",
        "        portfolio_data['short_only'] = {\n",
        "            'weights': short_weights,\n",
        "            'permnos': permnos.copy(),\n",
        "            'selected_permnos': permnos[bottom_idx] if len(bottom_idx) > 0 else np.array([])\n",
        "        }\n",
        "\n",
        "        # Long-Short portfolio (Top 做多 + Bottom 做空)\n",
        "        # -------------------------------------------------\n",
        "        ls_raw = long_weights + short_weights\n",
        "\n",
        "        gross_target = 2.0        #Gross ≈ 2，Net ≈ 0\n",
        "        current_gross = np.sum(np.abs(long_weights)) + np.sum(np.abs(short_weights))\n",
        "        scale = gross_target / current_gross if current_gross > 1e-8 else 0.0\n",
        "        ls_weights = scale * ls_raw\n",
        "        # -------------------------------------------------\n",
        "\n",
        "        ls_selected_permnos = np.concatenate([\n",
        "            permnos[top_idx] if len(top_idx) > 0 else np.array([]),\n",
        "            permnos[bottom_idx] if len(bottom_idx) > 0 else np.array([])\n",
        "        ])\n",
        "\n",
        "        portfolio_data['long_short'] = {\n",
        "            'weights': ls_weights,\n",
        "            'permnos': permnos.copy(),\n",
        "            'selected_permnos': ls_selected_permnos\n",
        "        }\n",
        "\n",
        "        return portfolio_data\n",
        "\n",
        "    def calculate_aligned_portfolio_return(self, portfolio_weights, portfolio_permnos, actual_returns, actual_permnos):\n",
        "        \"\"\"计算严格按permno对齐的组合收益\"\"\"\n",
        "        aligned_returns = np.zeros(len(portfolio_permnos))\n",
        "\n",
        "        # 构建permno到收益的映射\n",
        "        return_dict = dict(zip(actual_permnos, actual_returns))\n",
        "\n",
        "        # 按permno对齐收益\n",
        "        for i, permno in enumerate(portfolio_permnos):\n",
        "            if permno in return_dict:\n",
        "                aligned_returns[i] = return_dict[permno]\n",
        "            # 如果找不到对应收益，保持为0\n",
        "\n",
        "        # 计算组合收益\n",
        "        portfolio_return = np.sum(portfolio_weights * aligned_returns)\n",
        "        return portfolio_return, aligned_returns\n",
        "\n",
        "\n",
        "\n",
        "    def calculate_metrics(self, returns, turnover_series=None):\n",
        "        \"\"\"计算投资组合指标 - 仅返回summary指标，不包含长序列\"\"\"\n",
        "        returns = np.array(returns)\n",
        "\n",
        "        # 基础指标 - 改为日度年化\n",
        "        annual_return = np.mean(returns) * 252  # 日度年化\n",
        "        annual_vol = np.std(returns, ddof=1) * np.sqrt(252)  # 日度年化波动率\n",
        "        sharpe = annual_return / annual_vol if annual_vol > 0 else 0\n",
        "\n",
        "        # 最大回撤\n",
        "        log_cum = np.cumsum(np.log1p(returns))\n",
        "        peak_log = np.maximum.accumulate(log_cum)\n",
        "        dd_log = peak_log - log_cum\n",
        "        max_drawdown = 1 - np.exp(-dd_log.max())\n",
        "        #单日最大回撤\n",
        "        max_1d_loss = np.min(returns)\n",
        "\n",
        "        # 换手率\n",
        "        avg_turnover = np.mean(turnover_series) if turnover_series is not None else 0\n",
        "        # 新增Sortino Ratio和CVaR的计算\n",
        "\n",
        "        sortino = sortino_ratio(returns)\n",
        "        cvar95  = cvar(returns, alpha=0.95)\n",
        "\n",
        "\n",
        "        result = {\n",
        "            'annual_return': annual_return,\n",
        "            'annual_vol': annual_vol,\n",
        "            'sharpe': sharpe,\n",
        "            'max_drawdown': max_drawdown,\n",
        "            'max_1d_loss': max_1d_loss,\n",
        "            'avg_turnover': avg_turnover,\n",
        "            'sortino': sortino,\n",
        "            'cvar95': cvar95\n",
        "        }\n",
        "\n",
        "        return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUzfrXcL-BrM",
        "outputId": "a77352ed-9056-486e-ea97-edbe23ac333b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chronos prediction functions defined successfully\n"
          ]
        }
      ],
      "source": [
        "# ========== Chronos 零样本预测函数 ==========\n",
        "\n",
        "def chronos_predict_batch(pipeline, X_batch, prediction_length=1, num_samples=10):\n",
        "    \"\"\"\n",
        "    使用Chronos进行批量预测\n",
        "    Args:\n",
        "        pipeline: Chronos pipeline\n",
        "        X_batch: 输入数据 (batch_size, sequence_length)\n",
        "        prediction_length: 预测长度\n",
        "        num_samples: 采样数量\n",
        "    Returns:\n",
        "        predictions: 预测结果 (batch_size,)\n",
        "    \"\"\"\n",
        "    # 转换为torch.Tensor列表格式，Chronos要求每个序列是一个1维torch.Tensor\n",
        "    context_list = [torch.tensor(X_batch[i], dtype=torch.float32) for i in range(len(X_batch))]\n",
        "\n",
        "    # 使用Chronos进行预测\n",
        "    forecasts = pipeline.predict(\n",
        "        context=context_list,\n",
        "        prediction_length=prediction_length,\n",
        "        num_samples=num_samples\n",
        "    )\n",
        "\n",
        "    # 取第一步预测的均值作为最终预测\n",
        "    predictions = np.array([forecast[0].mean() for forecast in forecasts])\n",
        "\n",
        "    return predictions\n",
        "\n",
        "def chronos_rolling_prediction(pipeline, X_data, batch_size=256, prediction_length=1):\n",
        "    \"\"\"\n",
        "    使用滚动窗口进行Chronos预测\n",
        "    Args:\n",
        "        pipeline: Chronos模型\n",
        "        X_data: 输入特征 (n_samples, window_size)\n",
        "        batch_size: 批量大小\n",
        "        prediction_length: 预测长度\n",
        "    Returns:\n",
        "        predictions: 预测结果 (n_samples,)\n",
        "    \"\"\"\n",
        "    n_samples = len(X_data)\n",
        "    predictions = np.zeros(n_samples)\n",
        "\n",
        "    print(f\"Running Chronos prediction on {n_samples} samples with batch size {batch_size}\")\n",
        "\n",
        "    # 分批处理\n",
        "    for i in tqdm(range(0, n_samples, batch_size), desc=\"Chronos Prediction\"):\n",
        "        end_idx = min(i + batch_size, n_samples)\n",
        "        batch_X = X_data[i:end_idx]\n",
        "\n",
        "        # 使用Chronos进行预测\n",
        "        batch_predictions = chronos_predict_batch(\n",
        "            pipeline, batch_X, prediction_length=prediction_length\n",
        "        )\n",
        "\n",
        "        predictions[i:end_idx] = batch_predictions\n",
        "\n",
        "    return predictions\n",
        "\n",
        "print(\"Chronos prediction functions defined successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nm_F2bFA-BrM"
      },
      "outputs": [],
      "source": [
        "# ========== 日频预测隔日调仓Portfolio模拟主函数 ==========\n",
        "\n",
        "def run_chronos_portfolio_backtest(start_year=2016, end_year=2024, window_sizes=None, model_names=None,\n",
        "                                           npz_path=\"/content/drive/MyDrive/ERP Data/all_window_datasets_unscaled.npz\"):\n",
        "    \"\"\"\n",
        "Portfolio模拟（日频预测隔日调仓）：\n",
        "    1. 使用Chronos T5 Mini模型进行零样本预测\n",
        "    2. 日频预测→日频信号\n",
        "    3. 日频组合构建（T+1建仓，严格permno对齐）\n",
        "    4. 分离summary指标和时间序列数据\n",
        "    \"\"\"\n",
        "    if window_sizes is None:\n",
        "        window_sizes = [5, 21, 252, 512]\n",
        "    if model_names is None:\n",
        "        model_names = [\"chronos mini\"]\n",
        "\n",
        "    print(\"Starting Daily Rebalance Portfolio Backtesting Simulation\")\n",
        "\n",
        "    # 初始化回测器\n",
        "    backtester = PortfolioBacktester()\n",
        "\n",
        "    # 加载数据\n",
        "    datasets = load_datasets(npz_path)\n",
        "\n",
        "    # 存储结果\n",
        "    summary_results = []  # 仅存储summary指标\n",
        "    daily_series_data = []  # 存储日度时间序列\n",
        "    pred_rows = []          # 存所有预测\n",
        "\n",
        "    # 权重方案\n",
        "    WEIGHT_SCHEMES = [\"VW\", \"EW\"]\n",
        "\n",
        "    for window in window_sizes:\n",
        "        print(f\"Processing window size: {window}\")\n",
        "\n",
        "        # 加载测试数据\n",
        "        X_test = datasets[f\"X_test_{window}\"]\n",
        "        y_test = datasets[f\"y_test_{window}\"]\n",
        "        meta_test_dict = datasets[f\"meta_test_{window}\"].item()\n",
        "        meta_test = pd.DataFrame.from_dict(meta_test_dict)\n",
        "\n",
        "        # 提取所需信息\n",
        "        permnos_test = meta_test[\"PERMNO\"].values\n",
        "        meta_test[\"signal_date\"]  = pd.to_datetime(meta_test[\"date\"])\n",
        "        meta_test[\"ret_date\"]     = pd.to_datetime(meta_test[\"ret_date\"])\n",
        "        market_caps = meta_test.get(\"MKTCAP\", np.ones(len(permnos_test)))\n",
        "\n",
        "        # 确保日期列转换为datetime类型\n",
        "        meta_test['date'] = pd.to_datetime(meta_test[\"date\"])\n",
        "        dates_test = meta_test['signal_date']\n",
        "\n",
        "        for model_name in model_names:\n",
        "            for scheme in WEIGHT_SCHEMES:\n",
        "                all_y_true   = []\n",
        "                all_y_pred   = []\n",
        "                all_permnos  = []\n",
        "                all_meta     = []\n",
        "                print(f\"  Model: {model_name}, Scheme: {scheme}\")\n",
        "\n",
        "                            # 为每个组合类型单独存储数据\n",
        "                portfolio_daily_data = {\n",
        "                'long_only': {'returns': [], 'turnovers': [], 'dates': []},\n",
        "                'short_only': {'returns': [], 'turnovers': [], 'dates': []},\n",
        "                'long_short': {'returns': [], 'turnovers': [], 'dates': []}\n",
        "            }\n",
        "\n",
        "                # 存储上一期权重（为每个组合单独维护）\n",
        "                prev_portfolio_data = {'long_only': None, 'short_only': None, 'long_short': None}\n",
        "\n",
        "                # === 日频信号缓冲 ===\n",
        "                signals_buf = {}   # {datetime: DataFrame}\n",
        "\n",
        "                # 按年份循环进行Chronos预测\n",
        "                for year in range(start_year, min(end_year + 1, 2025)):\n",
        "                    print(f\"  Processing year: {year}\")\n",
        "\n",
        "                    # 筛选当年数据\n",
        "                    year_mask = (dates_test.dt.year == year)\n",
        "                    if not np.any(year_mask):\n",
        "                        continue\n",
        "\n",
        "                    X_year = X_test[year_mask]\n",
        "                    y_year = y_test[year_mask]\n",
        "                    permnos_year = permnos_test[year_mask]\n",
        "                    market_caps_year = market_caps[year_mask]\n",
        "                    dates_year = dates_test[year_mask]\n",
        "                    ret_dates_year = meta_test.loc[year_mask, 'ret_date'].values\n",
        "\n",
        "                    # 使用Chronos进行预测\n",
        "                    batch_size = get_batch_size(window)\n",
        "                    predictions_year = chronos_rolling_prediction(\n",
        "                        pipeline=pipeline,\n",
        "                        X_data=X_year,\n",
        "                        batch_size=batch_size,\n",
        "                        prediction_length=1\n",
        "                    )\n",
        "\n",
        "                    # 按日期分组处理（日频预测）\n",
        "                    df_quarter = pd.DataFrame({\n",
        "                        'signal_date': dates_year,\n",
        "                        'ret_date': ret_dates_year,\n",
        "                        'permno': permnos_year,\n",
        "                        'market_cap': market_caps_year,\n",
        "                        'actual_return': y_year,\n",
        "                        'prediction': predictions_year\n",
        "                    })\n",
        "\n",
        "                    # ====== 新增：只在第一个scheme里收集（不重复） ======\n",
        "                    if scheme == 'VW':\n",
        "                        df_q_save = df_quarter[['signal_date','ret_date','permno',\n",
        "                                                'actual_return','prediction','market_cap']].copy()\n",
        "                        df_q_save.rename(columns={'actual_return':'y_true',\n",
        "                                                  'prediction':'y_pred'}, inplace=True)\n",
        "                        df_q_save['model']  = model_name\n",
        "                        df_q_save['window'] = window\n",
        "                        pred_rows.append(df_q_save)\n",
        "                    # =====================================================\n",
        "\n",
        "                    # —— 用于方式1整体区间计算\n",
        "                    all_y_true.append(df_quarter['actual_return'].values)\n",
        "                    all_y_pred.append(df_quarter['prediction'].values)\n",
        "                    all_permnos.append(df_quarter['permno'].values)\n",
        "                    all_meta.append(meta_test.loc[year_mask, :])\n",
        "\n",
        "\n",
        "                    # 按日期循环（T+1建仓逻辑）\n",
        "                    for signal_date, sig_grp in df_quarter.groupby('signal_date'):\n",
        "\n",
        "                        # (1) 本日信号：只算出来，先放缓存，不建仓\n",
        "                        daily_signals = (\n",
        "                            sig_grp.groupby('permno')['prediction'].mean()        # 本日信号\n",
        "                                  .to_frame('prediction')\n",
        "                                  .join(sig_grp.groupby('permno')['market_cap'].mean())\n",
        "                        )\n",
        "                        signals_buf[signal_date] = daily_signals\n",
        "\n",
        "                        # (2) 必须等到 **下一日** 才用\"昨日信号\"建仓\n",
        "                        prev_date = signal_date - pd.tseries.offsets.BDay(1)\n",
        "                        if prev_date not in signals_buf:\n",
        "                            continue                             # 第一日无昨日信号 → 不交易\n",
        "\n",
        "                        sigs = signals_buf.pop(prev_date)       # ← 取出昨日的信号 DataFrame\n",
        "\n",
        "                        # (3) 用今日真实收益结算（ret_date == signal_date）\n",
        "                        ret_grp = df_quarter[df_quarter['ret_date'] == signal_date]\n",
        "                        if len(ret_grp) == 0:\n",
        "                            continue\n",
        "\n",
        "                        daily_actual_returns = (\n",
        "                            ret_grp.groupby('permno')['actual_return']\n",
        "                                   .mean()                                      # 日度收益直接取均值\n",
        "                                   .reindex(sigs.index, fill_value=0)          # 与信号股票全集对齐\n",
        "                                   .values\n",
        "                        )\n",
        "                        daily_permnos = sigs.index.values\n",
        "\n",
        "\n",
        "                        # (4) 生成 3 组权重\n",
        "                        portfolios_data = backtester.create_portfolios_with_permno_tracking(\n",
        "                            signals      = sigs['prediction'].values,\n",
        "                            market_caps  = sigs['market_cap'].values,\n",
        "                            permnos      = daily_permnos,\n",
        "                            weight_scheme= scheme\n",
        "                        )\n",
        "\n",
        "                        # 为每个组合类型单独处理\n",
        "                        for portfolio_type in ['long_only', 'short_only', 'long_short']:\n",
        "                            portfolio_info = portfolios_data[portfolio_type]\n",
        "\n",
        "                            # 计算严格对齐的组合收益\n",
        "                            portfolio_return, aligned_returns = backtester.calculate_aligned_portfolio_return(\n",
        "                                portfolio_weights=portfolio_info['weights'],\n",
        "                                portfolio_permnos=portfolio_info['permnos'],\n",
        "                                actual_returns=daily_actual_returns,\n",
        "                                actual_permnos=daily_permnos\n",
        "                            )\n",
        "\n",
        "                            if prev_portfolio_data[portfolio_type] is not None:\n",
        "                                # 1) 把昨日 & 今日权重都变成 Series，index=PERMNO\n",
        "                                prev_w_ser = pd.Series(\n",
        "                                    prev_portfolio_data[portfolio_type]['weights'],\n",
        "                                    index=prev_portfolio_data[portfolio_type]['permnos']\n",
        "                                )\n",
        "                                cur_w_ser = pd.Series(\n",
        "                                    portfolio_info['weights'],\n",
        "                                    index=portfolio_info['permnos']\n",
        "                                )\n",
        "\n",
        "                                # 2) 昨日真实收益向量也先做成 Series\n",
        "                                prev_r_ser = pd.Series(\n",
        "                                    prev_portfolio_data[portfolio_type]['aligned_returns'],\n",
        "                                    index=prev_portfolio_data[portfolio_type]['permnos']\n",
        "                                )\n",
        "\n",
        "                                # 3) 用 .reindex() 把\"昨日权重/收益\"补到与今日股票全集一致，缺位=0\n",
        "                                aligned_prev_w = prev_w_ser.reindex(cur_w_ser.index, fill_value=0).values\n",
        "                                aligned_prev_r = prev_r_ser.reindex(cur_w_ser.index, fill_value=0).values\n",
        "\n",
        "                                # 4) 今日目标权重 & 今日真实收益已天然对应 cur_w_ser.index\n",
        "                                aligned_cur_w = cur_w_ser.values\n",
        "\n",
        "                                # 5) 现在三条向量维度完全一致，可以安全算换手率\n",
        "                                turnover = backtester.calc_turnover(\n",
        "                                    w_t  = aligned_prev_w,\n",
        "                                    r_t  = aligned_prev_r,\n",
        "                                    w_tp1= aligned_cur_w\n",
        "                                )\n",
        "                            else:\n",
        "                                # 首日 / 上一期空仓 —— 全仓买入\n",
        "                                turnover = np.sum(np.abs(portfolio_info['weights']))\n",
        "\n",
        "\n",
        "                            # 存储数据\n",
        "                            portfolio_daily_data[portfolio_type]['returns'].append(portfolio_return)\n",
        "                            portfolio_daily_data[portfolio_type]['turnovers'].append(turnover)\n",
        "                            portfolio_daily_data[portfolio_type]['dates'].append(signal_date)\n",
        "\n",
        "                            # 更新上一期组合数据\n",
        "                            prev_portfolio_data[portfolio_type] = {\n",
        "                                'weights'        : portfolio_info['weights'],\n",
        "                                'permnos'        : portfolio_info['permnos'],\n",
        "                                'aligned_returns': aligned_returns\n",
        "                            }\n",
        "\n",
        "                # 计算最终指标并存储结果\n",
        "                for portfolio_type in ['long_only', 'short_only', 'long_short']:\n",
        "                    portfolio_data = portfolio_daily_data[portfolio_type]\n",
        "\n",
        "                    if len(portfolio_data['returns']) > 0:\n",
        "                        # 计算summary指标（日度年化）\n",
        "                        metrics = backtester.calculate_metrics(\n",
        "                            returns=portfolio_data['returns'],\n",
        "                            turnover_series=portfolio_data['turnovers']\n",
        "                        )\n",
        "\n",
        "                        # ---- 5/10/20/30/40 bps 逐档计算 ----\n",
        "                        rets = np.array(portfolio_data['returns'])\n",
        "                        tovs = np.array(portfolio_data['turnovers'])\n",
        "\n",
        "                        for tc in TC_GRID:\n",
        "                            tag = TC_TAG[tc]\n",
        "                            adj = rets - tovs * tc\n",
        "\n",
        "                            ann_ret = adj.mean() * 252\n",
        "                            ann_vol = adj.std(ddof=1) * np.sqrt(252)\n",
        "                            sharpe  = ann_ret / ann_vol if ann_vol > 0 else 0\n",
        "\n",
        "                            cum_adj = np.cumprod(1 + adj)\n",
        "                            mdd = ((cum_adj - np.maximum.accumulate(cum_adj)) /\n",
        "                                   np.maximum.accumulate(cum_adj)).min()\n",
        "\n",
        "                            metrics[f'{tag}_annual_return'] = ann_ret\n",
        "                            metrics[f'{tag}_annual_vol']    = ann_vol\n",
        "                            metrics[f'{tag}_sharpe']        = sharpe\n",
        "                            metrics[f'{tag}_max_drawdown']  = mdd\n",
        "\n",
        "                        # 存储summary结果 - 修复问题2：使用小写统一列名\n",
        "                        summary_results.append({\n",
        "                            'scheme': scheme,\n",
        "                            'model': model_name,\n",
        "                            'window': window,\n",
        "                            'portfolio_type': portfolio_type,\n",
        "                            **metrics\n",
        "                        })\n",
        "\n",
        "                        # ====== 先一次性算好序列 ======\n",
        "                        rets_arr = np.array(portfolio_data['returns'])\n",
        "                        tovs_arr = np.array(portfolio_data['turnovers'])\n",
        "                        cum_no_tc = np.log1p(rets_arr).cumsum()\n",
        "\n",
        "                        tc_ret_dict = {}\n",
        "                        tc_cum_dict = {}\n",
        "                        for tc in TC_GRID:\n",
        "                            tag = TC_TAG[tc]\n",
        "                            r = rets_arr - tovs_arr * tc\n",
        "                            tc_ret_dict[tag] = r\n",
        "                            tc_cum_dict[tag] = np.log1p(r).cumsum()\n",
        "\n",
        "                        # ====== 再逐日写入 ======\n",
        "                        for i, date in enumerate(portfolio_data['dates']):\n",
        "                            row = {\n",
        "                                'scheme'        : scheme,\n",
        "                                'model'         : model_name,\n",
        "                                'window'        : window,\n",
        "                                'portfolio_type': portfolio_type,\n",
        "                                'date'          : str(date),\n",
        "                                'return'        : rets_arr[i],\n",
        "                                'turnover'      : tovs_arr[i],\n",
        "                                'cumulative'    : cum_no_tc[i],\n",
        "                            }\n",
        "                            for tag in TC_TAG.values():\n",
        "                                row[f'{tag}_return']     = tc_ret_dict[tag][i]\n",
        "                                row[f'{tag}_cumulative'] = tc_cum_dict[tag][i]\n",
        "\n",
        "                            daily_series_data.append(row)\n",
        "\n",
        "                # 在完成当前scheme的所有portfolio_type处理后计算整体回归指标\n",
        "                if scheme == \"VW\" and len(all_y_true) > 0:\n",
        "                    y_all    = np.concatenate(all_y_true)\n",
        "                    yhat_all = np.concatenate(all_y_pred)\n",
        "                    perm_all = np.concatenate(all_permnos)\n",
        "                    meta_all = pd.concat(all_meta, ignore_index=True)\n",
        "\n",
        "                    k = X_test.shape[1]  # 自变量个数（同一 window 下固定）\n",
        "\n",
        "                    m1_metrics = overall_interval_metrics_method1(\n",
        "                        y_all, yhat_all, k,\n",
        "                        permnos_all=perm_all,\n",
        "                        meta_all=meta_all\n",
        "                    )\n",
        "\n",
        "                    # ==== 计算横截面 RankIC ====\n",
        "                    # pred_rows 里已经累计了每支股票每天的预测和真实收益\n",
        "                    full_pred_df = pd.concat(pred_rows, ignore_index=True)\n",
        "                    mean_ic, t_ic, pos_ic, _ = calc_ic_daily(full_pred_df, method='spearman')\n",
        "                    m1_metrics['RankIC_mean']  = mean_ic\n",
        "                    m1_metrics['RankIC_t']     = t_ic\n",
        "                    m1_metrics['RankIC_pos%']  = pos_ic\n",
        "\n",
        "                    save_metrics(m1_metrics, name=model_name, window=window,\n",
        "                        path=\"/content/drive/MyDrive/chronos_t5_mini_project_portfolio/chronos_results/portfolio_metrics.csv\")\n",
        "\n",
        "    # 创建DataFrame并确保所有TC指标都存在\n",
        "    summary_df = pd.DataFrame(summary_results)\n",
        "    daily_df = pd.DataFrame(daily_series_data) if daily_series_data else pd.DataFrame()\n",
        "\n",
        "    # 填充可能的NaN值\n",
        "    tc_columns = [c for c in summary_df.columns if c.startswith('tc')]\n",
        "    summary_df[tc_columns] = summary_df[tc_columns].fillna(0.0)\n",
        "\n",
        "    # 按 scheme 拆分保存到四个文件\n",
        "    def save_split_by_scheme(df, base_filename):\n",
        "        \"\"\"按scheme拆分保存文件的辅助函数\"\"\"\n",
        "        if df.empty:\n",
        "            print(f\"Warning: DataFrame is empty, skipping save for {base_filename}\")\n",
        "            return None, None\n",
        "\n",
        "        vw_df = df[df['scheme'] == 'VW']\n",
        "        ew_df = df[df['scheme'] == 'EW']\n",
        "\n",
        "        vw_filename = f\"/content/drive/MyDrive/chronos_t5_mini_project_portfolio/chronos_results/{base_filename}_VW.csv\"\n",
        "        ew_filename = f\"/content/drive/MyDrive/chronos_t5_mini_project_portfolio/chronos_results/{base_filename}_EW.csv\"\n",
        "\n",
        "        vw_df.to_csv(vw_filename, index=False)\n",
        "        ew_df.to_csv(ew_filename, index=False)\n",
        "\n",
        "        print(f\"VW results saved to {vw_filename}\")\n",
        "        print(f\"EW results saved to {ew_filename}\")\n",
        "\n",
        "        return vw_filename, ew_filename\n",
        "\n",
        "    # 保存拆分后的文件\n",
        "    save_split_by_scheme(summary_df, \"portfolio_results_daily_rebalance\")\n",
        "\n",
        "    if not daily_df.empty:\n",
        "        save_split_by_scheme(daily_df, \"portfolio_daily_series\")\n",
        "\n",
        "    # ===== 保存预测表 =====\n",
        "    if pred_rows:\n",
        "        pred_df = pd.concat(pred_rows, ignore_index=True)\n",
        "        pred_df.to_csv(\"/content/drive/MyDrive/chronos_t5_mini_project_portfolio/chronos_results/predictions_daily.csv\", index=False)\n",
        "        print(f\"Saved {len(pred_df)} prediction rows to predictions_daily.csv\")\n",
        "\n",
        "    print(f\"Generated {len(summary_results)} portfolio summary records\")\n",
        "    print(f\"Generated {len(daily_series_data)} daily series records\")\n",
        "\n",
        "    # 确保返回正确的DataFrame格式\n",
        "    return summary_df, daily_df, backtester\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "zU80Uqy7MjXg"
      },
      "outputs": [],
      "source": [
        "# ========== 结果保存 ==========\n",
        "\n",
        "import os\n",
        "\n",
        "# 创建结果目录\n",
        "results_dir = \"/content/drive/MyDrive/chronos_t5_mini_project_portfolio/chronos_results\"\n",
        "figures_dir = \"/content/drive/MyDrive/chronos_t5_mini_project_portfolio/chronos_figures\"\n",
        "\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "os.makedirs(figures_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OXrZ9L6Y9MZ",
        "outputId": "6f55be74-88a6-4908-c741-9a9197ecff57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Chronos T5-Mini Portfolio Backtesting...\n",
            "Starting Daily Rebalance Portfolio Backtesting Simulation\n",
            "Data loaded successfully! Keys: ['X_train_5', 'y_train_5', 'meta_train_5', 'market_caps_train_5', 'X_test_5', 'y_test_5', 'meta_test_5', 'market_caps_test_5', 'X_train_21', 'y_train_21', 'meta_train_21', 'market_caps_train_21', 'X_test_21', 'y_test_21', 'meta_test_21', 'market_caps_test_21', 'X_train_252', 'y_train_252', 'meta_train_252', 'market_caps_train_252', 'X_test_252', 'y_test_252', 'meta_test_252', 'market_caps_test_252', 'X_train_512', 'y_train_512', 'meta_train_512', 'market_caps_train_512', 'X_test_512', 'y_test_512', 'meta_test_512', 'market_caps_test_512']\n",
            "Available datasets: 32 files\n",
            "Processing window size: 5\n",
            "  Model: chronos mini, Scheme: VW\n",
            "  Processing year: 2016\n",
            "Running Chronos prediction on 12475 samples with batch size 1024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 13/13 [00:02<00:00,  4.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2017\n",
            "Running Chronos prediction on 12434 samples with batch size 1024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 13/13 [00:01<00:00,  6.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2018\n",
            "Running Chronos prediction on 12326 samples with batch size 1024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 13/13 [00:01<00:00,  7.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2019\n",
            "Running Chronos prediction on 12488 samples with batch size 1024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 13/13 [00:01<00:00,  7.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2020\n",
            "Running Chronos prediction on 11699 samples with batch size 1024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 12/12 [00:01<00:00,  7.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2021\n",
            "Running Chronos prediction on 12447 samples with batch size 1024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 13/13 [00:01<00:00,  7.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2022\n",
            "Running Chronos prediction on 12220 samples with batch size 1024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 12/12 [00:01<00:00,  6.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2023\n",
            "Running Chronos prediction on 12367 samples with batch size 1024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 13/13 [00:01<00:00,  7.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2024\n",
            "Running Chronos prediction on 12394 samples with batch size 1024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 13/13 [00:01<00:00,  7.80it/s]\n",
            "/tmp/ipython-input-486884232.py:54: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda g: g['y_pred'].corr(g['y_true'], method=method))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics saved for chronos mini_w5 to /content/drive/MyDrive/chronos_t5_mini_project_portfolio/chronos_results/portfolio_metrics.csv\n",
            "  Model: chronos mini, Scheme: EW\n",
            "  Processing year: 2016\n",
            "Running Chronos prediction on 12475 samples with batch size 1024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 13/13 [00:01<00:00,  7.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2017\n",
            "Running Chronos prediction on 12434 samples with batch size 1024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 13/13 [00:01<00:00,  6.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2018\n",
            "Running Chronos prediction on 12326 samples with batch size 1024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 13/13 [00:01<00:00,  6.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2019\n",
            "Running Chronos prediction on 12488 samples with batch size 1024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 13/13 [00:01<00:00,  7.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2020\n",
            "Running Chronos prediction on 11699 samples with batch size 1024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 12/12 [00:01<00:00,  7.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2021\n",
            "Running Chronos prediction on 12447 samples with batch size 1024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 13/13 [00:01<00:00,  7.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2022\n",
            "Running Chronos prediction on 12220 samples with batch size 1024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 12/12 [00:01<00:00,  6.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2023\n",
            "Running Chronos prediction on 12367 samples with batch size 1024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 13/13 [00:01<00:00,  7.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2024\n",
            "Running Chronos prediction on 12394 samples with batch size 1024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 13/13 [00:01<00:00,  7.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing window size: 21\n",
            "  Model: chronos mini, Scheme: VW\n",
            "  Processing year: 2016\n",
            "Running Chronos prediction on 12475 samples with batch size 256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 49/49 [00:03<00:00, 14.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2017\n",
            "Running Chronos prediction on 12434 samples with batch size 256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 49/49 [00:03<00:00, 14.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2018\n",
            "Running Chronos prediction on 12326 samples with batch size 256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 49/49 [00:02<00:00, 16.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2019\n",
            "Running Chronos prediction on 12488 samples with batch size 256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 49/49 [00:02<00:00, 16.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2020\n",
            "Running Chronos prediction on 11699 samples with batch size 256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 46/46 [00:02<00:00, 15.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2021\n",
            "Running Chronos prediction on 12447 samples with batch size 256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 49/49 [00:02<00:00, 16.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2022\n",
            "Running Chronos prediction on 12220 samples with batch size 256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 48/48 [00:02<00:00, 16.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2023\n",
            "Running Chronos prediction on 12367 samples with batch size 256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 49/49 [00:03<00:00, 14.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2024\n",
            "Running Chronos prediction on 12394 samples with batch size 256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 49/49 [00:03<00:00, 15.94it/s]\n",
            "/tmp/ipython-input-486884232.py:54: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda g: g['y_pred'].corr(g['y_true'], method=method))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics saved for chronos mini_w21 to /content/drive/MyDrive/chronos_t5_mini_project_portfolio/chronos_results/portfolio_metrics.csv\n",
            "  Model: chronos mini, Scheme: EW\n",
            "  Processing year: 2016\n",
            "Running Chronos prediction on 12475 samples with batch size 256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 49/49 [00:03<00:00, 15.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2017\n",
            "Running Chronos prediction on 12434 samples with batch size 256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 49/49 [00:03<00:00, 15.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2018\n",
            "Running Chronos prediction on 12326 samples with batch size 256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 49/49 [00:02<00:00, 16.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2019\n",
            "Running Chronos prediction on 12488 samples with batch size 256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 49/49 [00:03<00:00, 16.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2020\n",
            "Running Chronos prediction on 11699 samples with batch size 256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 46/46 [00:02<00:00, 15.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2021\n",
            "Running Chronos prediction on 12447 samples with batch size 256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 49/49 [00:03<00:00, 15.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2022\n",
            "Running Chronos prediction on 12220 samples with batch size 256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 48/48 [00:02<00:00, 16.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2023\n",
            "Running Chronos prediction on 12367 samples with batch size 256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 49/49 [00:02<00:00, 16.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2024\n",
            "Running Chronos prediction on 12394 samples with batch size 256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 49/49 [00:03<00:00, 15.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing window size: 252\n",
            "  Model: chronos mini, Scheme: VW\n",
            "  Processing year: 2016\n",
            "Running Chronos prediction on 12475 samples with batch size 32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 390/390 [00:27<00:00, 14.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2017\n",
            "Running Chronos prediction on 12434 samples with batch size 32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 389/389 [00:27<00:00, 14.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2018\n",
            "Running Chronos prediction on 12326 samples with batch size 32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 386/386 [00:27<00:00, 14.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2019\n",
            "Running Chronos prediction on 12488 samples with batch size 32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 391/391 [00:27<00:00, 14.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2020\n",
            "Running Chronos prediction on 11699 samples with batch size 32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 366/366 [00:25<00:00, 14.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2021\n",
            "Running Chronos prediction on 12447 samples with batch size 32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 389/389 [00:27<00:00, 14.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2022\n",
            "Running Chronos prediction on 12220 samples with batch size 32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 382/382 [00:26<00:00, 14.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2023\n",
            "Running Chronos prediction on 12367 samples with batch size 32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 387/387 [00:27<00:00, 14.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2024\n",
            "Running Chronos prediction on 12394 samples with batch size 32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 388/388 [00:27<00:00, 14.29it/s]\n",
            "/tmp/ipython-input-486884232.py:54: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda g: g['y_pred'].corr(g['y_true'], method=method))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics saved for chronos mini_w252 to /content/drive/MyDrive/chronos_t5_mini_project_portfolio/chronos_results/portfolio_metrics.csv\n",
            "  Model: chronos mini, Scheme: EW\n",
            "  Processing year: 2016\n",
            "Running Chronos prediction on 12475 samples with batch size 32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 390/390 [00:27<00:00, 14.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2017\n",
            "Running Chronos prediction on 12434 samples with batch size 32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 389/389 [00:27<00:00, 14.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2018\n",
            "Running Chronos prediction on 12326 samples with batch size 32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 386/386 [00:27<00:00, 14.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2019\n",
            "Running Chronos prediction on 12488 samples with batch size 32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 391/391 [00:27<00:00, 14.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2020\n",
            "Running Chronos prediction on 11699 samples with batch size 32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 366/366 [00:25<00:00, 14.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2021\n",
            "Running Chronos prediction on 12447 samples with batch size 32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 389/389 [00:27<00:00, 14.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2022\n",
            "Running Chronos prediction on 12220 samples with batch size 32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 382/382 [00:26<00:00, 14.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2023\n",
            "Running Chronos prediction on 12367 samples with batch size 32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 387/387 [00:27<00:00, 14.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2024\n",
            "Running Chronos prediction on 12394 samples with batch size 32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 388/388 [00:27<00:00, 14.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing window size: 512\n",
            "  Model: chronos mini, Scheme: VW\n",
            "  Processing year: 2016\n",
            "Running Chronos prediction on 12475 samples with batch size 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 780/780 [01:09<00:00, 11.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2017\n",
            "Running Chronos prediction on 12434 samples with batch size 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 778/778 [01:09<00:00, 11.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2018\n",
            "Running Chronos prediction on 12326 samples with batch size 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 771/771 [01:08<00:00, 11.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2019\n",
            "Running Chronos prediction on 12488 samples with batch size 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 781/781 [01:09<00:00, 11.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2020\n",
            "Running Chronos prediction on 11699 samples with batch size 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 732/732 [01:05<00:00, 11.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2021\n",
            "Running Chronos prediction on 12447 samples with batch size 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 778/778 [01:09<00:00, 11.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2022\n",
            "Running Chronos prediction on 12220 samples with batch size 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 764/764 [01:07<00:00, 11.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2023\n",
            "Running Chronos prediction on 12367 samples with batch size 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 773/773 [01:08<00:00, 11.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2024\n",
            "Running Chronos prediction on 12394 samples with batch size 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 775/775 [01:08<00:00, 11.26it/s]\n",
            "/tmp/ipython-input-486884232.py:54: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda g: g['y_pred'].corr(g['y_true'], method=method))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics saved for chronos mini_w512 to /content/drive/MyDrive/chronos_t5_mini_project_portfolio/chronos_results/portfolio_metrics.csv\n",
            "  Model: chronos mini, Scheme: EW\n",
            "  Processing year: 2016\n",
            "Running Chronos prediction on 12475 samples with batch size 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 780/780 [01:09<00:00, 11.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2017\n",
            "Running Chronos prediction on 12434 samples with batch size 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 778/778 [01:09<00:00, 11.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2018\n",
            "Running Chronos prediction on 12326 samples with batch size 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 771/771 [01:08<00:00, 11.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2019\n",
            "Running Chronos prediction on 12488 samples with batch size 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 781/781 [01:09<00:00, 11.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2020\n",
            "Running Chronos prediction on 11699 samples with batch size 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 732/732 [01:05<00:00, 11.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2021\n",
            "Running Chronos prediction on 12447 samples with batch size 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 778/778 [01:09<00:00, 11.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2022\n",
            "Running Chronos prediction on 12220 samples with batch size 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 764/764 [01:08<00:00, 11.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2023\n",
            "Running Chronos prediction on 12367 samples with batch size 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 773/773 [01:08<00:00, 11.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processing year: 2024\n",
            "Running Chronos prediction on 12394 samples with batch size 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chronos Prediction: 100%|██████████| 775/775 [01:08<00:00, 11.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VW results saved to /content/drive/MyDrive/chronos_t5_mini_project_portfolio/chronos_results/portfolio_results_daily_rebalance_VW.csv\n",
            "EW results saved to /content/drive/MyDrive/chronos_t5_mini_project_portfolio/chronos_results/portfolio_results_daily_rebalance_EW.csv\n",
            "VW results saved to /content/drive/MyDrive/chronos_t5_mini_project_portfolio/chronos_results/portfolio_daily_series_VW.csv\n",
            "EW results saved to /content/drive/MyDrive/chronos_t5_mini_project_portfolio/chronos_results/portfolio_daily_series_EW.csv\n",
            "Saved 443400 prediction rows to predictions_daily.csv\n",
            "Generated 24 portfolio summary records\n",
            "Generated 52272 daily series records\n",
            "\\n============================================================\n",
            "CHRONOS T5-Mini PORTFOLIO BACKTESTING RESULTS\n",
            "============================================================\n",
            "\\nSummary Results:\n",
            "   scheme         model  window portfolio_type  annual_return  annual_vol  \\\n",
            "0      VW  chronos mini       5      long_only         0.1282      0.1947   \n",
            "1      VW  chronos mini       5     short_only        -0.1711      0.2049   \n",
            "2      VW  chronos mini       5     long_short        -0.0430      0.2105   \n",
            "3      EW  chronos mini       5      long_only         0.1397      0.1993   \n",
            "4      EW  chronos mini       5     short_only        -0.1593      0.2122   \n",
            "5      EW  chronos mini       5     long_short        -0.0196      0.2030   \n",
            "6      VW  chronos mini      21      long_only         0.1120      0.1965   \n",
            "7      VW  chronos mini      21     short_only        -0.1297      0.2051   \n",
            "8      VW  chronos mini      21     long_short        -0.0177      0.2083   \n",
            "9      EW  chronos mini      21      long_only         0.1472      0.2013   \n",
            "10     EW  chronos mini      21     short_only        -0.0765      0.2162   \n",
            "11     EW  chronos mini      21     long_short         0.0707      0.1935   \n",
            "12     VW  chronos mini     252      long_only         0.0911      0.2028   \n",
            "13     VW  chronos mini     252     short_only        -0.2093      0.2156   \n",
            "14     VW  chronos mini     252     long_short        -0.1182      0.2040   \n",
            "15     EW  chronos mini     252      long_only         0.1559      0.1984   \n",
            "16     EW  chronos mini     252     short_only        -0.1724      0.2209   \n",
            "17     EW  chronos mini     252     long_short        -0.0165      0.1766   \n",
            "18     VW  chronos mini     512      long_only         0.1401      0.2107   \n",
            "19     VW  chronos mini     512     short_only        -0.2397      0.2019   \n",
            "20     VW  chronos mini     512     long_short        -0.0995      0.2054   \n",
            "21     EW  chronos mini     512      long_only         0.1251      0.2046   \n",
            "22     EW  chronos mini     512     short_only        -0.1613      0.2137   \n",
            "23     EW  chronos mini     512     long_short        -0.0362      0.1725   \n",
            "\n",
            "    sharpe  max_drawdown  max_1d_loss  avg_turnover  ...  tc20_sharpe  \\\n",
            "0   0.6581        0.2530      -0.0521        1.7019  ...      -3.7343   \n",
            "1  -0.8354        0.8348      -0.0533        1.7535  ...      -5.1405   \n",
            "2  -0.2042        0.5269      -0.0623        3.4557  ...      -8.4413   \n",
            "3   0.7010        0.3141      -0.0619        1.5630  ...      -3.2518   \n",
            "4  -0.7508        0.8137      -0.0623        1.6296  ...      -4.6204   \n",
            "5  -0.0966        0.5001      -0.0736        3.1933  ...      -7.9997   \n",
            "6   0.5700        0.3566      -0.0506        1.6408  ...      -3.6335   \n",
            "7  -0.6324        0.7686      -0.0534        1.6667  ...      -4.7103   \n",
            "8  -0.0851        0.5514      -0.0724        3.3075  ...      -8.0510   \n",
            "9   0.7316        0.2454      -0.0525        1.4804  ...      -2.9741   \n",
            "10 -0.3539        0.6543      -0.0500        1.5017  ...      -3.8552   \n",
            "11  0.3657        0.4657      -0.0724        2.9820  ...      -7.3913   \n",
            "12  0.4491        0.4252      -0.0471        1.6898  ...      -3.7421   \n",
            "13 -0.9707        0.8872      -0.0734        1.7109  ...      -4.9583   \n",
            "14 -0.5795        0.7741      -0.0556        3.4009  ...      -8.9546   \n",
            "15  0.7855        0.2330      -0.0512        1.5584  ...      -3.1658   \n",
            "16 -0.7806        0.8532      -0.0734        1.5240  ...      -4.2632   \n",
            "17 -0.0936        0.5177      -0.0611        3.0823  ...      -8.8690   \n",
            "18  0.6651        0.3045      -0.0537        1.6979  ...      -3.3940   \n",
            "19 -1.1868        0.9018      -0.0546        1.6916  ...      -5.3883   \n",
            "20 -0.4848        0.7374      -0.0660        3.3894  ...      -8.7947   \n",
            "21  0.6112        0.3011      -0.0477        1.5585  ...      -3.2212   \n",
            "22 -0.7546        0.8460      -0.0555        1.5461  ...      -4.3960   \n",
            "23 -0.2098        0.5680      -0.0542        3.1044  ...      -9.2522   \n",
            "\n",
            "    tc20_max_drawdown  tc30_annual_return  tc30_annual_vol  tc30_sharpe  \\\n",
            "0             -0.9985             -1.1585           0.1961      -5.9090   \n",
            "1             -0.9999             -1.4968           0.2057      -7.2771   \n",
            "2             -1.0000             -2.6555           0.2125     -12.4950   \n",
            "3             -0.9969             -1.0419           0.1995      -5.2217   \n",
            "4             -0.9998             -1.3913           0.2125      -6.5479   \n",
            "5             -1.0000             -2.4338           0.2045     -11.9025   \n",
            "6             -0.9983             -1.1284           0.1973      -5.7183   \n",
            "7             -0.9998             -1.3898           0.2066      -6.7258   \n",
            "8             -1.0000             -2.5182           0.2105     -11.9625   \n",
            "9             -0.9953             -0.9719           0.2016      -4.8200   \n",
            "10            -0.9994             -1.2118           0.2164      -5.5999   \n",
            "11            -1.0000             -2.1837           0.1945     -11.2298   \n",
            "12            -0.9988             -1.1865           0.2039      -5.8193   \n",
            "13            -0.9999             -1.5027           0.2167      -6.9355   \n",
            "14            -1.0000             -2.6893           0.2056     -13.0775   \n",
            "15            -0.9964             -1.0223           0.1993      -5.1293   \n",
            "16            -0.9998             -1.3246           0.2207      -6.0017   \n",
            "17            -1.0000             -2.3468           0.1778     -13.2022   \n",
            "18            -0.9983             -1.1435           0.2113      -5.4116   \n",
            "19            -0.9999             -1.5185           0.2034      -7.4641   \n",
            "20            -1.0000             -2.6619           0.2064     -12.8976   \n",
            "21            -0.9973             -1.0532           0.2054      -5.1266   \n",
            "22            -0.9998             -1.3301           0.2143      -6.2072   \n",
            "23            -1.0000             -2.3831           0.1738     -13.7083   \n",
            "\n",
            "    tc30_max_drawdown  tc40_annual_return  tc40_annual_vol  tc40_sharpe  \\\n",
            "0             -1.0000             -1.5874           0.1970      -8.0591   \n",
            "1             -1.0000             -1.9387           0.2064      -9.3953   \n",
            "2             -1.0000             -3.5263           0.2141     -16.4740   \n",
            "3             -0.9999             -1.4358           0.1999      -7.1810   \n",
            "4             -1.0000             -1.8020           0.2129      -8.4653   \n",
            "5             -1.0000             -3.2385           0.2056     -15.7475   \n",
            "6             -1.0000             -1.5419           0.1982      -7.7803   \n",
            "7             -1.0000             -1.8098           0.2076      -8.7163   \n",
            "8             -1.0000             -3.3517           0.2123     -15.7899   \n",
            "9             -0.9998             -1.3450           0.2021      -6.6553   \n",
            "10            -1.0000             -1.5902           0.2168      -7.3355   \n",
            "11            -1.0000             -2.9351           0.1955     -15.0142   \n",
            "12            -1.0000             -1.6123           0.2048      -7.8742   \n",
            "13            -1.0000             -1.9339           0.2174      -8.8945   \n",
            "14            -1.0000             -3.5463           0.2071     -17.1198   \n",
            "15            -0.9999             -1.4150           0.1999      -7.0791   \n",
            "16            -1.0000             -1.7086           0.2209      -7.7336   \n",
            "17            -1.0000             -3.1235           0.1788     -17.4657   \n",
            "18            -1.0000             -1.5713           0.2120      -7.4121   \n",
            "19            -1.0000             -1.9448           0.2044      -9.5138   \n",
            "20            -1.0000             -3.5160           0.2077     -16.9279   \n",
            "21            -0.9999             -1.4459           0.2060      -7.0196   \n",
            "22            -1.0000             -1.7197           0.2148      -8.0070   \n",
            "23            -1.0000             -3.1655           0.1750     -18.0854   \n",
            "\n",
            "    tc40_max_drawdown  \n",
            "0                -1.0  \n",
            "1                -1.0  \n",
            "2                -1.0  \n",
            "3                -1.0  \n",
            "4                -1.0  \n",
            "5                -1.0  \n",
            "6                -1.0  \n",
            "7                -1.0  \n",
            "8                -1.0  \n",
            "9                -1.0  \n",
            "10               -1.0  \n",
            "11               -1.0  \n",
            "12               -1.0  \n",
            "13               -1.0  \n",
            "14               -1.0  \n",
            "15               -1.0  \n",
            "16               -1.0  \n",
            "17               -1.0  \n",
            "18               -1.0  \n",
            "19               -1.0  \n",
            "20               -1.0  \n",
            "21               -1.0  \n",
            "22               -1.0  \n",
            "23               -1.0  \n",
            "\n",
            "[24 rows x 32 columns]\n"
          ]
        }
      ],
      "source": [
        "# ========== 运行回测 ==========\n",
        "\n",
        "print(\"Starting Chronos T5-Mini Portfolio Backtesting...\")\n",
        "\n",
        "# 运行回测，注意END_YEAR应该是2024而不是2023\n",
        "summary_results, daily_series, backtester = run_chronos_portfolio_backtest(\n",
        "    start_year=START_YEAR,\n",
        "    end_year=END_YEAR,\n",
        "    window_sizes=WINDOW_SIZES\n",
        ")\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*60)\n",
        "print(\"CHRONOS T5-Mini PORTFOLIO BACKTESTING RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 显示汇总结果\n",
        "print(\"\\\\nSummary Results:\")\n",
        "print(summary_results.round(4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGyZHPvAY9Ma",
        "outputId": "fb6cb1aa-37f8-427f-940a-e7c64cd43225"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Saved] 5_factor_analysis_VW_gross.csv \n",
            "[Saved] 5_factor_analysis_VW_net.csv \n",
            "[Saved] 5_factor_analysis_EW_gross.csv \n",
            "[Saved] 5_factor_analysis_EW_net.csv \n"
          ]
        }
      ],
      "source": [
        "# ---------- 5因子回归主函数 -----------\n",
        "def run_factor_regression(port_ret, factors, use_excess=True):\n",
        "    df = pd.concat([port_ret, factors], axis=1, join='inner').dropna()\n",
        "    df.columns = ['ret'] + list(factors.columns)\n",
        "\n",
        "    if use_excess:\n",
        "        y = df['ret'].values\n",
        "    else:\n",
        "        y = df['ret'].values - df['rf'].values\n",
        "\n",
        "    X = df[['mktrf','smb','hml','rmw','cma','umd']].values\n",
        "    X = sm.add_constant(X)\n",
        "\n",
        "    model = sm.OLS(y, X)\n",
        "    res = model.fit()\n",
        "    alpha = res.params[0]          # 日度 α\n",
        "    resid_std = res.resid.std(ddof=1)\n",
        "\n",
        "    ir_daily = alpha / resid_std          # 日度 IR\n",
        "    ir_annual = ir_daily * np.sqrt(252)   # 年化 IR\n",
        "\n",
        "\n",
        "    y_hat = np.asarray(res.fittedvalues)  # 改这里\n",
        "\n",
        "    out = {\n",
        "        'N_obs'            : len(y),\n",
        "        'alpha_daily'      : alpha,\n",
        "        'alpha_annual'     : alpha*252,\n",
        "        't_alpha'          : res.tvalues[0],\n",
        "        'IR_daily'         : ir_daily,\n",
        "        'IR_annual'        : ir_annual,\n",
        "        'R2_zero'          : r2_zero(y, y_hat),\n",
        "    }\n",
        "\n",
        "    factor_names = ['MKT','SMB','HML','RMW','CMA','UMD']\n",
        "    for i, fac in enumerate(factor_names, start=1):\n",
        "        out[f'beta_{fac}'] = res.params[i]\n",
        "        out[f't_{fac}']    = res.tvalues[i]\n",
        "\n",
        "    return out\n",
        "\n",
        "# ---------- 3. 批量跑（EW/VW、三种组合） ----------\n",
        "def batch_factor_analysis(\n",
        "    daily_df: pd.DataFrame,\n",
        "    factors_path: str,\n",
        "    scheme: str,\n",
        "    tc_levels=(0, 5, 10, 20, 40),\n",
        "    portfolio_types=('long_only','short_only','long_short'),\n",
        "    model_filter=None,\n",
        "    window_filter=None,\n",
        "    gross_only=False,            # ← True 时只算 tc=0\n",
        "    out_dir='/content/drive/MyDrive/chronos_t5_mini_project_portfolio/chronos_results/factor_IR_results',\n",
        "):\n",
        "    \"\"\"\n",
        "    生成一张含 IR 的 CSV。\n",
        "    gross_only=True  → 仅 tc=0；False → tc_levels 全部。\n",
        "    \"\"\"\n",
        "    import os\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    # 读取因子\n",
        "    fac = (pd.read_csv(factors_path, parse_dates=['date'])\n",
        "             .set_index('date')\n",
        "             .sort_index())\n",
        "\n",
        "    # 过滤组合表\n",
        "    sub = daily_df[daily_df['scheme'] == scheme].copy()\n",
        "    if model_filter is not None:\n",
        "        sub = sub[sub['model'].isin(model_filter)]\n",
        "    if window_filter is not None:\n",
        "        sub = sub[sub['window'].isin(window_filter)]\n",
        "\n",
        "    tc_iter = (0,) if gross_only else tc_levels\n",
        "    results = []\n",
        "\n",
        "    for (model, win, ptype), g in sub.groupby(['model','window','portfolio_type']):\n",
        "        g = g.sort_values('date').set_index(pd.to_datetime(g['date']))\n",
        "\n",
        "        for tc in tc_iter:\n",
        "            col = 'return' if tc == 0 else f'tc{tc}_return'\n",
        "            if col not in g.columns:\n",
        "                continue  # 防御\n",
        "            port_ret = g[col]\n",
        "            stats = run_factor_regression(port_ret, fac, use_excess=True)\n",
        "            stats.update({\n",
        "                'scheme'        : scheme,\n",
        "                'model'         : model,\n",
        "                'window'        : win,\n",
        "                'portfolio_type': ptype,\n",
        "                'tc_bps'        : tc,\n",
        "            })\n",
        "            results.append(stats)\n",
        "\n",
        "    df_out = pd.DataFrame(results)[[\n",
        "        'scheme','model','window','portfolio_type','tc_bps','N_obs',\n",
        "        'alpha_daily','alpha_annual','t_alpha',\n",
        "        'IR_daily','IR_annual','R2_zero',\n",
        "        'beta_MKT','t_MKT','beta_SMB','t_SMB',\n",
        "        'beta_HML','t_HML','beta_RMW','t_RMW',\n",
        "        'beta_CMA','t_CMA','beta_UMD','t_UMD'\n",
        "    ]]\n",
        "\n",
        "    tag = 'gross' if gross_only else 'net'\n",
        "    fname = f'5_factor_analysis_{scheme}_{tag}.csv'\n",
        "    df_out.to_csv(os.path.join(out_dir, fname), index=False)\n",
        "    print(f'[Saved] {fname} ')\n",
        "    return df_out\n",
        "\n",
        "\n",
        "\n",
        "def run_all_factor_tests(vw_csv=\"/content/drive/MyDrive/chronos_t5_mini_project_portfolio/chronos_results/portfolio_daily_series_VW.csv\",\n",
        "                         ew_csv=\"/content/drive/MyDrive/chronos_t5_mini_project_portfolio/chronos_results/portfolio_daily_series_EW.csv\",\n",
        "                         factor_csv=\"/content/drive/MyDrive/ERP Data/5_Factors_Plus_Momentum.csv\",\n",
        "                         save_dir=\"/content/drive/MyDrive/chronos_t5_mini_project_portfolio/chronos_results\",\n",
        "                         y_is_excess=True,\n",
        "                         hac_lags=5,\n",
        "                         save_txt=True):\n",
        "    vw_df = pd.read_csv(vw_csv)\n",
        "    ew_df = pd.read_csv(ew_csv)\n",
        "\n",
        "    # ----- VW -----\n",
        "    vw_gross = batch_factor_analysis(\n",
        "        vw_df, factor_csv, scheme='VW', gross_only=True)    # 只 tc=0\n",
        "    vw_net   = batch_factor_analysis(\n",
        "        vw_df, factor_csv, scheme='VW', gross_only=False)   # 含 0/5/10/20/40\n",
        "\n",
        "    # ----- EW -----\n",
        "    ew_gross = batch_factor_analysis(\n",
        "        ew_df, factor_csv, scheme='EW', gross_only=True)\n",
        "    ew_net   = batch_factor_analysis(\n",
        "        ew_df, factor_csv, scheme='EW', gross_only=False)\n",
        "\n",
        "    # 想要什么就 return 什么；这里示例全部返回\n",
        "    return vw_gross, vw_net, ew_gross, ew_net\n",
        "\n",
        "\n",
        "vw_gross, vw_net, ew_gross, ew_net = run_all_factor_tests()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hI8RXVQL-BrO",
        "outputId": "610fb9c4-edb6-44dc-c858-29ae3f8dfd05"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-270096965.py:27: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  for _, group in df.groupby([\"scheme\", \"model\", \"window\", \"portfolio_type\"], sort=False):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finish: /content/drive/MyDrive/chronos_t5_mini_project_portfolio/chronos_results/portfolio_daily_series_VW_with_rf.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-270096965.py:27: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  for _, group in df.groupby([\"scheme\", \"model\", \"window\", \"portfolio_type\"], sort=False):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finish: /content/drive/MyDrive/chronos_t5_mini_project_portfolio/chronos_results/portfolio_daily_series_EW_with_rf.csv\n"
          ]
        }
      ],
      "source": [
        "# === 文件路径 ===\n",
        "rf_file = \"/content/drive/MyDrive/ERP Data/CRSP_2016_2024_top50_with_exret.csv\"\n",
        "vw_file = \"/content/drive/MyDrive/chronos_t5_mini_project_portfolio/chronos_results/portfolio_daily_series_VW.csv\"  # 本地路径\n",
        "ew_file = \"/content/drive/MyDrive/chronos_t5_mini_project_portfolio/chronos_results/portfolio_daily_series_EW.csv\"  # 本地路径\n",
        "\n",
        "# === 加载 rf (无风险利率) 数据 ===\n",
        "\n",
        "rf_df = pd.read_csv(rf_file, usecols=[\"date\", \"rf\"])\n",
        "rf_df[\"date\"] = pd.to_datetime(rf_df[\"date\"])\n",
        "rf_dict = dict(zip(rf_df[\"date\"], rf_df[\"rf\"]))  # 转成字典方便查找\n",
        "\n",
        "\n",
        "def adjust_returns_with_rf_grouped(file_path, output_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    # 使用 format='mixed' 或 dayfirst=True 来处理不同的日期格式\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"], format='mixed', dayfirst=True)\n",
        "\n",
        "    # 找出所有 return 列（包括 tc5_return, tc10_return 等）\n",
        "    return_cols = [col for col in df.columns if \"return\" in col and \"cumul\" not in col]\n",
        "\n",
        "    # 强制 portfolio_type 顺序，避免 groupby 自动排序错乱\n",
        "    order = [\"long_only\", \"short_only\", \"long_short\"]\n",
        "    df[\"portfolio_type\"] = pd.Categorical(df[\"portfolio_type\"], categories=order, ordered=True)\n",
        "\n",
        "    df_list = []\n",
        "    # 按策略/模型/窗口分组，分别加 rf 和重新算 cumulative\n",
        "    for _, group in df.groupby([\"scheme\", \"model\", \"window\", \"portfolio_type\"], sort=False):\n",
        "        group = group.sort_values(\"date\").copy()\n",
        "        for col in return_cols:\n",
        "            # 每天的 return 加回 rf\n",
        "            group[col] = group.apply(lambda row: row[col] + rf_dict.get(row[\"date\"], 0), axis=1)\n",
        "\n",
        "            # 找到对应的 cumulative 列（和原表一致命名）\n",
        "            cum_col = col.replace(\"return\", \"cumulative\")\n",
        "            group[cum_col] = np.log1p(group[col]).cumsum()\n",
        "        df_list.append(group)\n",
        "\n",
        "    # 合并结果并按顺序输出\n",
        "    df_new = pd.concat(df_list).sort_values([\"scheme\", \"model\", \"window\", \"portfolio_type\", \"date\"])\n",
        "    df_new.to_csv(output_path, index=False)\n",
        "    print(f\"Finish: {output_path}\")\n",
        "\n",
        "# === 分别处理 VW 和 EW 文件 ===\n",
        "adjust_returns_with_rf_grouped(vw_file, \"/content/drive/MyDrive/chronos_t5_mini_project_portfolio/chronos_results/portfolio_daily_series_VW_with_rf.csv\")\n",
        "adjust_returns_with_rf_grouped(ew_file, \"/content/drive/MyDrive/chronos_t5_mini_project_portfolio/chronos_results/portfolio_daily_series_EW_with_rf.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qc1BDMeX-BrO",
        "outputId": "0c68edde-95e3-4b87-ec72-c096e5d1dca2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-453910512.py:2: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  sp500 = yf.download(\"^GSPC\", start=\"2016-01-01\", end=\"2024-12-31\")\n",
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All figures have been generated and saved to: /content/drive/MyDrive/chronos_t5_mini_project_portfolio/chronos_figures/\n"
          ]
        }
      ],
      "source": [
        "# ========== 下载 S&P500 (2016-2024) ==========\n",
        "sp500 = yf.download(\"^GSPC\", start=\"2016-01-01\", end=\"2024-12-31\")\n",
        "price_col = \"Adj Close\" if \"Adj Close\" in sp500.columns else \"Close\"\n",
        "sp500[\"daily_return\"] = sp500[price_col].pct_change().fillna(0)\n",
        "# 论文公式：累计 log return\n",
        "sp500[\"cum_return\"] = np.cumsum(np.log1p(sp500[\"daily_return\"]))\n",
        "sp500 = sp500[[\"cum_return\"]]\n",
        "sp500.index = pd.to_datetime(sp500.index)\n",
        "\n",
        "# ========== 配置 ==========\n",
        "files = [\n",
        "    (\"VW\", \"/content/drive/MyDrive/chronos_t5_mini_project_portfolio/chronos_results/portfolio_daily_series_VW_with_rf.csv\"),\n",
        "    (\"EW\", \"/content/drive/MyDrive/chronos_t5_mini_project_portfolio/chronos_results/portfolio_daily_series_EW_with_rf.csv\")\n",
        "]\n",
        "tc_levels = [0, 5, 10, 20, 40]      # 交易成本 (bps)\n",
        "windows = [5, 21, 252, 512]         # 窗口\n",
        "strategies = [\"long_only\", \"short_only\", \"long_short\"]\n",
        "\n",
        "output_dir = \"/content/drive/MyDrive/chronos_t5_mini_project_portfolio/chronos_figures\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# 灰色经济事件区间\n",
        "crisis_periods = [\n",
        "    (datetime(2018, 6, 1), datetime(2019, 1, 1), \"US-China Trade War\"),\n",
        "    (datetime(2020, 2, 1), datetime(2020, 7, 1), \"COVID-19\"),\n",
        "    (datetime(2022, 2, 1), datetime(2022, 6, 1), \"Russia-Ukraine War\"),\n",
        "    (datetime(2023, 1, 1), datetime(2023, 4, 1), \"US Bank Crisis\"),\n",
        "]\n",
        "\n",
        "def plot_comparison_styled(df, scheme, tc, window):\n",
        "    plt.figure(figsize=(15, 12))\n",
        "    model_names = df[\"model\"].unique()\n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, len(model_names)))\n",
        "\n",
        "    offset_step = 0.02\n",
        "\n",
        "    for i, strat in enumerate(strategies, 1):\n",
        "        ax = plt.subplot(3, 1, i)\n",
        "\n",
        "        # baseline ----------------------------------------------------------------\n",
        "        plt.plot(sp500.index, sp500[\"cum_return\"],\n",
        "                 color=\"black\", lw=2.5, label=\"S&P500 (Total Return)\", zorder=10)\n",
        "\n",
        "        # -------------------------------------------------------------------------\n",
        "        for idx, model_name in enumerate(model_names):\n",
        "            sub = df[(df[\"window\"] == window) &\n",
        "                     (df[\"portfolio_type\"] == strat) &\n",
        "                     (df[\"model\"] == model_name)].sort_values(\"date\")\n",
        "            if sub.empty:\n",
        "                continue\n",
        "\n",
        "            # --------- ① 关键：按 tc 取列名 -------------\n",
        "            if tc == 0:\n",
        "                ret_col = \"return\"          # 原始超额收益\n",
        "            else:\n",
        "                ret_col = f\"tc{tc}_return\"  # 含交易成本的列\n",
        "            # -------------------------------------------\n",
        "\n",
        "            if ret_col not in sub.columns:\n",
        "                # 防呆：列不存在就跳过\n",
        "                continue\n",
        "\n",
        "            # --------- ② 仍然用论文公式做累计 ----------\n",
        "            log_cum = np.cumsum(np.log1p(sub[ret_col].values))\n",
        "            # -------------------------------------------\n",
        "\n",
        "            y_shift = idx * offset_step\n",
        "            plt.plot(sub[\"date\"], log_cum + y_shift,\n",
        "                     label=f\"{model_name} ({strat.replace('_',' ').title()})\",\n",
        "                     lw=2, color=colors[idx], alpha=0.9)\n",
        "\n",
        "        # 灰色阴影、省略号 …… (保持和原来一样)\n",
        "        # ------------------------------------------------\n",
        "        for start, end, label in crisis_periods:\n",
        "            ax.axvspan(start, end, color='grey', alpha=0.3)\n",
        "            ax.text(start + pd.Timedelta(days=10),\n",
        "                    ax.get_ylim()[1]*0.92, label, fontsize=8, color='grey')\n",
        "        ax.xaxis.set_major_locator(mdates.YearLocator())\n",
        "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "        ax.set_ylabel(\"Cumulative log return (start = 0)\")\n",
        "        ax.set_title(f\"{scheme} | Window={window} | Strategy={strat} | TC={tc} bps\")\n",
        "        ax.grid(alpha=0.3)\n",
        "        plt.xticks(rotation=30)\n",
        "        plt.legend(bbox_to_anchor=(1.04, 1), loc='upper left', fontsize=8)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    fname = f\"{scheme}_window{window}_TC{tc}.png\"\n",
        "    plt.savefig(os.path.join(output_dir, fname), dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# ========== 主循环，生成所有图 ==========\n",
        "for scheme, file_path in files:\n",
        "    df = pd.read_csv(file_path)\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "    for tc in tc_levels:\n",
        "        for window in windows:\n",
        "            plot_comparison_styled(df, scheme, tc, window)\n",
        "\n",
        "print(f\"All figures have been generated and saved to: {output_dir}/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiZUWpb3-BrO",
        "outputId": "9c521bb8-050a-4a4f-f88a-cb11197da0a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Update] ΔSharpe has been written to /content/drive/MyDrive/chronos_t5_mini_project_portfolio/chronos_results/portfolio_results_daily_rebalance_VW.csv\n",
            "[Update] ΔSharpe has been written to /content/drive/MyDrive/chronos_t5_mini_project_portfolio/chronos_results/portfolio_results_daily_rebalance_EW.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# === 3. 加载 portfolio_metrics.csv 里的 R²_zero ===\n",
        "metrics_df = pd.read_csv(\"/content/drive/MyDrive/chronos_t5_mini_project_portfolio/chronos_results/portfolio_metrics.csv\")[[\"Model\", \"Window\", \"R²_zero\"]]\n",
        "metrics_df.rename(columns={\"Model\": \"model\", \"Window\": \"window\"}, inplace=True)\n",
        "\n",
        "# === 4. 处理 VW/EW 两个文件 ===\n",
        "for fname in [\"/content/drive/MyDrive/chronos_t5_mini_project_portfolio/chronos_results/portfolio_results_daily_rebalance_VW.csv\", \"/content/drive/MyDrive/chronos_t5_mini_project_portfolio/chronos_results/portfolio_results_daily_rebalance_EW.csv\"]:\n",
        "    df = pd.read_csv(fname)\n",
        "\n",
        "    # 按 model+window 合并 R²_zero\n",
        "    df = df.merge(metrics_df, on=[\"model\", \"window\"], how=\"left\")\n",
        "\n",
        "    # 循环计算 ΔSharpe 和 Sharpe*\n",
        "    rows = []\n",
        "    for _, row in df.iterrows():\n",
        "        r2 = float(row[\"R²_zero\"]) if not pd.isna(row[\"R²_zero\"]) else 0.0\n",
        "        if row[\"portfolio_type\"] == \"long_only\":\n",
        "            d_sr, sr_star = delta_sharpe(r2, SR_MKT_EX)  # 基准=SPX Excess\n",
        "            row[\"ΔSharpe\"]  = d_sr\n",
        "            row[\"Sharpe*\"]  = sr_star\n",
        "            row[\"baseline\"] = f\"SPX_excess ({SR_MKT_EX:.2f})\"\n",
        "        else:\n",
        "            d_sr, sr_star = delta_sharpe(r2, 0)  # 基准=现金\n",
        "            row[\"ΔSharpe\"]  = d_sr\n",
        "            row[\"Sharpe*\"]  = sr_star\n",
        "            row[\"baseline\"] = \"cash (0)\"\n",
        "        rows.append(row)\n",
        "\n",
        "    pd.DataFrame(rows).to_csv(fname, index=False)\n",
        "    print(f\"[Update] ΔSharpe has been written to {fname}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/wj/7nbyftys3g77fcp37r3t0_lm0000gn/T/ipykernel_62181/1104344638.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  ics = (df_group.groupby(\"signal_date\", observed=True).apply(_day_ic).dropna())\n",
            "/var/folders/wj/7nbyftys3g77fcp37r3t0_lm0000gn/T/ipykernel_62181/1104344638.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  ics = (df_group.groupby(\"signal_date\", observed=True).apply(_day_ic).dropna())\n",
            "/var/folders/wj/7nbyftys3g77fcp37r3t0_lm0000gn/T/ipykernel_62181/1104344638.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  ics = (df_group.groupby(\"signal_date\", observed=True).apply(_day_ic).dropna())\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] Overwrote portfolio_metrics.csv with new RankIC )\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/wj/7nbyftys3g77fcp37r3t0_lm0000gn/T/ipykernel_62181/1104344638.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  ics = (df_group.groupby(\"signal_date\", observed=True).apply(_day_ic).dropna())\n",
            "/var/folders/wj/7nbyftys3g77fcp37r3t0_lm0000gn/T/ipykernel_62181/1104344638.py:33: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(rankic_stats)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from math import sqrt\n",
        "\n",
        "PRED_PATH = \"predictions_daily.csv\"\n",
        "METRICS_PATH = \"portfolio_metrics.csv\"\n",
        "TREAT_CONSTANT_DAY_AS_ZERO = False\n",
        "MIN_DAYS_FOR_STATS = 1\n",
        "\n",
        "def _day_ic(g):\n",
        "    if g[\"y_pred\"].nunique(dropna=True) <= 1 or g[\"y_true\"].nunique(dropna=True) <= 1:\n",
        "        return 0.0 if TREAT_CONSTANT_DAY_AS_ZERO else np.nan\n",
        "    return g[\"y_pred\"].corr(g[\"y_true\"], method=\"spearman\")\n",
        "\n",
        "def rankic_stats(df_group):\n",
        "    ics = (df_group.groupby(\"signal_date\", observed=True).apply(_day_ic).dropna())\n",
        "    n = int(ics.shape[0])\n",
        "    if n < MIN_DAYS_FOR_STATS:\n",
        "        return pd.Series({\"RankIC_mean\": np.nan, \"RankIC_t\": np.nan, \"RankIC_pos%\": np.nan, \"N_days\": n})\n",
        "    mean_ic = float(ics.mean())\n",
        "    std_ic  = float(ics.std(ddof=1))\n",
        "    t_ic    = mean_ic / (std_ic / np.sqrt(n)) if std_ic > 0 else np.nan\n",
        "    pos_pct = float((ics > 0).mean())\n",
        "    return pd.Series({\"RankIC_mean\": mean_ic, \"RankIC_t\": t_ic, \"RankIC_pos%\": pos_pct, \"N_days\": n})\n",
        "\n",
        "# 1) 读数据并计算 RankIC\n",
        "pred = pd.read_csv(PRED_PATH)\n",
        "pred[\"signal_date\"] = pd.to_datetime(pred[\"signal_date\"], errors=\"coerce\")\n",
        "pred = pred.dropna(subset=[\"signal_date\", \"y_true\", \"y_pred\", \"model\", \"window\"])\n",
        "pred[\"window\"] = pd.to_numeric(pred[\"window\"], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "rankic_df = (pred.groupby([\"model\", \"window\"], dropna=False)\n",
        "                .apply(rankic_stats)\n",
        "                .reset_index()\n",
        "                .rename(columns={\"model\":\"Model\",\"window\":\"Window\"}))\n",
        "\n",
        "# 2) 合并：右表(新的 RankIC)保留原名，左表(旧 metrics)加后缀 _old\n",
        "metrics = pd.read_csv(METRICS_PATH)\n",
        "metrics[\"Window\"] = pd.to_numeric(metrics[\"Window\"], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "merged = metrics.merge(rankic_df, on=[\"Model\",\"Window\"], how=\"left\", suffixes=(\"_old\",\"\"))\n",
        "\n",
        "# 3) 丢掉旧列（带 _old 后缀）\n",
        "to_drop = [c for c in merged.columns if c.endswith(\"_old\")]\n",
        "merged = merged.drop(columns=to_drop)\n",
        "\n",
        "# 4) 保存覆盖\n",
        "merged.to_csv(METRICS_PATH, index=False)\n",
        "print(\"[OK] Overwrote portfolio_metrics.csv with new RankIC )\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (tf-mac)",
      "language": "python",
      "name": "tf-mac"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
