{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps (Apple Metal GPU)\n",
      " MPS cache cleared\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import optuna\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import f as f_dist\n",
    "import yfinance as yf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration: prefer MPS (Mac GPU), then CUDA, then CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(f\"Using device: {device} (Apple Metal GPU)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Using device: {device} (NVIDIA GPU)\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(f\"Using device: {device} (CPU only)\")\n",
    "\n",
    "if device.type == 'mps':\n",
    "    torch.mps.empty_cache()\n",
    "    print(\"MPS cache cleared\")\n",
    "\n",
    "def get_device_info():\n",
    "    \"\"\"Get device information\"\"\"\n",
    "    if device.type == 'mps':\n",
    "        return f\"Apple Metal GPU (MPS) - Mac M-series chip\"\n",
    "    elif device.type == 'cuda':\n",
    "        return f\"NVIDIA GPU: {torch.cuda.get_device_name()}\"\n",
    "    else:\n",
    "        return \"CPU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] S&P500 Excess Sharpe (2016–24) = 0.652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ========== Core Function Definitions ==========\n",
    "def r2_zero(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute zero-based R² (baseline is 0)\n",
    "    y_true: array of true values (N,)\n",
    "    y_pred: array of predicted values (N,)\n",
    "    \"\"\"\n",
    "    rss = np.sum((y_true - y_pred)**2)  \n",
    "    tss = np.sum(y_true**2)            \n",
    "    return 1 - rss / tss\n",
    "\n",
    "def calc_ic_daily(df, method='spearman'):\n",
    "    \"\"\"\n",
    "    Calculate daily cross-sectional RankIC.\n",
    "    df: must contain ['signal_date','y_true','y_pred']\n",
    "    \"\"\"\n",
    "    ics = (df.groupby('signal_date')\n",
    "             .apply(lambda g: g['y_pred'].corr(g['y_true'], method=method))\n",
    "             .dropna())\n",
    "    mean_ic = ics.mean()\n",
    "    std_ic  = ics.std(ddof=1)\n",
    "    t_ic    = mean_ic / (std_ic / np.sqrt(len(ics))) if std_ic > 0 else np.nan\n",
    "    pos_ratio = (ics > 0).mean()\n",
    "    return mean_ic, t_ic, pos_ratio, ics\n",
    "\n",
    "def annual_sharpe(rets, freq=252):\n",
    "    mu = float(np.mean(rets)) * freq\n",
    "    sd = float(np.std(rets, ddof=1)) * np.sqrt(freq)\n",
    "    return mu / sd if sd > 0 else 0\n",
    "\n",
    "def delta_sharpe(r2_zero, sr_base):\n",
    "    sr_star = np.sqrt(sr_base**2 + r2_zero) / np.sqrt(1 - r2_zero)\n",
    "    return sr_star - sr_base, sr_star\n",
    "\n",
    "# Load risk-free rate & calculate S&P500 Excess Sharpe\n",
    "rf_file = \"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/CRSP_2016_2024_top50_with_exret.csv\"\n",
    "rf_df = pd.read_csv(rf_file, usecols=[\"date\", \"rf\"])\n",
    "rf_df[\"date\"] = pd.to_datetime(rf_df[\"date\"])\n",
    "rf_df = rf_df.drop_duplicates(\"date\").set_index(\"date\").sort_index()\n",
    "rf_series = rf_df[\"rf\"].astype(float)\n",
    "\n",
    "px = yf.download(\"^GSPC\", start=\"2016-01-01\", end=\"2024-12-31\")[\"Close\"]\n",
    "sp_ret = px.pct_change().dropna()\n",
    "rf_align = rf_series.reindex(sp_ret.index).fillna(method=\"ffill\")\n",
    "sp_excess = sp_ret.values - rf_align.values\n",
    "\n",
    "SR_MKT_EX = annual_sharpe(sp_excess)\n",
    "print(f\"[INFO] S&P500 Excess Sharpe (2016–24) = {SR_MKT_EX:.3f}\")\n",
    "\n",
    "def calc_directional_metrics(y_true, y_pred, permnos=None):\n",
    "    \"\"\"\n",
    "    Improved version:\n",
    "    - Sample-level sign prediction\n",
    "    - If grouped by stock, calculate Overall, Up, Down for each stock and then average\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    if permnos is None:\n",
    "        s_true = np.sign(y_true)\n",
    "        s_pred = np.sign(y_pred)\n",
    "        mask = s_true != 0\n",
    "        s_true = s_true[mask]\n",
    "        s_pred = s_pred[mask]\n",
    "\n",
    "        overall_acc = np.mean(s_true == s_pred)\n",
    "\n",
    "        up_mask = s_true > 0\n",
    "        down_mask = s_true < 0\n",
    "        up_acc = np.mean(s_true[up_mask] == s_pred[up_mask]) if np.any(up_mask) else 0\n",
    "        down_acc = np.mean(s_true[down_mask] == s_pred[down_mask]) if np.any(down_mask) else 0\n",
    "\n",
    "    else:\n",
    "        df = pd.DataFrame({\"permno\": permnos, \"yt\": y_true, \"yp\": y_pred})\n",
    "        overall_accs = []\n",
    "        up_accs = []\n",
    "        down_accs = []\n",
    "\n",
    "        for _, g in df.groupby(\"permno\"):\n",
    "            s_true = np.sign(g[\"yt\"].values)\n",
    "            s_pred = np.sign(g[\"yp\"].values)\n",
    "            mask = s_true != 0\n",
    "            s_true = s_true[mask]\n",
    "            s_pred = s_pred[mask]\n",
    "            if len(s_true) == 0:\n",
    "                continue\n",
    "            overall_accs.append(np.mean(s_true == s_pred))\n",
    "\n",
    "            up_mask = s_true > 0\n",
    "            down_mask = s_true < 0\n",
    "            up_accs.append(np.mean(s_true[up_mask] == s_pred[up_mask]) if np.any(up_mask) else np.nan)\n",
    "            down_accs.append(np.mean(s_true[down_mask] == s_pred[down_mask]) if np.any(down_mask) else np.nan)\n",
    "\n",
    "        overall_acc = np.nanmean(overall_accs)\n",
    "        up_acc = np.nanmean(up_accs)\n",
    "        down_acc = np.nanmean(down_accs)\n",
    "\n",
    "    return overall_acc, up_acc, down_acc\n",
    "\n",
    "def regression_metrics(y_true, y_pred, k, meta=None, permnos=None):\n",
    "    \"\"\"\n",
    "    Includes:\n",
    "    - Regression metrics\n",
    "    - Pointwise directional accuracy\n",
    "    - Market cap group metrics\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    n = len(y_true)\n",
    "\n",
    "    r2 = r2_zero(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "\n",
    "    dir_acc, up_acc, down_acc = calc_directional_metrics(y_true, y_pred, permnos)\n",
    "\n",
    "    metrics = {\n",
    "        \"R²_zero\": r2,\n",
    "        \"RMSE\": rmse,\n",
    "        \"MAE\": mae,\n",
    "        \"MSE\": mse,\n",
    "        \"Directional Accuracy\": dir_acc,\n",
    "        \"Up_Directional_Acc\": up_acc,\n",
    "        \"Down_Directional_Acc\": down_acc\n",
    "    }\n",
    "\n",
    "    if meta is not None and \"MKTCAP_PERCENTILE\" in meta:\n",
    "        top_mask = meta[\"MKTCAP_PERCENTILE\"] >= 0.75\n",
    "        bottom_mask = meta[\"MKTCAP_PERCENTILE\"] <= 0.25\n",
    "\n",
    "        if np.any(top_mask):\n",
    "            yt_top = y_true[top_mask]\n",
    "            yp_top = y_pred[top_mask]\n",
    "            perm_top = permnos[top_mask] if permnos is not None else None\n",
    "            r2_top = r2_zero(yt_top, yp_top)\n",
    "            rmse_top = np.sqrt(mean_squared_error(yt_top, yp_top))\n",
    "            mae_top = mean_absolute_error(yt_top, yp_top)\n",
    "            mse_top = mean_squared_error(yt_top, yp_top)\n",
    "            dir_top, up_top, down_top = calc_directional_metrics(yt_top, yp_top, perm_top)\n",
    "            metrics.update({\n",
    "                \"Top25_R2_zero\": r2_top,\n",
    "                \"Top25_RMSE\": rmse_top,\n",
    "                \"Top25_MAE\": mae_top,\n",
    "                \"Top25_MSE\": mse_top,\n",
    "                \"Top25_Dir_Acc\": dir_top,\n",
    "                \"Top25_Up_Acc\": up_top,\n",
    "                \"Top25_Down_Acc\": down_top\n",
    "            })\n",
    "\n",
    "        if np.any(bottom_mask):\n",
    "            yt_bot = y_true[bottom_mask]\n",
    "            yp_bot = y_pred[bottom_mask]\n",
    "            perm_bot = permnos[bottom_mask] if permnos is not None else None\n",
    "            r2_bot = r2_zero(yt_bot, yp_bot)\n",
    "            rmse_bot = np.sqrt(mean_squared_error(yt_bot, yp_bot))\n",
    "            mae_bot = mean_absolute_error(yt_bot, yp_bot)\n",
    "            mse_bot = mean_squared_error(yt_bot, yp_bot)\n",
    "            dir_bot, up_bot, down_bot = calc_directional_metrics(yt_bot, yp_bot, perm_bot)\n",
    "            metrics.update({\n",
    "                \"Bottom25_R2_zero\": r2_bot,\n",
    "                \"Bottom25_RMSE\": rmse_bot,\n",
    "                \"Bottom25_MAE\": mae_bot,\n",
    "                \"Bottom25_MSE\": mse_bot,\n",
    "                \"Bottom25_Dir_Acc\": dir_bot,\n",
    "                \"Bottom25_Up_Acc\": up_bot,\n",
    "                \"Bottom25_Down_Acc\": down_bot\n",
    "            })\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def f_statistic(y_true, y_pred, k):\n",
    "    \"\"\"Return F statistic and corresponding p-value\"\"\"\n",
    "    n   = len(y_true)\n",
    "    rss = np.sum((y_true - y_pred) ** 2)\n",
    "    tss = np.sum(y_true ** 2)\n",
    "    r2  = 1 - rss / tss\n",
    "    if (r2 <= 0) or (n <= k):\n",
    "        return 0.0, 1.0\n",
    "    F = (r2 / k) / ((1 - r2) / (n - k))\n",
    "    p = f_dist.sf(F, k, n - k)\n",
    "    return F, p\n",
    "\n",
    "def overall_interval_metrics_method1(y_all, yhat_all, k, permnos_all=None, meta_all=None):\n",
    "    \"\"\"\n",
    "    Method 1: Calculate metrics for the entire interval at once (2016-2024, all samples concatenated)\n",
    "    Returns: a dict, can be directly passed to save_metrics()\n",
    "    \"\"\"\n",
    "    base = regression_metrics(\n",
    "        y_true=y_all, \n",
    "        y_pred=yhat_all, \n",
    "        k=k, \n",
    "        meta=meta_all, \n",
    "        permnos=permnos_all\n",
    "    )\n",
    "    F, p = f_statistic(y_all, yhat_all, k)\n",
    "    base[\"F_stat\"]     = F\n",
    "    base[\"F_pvalue\"]   = p\n",
    "    base[\"N_obs\"] = len(y_all)\n",
    "    \n",
    "    delta_cash, sr_star_cash = delta_sharpe(base[\"R²_zero\"], sr_base=0)\n",
    "    base[\"ΔSharpe_cash\"]      = delta_cash\n",
    "    base[\"Sharpe*_cash\"]      = sr_star_cash\n",
    "\n",
    "    delta_mkt , sr_star_mkt  = delta_sharpe(base[\"R²_zero\"], sr_base=SR_MKT_EX)\n",
    "    base[\"ΔSharpe_mkt\"]       = delta_mkt\n",
    "    base[\"Sharpe*_mkt\"]       = sr_star_mkt\n",
    "    \n",
    "    return base\n",
    "\n",
    "def sortino_ratio(rets, freq=252):\n",
    "    \"\"\"Calculate Sortino Ratio\"\"\"\n",
    "    downside = rets[rets < 0]\n",
    "    if len(downside) == 0:\n",
    "        return np.inf\n",
    "    mu = rets.mean() * freq\n",
    "    sigma = np.sqrt((downside ** 2).mean()) * np.sqrt(freq)\n",
    "    return mu / sigma\n",
    "\n",
    "def cvar(rets, alpha=0.95):\n",
    "    \"\"\"Calculate CVaR\"\"\"\n",
    "    q = np.quantile(rets, 1 - alpha)\n",
    "    return rets[rets <= q].mean()\n",
    "\n",
    "def save_predictions(model_name, window_size, y_true, y_pred, permnos, path=\"predictions/\"):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        \"PERMNO\": permnos,\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred\n",
    "    })\n",
    "\n",
    "    filename = f\"{model_name}_w{window_size}.csv\"\n",
    "    df.to_csv(os.path.join(path, filename), index=False)\n",
    "    print(f\"[Save] {filename}\")\n",
    "\n",
    "def save_metrics(metrics_dict, name, window, path=\"results.csv\"):\n",
    "    \"\"\"Save evaluation metrics\"\"\"\n",
    "    row = pd.DataFrame([metrics_dict])\n",
    "    row.insert(0, \"Model\", name)\n",
    "    row.insert(1, \"Window\", window)\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path)\n",
    "        df = df[~((df[\"Model\"] == name) & (df[\"Window\"] == window))]\n",
    "        df = pd.concat([df, row], ignore_index=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        print(f\"[Update] Metrics updated for {name} w={window}\")\n",
    "    else:\n",
    "        row.to_csv(path, index=False)\n",
    "        print(f\"[Create] New metrics file created with {name} w={window}\")\n",
    "\n",
    "def get_quarter_periods(start_year=2015, end_year=2024):\n",
    "    \"\"\"Generate list of quarters\"\"\"\n",
    "    quarters = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for q in range(1, 5):\n",
    "            quarters.append((year, q))\n",
    "    return quarters\n",
    "\n",
    "def save_model_with_quarter(mlp_wrapper, name, window, year, quarter,\n",
    "                            path=\"models/\"):\n",
    "    \"\"\"\n",
    "    Safely save MLP model parameters, handle MPS device compatibility and reduce file size\n",
    "    \"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    mlp_wrapper.clear_loss_history()\n",
    "    \n",
    "    original_device = mlp_wrapper.training_device\n",
    "    mlp_wrapper.model.to('cpu')\n",
    "    \n",
    "    pth_file = f\"{name}_w{window}_{year}Q{quarter}.pth\"\n",
    "    torch.save(mlp_wrapper.model.state_dict(),\n",
    "               os.path.join(path, pth_file))\n",
    "    \n",
    "    mlp_wrapper.model.to(original_device)\n",
    "    \n",
    "    print(f\"[Saved] {pth_file}\")\n",
    "    \n",
    "def load_datasets(npz_path):\n",
    "    \"\"\"Load dataset\"\"\"\n",
    "    data = np.load(npz_path, allow_pickle=True) \n",
    "    datasets = {}\n",
    "    for key in data.files:\n",
    "        datasets[key] = data[key]\n",
    "    return datasets\n",
    "\n",
    "def find_coef_step(model):\n",
    "    \"\"\"\n",
    "    Get model coefficients, handle Pipeline and standalone models.\n",
    "    For nonlinear models (e.g., MLP), return None to indicate no coefficients.\n",
    "    \"\"\"\n",
    "    if hasattr(model, 'named_steps'):\n",
    "        for name, est in model.named_steps.items():\n",
    "            if hasattr(est, 'coef_'):\n",
    "                return name, est\n",
    "            if isinstance(est, Pipeline):\n",
    "                for subname, subest in est.named_steps.items():\n",
    "                    if hasattr(subest, 'coef_'):\n",
    "                        return f\"{name}__{subname}\", subest\n",
    "        return None, None\n",
    "    else:\n",
    "        if hasattr(model, 'coef_'):\n",
    "            return 'model', model\n",
    "    \n",
    "    raise ValueError(\"No estimator with coef_ found in model\")\n",
    "    \n",
    "def train_or_skip(model, train_loader, valid_loader, window_size, year, quarter, **train_kwargs):\n",
    "    save_path = f\"models/NN1_w{window_size}_{year}Q{quarter}.pth\"\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"[Skip Training] Model already exists: {save_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"[Training] Start training model: {save_path}\")\n",
    "    train_model(model, train_loader, valid_loader, **train_kwargs)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"[Done] Model saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== MLP Model Definition ==========\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping mechanism\"\"\"\n",
    "    def __init__(self, patience=20, min_delta=0, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            \n",
    "        if self.counter >= self.patience:\n",
    "            if self.restore_best_weights:\n",
    "                model.load_state_dict(self.best_weights)\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def save_checkpoint(self, model):\n",
    "        \"\"\"Save best weights\"\"\"\n",
    "        self.best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "class MLPNet(nn.Module):\n",
    "    \"\"\"MLP neural network model\"\"\"\n",
    "    def __init__(self, input_size, hidden_sizes, dropout_rate=0.2, activation='relu'):\n",
    "        super(MLPNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        activations = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'tanh': nn.Tanh(),\n",
    "            'leaky_relu': nn.LeakyReLU()\n",
    "        }\n",
    "        self.activation = activations.get(activation, nn.ReLU())\n",
    "            \n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                self.activation,\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "            \n",
    "        layers.append(nn.Linear(prev_size, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, m):\n",
    "        \"\"\"Xavier initialization\"\"\"\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze()\n",
    "\n",
    "def get_mlp_model(model_name, input_size, dropout_rate=0.1, training_device='cpu'):\n",
    "    \"\"\"Create MLP model, specify training device\"\"\"\n",
    "    hidden_configs = {\n",
    "        'NN1': [32],\n",
    "        'NN2': [64,32],\n",
    "        'NN3': [128,64,32],\n",
    "        'NN4': [256,128,64,32],\n",
    "        'NN5': [512,256,128,64,32]\n",
    "    }\n",
    "    return MLPNet(input_size=input_size,\n",
    "                  hidden_sizes=hidden_configs[model_name],\n",
    "                  dropout_rate=dropout_rate).to(training_device)\n",
    "\n",
    "# ========== Core Training and Prediction Functions ==========\n",
    "\n",
    "class MLPWrapper:\n",
    "    \"\"\"MLP model wrapper with training logic\"\"\"\n",
    "    def __init__(self, model_name, input_size, learning_rate=0.001, batch_size=512, max_epochs=100, warm_start_epochs=15, dropout_rate=0.1, training_device=None):\n",
    "        self.model_name = model_name\n",
    "        self.input_size = input_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.max_epochs = max_epochs\n",
    "        self.warm_start_epochs = warm_start_epochs\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.training_device = training_device if training_device is not None else device\n",
    "        \n",
    "        self.model = get_mlp_model(model_name, input_size, dropout_rate, self.training_device)\n",
    "        self._init_training_components()\n",
    "        self.is_fitted = False\n",
    "        \n",
    "    def _init_training_components(self):\n",
    "        \"\"\"Initialize training components\"\"\"\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode='min', factor=0.5, patience=5, verbose=False)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.early_stopping = EarlyStopping(patience=5)\n",
    "        \n",
    "    def fit(self, X, y, validation_split=0.1, warm_start=False, verbose=True):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        if not warm_start:\n",
    "            self.model = get_mlp_model(self.model_name, self.input_size, self.dropout_rate, self.training_device)\n",
    "            self._init_training_components()\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"    [Warm Start] Retaining model weights, continuing training...\")\n",
    "            self.early_stopping = EarlyStopping(patience=5)\n",
    "        \n",
    "        val_size = int(len(X) * validation_split) if validation_split > 0 else max(1, int(len(X)*0.1))\n",
    "        X_train, X_val = X[:-val_size], X[-val_size:]\n",
    "        y_train, y_val = y[:-val_size], y[-val_size:]\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            TensorDataset(torch.FloatTensor(X_train).to(self.training_device), torch.FloatTensor(y_train).to(self.training_device)),\n",
    "            batch_size=self.batch_size, shuffle=False, drop_last=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            TensorDataset(torch.FloatTensor(X_val).to(self.training_device), torch.FloatTensor(y_val).to(self.training_device)),\n",
    "            batch_size=self.batch_size, shuffle=False\n",
    "        )\n",
    "        \n",
    "        actual_epochs = self.warm_start_epochs if warm_start else self.max_epochs\n",
    "        if verbose:\n",
    "            print(f\"    Training for {actual_epochs} epochs ({'warm-start fine-tuning' if warm_start else 'full training'}) on {self.training_device}\")\n",
    "        \n",
    "        losses = {'train': [], 'val': []}\n",
    "        \n",
    "        for epoch in range(actual_epochs):\n",
    "            self.model.train()\n",
    "            train_loss = sum(\n",
    "                self._train_batch(batch_X, batch_y) for batch_X, batch_y in train_loader\n",
    "            ) / len(train_loader)\n",
    "            \n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = sum(\n",
    "                    self.criterion(self.model(batch_X), batch_y).item() \n",
    "                    for batch_X, batch_y in val_loader\n",
    "                ) / len(val_loader)\n",
    "            \n",
    "            losses['train'].append(train_loss)\n",
    "            losses['val'].append(val_loss)\n",
    "            \n",
    "            if verbose and ((epoch + 1) % 5 == 0 or epoch == 0):\n",
    "                print(f\"    Epoch {epoch+1}/{actual_epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "            \n",
    "            self.scheduler.step(val_loss)\n",
    "            \n",
    "            if self.early_stopping(val_loss, self.model):\n",
    "                if verbose:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "                break\n",
    "            \n",
    "            if self.training_device.type == 'mps' and (epoch + 1) % 10 == 0:\n",
    "                torch.mps.empty_cache()\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"    Final - Train Loss: {losses['train'][-1]:.6f}, Val Loss: {losses['val'][-1]:.6f}\")\n",
    "        \n",
    "        self.loss_history = losses\n",
    "        if self.training_device.type == 'mps':\n",
    "            torch.mps.empty_cache()\n",
    "        self.is_fitted = True\n",
    "        \n",
    "    def clear_loss_history(self):\n",
    "        \"\"\"Clear loss history to reduce file size\"\"\"\n",
    "        if hasattr(self, 'loss_history'):\n",
    "            del self.loss_history\n",
    "        \n",
    "    def _train_batch(self, batch_X, batch_y):\n",
    "        \"\"\"Train a single batch\"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        outputs = self.model(batch_X)\n",
    "        loss = self.criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model must be fitted before predicting\")\n",
    "            \n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.FloatTensor(X).to(self.training_device)\n",
    "            predictions = self.model(X_tensor).cpu().numpy()\n",
    "            \n",
    "        return predictions\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        \"\"\"Set hyperparameters\"\"\"\n",
    "        for key, value in params.items():\n",
    "            if hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "        self._init_training_components()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "TUNED_MODELS = {\"NN1\"}  # Only tune NN1\n",
    "\n",
    "def tune_mlp_with_optuna(model_name, X, y, n_trials=25,verbose=False):\n",
    "    \"\"\"Optuna hyperparameter tuning for MLP model\"\"\"\n",
    "    if model_name not in TUNED_MODELS:\n",
    "        print(f\"Skip {model_name} - not tunable\")\n",
    "        return None\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    input_size = X.shape[1]\n",
    "\n",
    "    def objective(trial):\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "        batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
    "        dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.5)\n",
    "        \n",
    "        mlp = MLPWrapper(\n",
    "            model_name=model_name,\n",
    "            input_size=input_size,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size,\n",
    "            max_epochs=20,\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "        \n",
    "        cv_mse = []\n",
    "        for train_idx, val_idx in tscv.split(X):\n",
    "            X_tr, X_val = X[train_idx], X[val_idx]\n",
    "            y_tr, y_val = y[train_idx], y[val_idx]\n",
    "            \n",
    "            mlp.fit(X_tr, y_tr, validation_split=0.0, warm_start=False, verbose=verbose)\n",
    "            preds = mlp.predict(X_val)\n",
    "            cv_mse.append(mean_squared_error(y_val, preds))\n",
    "            \n",
    "        return np.mean(cv_mse)\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\",\n",
    "                                sampler=optuna.samplers.TPESampler(seed=42))\n",
    "    study.optimize(objective, n_trials=n_trials, n_jobs=-1)\n",
    "\n",
    "    print(f\"[Hyper] {model_name}: best_MSE={study.best_value:.6f}, params={study.best_params}\")\n",
    "    return study.best_params\n",
    "\n",
    "# ========== Expanding Window Training per Quarter ==========\n",
    "\n",
    "def train_mlp_models_expanding_quarterly(start_year=2015, end_year=2024, window_sizes=None, model_names=None, \n",
    "                          npz_path=\"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/all_window_datasets.npz\"):\n",
    "    \"\"\"\n",
    "    MLP expanding window training per quarter:\n",
    "    1. 2015Q4 (first quarter): use 2000-2015 data for tuning and training\n",
    "    2. 2015Q4: use 2000-2015Q2 data (train) + 2015Q3 data (test from previous quarter) for training\n",
    "    3. Continue, adding previous quarter's test data to the training set each quarter\n",
    "    4. Only tune hyperparameters for NN1, other models share the same hyperparameters\n",
    "    \"\"\"\n",
    "    if window_sizes is None:\n",
    "        window_sizes = [5, 21, 252, 512]\n",
    "    if model_names is None:\n",
    "        model_names = [\"NN1\", \"NN2\", \"NN3\", \"NN4\", \"NN5\"]\n",
    "    \n",
    "    print(f\"Starting MLP Quarterly Expanding Window Training ({start_year}-{end_year})\")\n",
    "    \n",
    "    datasets = load_datasets(npz_path)\n",
    "    \n",
    "    best_params_cache = {}\n",
    "    quarters_to_tune = {(2015, 4), (2020, 4)}\n",
    "    \n",
    "    for window in window_sizes:\n",
    "        print(f\"Processing window size: {window}\")\n",
    "        \n",
    "        X_train_initial = datasets[f\"X_train_{window}\"]\n",
    "        y_train_initial = datasets[f\"y_train_{window}\"]\n",
    "        X_test_full = datasets[f\"X_test_{window}\"]\n",
    "        y_test_full = datasets[f\"y_test_{window}\"]\n",
    "        \n",
    "        meta_train_dict = datasets[f\"meta_train_{window}\"].item()\n",
    "        meta_test_dict = datasets[f\"meta_test_{window}\"].item()\n",
    "        meta_train = pd.DataFrame.from_dict(meta_train_dict)\n",
    "        meta_test = pd.DataFrame.from_dict(meta_test_dict)\n",
    "        meta_test['ret_date'] = pd.to_datetime(meta_test['ret_date'])\n",
    "        \n",
    "        input_size = X_train_initial.shape[1]\n",
    "        \n",
    "        X_expanding = X_train_initial.copy()\n",
    "        y_expanding = y_train_initial.copy()\n",
    "\n",
    "        for model_name in model_names:\n",
    "            cache_key = f\"{model_name}_w{window}\"\n",
    "            default_params = {'learning_rate': 0.001, 'batch_size': 512, 'dropout_rate': 0.1}\n",
    "            best_params_cache[cache_key] = default_params\n",
    "        \n",
    "        mlp_models = {}\n",
    "        \n",
    "        quarter_periods = get_quarter_periods(start_year, end_year)\n",
    "\n",
    "        quarter_periods = [\n",
    "            (y, q) for (y, q) in quarter_periods\n",
    "            if not (y == start_year and q < 4)\n",
    "            and not (y == end_year and q > 3)\n",
    "        ]\n",
    "        \n",
    "        for year, quarter in quarter_periods:\n",
    "            print(f\"  Quarter {year}Q{quarter}: Training models with expanding data\")\n",
    "            \n",
    "            if not (year == start_year and quarter == 1):\n",
    "                if quarter == 1:\n",
    "                    prev_year, prev_quarter = year - 1, 4\n",
    "                else:\n",
    "                    prev_year, prev_quarter = year, quarter - 1\n",
    "                \n",
    "                prev_quarter_mask = (\n",
    "                    (meta_test['ret_date'].dt.year == prev_year) & \n",
    "                    (meta_test['ret_date'].dt.quarter == prev_quarter)\n",
    "                )\n",
    "                if np.any(prev_quarter_mask):\n",
    "                    X_prev_quarter = X_test_full[prev_quarter_mask]\n",
    "                    y_prev_quarter = y_test_full[prev_quarter_mask]\n",
    "                    \n",
    "                    X_expanding = np.vstack([X_expanding, X_prev_quarter])\n",
    "                    y_expanding = np.hstack([y_expanding, y_prev_quarter])\n",
    "                    \n",
    "                    print(f\"    Added {np.sum(prev_quarter_mask)} samples from {prev_year}Q{prev_quarter} to training set\")\n",
    "            \n",
    "            for model_name in model_names:\n",
    "                model_path = f\"models/{model_name}_w{window}_{year}Q{quarter}.pth\"\n",
    "                if os.path.exists(model_path):\n",
    "                    print(f\"    [Skip Training] {model_path} already exists\")\n",
    "                    continue\n",
    "                cache_key = f\"{model_name}_w{window}\"\n",
    "\n",
    "                if cache_key in best_params_cache:\n",
    "                    best_params = best_params_cache[cache_key]\n",
    "                else:\n",
    "                    best_params = {'learning_rate': 0.001, 'batch_size': 512, 'dropout_rate': 0.1}\n",
    "\n",
    "                need_rebuild = False\n",
    "                if ((year, quarter) in quarters_to_tune) and (model_name == \"NN1\"):\n",
    "                    print(f\"    [Re-tuning] {model_name} for {year}Q{quarter}\")\n",
    "                    new_params = tune_mlp_with_optuna(model_name, X_expanding, y_expanding, n_trials=5)\n",
    "                    if new_params is not None:\n",
    "                        old_params = best_params_cache.get(cache_key, {})\n",
    "                        if 'dropout_rate' in new_params and old_params.get('dropout_rate') != new_params.get('dropout_rate'):\n",
    "                            print(f\"    [Structure Change] dropout_rate changed: {old_params.get('dropout_rate', 'N/A')} -> {new_params.get('dropout_rate')}\")\n",
    "                            need_rebuild = True\n",
    "                            \n",
    "                        best_params = new_params\n",
    "                        for mn in model_names:\n",
    "                            best_params_cache[f\"{mn}_w{window}\"] = best_params\n",
    "\n",
    "                if (year == start_year and quarter == 4) or model_name not in mlp_models or need_rebuild:\n",
    "                    if need_rebuild:\n",
    "                        print(f\"    Rebuilding {model_name} model due to structure change\")\n",
    "                    else:\n",
    "                        print(f\"    Creating new {model_name} model\")\n",
    "                        \n",
    "                    mlp = MLPWrapper(\n",
    "                        model_name=model_name,\n",
    "                        input_size=input_size,\n",
    "                        learning_rate=best_params.get('learning_rate', 0.001),\n",
    "                        batch_size=best_params.get('batch_size', 512),\n",
    "                        max_epochs=50,\n",
    "                        warm_start_epochs=20,\n",
    "                        dropout_rate=best_params.get('dropout_rate', 0.1)   \n",
    "                    )\n",
    "                    \n",
    "                    mlp_models[model_name] = mlp\n",
    "                    warm_start = False\n",
    "                else:\n",
    "                    print(f\"    Using warm-start for {model_name} model (retaining previous weights)\")\n",
    "                    mlp = mlp_models[model_name]\n",
    "                    mlp.learning_rate = best_params.get('learning_rate', 0.001)\n",
    "                    mlp.batch_size = best_params.get('batch_size', 512)\n",
    "                    mlp.max_epochs = 50\n",
    "                    mlp.warm_start_epochs = 20\n",
    "                    mlp.optimizer = optim.Adam(mlp.model.parameters(), lr=mlp.learning_rate)\n",
    "                    mlp.scheduler = ReduceLROnPlateau(mlp.optimizer, mode='min', factor=0.5, patience=5, verbose=False)\n",
    "                    mlp.early_stopping = EarlyStopping(patience=5)\n",
    "                    warm_start = True\n",
    "\n",
    "                mlp.fit(X_expanding, y_expanding, validation_split=0.2, warm_start=warm_start)\n",
    "                \n",
    "                save_model_with_quarter(mlp, model_name, window, year, quarter)\n",
    "                \n",
    "            print(f\"    Training data size: {len(X_expanding)} samples\")\n",
    "            gc.collect()\n",
    "            if torch.backends.mps.is_available():\n",
    "                torch.mps.empty_cache()\n",
    "            elif torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    print(\"MLP Quarterly Expanding Window Training Completed\")\n",
    "    return best_params_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Portfolio Core Class ==========\n",
    "# ========== Transaction Cost Settings ==========\n",
    "TC_GRID = [0.0005, 0.001, 0.002, 0.003, 0.004]  # 5, 10, 20, 30, 40 bps\n",
    "TC_TAG  = {\n",
    "    0.0005: \"tc5\",\n",
    "    0.001:  \"tc10\", \n",
    "    0.002:  \"tc20\",\n",
    "    0.003:  \"tc30\",\n",
    "    0.004:  \"tc40\"\n",
    "}\n",
    "\n",
    "class PortfolioBacktester:\n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "        \n",
    "    def calc_turnover(self, w_t, r_t, w_tp1):\n",
    "        \"\"\"Calculate turnover using the standard formula provided by the user\"\"\"\n",
    "        if w_t is None:\n",
    "            return np.sum(np.abs(w_tp1))\n",
    "        \n",
    "        gross_ret = np.sum(w_t * r_t)\n",
    "        if abs(1 + gross_ret) < 1e-8:\n",
    "            return np.sum(np.abs(w_tp1))\n",
    "        \n",
    "        passive_weight = w_t * (1 + r_t) / (1 + gross_ret)\n",
    "        turnover = np.sum(np.abs(w_tp1 - passive_weight))\n",
    "        return turnover\n",
    "    \n",
    "    def create_portfolios_with_permno_tracking(self, signals, market_caps, permnos, top_pct=0.1, bottom_pct=0.1, weight_scheme=\"VW\"):\n",
    "        \"\"\"\n",
    "        Create portfolio weights based on signals, strictly tracking permno alignment.\n",
    "        weight_scheme: 'VW' for value-weighted, 'EW' for equal-weighted\n",
    "        \"\"\"\n",
    "        n_stocks = len(signals)\n",
    "        top_n    = max(1, int(round(n_stocks * top_pct)))\n",
    "        bottom_n = max(1, int(round(n_stocks * bottom_pct)))\n",
    "        \n",
    "        sorted_idx = np.argsort(signals)[::-1]\n",
    "        \n",
    "        top_idx = sorted_idx[:top_n]\n",
    "        bottom_idx = sorted_idx[-bottom_n:]\n",
    "        \n",
    "        portfolio_data = {}\n",
    "        \n",
    "        long_weights = np.zeros(n_stocks)\n",
    "        if len(top_idx) > 0:\n",
    "            if weight_scheme == \"VW\":\n",
    "                top_market_caps = market_caps[top_idx]\n",
    "                if np.sum(top_market_caps) > 0:\n",
    "                    long_weights[top_idx] = top_market_caps / np.sum(top_market_caps)\n",
    "            else:\n",
    "                long_weights[top_idx] = 1.0 / len(top_idx)\n",
    "        \n",
    "        portfolio_data['long_only'] = {\n",
    "            'weights': long_weights,\n",
    "            'permnos': permnos.copy(),\n",
    "            'selected_permnos': permnos[top_idx] if len(top_idx) > 0 else np.array([])\n",
    "        }\n",
    "        \n",
    "        short_weights = np.zeros(n_stocks)\n",
    "        if len(bottom_idx) > 0:\n",
    "            if weight_scheme == \"VW\":\n",
    "                bottom_market_caps = market_caps[bottom_idx]\n",
    "                if np.sum(bottom_market_caps) > 0:\n",
    "                    short_weights[bottom_idx] = -bottom_market_caps / np.sum(bottom_market_caps)\n",
    "            else:\n",
    "                short_weights[bottom_idx] = -1.0 / len(bottom_idx)\n",
    "        \n",
    "        portfolio_data['short_only'] = {\n",
    "            'weights': short_weights,\n",
    "            'permnos': permnos.copy(),\n",
    "            'selected_permnos': permnos[bottom_idx] if len(bottom_idx) > 0 else np.array([])\n",
    "        }\n",
    "        \n",
    "        ls_raw = long_weights + short_weights\n",
    "\n",
    "        gross_target = 2.0\n",
    "        current_gross = np.sum(np.abs(long_weights)) + np.sum(np.abs(short_weights))\n",
    "        scale = gross_target / current_gross if current_gross > 1e-8 else 0.0\n",
    "        ls_weights = scale * ls_raw\n",
    "\n",
    "        ls_selected_permnos = np.concatenate([\n",
    "            permnos[top_idx] if len(top_idx) > 0 else np.array([]),\n",
    "            permnos[bottom_idx] if len(bottom_idx) > 0 else np.array([])\n",
    "        ])\n",
    "\n",
    "        portfolio_data['long_short'] = {\n",
    "            'weights': ls_weights,\n",
    "            'permnos': permnos.copy(),\n",
    "            'selected_permnos': ls_selected_permnos\n",
    "        }\n",
    "\n",
    "        return portfolio_data\n",
    "    \n",
    "    def calculate_aligned_portfolio_return(self, portfolio_weights, portfolio_permnos, actual_returns, actual_permnos):\n",
    "        \"\"\"Calculate portfolio return strictly aligned by permno\"\"\"\n",
    "        aligned_returns = np.zeros(len(portfolio_permnos))\n",
    "        \n",
    "        return_dict = dict(zip(actual_permnos, actual_returns))\n",
    "        \n",
    "        for i, permno in enumerate(portfolio_permnos):\n",
    "            if permno in return_dict:\n",
    "                aligned_returns[i] = return_dict[permno]\n",
    "        \n",
    "        portfolio_return = np.sum(portfolio_weights * aligned_returns)\n",
    "        return portfolio_return, aligned_returns\n",
    "\n",
    "    def calculate_metrics(self, returns, turnover_series=None):\n",
    "        \"\"\"Calculate portfolio metrics - only returns summary metrics, not long series\"\"\"\n",
    "        returns = np.array(returns)\n",
    "        \n",
    "        annual_return = np.mean(returns) * 252\n",
    "        annual_vol = np.std(returns, ddof=1) * np.sqrt(252)\n",
    "        sharpe = annual_return / annual_vol if annual_vol > 0 else 0\n",
    "        \n",
    "        log_cum = np.cumsum(np.log1p(returns))\n",
    "        peak_log = np.maximum.accumulate(log_cum)\n",
    "        dd_log = peak_log - log_cum\n",
    "        max_drawdown = 1 - np.exp(-dd_log.max()) \n",
    "        max_1d_loss = np.min(returns) \n",
    "        \n",
    "        avg_turnover = np.mean(turnover_series) if turnover_series is not None else 0\n",
    "        \n",
    "        sortino = sortino_ratio(returns)\n",
    "        cvar95  = cvar(returns, alpha=0.95)\n",
    "\n",
    "        result = {\n",
    "            'annual_return': annual_return,\n",
    "            'annual_vol': annual_vol,\n",
    "            'sharpe': sharpe,\n",
    "            'max_drawdown': max_drawdown,\n",
    "            'max_1d_loss': max_1d_loss,\n",
    "            'avg_turnover': avg_turnover,\n",
    "            'sortino': sortino,\n",
    "            'cvar95': cvar95\n",
    "        }\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_portfolio_simulation_daily_rebalance(start_year=2016, end_year=2024, window_sizes=None, model_names=None,\n",
    "                                           npz_path=\"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/all_window_datasets.npz\"):\n",
    "    \"\"\"\n",
    "    Portfolio simulation (daily prediction, next-day rebalance):\n",
    "        1. Load quarterly models (trained with quarterly expanding window)\n",
    "        2. Daily prediction to daily signal\n",
    "        3. Daily portfolio construction (T+1 rebalance, strict permno alignment)\n",
    "        4. Separate summary metrics and time series data\n",
    "    \"\"\"\n",
    "    if window_sizes is None:\n",
    "        window_sizes = [5, 21, 252, 512]\n",
    "    if model_names is None:\n",
    "        model_names = [\"NN1\", \"NN2\", \"NN3\", \"NN4\", \"NN5\"]\n",
    "    \n",
    "    print(\"Starting daily rebalance portfolio backtesting simulation\")\n",
    "    \n",
    "    backtester = PortfolioBacktester()\n",
    "    datasets = load_datasets(npz_path)\n",
    "    \n",
    "    summary_results = []\n",
    "    daily_series_data = []\n",
    "    pred_rows = []\n",
    "    \n",
    "    WEIGHT_SCHEMES = [\"VW\", \"EW\"]\n",
    "    \n",
    "    for window in window_sizes:\n",
    "        print(f\"Processing window size: {window}\")\n",
    "        \n",
    "        X_test = datasets[f\"X_test_{window}\"]\n",
    "        y_test = datasets[f\"y_test_{window}\"]\n",
    "        input_size = X_test.shape[1]\n",
    "        meta_test_dict = datasets[f\"meta_test_{window}\"].item()\n",
    "        meta_test = pd.DataFrame.from_dict(meta_test_dict)\n",
    "        \n",
    "        permnos_test = meta_test[\"PERMNO\"].values\n",
    "        meta_test[\"signal_date\"]  = pd.to_datetime(meta_test[\"date\"])\n",
    "        meta_test[\"ret_date\"]     = pd.to_datetime(meta_test[\"ret_date\"])\n",
    "        market_caps = meta_test.get(\"MKTCAP\", np.ones(len(permnos_test)))\n",
    "        \n",
    "        meta_test['date'] = pd.to_datetime(meta_test[\"date\"])\n",
    "        dates_test = meta_test['signal_date']\n",
    "        \n",
    "        for model_name in model_names:\n",
    "            for scheme in WEIGHT_SCHEMES:\n",
    "                all_y_true   = []\n",
    "                all_y_pred   = []\n",
    "                all_permnos  = []\n",
    "                all_meta     = []\n",
    "                print(f\"  Model: {model_name}, Scheme: {scheme}\")\n",
    "                \n",
    "                portfolio_daily_data = {\n",
    "                    'long_only': {'returns': [], 'turnovers': [], 'dates': []},\n",
    "                    'short_only': {'returns': [], 'turnovers': [], 'dates': []},\n",
    "                    'long_short': {'returns': [], 'turnovers': [], 'dates': []}\n",
    "                }\n",
    "                \n",
    "                prev_portfolio_data = {'long_only': None, 'short_only': None, 'long_short': None}\n",
    "                \n",
    "                signals_buf = {}\n",
    "                \n",
    "                for year in range(start_year, min(end_year + 1, 2025)):\n",
    "                    for quarter in range(1, 5):\n",
    "                        # Determine model file year and quarter (T+1 logic: use previous quarter's model to predict current quarter)\n",
    "                        if quarter == 1:\n",
    "                            model_file_year, model_file_quarter = year - 1, 4\n",
    "                        else:\n",
    "                            model_file_year, model_file_quarter = year, quarter - 1\n",
    "                            \n",
    "                        pth_path = f\"models/{model_name}_w{window}_{model_file_year}Q{model_file_quarter}.pth\"\n",
    "                        if not os.path.exists(pth_path):\n",
    "                            print(f\"      Skip: Model file not found {pth_path}\")\n",
    "                            continue\n",
    "\n",
    "                        mlp = MLPWrapper(model_name=model_name,\n",
    "                                         input_size=input_size,\n",
    "                                         max_epochs=0)\n",
    "                        mlp.model.load_state_dict(torch.load(pth_path, map_location=device))\n",
    "                        mlp.is_fitted = True\n",
    "                        model = mlp\n",
    "                        \n",
    "                        quarter_mask = (\n",
    "                            (dates_test.dt.year == year) & \n",
    "                            (dates_test.dt.quarter == quarter)\n",
    "                        )\n",
    "                        if not np.any(quarter_mask):\n",
    "                            continue\n",
    "                        \n",
    "                        X_quarter = X_test[quarter_mask]\n",
    "                        y_quarter = y_test[quarter_mask]\n",
    "                        permnos_quarter = permnos_test[quarter_mask]\n",
    "                        market_caps_quarter = market_caps[quarter_mask]\n",
    "                        dates_quarter = dates_test[quarter_mask]\n",
    "                        ret_dates_quarter = meta_test.loc[quarter_mask, 'ret_date'].values\n",
    "                        \n",
    "                        df_quarter = pd.DataFrame({\n",
    "                            'signal_date': dates_quarter,\n",
    "                            'ret_date': ret_dates_quarter,\n",
    "                            'permno': permnos_quarter,\n",
    "                            'market_cap': market_caps_quarter,\n",
    "                            'actual_return': y_quarter,                 \n",
    "                            'prediction': model.predict(X_quarter)   \n",
    "                        })\n",
    "                        \n",
    "                        if scheme == 'VW':\n",
    "                            df_q_save = df_quarter[['signal_date','ret_date','permno',\n",
    "                                                    'actual_return','prediction','market_cap']].copy()\n",
    "                            df_q_save.rename(columns={'actual_return':'y_true',\n",
    "                                                      'prediction':'y_pred'}, inplace=True)\n",
    "                            df_q_save['model']  = model_name\n",
    "                            df_q_save['window'] = window\n",
    "                            pred_rows.append(df_q_save)\n",
    "                        \n",
    "                        all_y_true.append(df_quarter['actual_return'].values)\n",
    "                        all_y_pred.append(df_quarter['prediction'].values)\n",
    "                        all_permnos.append(df_quarter['permno'].values)\n",
    "                        all_meta.append(meta_test.loc[quarter_mask, :])   \n",
    "\n",
    "                        for signal_date, sig_grp in df_quarter.groupby('signal_date'):\n",
    "                            # (1) Calculate today's signals and store in buffer, do not rebalance yet\n",
    "                            daily_signals = (\n",
    "                                sig_grp.groupby('permno')['prediction'].mean()\n",
    "                                      .to_frame('prediction')\n",
    "                                      .join(sig_grp.groupby('permno')['market_cap'].mean())\n",
    "                            )\n",
    "                            signals_buf[signal_date] = daily_signals\n",
    "\n",
    "                            # (2) Use previous day's signals to rebalance today\n",
    "                            prev_date = signal_date - pd.tseries.offsets.BDay(1)\n",
    "                            if prev_date not in signals_buf:\n",
    "                                continue\n",
    "\n",
    "                            sigs = signals_buf.pop(prev_date)\n",
    "                            if prev_date in signals_buf:\n",
    "                                del signals_buf[prev_date]\n",
    "\n",
    "                            # (3) Use today's realized returns (ret_date == signal_date)\n",
    "                            ret_grp = df_quarter[df_quarter['ret_date'] == signal_date]\n",
    "                            if len(ret_grp) == 0:\n",
    "                                continue\n",
    "\n",
    "                            daily_actual_returns = (\n",
    "                                ret_grp.groupby('permno')['actual_return']\n",
    "                                       .mean()\n",
    "                                       .reindex(sigs.index, fill_value=0)\n",
    "                                       .values\n",
    "                            )\n",
    "                            daily_permnos = sigs.index.values\n",
    "\n",
    "                            # (4) Generate 3 sets of weights\n",
    "                            portfolios_data = backtester.create_portfolios_with_permno_tracking(\n",
    "                                signals      = sigs['prediction'].values,\n",
    "                                market_caps  = sigs['market_cap'].values,\n",
    "                                permnos      = daily_permnos,\n",
    "                                weight_scheme= scheme\n",
    "                            )\n",
    "                            \n",
    "                            for portfolio_type in ['long_only', 'short_only', 'long_short']:\n",
    "                                portfolio_info = portfolios_data[portfolio_type]\n",
    "                                \n",
    "                                portfolio_return, aligned_returns = backtester.calculate_aligned_portfolio_return(\n",
    "                                    portfolio_weights=portfolio_info['weights'],\n",
    "                                    portfolio_permnos=portfolio_info['permnos'],\n",
    "                                    actual_returns=daily_actual_returns,\n",
    "                                    actual_permnos=daily_permnos\n",
    "                                )\n",
    "                                \n",
    "                                if prev_portfolio_data[portfolio_type] is not None:\n",
    "                                    prev_w_ser = pd.Series(\n",
    "                                        prev_portfolio_data[portfolio_type]['weights'],\n",
    "                                        index=prev_portfolio_data[portfolio_type]['permnos']\n",
    "                                    )\n",
    "                                    cur_w_ser = pd.Series(\n",
    "                                        portfolio_info['weights'],\n",
    "                                        index=portfolio_info['permnos']\n",
    "                                    )\n",
    "\n",
    "                                    prev_r_ser = pd.Series(\n",
    "                                        prev_portfolio_data[portfolio_type]['aligned_returns'],\n",
    "                                        index=prev_portfolio_data[portfolio_type]['permnos']\n",
    "                                    )\n",
    "\n",
    "                                    aligned_prev_w = prev_w_ser.reindex(cur_w_ser.index, fill_value=0).values\n",
    "                                    aligned_prev_r = prev_r_ser.reindex(cur_w_ser.index, fill_value=0).values\n",
    "\n",
    "                                    aligned_cur_w = cur_w_ser.values\n",
    "\n",
    "                                    turnover = backtester.calc_turnover(\n",
    "                                        w_t  = aligned_prev_w,\n",
    "                                        r_t  = aligned_prev_r,\n",
    "                                        w_tp1= aligned_cur_w\n",
    "                                    )\n",
    "                                else:\n",
    "                                    turnover = np.sum(np.abs(portfolio_info['weights']))\n",
    "                                \n",
    "                                portfolio_daily_data[portfolio_type]['returns'].append(portfolio_return)\n",
    "                                portfolio_daily_data[portfolio_type]['turnovers'].append(turnover)\n",
    "                                portfolio_daily_data[portfolio_type]['dates'].append(signal_date)\n",
    "                                \n",
    "                                prev_portfolio_data[portfolio_type] = {\n",
    "                                    'weights'        : portfolio_info['weights'],\n",
    "                                    'permnos'        : portfolio_info['permnos'],\n",
    "                                    'aligned_returns': aligned_returns      \n",
    "                                }\n",
    "                \n",
    "                for portfolio_type in ['long_only', 'short_only', 'long_short']:\n",
    "                    portfolio_data = portfolio_daily_data[portfolio_type]\n",
    "                    \n",
    "                    if len(portfolio_data['returns']) > 0:\n",
    "                        metrics = backtester.calculate_metrics(\n",
    "                            returns=portfolio_data['returns'],\n",
    "                            turnover_series=portfolio_data['turnovers']\n",
    "                        )\n",
    "                        \n",
    "                        rets = np.array(portfolio_data['returns'])\n",
    "                        tovs = np.array(portfolio_data['turnovers'])\n",
    "\n",
    "                        for tc in TC_GRID:\n",
    "                            tag = TC_TAG[tc]\n",
    "                            adj = rets - tovs * tc\n",
    "\n",
    "                            ann_ret = adj.mean() * 252\n",
    "                            ann_vol = adj.std(ddof=1) * np.sqrt(252)\n",
    "                            sharpe  = ann_ret / ann_vol if ann_vol > 0 else 0\n",
    "\n",
    "                            cum_adj = np.cumprod(1 + adj)\n",
    "                            mdd = ((cum_adj - np.maximum.accumulate(cum_adj)) /\n",
    "                                   np.maximum.accumulate(cum_adj)).min()\n",
    "\n",
    "                            metrics[f'{tag}_annual_return'] = ann_ret\n",
    "                            metrics[f'{tag}_annual_vol']    = ann_vol\n",
    "                            metrics[f'{tag}_sharpe']        = sharpe\n",
    "                            metrics[f'{tag}_max_drawdown']  = mdd\n",
    "                        \n",
    "                        summary_results.append({\n",
    "                            'scheme': scheme,\n",
    "                            'model': model_name,\n",
    "                            'window': window,\n",
    "                            'portfolio_type': portfolio_type,\n",
    "                            **metrics\n",
    "                        })\n",
    "                        \n",
    "                        rets_arr = np.array(portfolio_data['returns'])\n",
    "                        tovs_arr = np.array(portfolio_data['turnovers'])\n",
    "                        cum_no_tc = np.log1p(rets_arr).cumsum()\n",
    "\n",
    "                        tc_ret_dict = {}\n",
    "                        tc_cum_dict = {}\n",
    "                        for tc in TC_GRID:\n",
    "                            tag = TC_TAG[tc]\n",
    "                            r = rets_arr - tovs_arr * tc\n",
    "                            tc_ret_dict[tag] = r\n",
    "                            tc_cum_dict[tag] = np.log1p(r).cumsum()\n",
    "\n",
    "                        for i, date in enumerate(portfolio_data['dates']):\n",
    "                            row = {\n",
    "                                'scheme'        : scheme,\n",
    "                                'model'         : model_name,\n",
    "                                'window'        : window,\n",
    "                                'portfolio_type': portfolio_type,\n",
    "                                'date'          : str(date),\n",
    "                                'return'        : rets_arr[i],\n",
    "                                'turnover'      : tovs_arr[i],\n",
    "                                'cumulative'    : cum_no_tc[i],\n",
    "                            }\n",
    "                            for tag in TC_TAG.values():\n",
    "                                row[f'{tag}_return']     = tc_ret_dict[tag][i]\n",
    "                                row[f'{tag}_cumulative'] = tc_cum_dict[tag][i]\n",
    "\n",
    "                            daily_series_data.append(row)\n",
    "\n",
    "                if scheme == \"VW\" and len(all_y_true) > 0:\n",
    "                    y_all    = np.concatenate(all_y_true)\n",
    "                    yhat_all = np.concatenate(all_y_pred)\n",
    "                    perm_all = np.concatenate(all_permnos)\n",
    "                    meta_all = pd.concat(all_meta, ignore_index=True)\n",
    "\n",
    "                    k = X_test.shape[1]\n",
    "\n",
    "                    m1_metrics = overall_interval_metrics_method1(\n",
    "                        y_all, yhat_all, k,\n",
    "                        permnos_all=perm_all,\n",
    "                        meta_all=meta_all\n",
    "                    )\n",
    "\n",
    "                    full_pred_df = pd.concat(pred_rows, ignore_index=True)\n",
    "                    mean_ic, t_ic, pos_ic, _ = calc_ic_daily(full_pred_df, method='spearman')\n",
    "                    m1_metrics['RankIC_mean']  = mean_ic\n",
    "                    m1_metrics['RankIC_t']     = t_ic\n",
    "                    m1_metrics['RankIC_pos%']  = pos_ic\n",
    "\n",
    "                    save_metrics(m1_metrics, name=model_name, window=window,\n",
    "                        path=\"portfolio_metrics.csv\")\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_results)\n",
    "    daily_df = pd.DataFrame(daily_series_data) if daily_series_data else pd.DataFrame()\n",
    "    \n",
    "    tc_columns = [c for c in summary_df.columns if c.startswith('tc')]\n",
    "    summary_df[tc_columns] = summary_df[tc_columns].fillna(0.0)\n",
    "    \n",
    "    def save_split_by_scheme(df, base_filename):\n",
    "        \"\"\"Helper function to save files split by scheme\"\"\"\n",
    "        if df.empty:\n",
    "            print(f\"Warning: DataFrame is empty, skipping save for {base_filename}\")\n",
    "            return None, None\n",
    "            \n",
    "        vw_df = df[df['scheme'] == 'VW']\n",
    "        ew_df = df[df['scheme'] == 'EW']\n",
    "        \n",
    "        vw_filename = f\"{base_filename}_VW.csv\"\n",
    "        ew_filename = f\"{base_filename}_EW.csv\"\n",
    "        \n",
    "        vw_df.to_csv(vw_filename, index=False)\n",
    "        ew_df.to_csv(ew_filename, index=False)\n",
    "        \n",
    "        print(f\"VW results saved to {vw_filename}\")\n",
    "        print(f\"EW results saved to {ew_filename}\")\n",
    "        \n",
    "        return vw_filename, ew_filename\n",
    "    \n",
    "    save_split_by_scheme(summary_df, \"portfolio_results_daily_rebalance\")\n",
    "    \n",
    "    if not daily_df.empty:\n",
    "        save_split_by_scheme(daily_df, \"portfolio_daily_series\")\n",
    "    \n",
    "    if pred_rows:\n",
    "        pred_df = pd.concat(pred_rows, ignore_index=True)\n",
    "        pred_df.to_csv(\"predictions_daily.csv\", index=False)\n",
    "        print(f\"Saved {len(pred_df)} prediction rows to predictions_daily.csv\")\n",
    "    \n",
    "    print(f\"Generated {len(summary_results)} portfolio summary records\")\n",
    "    print(f\"Generated {len(daily_series_data)} daily series records\")\n",
    "    \n",
    "    return summary_df, daily_df, backtester\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MLP Quarterly Expanding Window Training (2015-2024)\n",
      "Processing window size: 5\n",
      "  Quarter 2015Q4: Training models with expanding data\n",
      "    [Re-tuning] NN1 for 2015Q4\n",
      "[Hyper] NN1: best_MSE=0.000288, params={'learning_rate': 0.002881273227171111, 'batch_size': 128, 'dropout_rate': 0.2835072377864848}\n",
      "    [Structure Change] dropout_rate changed: 0.1 -> 0.2835072377864848\n",
      "    Rebuilding NN1 model due to structure change\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 0.023575, Val Loss: 0.000178\n",
      "    Epoch 5/50, Train Loss: 0.000496, Val Loss: 0.000170\n",
      "    Epoch 10/50, Train Loss: 0.000445, Val Loss: 0.000170\n",
      "Early stopping at epoch 11, Train Loss: 0.000445, Val Loss: 0.000171\n",
      "    Final - Train Loss: 0.000445, Val Loss: 0.000171\n",
      "[Saved] NN1_w5_2015Q4.pth\n",
      "    Creating new NN2 model\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 0.041906, Val Loss: 0.000185\n",
      "    Epoch 5/50, Train Loss: 0.000483, Val Loss: 0.000169\n",
      "Early stopping at epoch 8, Train Loss: 0.000456, Val Loss: 0.000170\n",
      "    Final - Train Loss: 0.000456, Val Loss: 0.000170\n",
      "[Saved] NN2_w5_2015Q4.pth\n",
      "    Creating new NN3 model\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 0.014119, Val Loss: 0.000170\n",
      "    Epoch 5/50, Train Loss: 0.000461, Val Loss: 0.000171\n",
      "Early stopping at epoch 7, Train Loss: 0.000447, Val Loss: 0.000170\n",
      "    Final - Train Loss: 0.000447, Val Loss: 0.000170\n",
      "[Saved] NN3_w5_2015Q4.pth\n",
      "    Creating new NN4 model\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 0.023454, Val Loss: 0.000174\n",
      "    Epoch 5/50, Train Loss: 0.000463, Val Loss: 0.000169\n",
      "Early stopping at epoch 7, Train Loss: 0.000449, Val Loss: 0.000170\n",
      "    Final - Train Loss: 0.000449, Val Loss: 0.000170\n",
      "[Saved] NN4_w5_2015Q4.pth\n",
      "    Creating new NN5 model\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 0.019624, Val Loss: 0.000185\n",
      "    Epoch 5/50, Train Loss: 0.000456, Val Loss: 0.000172\n",
      "Early stopping at epoch 8, Train Loss: 0.000446, Val Loss: 0.000171\n",
      "    Final - Train Loss: 0.000446, Val Loss: 0.000171\n",
      "[Saved] NN5_w5_2015Q4.pth\n",
      "    Training data size: 196920 samples\n",
      "  Quarter 2016Q1: Training models with expanding data\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000455, Val Loss: 0.000171\n",
      "    Epoch 5/20, Train Loss: 0.000444, Val Loss: 0.000170\n",
      "    Epoch 10/20, Train Loss: 0.000443, Val Loss: 0.000170\n",
      "    Epoch 15/20, Train Loss: 0.000442, Val Loss: 0.000170\n",
      "Early stopping at epoch 15, Train Loss: 0.000442, Val Loss: 0.000170\n",
      "    Final - Train Loss: 0.000442, Val Loss: 0.000170\n",
      "[Saved] NN1_w5_2016Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000454, Val Loss: 0.000170\n",
      "    Epoch 5/20, Train Loss: 0.000443, Val Loss: 0.000171\n",
      "Early stopping at epoch 6, Train Loss: 0.000444, Val Loss: 0.000173\n",
      "    Final - Train Loss: 0.000444, Val Loss: 0.000173\n",
      "[Saved] NN2_w5_2016Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000452, Val Loss: 0.000170\n",
      "    Epoch 5/20, Train Loss: 0.000444, Val Loss: 0.000172\n",
      "Early stopping at epoch 7, Train Loss: 0.000442, Val Loss: 0.000174\n",
      "    Final - Train Loss: 0.000442, Val Loss: 0.000174\n",
      "[Saved] NN3_w5_2016Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000450, Val Loss: 0.000169\n",
      "    Epoch 5/20, Train Loss: 0.000443, Val Loss: 0.000170\n",
      "Early stopping at epoch 6, Train Loss: 0.000443, Val Loss: 0.000170\n",
      "    Final - Train Loss: 0.000443, Val Loss: 0.000170\n",
      "[Saved] NN4_w5_2016Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000449, Val Loss: 0.000170\n",
      "    Epoch 5/20, Train Loss: 0.000444, Val Loss: 0.000169\n",
      "    Epoch 10/20, Train Loss: 0.000443, Val Loss: 0.000168\n",
      "Early stopping at epoch 12, Train Loss: 0.000443, Val Loss: 0.000169\n",
      "    Final - Train Loss: 0.000443, Val Loss: 0.000169\n",
      "[Saved] NN5_w5_2016Q1.pth\n",
      "    Training data size: 196920 samples\n",
      "  Quarter 2016Q2: Training models with expanding data\n",
      "    Added 2956 samples from 2016Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000439, Val Loss: 0.000176\n",
      "    Epoch 5/20, Train Loss: 0.000439, Val Loss: 0.000176\n",
      "    Epoch 10/20, Train Loss: 0.000439, Val Loss: 0.000176\n",
      "Early stopping at epoch 11, Train Loss: 0.000439, Val Loss: 0.000176\n",
      "    Final - Train Loss: 0.000439, Val Loss: 0.000176\n",
      "[Saved] NN1_w5_2016Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000441, Val Loss: 0.000176\n",
      "    Epoch 5/20, Train Loss: 0.000440, Val Loss: 0.000176\n",
      "    Epoch 10/20, Train Loss: 0.000439, Val Loss: 0.000176\n",
      "Early stopping at epoch 11, Train Loss: 0.000439, Val Loss: 0.000176\n",
      "    Final - Train Loss: 0.000439, Val Loss: 0.000176\n",
      "[Saved] NN2_w5_2016Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000439, Val Loss: 0.000175\n",
      "    Epoch 5/20, Train Loss: 0.000439, Val Loss: 0.000175\n",
      "    Epoch 10/20, Train Loss: 0.000439, Val Loss: 0.000175\n",
      "Early stopping at epoch 11, Train Loss: 0.000438, Val Loss: 0.000176\n",
      "    Final - Train Loss: 0.000438, Val Loss: 0.000176\n",
      "[Saved] NN3_w5_2016Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000440, Val Loss: 0.000175\n",
      "    Epoch 5/20, Train Loss: 0.000440, Val Loss: 0.000175\n",
      "    Epoch 10/20, Train Loss: 0.000439, Val Loss: 0.000175\n",
      "    Epoch 15/20, Train Loss: 0.000439, Val Loss: 0.000175\n",
      "Early stopping at epoch 15, Train Loss: 0.000439, Val Loss: 0.000175\n",
      "    Final - Train Loss: 0.000439, Val Loss: 0.000175\n",
      "[Saved] NN4_w5_2016Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000439, Val Loss: 0.000175\n",
      "    Epoch 5/20, Train Loss: 0.000439, Val Loss: 0.000175\n",
      "Early stopping at epoch 8, Train Loss: 0.000439, Val Loss: 0.000175\n",
      "    Final - Train Loss: 0.000439, Val Loss: 0.000175\n",
      "[Saved] NN5_w5_2016Q2.pth\n",
      "    Training data size: 199876 samples\n",
      "  Quarter 2016Q3: Training models with expanding data\n",
      "    Added 3170 samples from 2016Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000434, Val Loss: 0.000174\n",
      "    Epoch 5/20, Train Loss: 0.000434, Val Loss: 0.000174\n",
      "    Epoch 10/20, Train Loss: 0.000434, Val Loss: 0.000174\n",
      "Early stopping at epoch 12, Train Loss: 0.000434, Val Loss: 0.000174\n",
      "    Final - Train Loss: 0.000434, Val Loss: 0.000174\n",
      "[Saved] NN1_w5_2016Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000435, Val Loss: 0.000174\n",
      "    Epoch 5/20, Train Loss: 0.000434, Val Loss: 0.000174\n",
      "    Epoch 10/20, Train Loss: 0.000434, Val Loss: 0.000174\n",
      "Early stopping at epoch 14, Train Loss: 0.000434, Val Loss: 0.000174\n",
      "    Final - Train Loss: 0.000434, Val Loss: 0.000174\n",
      "[Saved] NN2_w5_2016Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000434, Val Loss: 0.000174\n",
      "    Epoch 5/20, Train Loss: 0.000434, Val Loss: 0.000174\n",
      "Early stopping at epoch 6, Train Loss: 0.000434, Val Loss: 0.000174\n",
      "    Final - Train Loss: 0.000434, Val Loss: 0.000174\n",
      "[Saved] NN3_w5_2016Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000434, Val Loss: 0.000174\n",
      "    Epoch 5/20, Train Loss: 0.000434, Val Loss: 0.000174\n",
      "Early stopping at epoch 8, Train Loss: 0.000434, Val Loss: 0.000174\n",
      "    Final - Train Loss: 0.000434, Val Loss: 0.000174\n",
      "[Saved] NN4_w5_2016Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000434, Val Loss: 0.000174\n",
      "    Epoch 5/20, Train Loss: 0.000434, Val Loss: 0.000174\n",
      "Early stopping at epoch 8, Train Loss: 0.000434, Val Loss: 0.000174\n",
      "    Final - Train Loss: 0.000434, Val Loss: 0.000174\n",
      "[Saved] NN5_w5_2016Q3.pth\n",
      "    Training data size: 203046 samples\n",
      "  Quarter 2016Q4: Training models with expanding data\n",
      "    Added 3176 samples from 2016Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000430, Val Loss: 0.000170\n",
      "    Epoch 5/20, Train Loss: 0.000430, Val Loss: 0.000170\n",
      "    Epoch 10/20, Train Loss: 0.000430, Val Loss: 0.000170\n",
      "Early stopping at epoch 13, Train Loss: 0.000430, Val Loss: 0.000170\n",
      "    Final - Train Loss: 0.000430, Val Loss: 0.000170\n",
      "[Saved] NN1_w5_2016Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000430, Val Loss: 0.000170\n",
      "    Epoch 5/20, Train Loss: 0.000430, Val Loss: 0.000170\n",
      "    Epoch 10/20, Train Loss: 0.000430, Val Loss: 0.000170\n",
      "Early stopping at epoch 11, Train Loss: 0.000430, Val Loss: 0.000170\n",
      "    Final - Train Loss: 0.000430, Val Loss: 0.000170\n",
      "[Saved] NN2_w5_2016Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000430, Val Loss: 0.000170\n",
      "    Epoch 5/20, Train Loss: 0.000430, Val Loss: 0.000170\n",
      "Early stopping at epoch 6, Train Loss: 0.000430, Val Loss: 0.000170\n",
      "    Final - Train Loss: 0.000430, Val Loss: 0.000170\n",
      "[Saved] NN3_w5_2016Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000430, Val Loss: 0.000170\n",
      "    Epoch 5/20, Train Loss: 0.000430, Val Loss: 0.000170\n",
      "Early stopping at epoch 6, Train Loss: 0.000430, Val Loss: 0.000170\n",
      "    Final - Train Loss: 0.000430, Val Loss: 0.000170\n",
      "[Saved] NN4_w5_2016Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000431, Val Loss: 0.000170\n",
      "    Epoch 5/20, Train Loss: 0.000430, Val Loss: 0.000170\n",
      "Early stopping at epoch 7, Train Loss: 0.000430, Val Loss: 0.000170\n",
      "    Final - Train Loss: 0.000430, Val Loss: 0.000170\n",
      "[Saved] NN5_w5_2016Q4.pth\n",
      "    Training data size: 206222 samples\n",
      "  Quarter 2017Q1: Training models with expanding data\n",
      "    Added 3123 samples from 2016Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000426, Val Loss: 0.000172\n",
      "    Epoch 5/20, Train Loss: 0.000426, Val Loss: 0.000172\n",
      "Early stopping at epoch 9, Train Loss: 0.000422, Val Loss: 0.000172\n",
      "    Final - Train Loss: 0.000422, Val Loss: 0.000172\n",
      "[Saved] NN1_w5_2017Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000426, Val Loss: 0.000171\n",
      "    Epoch 5/20, Train Loss: 0.000426, Val Loss: 0.000171\n",
      "    Epoch 10/20, Train Loss: 0.000425, Val Loss: 0.000171\n",
      "Early stopping at epoch 14, Train Loss: 0.000426, Val Loss: 0.000171\n",
      "    Final - Train Loss: 0.000426, Val Loss: 0.000171\n",
      "[Saved] NN2_w5_2017Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000426, Val Loss: 0.000171\n",
      "    Epoch 5/20, Train Loss: 0.000426, Val Loss: 0.000171\n",
      "Early stopping at epoch 8, Train Loss: 0.000426, Val Loss: 0.000171\n",
      "    Final - Train Loss: 0.000426, Val Loss: 0.000171\n",
      "[Saved] NN3_w5_2017Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000426, Val Loss: 0.000172\n",
      "    Epoch 5/20, Train Loss: 0.000425, Val Loss: 0.000172\n",
      "Early stopping at epoch 8, Train Loss: 0.000426, Val Loss: 0.000172\n",
      "    Final - Train Loss: 0.000426, Val Loss: 0.000172\n",
      "[Saved] NN4_w5_2017Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000426, Val Loss: 0.000172\n",
      "    Epoch 5/20, Train Loss: 0.000426, Val Loss: 0.000172\n",
      "Early stopping at epoch 6, Train Loss: 0.000426, Val Loss: 0.000172\n",
      "    Final - Train Loss: 0.000426, Val Loss: 0.000172\n",
      "[Saved] NN5_w5_2017Q1.pth\n",
      "    Training data size: 209345 samples\n",
      "  Quarter 2017Q2: Training models with expanding data\n",
      "    Added 3083 samples from 2017Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000422, Val Loss: 0.000174\n",
      "    Epoch 5/20, Train Loss: 0.000422, Val Loss: 0.000174\n",
      "    Epoch 10/20, Train Loss: 0.000418, Val Loss: 0.000170\n",
      "    Epoch 15/20, Train Loss: 0.000417, Val Loss: 0.000170\n",
      "Early stopping at epoch 17, Train Loss: 0.000418, Val Loss: 0.000170\n",
      "    Final - Train Loss: 0.000418, Val Loss: 0.000170\n",
      "[Saved] NN1_w5_2017Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000422, Val Loss: 0.000171\n",
      "    Epoch 5/20, Train Loss: 0.000422, Val Loss: 0.000171\n",
      "Early stopping at epoch 8, Train Loss: 0.000422, Val Loss: 0.000171\n",
      "    Final - Train Loss: 0.000422, Val Loss: 0.000171\n",
      "[Saved] NN2_w5_2017Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000422, Val Loss: 0.000171\n",
      "    Epoch 5/20, Train Loss: 0.000422, Val Loss: 0.000171\n",
      "    Epoch 10/20, Train Loss: 0.000421, Val Loss: 0.000171\n",
      "Early stopping at epoch 13, Train Loss: 0.000422, Val Loss: 0.000171\n",
      "    Final - Train Loss: 0.000422, Val Loss: 0.000171\n",
      "[Saved] NN3_w5_2017Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000422, Val Loss: 0.000173\n",
      "    Epoch 5/20, Train Loss: 0.000422, Val Loss: 0.000173\n",
      "Early stopping at epoch 6, Train Loss: 0.000422, Val Loss: 0.000173\n",
      "    Final - Train Loss: 0.000422, Val Loss: 0.000173\n",
      "[Saved] NN4_w5_2017Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000422, Val Loss: 0.000172\n",
      "    Epoch 5/20, Train Loss: 0.000422, Val Loss: 0.000173\n",
      "Early stopping at epoch 6, Train Loss: 0.000422, Val Loss: 0.000173\n",
      "    Final - Train Loss: 0.000422, Val Loss: 0.000173\n",
      "[Saved] NN5_w5_2017Q2.pth\n",
      "    Training data size: 212428 samples\n",
      "  Quarter 2017Q3: Training models with expanding data\n",
      "    Added 3122 samples from 2017Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000418, Val Loss: 0.000171\n",
      "    Epoch 5/20, Train Loss: 0.000418, Val Loss: 0.000170\n",
      "    Epoch 10/20, Train Loss: 0.000418, Val Loss: 0.000171\n",
      "    Epoch 15/20, Train Loss: 0.000418, Val Loss: 0.000170\n",
      "    Epoch 20/20, Train Loss: 0.000414, Val Loss: 0.000170\n",
      "    Final - Train Loss: 0.000414, Val Loss: 0.000170\n",
      "[Saved] NN1_w5_2017Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000418, Val Loss: 0.000171\n",
      "    Epoch 5/20, Train Loss: 0.000417, Val Loss: 0.000171\n",
      "Early stopping at epoch 6, Train Loss: 0.000418, Val Loss: 0.000171\n",
      "    Final - Train Loss: 0.000418, Val Loss: 0.000171\n",
      "[Saved] NN2_w5_2017Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000417, Val Loss: 0.000170\n",
      "    Epoch 5/20, Train Loss: 0.000417, Val Loss: 0.000170\n",
      "    Epoch 10/20, Train Loss: 0.000417, Val Loss: 0.000170\n",
      "Early stopping at epoch 11, Train Loss: 0.000417, Val Loss: 0.000170\n",
      "    Final - Train Loss: 0.000417, Val Loss: 0.000170\n",
      "[Saved] NN3_w5_2017Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000418, Val Loss: 0.000171\n",
      "    Epoch 5/20, Train Loss: 0.000418, Val Loss: 0.000170\n",
      "    Epoch 10/20, Train Loss: 0.000418, Val Loss: 0.000170\n",
      "    Epoch 15/20, Train Loss: 0.000413, Val Loss: 0.000170\n",
      "Early stopping at epoch 18, Train Loss: 0.000412, Val Loss: 0.000170\n",
      "    Final - Train Loss: 0.000412, Val Loss: 0.000170\n",
      "[Saved] NN4_w5_2017Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000418, Val Loss: 0.000170\n",
      "    Epoch 5/20, Train Loss: 0.000417, Val Loss: 0.000171\n",
      "Early stopping at epoch 8, Train Loss: 0.000417, Val Loss: 0.000171\n",
      "    Final - Train Loss: 0.000417, Val Loss: 0.000171\n",
      "[Saved] NN5_w5_2017Q3.pth\n",
      "    Training data size: 215550 samples\n",
      "  Quarter 2017Q4: Training models with expanding data\n",
      "    Added 3114 samples from 2017Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000415, Val Loss: 0.000172\n",
      "    Epoch 5/20, Train Loss: 0.000415, Val Loss: 0.000171\n",
      "Early stopping at epoch 9, Train Loss: 0.000415, Val Loss: 0.000172\n",
      "    Final - Train Loss: 0.000415, Val Loss: 0.000172\n",
      "[Saved] NN1_w5_2017Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000414, Val Loss: 0.000171\n",
      "    Epoch 5/20, Train Loss: 0.000414, Val Loss: 0.000171\n",
      "    Epoch 10/20, Train Loss: 0.000414, Val Loss: 0.000171\n",
      "    Epoch 15/20, Train Loss: 0.000414, Val Loss: 0.000171\n",
      "Early stopping at epoch 15, Train Loss: 0.000414, Val Loss: 0.000171\n",
      "    Final - Train Loss: 0.000414, Val Loss: 0.000171\n",
      "[Saved] NN2_w5_2017Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000414, Val Loss: 0.000171\n",
      "    Epoch 5/20, Train Loss: 0.000414, Val Loss: 0.000172\n",
      "    Epoch 10/20, Train Loss: 0.000414, Val Loss: 0.000171\n",
      "Early stopping at epoch 11, Train Loss: 0.000413, Val Loss: 0.000171\n",
      "    Final - Train Loss: 0.000413, Val Loss: 0.000171\n",
      "[Saved] NN3_w5_2017Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000414, Val Loss: 0.000172\n",
      "    Epoch 5/20, Train Loss: 0.000414, Val Loss: 0.000172\n",
      "    Epoch 10/20, Train Loss: 0.000414, Val Loss: 0.000171\n",
      "Early stopping at epoch 11, Train Loss: 0.000413, Val Loss: 0.000171\n",
      "    Final - Train Loss: 0.000413, Val Loss: 0.000171\n",
      "[Saved] NN4_w5_2017Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000414, Val Loss: 0.000171\n",
      "    Epoch 5/20, Train Loss: 0.000414, Val Loss: 0.000171\n",
      "    Epoch 10/20, Train Loss: 0.000414, Val Loss: 0.000172\n",
      "Early stopping at epoch 11, Train Loss: 0.000414, Val Loss: 0.000171\n",
      "    Final - Train Loss: 0.000414, Val Loss: 0.000171\n",
      "[Saved] NN5_w5_2017Q4.pth\n",
      "    Training data size: 218664 samples\n",
      "  Quarter 2018Q1: Training models with expanding data\n",
      "    Added 3115 samples from 2017Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000411, Val Loss: 0.000174\n",
      "    Epoch 5/20, Train Loss: 0.000411, Val Loss: 0.000174\n",
      "Early stopping at epoch 7, Train Loss: 0.000411, Val Loss: 0.000174\n",
      "    Final - Train Loss: 0.000411, Val Loss: 0.000174\n",
      "[Saved] NN1_w5_2018Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000411, Val Loss: 0.000172\n",
      "    Epoch 5/20, Train Loss: 0.000411, Val Loss: 0.000172\n",
      "Early stopping at epoch 6, Train Loss: 0.000411, Val Loss: 0.000173\n",
      "    Final - Train Loss: 0.000411, Val Loss: 0.000173\n",
      "[Saved] NN2_w5_2018Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000410, Val Loss: 0.000173\n",
      "    Epoch 5/20, Train Loss: 0.000410, Val Loss: 0.000173\n",
      "Early stopping at epoch 9, Train Loss: 0.000410, Val Loss: 0.000173\n",
      "    Final - Train Loss: 0.000410, Val Loss: 0.000173\n",
      "[Saved] NN3_w5_2018Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000410, Val Loss: 0.000173\n",
      "    Epoch 5/20, Train Loss: 0.000410, Val Loss: 0.000173\n",
      "    Epoch 10/20, Train Loss: 0.000405, Val Loss: 0.000171\n",
      "    Epoch 15/20, Train Loss: 0.000404, Val Loss: 0.000172\n",
      "Early stopping at epoch 16, Train Loss: 0.000404, Val Loss: 0.000172\n",
      "    Final - Train Loss: 0.000404, Val Loss: 0.000172\n",
      "[Saved] NN4_w5_2018Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000410, Val Loss: 0.000173\n",
      "    Epoch 5/20, Train Loss: 0.000410, Val Loss: 0.000173\n",
      "Early stopping at epoch 7, Train Loss: 0.000410, Val Loss: 0.000174\n",
      "    Final - Train Loss: 0.000410, Val Loss: 0.000174\n",
      "[Saved] NN5_w5_2018Q1.pth\n",
      "    Training data size: 221779 samples\n",
      "  Quarter 2018Q2: Training models with expanding data\n",
      "    Added 2996 samples from 2018Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000407, Val Loss: 0.000181\n",
      "    Epoch 5/20, Train Loss: 0.000407, Val Loss: 0.000181\n",
      "    Epoch 10/20, Train Loss: 0.000407, Val Loss: 0.000181\n",
      "Early stopping at epoch 10, Train Loss: 0.000407, Val Loss: 0.000181\n",
      "    Final - Train Loss: 0.000407, Val Loss: 0.000181\n",
      "[Saved] NN1_w5_2018Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000407, Val Loss: 0.000182\n",
      "    Epoch 5/20, Train Loss: 0.000407, Val Loss: 0.000182\n",
      "    Epoch 10/20, Train Loss: 0.000407, Val Loss: 0.000182\n",
      "Early stopping at epoch 10, Train Loss: 0.000407, Val Loss: 0.000182\n",
      "    Final - Train Loss: 0.000407, Val Loss: 0.000182\n",
      "[Saved] NN2_w5_2018Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000406, Val Loss: 0.000182\n",
      "    Epoch 5/20, Train Loss: 0.000406, Val Loss: 0.000182\n",
      "Early stopping at epoch 6, Train Loss: 0.000406, Val Loss: 0.000182\n",
      "    Final - Train Loss: 0.000406, Val Loss: 0.000182\n",
      "[Saved] NN3_w5_2018Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000405, Val Loss: 0.000182\n",
      "    Epoch 5/20, Train Loss: 0.000406, Val Loss: 0.000182\n",
      "Early stopping at epoch 9, Train Loss: 0.000406, Val Loss: 0.000182\n",
      "    Final - Train Loss: 0.000406, Val Loss: 0.000182\n",
      "[Saved] NN4_w5_2018Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000407, Val Loss: 0.000182\n",
      "    Epoch 5/20, Train Loss: 0.000407, Val Loss: 0.000182\n",
      "    Epoch 10/20, Train Loss: 0.000406, Val Loss: 0.000182\n",
      "Early stopping at epoch 12, Train Loss: 0.000406, Val Loss: 0.000182\n",
      "    Final - Train Loss: 0.000406, Val Loss: 0.000182\n",
      "[Saved] NN5_w5_2018Q2.pth\n",
      "    Training data size: 224775 samples\n",
      "  Quarter 2018Q3: Training models with expanding data\n",
      "    Added 3160 samples from 2018Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000404, Val Loss: 0.000183\n",
      "    Epoch 5/20, Train Loss: 0.000404, Val Loss: 0.000183\n",
      "Early stopping at epoch 7, Train Loss: 0.000404, Val Loss: 0.000183\n",
      "    Final - Train Loss: 0.000404, Val Loss: 0.000183\n",
      "[Saved] NN1_w5_2018Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000403, Val Loss: 0.000184\n",
      "    Epoch 5/20, Train Loss: 0.000404, Val Loss: 0.000184\n",
      "Early stopping at epoch 7, Train Loss: 0.000403, Val Loss: 0.000184\n",
      "    Final - Train Loss: 0.000403, Val Loss: 0.000184\n",
      "[Saved] NN2_w5_2018Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000403, Val Loss: 0.000184\n",
      "    Epoch 5/20, Train Loss: 0.000402, Val Loss: 0.000184\n",
      "    Epoch 10/20, Train Loss: 0.000403, Val Loss: 0.000184\n",
      "Early stopping at epoch 12, Train Loss: 0.000402, Val Loss: 0.000184\n",
      "    Final - Train Loss: 0.000402, Val Loss: 0.000184\n",
      "[Saved] NN3_w5_2018Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000402, Val Loss: 0.000183\n",
      "    Epoch 5/20, Train Loss: 0.000402, Val Loss: 0.000183\n",
      "Early stopping at epoch 9, Train Loss: 0.000402, Val Loss: 0.000183\n",
      "    Final - Train Loss: 0.000402, Val Loss: 0.000183\n",
      "[Saved] NN4_w5_2018Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000403, Val Loss: 0.000183\n",
      "    Epoch 5/20, Train Loss: 0.000403, Val Loss: 0.000183\n",
      "    Epoch 10/20, Train Loss: 0.000403, Val Loss: 0.000183\n",
      "Early stopping at epoch 12, Train Loss: 0.000402, Val Loss: 0.000183\n",
      "    Final - Train Loss: 0.000402, Val Loss: 0.000183\n",
      "[Saved] NN5_w5_2018Q3.pth\n",
      "    Training data size: 227935 samples\n",
      "  Quarter 2018Q4: Training models with expanding data\n",
      "    Added 3125 samples from 2018Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000400, Val Loss: 0.000184\n",
      "    Epoch 5/20, Train Loss: 0.000401, Val Loss: 0.000184\n",
      "    Epoch 10/20, Train Loss: 0.000400, Val Loss: 0.000184\n",
      "Early stopping at epoch 14, Train Loss: 0.000401, Val Loss: 0.000184\n",
      "    Final - Train Loss: 0.000401, Val Loss: 0.000184\n",
      "[Saved] NN1_w5_2018Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000400, Val Loss: 0.000185\n",
      "    Epoch 5/20, Train Loss: 0.000400, Val Loss: 0.000185\n",
      "Early stopping at epoch 8, Train Loss: 0.000400, Val Loss: 0.000186\n",
      "    Final - Train Loss: 0.000400, Val Loss: 0.000186\n",
      "[Saved] NN2_w5_2018Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000399, Val Loss: 0.000185\n",
      "    Epoch 5/20, Train Loss: 0.000399, Val Loss: 0.000186\n",
      "    Epoch 10/20, Train Loss: 0.000398, Val Loss: 0.000185\n",
      "Early stopping at epoch 13, Train Loss: 0.000399, Val Loss: 0.000185\n",
      "    Final - Train Loss: 0.000399, Val Loss: 0.000185\n",
      "[Saved] NN3_w5_2018Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000399, Val Loss: 0.000184\n",
      "    Epoch 5/20, Train Loss: 0.000398, Val Loss: 0.000185\n",
      "    Epoch 10/20, Train Loss: 0.000398, Val Loss: 0.000185\n",
      "    Epoch 15/20, Train Loss: 0.000394, Val Loss: 0.000183\n",
      "    Epoch 20/20, Train Loss: 0.000393, Val Loss: 0.000183\n",
      "    Final - Train Loss: 0.000393, Val Loss: 0.000183\n",
      "[Saved] NN4_w5_2018Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000399, Val Loss: 0.000185\n",
      "    Epoch 5/20, Train Loss: 0.000399, Val Loss: 0.000185\n",
      "Early stopping at epoch 8, Train Loss: 0.000399, Val Loss: 0.000186\n",
      "    Final - Train Loss: 0.000399, Val Loss: 0.000186\n",
      "[Saved] NN5_w5_2018Q4.pth\n",
      "    Training data size: 231060 samples\n",
      "  Quarter 2019Q1: Training models with expanding data\n",
      "    Added 3045 samples from 2018Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000397, Val Loss: 0.000200\n",
      "    Epoch 5/20, Train Loss: 0.000397, Val Loss: 0.000200\n",
      "    Epoch 10/20, Train Loss: 0.000398, Val Loss: 0.000200\n",
      "Early stopping at epoch 12, Train Loss: 0.000397, Val Loss: 0.000200\n",
      "    Final - Train Loss: 0.000397, Val Loss: 0.000200\n",
      "[Saved] NN1_w5_2019Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000397, Val Loss: 0.000198\n",
      "    Epoch 5/20, Train Loss: 0.000397, Val Loss: 0.000199\n",
      "Early stopping at epoch 6, Train Loss: 0.000397, Val Loss: 0.000199\n",
      "    Final - Train Loss: 0.000397, Val Loss: 0.000199\n",
      "[Saved] NN2_w5_2019Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000396, Val Loss: 0.000200\n",
      "    Epoch 5/20, Train Loss: 0.000396, Val Loss: 0.000199\n",
      "Early stopping at epoch 7, Train Loss: 0.000396, Val Loss: 0.000200\n",
      "    Final - Train Loss: 0.000396, Val Loss: 0.000200\n",
      "[Saved] NN3_w5_2019Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000394, Val Loss: 0.000198\n",
      "    Epoch 5/20, Train Loss: 0.000395, Val Loss: 0.000199\n",
      "Early stopping at epoch 9, Train Loss: 0.000394, Val Loss: 0.000197\n",
      "    Final - Train Loss: 0.000394, Val Loss: 0.000197\n",
      "[Saved] NN4_w5_2019Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000396, Val Loss: 0.000197\n",
      "    Epoch 5/20, Train Loss: 0.000395, Val Loss: 0.000197\n",
      "Early stopping at epoch 8, Train Loss: 0.000395, Val Loss: 0.000197\n",
      "    Final - Train Loss: 0.000395, Val Loss: 0.000197\n",
      "[Saved] NN5_w5_2019Q1.pth\n",
      "    Training data size: 234105 samples\n",
      "  Quarter 2019Q2: Training models with expanding data\n",
      "    Added 3022 samples from 2019Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000394, Val Loss: 0.000199\n",
      "    Epoch 5/20, Train Loss: 0.000394, Val Loss: 0.000199\n",
      "    Epoch 10/20, Train Loss: 0.000394, Val Loss: 0.000198\n",
      "    Epoch 15/20, Train Loss: 0.000394, Val Loss: 0.000198\n",
      "Early stopping at epoch 15, Train Loss: 0.000394, Val Loss: 0.000198\n",
      "    Final - Train Loss: 0.000394, Val Loss: 0.000198\n",
      "[Saved] NN1_w5_2019Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000393, Val Loss: 0.000198\n",
      "    Epoch 5/20, Train Loss: 0.000394, Val Loss: 0.000198\n",
      "Early stopping at epoch 6, Train Loss: 0.000394, Val Loss: 0.000199\n",
      "    Final - Train Loss: 0.000394, Val Loss: 0.000199\n",
      "[Saved] NN2_w5_2019Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000392, Val Loss: 0.000199\n",
      "    Epoch 5/20, Train Loss: 0.000392, Val Loss: 0.000198\n",
      "Early stopping at epoch 8, Train Loss: 0.000392, Val Loss: 0.000199\n",
      "    Final - Train Loss: 0.000392, Val Loss: 0.000199\n",
      "[Saved] NN3_w5_2019Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000391, Val Loss: 0.000198\n",
      "    Epoch 5/20, Train Loss: 0.000391, Val Loss: 0.000197\n",
      "    Epoch 10/20, Train Loss: 0.000391, Val Loss: 0.000197\n",
      "Early stopping at epoch 12, Train Loss: 0.000390, Val Loss: 0.000198\n",
      "    Final - Train Loss: 0.000390, Val Loss: 0.000198\n",
      "[Saved] NN4_w5_2019Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000392, Val Loss: 0.000198\n",
      "    Epoch 5/20, Train Loss: 0.000392, Val Loss: 0.000198\n",
      "    Epoch 10/20, Train Loss: 0.000391, Val Loss: 0.000198\n",
      "    Epoch 15/20, Train Loss: 0.000391, Val Loss: 0.000198\n",
      "Early stopping at epoch 18, Train Loss: 0.000391, Val Loss: 0.000198\n",
      "    Final - Train Loss: 0.000391, Val Loss: 0.000198\n",
      "[Saved] NN5_w5_2019Q2.pth\n",
      "    Training data size: 237127 samples\n",
      "  Quarter 2019Q3: Training models with expanding data\n",
      "    Added 3120 samples from 2019Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000391, Val Loss: 0.000201\n",
      "    Epoch 5/20, Train Loss: 0.000391, Val Loss: 0.000201\n",
      "Early stopping at epoch 6, Train Loss: 0.000391, Val Loss: 0.000201\n",
      "    Final - Train Loss: 0.000391, Val Loss: 0.000201\n",
      "[Saved] NN1_w5_2019Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000390, Val Loss: 0.000200\n",
      "    Epoch 5/20, Train Loss: 0.000391, Val Loss: 0.000200\n",
      "    Epoch 10/20, Train Loss: 0.000390, Val Loss: 0.000200\n",
      "    Epoch 15/20, Train Loss: 0.000390, Val Loss: 0.000200\n",
      "Early stopping at epoch 15, Train Loss: 0.000390, Val Loss: 0.000200\n",
      "    Final - Train Loss: 0.000390, Val Loss: 0.000200\n",
      "[Saved] NN2_w5_2019Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000389, Val Loss: 0.000200\n",
      "    Epoch 5/20, Train Loss: 0.000389, Val Loss: 0.000200\n",
      "Early stopping at epoch 7, Train Loss: 0.000389, Val Loss: 0.000200\n",
      "    Final - Train Loss: 0.000389, Val Loss: 0.000200\n",
      "[Saved] NN3_w5_2019Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000387, Val Loss: 0.000200\n",
      "    Epoch 5/20, Train Loss: 0.000388, Val Loss: 0.000200\n",
      "Early stopping at epoch 6, Train Loss: 0.000388, Val Loss: 0.000200\n",
      "    Final - Train Loss: 0.000388, Val Loss: 0.000200\n",
      "[Saved] NN4_w5_2019Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000387, Val Loss: 0.000201\n",
      "    Epoch 5/20, Train Loss: 0.000388, Val Loss: 0.000200\n",
      "    Epoch 10/20, Train Loss: 0.000383, Val Loss: 0.000201\n",
      "    Epoch 15/20, Train Loss: 0.000382, Val Loss: 0.000201\n",
      "Early stopping at epoch 19, Train Loss: 0.000382, Val Loss: 0.000201\n",
      "    Final - Train Loss: 0.000382, Val Loss: 0.000201\n",
      "[Saved] NN5_w5_2019Q3.pth\n",
      "    Training data size: 240247 samples\n",
      "  Quarter 2019Q4: Training models with expanding data\n",
      "    Added 3167 samples from 2019Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000389, Val Loss: 0.000207\n",
      "    Epoch 5/20, Train Loss: 0.000390, Val Loss: 0.000207\n",
      "Early stopping at epoch 7, Train Loss: 0.000390, Val Loss: 0.000207\n",
      "    Final - Train Loss: 0.000390, Val Loss: 0.000207\n",
      "[Saved] NN1_w5_2019Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000389, Val Loss: 0.000203\n",
      "    Epoch 5/20, Train Loss: 0.000389, Val Loss: 0.000204\n",
      "Early stopping at epoch 6, Train Loss: 0.000390, Val Loss: 0.000204\n",
      "    Final - Train Loss: 0.000390, Val Loss: 0.000204\n",
      "[Saved] NN2_w5_2019Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000388, Val Loss: 0.000204\n",
      "    Epoch 5/20, Train Loss: 0.000387, Val Loss: 0.000204\n",
      "    Epoch 10/20, Train Loss: 0.000387, Val Loss: 0.000203\n",
      "    Epoch 15/20, Train Loss: 0.000388, Val Loss: 0.000204\n",
      "Early stopping at epoch 16, Train Loss: 0.000388, Val Loss: 0.000204\n",
      "    Final - Train Loss: 0.000388, Val Loss: 0.000204\n",
      "[Saved] NN3_w5_2019Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000386, Val Loss: 0.000210\n",
      "    Epoch 5/20, Train Loss: 0.000387, Val Loss: 0.000208\n",
      "    Epoch 10/20, Train Loss: 0.000386, Val Loss: 0.000208\n",
      "Early stopping at epoch 10, Train Loss: 0.000386, Val Loss: 0.000208\n",
      "    Final - Train Loss: 0.000386, Val Loss: 0.000208\n",
      "[Saved] NN4_w5_2019Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000385, Val Loss: 0.000216\n",
      "    Epoch 5/20, Train Loss: 0.000386, Val Loss: 0.000211\n",
      "Early stopping at epoch 9, Train Loss: 0.000386, Val Loss: 0.000211\n",
      "    Final - Train Loss: 0.000386, Val Loss: 0.000211\n",
      "[Saved] NN5_w5_2019Q4.pth\n",
      "    Training data size: 243414 samples\n",
      "  Quarter 2020Q1: Training models with expanding data\n",
      "    Added 3179 samples from 2019Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000387, Val Loss: 0.000211\n",
      "    Epoch 5/20, Train Loss: 0.000387, Val Loss: 0.000211\n",
      "Early stopping at epoch 7, Train Loss: 0.000388, Val Loss: 0.000211\n",
      "    Final - Train Loss: 0.000388, Val Loss: 0.000211\n",
      "[Saved] NN1_w5_2020Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000387, Val Loss: 0.000213\n",
      "    Epoch 5/20, Train Loss: 0.000387, Val Loss: 0.000213\n",
      "Early stopping at epoch 8, Train Loss: 0.000387, Val Loss: 0.000212\n",
      "    Final - Train Loss: 0.000387, Val Loss: 0.000212\n",
      "[Saved] NN2_w5_2020Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000385, Val Loss: 0.000217\n",
      "    Epoch 5/20, Train Loss: 0.000385, Val Loss: 0.000212\n",
      "    Epoch 10/20, Train Loss: 0.000385, Val Loss: 0.000218\n",
      "Early stopping at epoch 10, Train Loss: 0.000385, Val Loss: 0.000218\n",
      "    Final - Train Loss: 0.000385, Val Loss: 0.000218\n",
      "[Saved] NN3_w5_2020Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000385, Val Loss: 0.000206\n",
      "    Epoch 5/20, Train Loss: 0.000384, Val Loss: 0.000208\n",
      "Early stopping at epoch 6, Train Loss: 0.000385, Val Loss: 0.000208\n",
      "    Final - Train Loss: 0.000385, Val Loss: 0.000208\n",
      "[Saved] NN4_w5_2020Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000384, Val Loss: 0.000210\n",
      "    Epoch 5/20, Train Loss: 0.000383, Val Loss: 0.000211\n",
      "Early stopping at epoch 6, Train Loss: 0.000385, Val Loss: 0.000213\n",
      "    Final - Train Loss: 0.000385, Val Loss: 0.000213\n",
      "[Saved] NN5_w5_2020Q1.pth\n",
      "    Training data size: 246593 samples\n",
      "  Quarter 2020Q2: Training models with expanding data\n",
      "    Added 2595 samples from 2020Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000387, Val Loss: 0.000244\n",
      "    Epoch 5/20, Train Loss: 0.000387, Val Loss: 0.000246\n",
      "    Epoch 10/20, Train Loss: 0.000387, Val Loss: 0.000244\n",
      "Early stopping at epoch 11, Train Loss: 0.000387, Val Loss: 0.000246\n",
      "    Final - Train Loss: 0.000387, Val Loss: 0.000246\n",
      "[Saved] NN1_w5_2020Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000386, Val Loss: 0.000238\n",
      "    Epoch 5/20, Train Loss: 0.000387, Val Loss: 0.000240\n",
      "    Epoch 10/20, Train Loss: 0.000387, Val Loss: 0.000238\n",
      "Early stopping at epoch 13, Train Loss: 0.000387, Val Loss: 0.000238\n",
      "    Final - Train Loss: 0.000387, Val Loss: 0.000238\n",
      "[Saved] NN2_w5_2020Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000384, Val Loss: 0.000239\n",
      "    Epoch 5/20, Train Loss: 0.000385, Val Loss: 0.000238\n",
      "Early stopping at epoch 8, Train Loss: 0.000385, Val Loss: 0.000238\n",
      "    Final - Train Loss: 0.000385, Val Loss: 0.000238\n",
      "[Saved] NN3_w5_2020Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000383, Val Loss: 0.000238\n",
      "    Epoch 5/20, Train Loss: 0.000384, Val Loss: 0.000240\n",
      "Early stopping at epoch 8, Train Loss: 0.000384, Val Loss: 0.000240\n",
      "    Final - Train Loss: 0.000384, Val Loss: 0.000240\n",
      "[Saved] NN4_w5_2020Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000383, Val Loss: 0.000242\n",
      "    Epoch 5/20, Train Loss: 0.000384, Val Loss: 0.000243\n",
      "Early stopping at epoch 8, Train Loss: 0.000383, Val Loss: 0.000245\n",
      "    Final - Train Loss: 0.000383, Val Loss: 0.000245\n",
      "[Saved] NN5_w5_2020Q2.pth\n",
      "    Training data size: 249188 samples\n",
      "  Quarter 2020Q3: Training models with expanding data\n",
      "    Added 2839 samples from 2020Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000384, Val Loss: 0.000256\n",
      "    Epoch 5/20, Train Loss: 0.000384, Val Loss: 0.000256\n",
      "Early stopping at epoch 7, Train Loss: 0.000384, Val Loss: 0.000256\n",
      "    Final - Train Loss: 0.000384, Val Loss: 0.000256\n",
      "[Saved] NN1_w5_2020Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000384, Val Loss: 0.000259\n",
      "    Epoch 5/20, Train Loss: 0.000384, Val Loss: 0.000259\n",
      "Early stopping at epoch 9, Train Loss: 0.000384, Val Loss: 0.000259\n",
      "    Final - Train Loss: 0.000384, Val Loss: 0.000259\n",
      "[Saved] NN2_w5_2020Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000382, Val Loss: 0.000255\n",
      "    Epoch 5/20, Train Loss: 0.000381, Val Loss: 0.000259\n",
      "    Epoch 10/20, Train Loss: 0.000381, Val Loss: 0.000258\n",
      "Early stopping at epoch 11, Train Loss: 0.000382, Val Loss: 0.000257\n",
      "    Final - Train Loss: 0.000382, Val Loss: 0.000257\n",
      "[Saved] NN3_w5_2020Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000381, Val Loss: 0.000256\n",
      "    Epoch 5/20, Train Loss: 0.000381, Val Loss: 0.000253\n",
      "    Epoch 10/20, Train Loss: 0.000381, Val Loss: 0.000248\n",
      "Early stopping at epoch 14, Train Loss: 0.000381, Val Loss: 0.000247\n",
      "    Final - Train Loss: 0.000381, Val Loss: 0.000247\n",
      "[Saved] NN4_w5_2020Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000380, Val Loss: 0.000254\n",
      "    Epoch 5/20, Train Loss: 0.000380, Val Loss: 0.000254\n",
      "Early stopping at epoch 9, Train Loss: 0.000380, Val Loss: 0.000251\n",
      "    Final - Train Loss: 0.000380, Val Loss: 0.000251\n",
      "[Saved] NN5_w5_2020Q3.pth\n",
      "    Training data size: 252027 samples\n",
      "  Quarter 2020Q4: Training models with expanding data\n",
      "    Added 3151 samples from 2020Q3 to training set\n",
      "    [Re-tuning] NN1 for 2020Q4\n",
      "[Hyper] NN1: best_MSE=0.000273, params={'learning_rate': 0.0003202493137065723, 'batch_size': 128, 'dropout_rate': 0.498601860763653}\n",
      "    [Structure Change] dropout_rate changed: 0.2835072377864848 -> 0.498601860763653\n",
      "    Rebuilding NN1 model due to structure change\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 0.517694, Val Loss: 0.012352\n",
      "    Epoch 5/50, Train Loss: 0.000379, Val Loss: 0.000252\n",
      "Early stopping at epoch 9, Train Loss: 0.000386, Val Loss: 0.000267\n",
      "    Final - Train Loss: 0.000386, Val Loss: 0.000267\n",
      "[Saved] NN1_w5_2020Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000373, Val Loss: 0.000244\n",
      "    Epoch 5/20, Train Loss: 0.000373, Val Loss: 0.000244\n",
      "Early stopping at epoch 6, Train Loss: 0.000373, Val Loss: 0.000244\n",
      "    Final - Train Loss: 0.000373, Val Loss: 0.000244\n",
      "[Saved] NN2_w5_2020Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000371, Val Loss: 0.000244\n",
      "    Epoch 5/20, Train Loss: 0.000371, Val Loss: 0.000244\n",
      "Early stopping at epoch 6, Train Loss: 0.000370, Val Loss: 0.000244\n",
      "    Final - Train Loss: 0.000370, Val Loss: 0.000244\n",
      "[Saved] NN3_w5_2020Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000371, Val Loss: 0.000245\n",
      "    Epoch 5/20, Train Loss: 0.000371, Val Loss: 0.000245\n",
      "Early stopping at epoch 7, Train Loss: 0.000370, Val Loss: 0.000245\n",
      "    Final - Train Loss: 0.000370, Val Loss: 0.000245\n",
      "[Saved] NN4_w5_2020Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000370, Val Loss: 0.000245\n",
      "    Epoch 5/20, Train Loss: 0.000370, Val Loss: 0.000245\n",
      "Early stopping at epoch 7, Train Loss: 0.000370, Val Loss: 0.000245\n",
      "    Final - Train Loss: 0.000370, Val Loss: 0.000245\n",
      "[Saved] NN5_w5_2020Q4.pth\n",
      "    Training data size: 255178 samples\n",
      "  Quarter 2021Q1: Training models with expanding data\n",
      "    Added 3114 samples from 2020Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000381, Val Loss: 0.000270\n",
      "    Epoch 5/20, Train Loss: 0.000374, Val Loss: 0.000261\n",
      "    Epoch 10/20, Train Loss: 0.000372, Val Loss: 0.000259\n",
      "    Epoch 15/20, Train Loss: 0.000371, Val Loss: 0.000258\n",
      "    Epoch 20/20, Train Loss: 0.000371, Val Loss: 0.000258\n",
      "Early stopping at epoch 20, Train Loss: 0.000371, Val Loss: 0.000258\n",
      "    Final - Train Loss: 0.000371, Val Loss: 0.000258\n",
      "[Saved] NN1_w5_2021Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000370, Val Loss: 0.000257\n",
      "    Epoch 5/20, Train Loss: 0.000370, Val Loss: 0.000257\n",
      "Early stopping at epoch 6, Train Loss: 0.000370, Val Loss: 0.000257\n",
      "    Final - Train Loss: 0.000370, Val Loss: 0.000257\n",
      "[Saved] NN2_w5_2021Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000368, Val Loss: 0.000257\n",
      "    Epoch 5/20, Train Loss: 0.000368, Val Loss: 0.000258\n",
      "Early stopping at epoch 6, Train Loss: 0.000368, Val Loss: 0.000258\n",
      "    Final - Train Loss: 0.000368, Val Loss: 0.000258\n",
      "[Saved] NN3_w5_2021Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000367, Val Loss: 0.000257\n",
      "    Epoch 5/20, Train Loss: 0.000368, Val Loss: 0.000258\n",
      "    Epoch 10/20, Train Loss: 0.000367, Val Loss: 0.000257\n",
      "Early stopping at epoch 13, Train Loss: 0.000367, Val Loss: 0.000257\n",
      "    Final - Train Loss: 0.000367, Val Loss: 0.000257\n",
      "[Saved] NN4_w5_2021Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000367, Val Loss: 0.000258\n",
      "    Epoch 5/20, Train Loss: 0.000367, Val Loss: 0.000258\n",
      "    Epoch 10/20, Train Loss: 0.000366, Val Loss: 0.000258\n",
      "Early stopping at epoch 12, Train Loss: 0.000366, Val Loss: 0.000258\n",
      "    Final - Train Loss: 0.000366, Val Loss: 0.000258\n",
      "[Saved] NN5_w5_2021Q1.pth\n",
      "    Training data size: 258292 samples\n",
      "  Quarter 2021Q2: Training models with expanding data\n",
      "    Added 2989 samples from 2021Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000369, Val Loss: 0.000269\n",
      "    Epoch 5/20, Train Loss: 0.000369, Val Loss: 0.000268\n",
      "Early stopping at epoch 9, Train Loss: 0.000368, Val Loss: 0.000269\n",
      "    Final - Train Loss: 0.000368, Val Loss: 0.000269\n",
      "[Saved] NN1_w5_2021Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000368, Val Loss: 0.000268\n",
      "    Epoch 5/20, Train Loss: 0.000368, Val Loss: 0.000268\n",
      "    Epoch 10/20, Train Loss: 0.000368, Val Loss: 0.000268\n",
      "Early stopping at epoch 10, Train Loss: 0.000368, Val Loss: 0.000268\n",
      "    Final - Train Loss: 0.000368, Val Loss: 0.000268\n",
      "[Saved] NN2_w5_2021Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000366, Val Loss: 0.000268\n",
      "    Epoch 5/20, Train Loss: 0.000365, Val Loss: 0.000268\n",
      "Early stopping at epoch 9, Train Loss: 0.000366, Val Loss: 0.000268\n",
      "    Final - Train Loss: 0.000366, Val Loss: 0.000268\n",
      "[Saved] NN3_w5_2021Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000365, Val Loss: 0.000268\n",
      "    Epoch 5/20, Train Loss: 0.000365, Val Loss: 0.000268\n",
      "Early stopping at epoch 9, Train Loss: 0.000365, Val Loss: 0.000268\n",
      "    Final - Train Loss: 0.000365, Val Loss: 0.000268\n",
      "[Saved] NN4_w5_2021Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000364, Val Loss: 0.000268\n",
      "    Epoch 5/20, Train Loss: 0.000364, Val Loss: 0.000268\n",
      "Early stopping at epoch 8, Train Loss: 0.000365, Val Loss: 0.000268\n",
      "    Final - Train Loss: 0.000365, Val Loss: 0.000268\n",
      "[Saved] NN5_w5_2021Q2.pth\n",
      "    Training data size: 261281 samples\n",
      "  Quarter 2021Q3: Training models with expanding data\n",
      "    Added 3132 samples from 2021Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000366, Val Loss: 0.000271\n",
      "    Epoch 5/20, Train Loss: 0.000366, Val Loss: 0.000272\n",
      "Early stopping at epoch 7, Train Loss: 0.000366, Val Loss: 0.000272\n",
      "    Final - Train Loss: 0.000366, Val Loss: 0.000272\n",
      "[Saved] NN1_w5_2021Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000366, Val Loss: 0.000271\n",
      "    Epoch 5/20, Train Loss: 0.000365, Val Loss: 0.000271\n",
      "Early stopping at epoch 6, Train Loss: 0.000365, Val Loss: 0.000271\n",
      "    Final - Train Loss: 0.000365, Val Loss: 0.000271\n",
      "[Saved] NN2_w5_2021Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000363, Val Loss: 0.000271\n",
      "    Epoch 5/20, Train Loss: 0.000363, Val Loss: 0.000271\n",
      "    Epoch 10/20, Train Loss: 0.000363, Val Loss: 0.000271\n",
      "    Epoch 15/20, Train Loss: 0.000362, Val Loss: 0.000271\n",
      "Early stopping at epoch 16, Train Loss: 0.000362, Val Loss: 0.000271\n",
      "    Final - Train Loss: 0.000362, Val Loss: 0.000271\n",
      "[Saved] NN3_w5_2021Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000362, Val Loss: 0.000271\n",
      "    Epoch 5/20, Train Loss: 0.000361, Val Loss: 0.000271\n",
      "Early stopping at epoch 6, Train Loss: 0.000363, Val Loss: 0.000271\n",
      "    Final - Train Loss: 0.000363, Val Loss: 0.000271\n",
      "[Saved] NN4_w5_2021Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000361, Val Loss: 0.000272\n",
      "    Epoch 5/20, Train Loss: 0.000362, Val Loss: 0.000272\n",
      "Early stopping at epoch 9, Train Loss: 0.000361, Val Loss: 0.000273\n",
      "    Final - Train Loss: 0.000361, Val Loss: 0.000273\n",
      "[Saved] NN5_w5_2021Q3.pth\n",
      "    Training data size: 264413 samples\n",
      "  Quarter 2021Q4: Training models with expanding data\n",
      "    Added 3173 samples from 2021Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000363, Val Loss: 0.000278\n",
      "    Epoch 5/20, Train Loss: 0.000363, Val Loss: 0.000279\n",
      "Early stopping at epoch 7, Train Loss: 0.000363, Val Loss: 0.000279\n",
      "    Final - Train Loss: 0.000363, Val Loss: 0.000279\n",
      "[Saved] NN1_w5_2021Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000362, Val Loss: 0.000276\n",
      "    Epoch 5/20, Train Loss: 0.000362, Val Loss: 0.000276\n",
      "    Epoch 10/20, Train Loss: 0.000362, Val Loss: 0.000277\n",
      "Early stopping at epoch 11, Train Loss: 0.000362, Val Loss: 0.000276\n",
      "    Final - Train Loss: 0.000362, Val Loss: 0.000276\n",
      "[Saved] NN2_w5_2021Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000360, Val Loss: 0.000276\n",
      "    Epoch 5/20, Train Loss: 0.000360, Val Loss: 0.000276\n",
      "Early stopping at epoch 8, Train Loss: 0.000360, Val Loss: 0.000276\n",
      "    Final - Train Loss: 0.000360, Val Loss: 0.000276\n",
      "[Saved] NN3_w5_2021Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000360, Val Loss: 0.000277\n",
      "    Epoch 5/20, Train Loss: 0.000359, Val Loss: 0.000277\n",
      "    Epoch 10/20, Train Loss: 0.000359, Val Loss: 0.000277\n",
      "Early stopping at epoch 10, Train Loss: 0.000359, Val Loss: 0.000277\n",
      "    Final - Train Loss: 0.000359, Val Loss: 0.000277\n",
      "[Saved] NN4_w5_2021Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000358, Val Loss: 0.000277\n",
      "    Epoch 5/20, Train Loss: 0.000358, Val Loss: 0.000277\n",
      "    Epoch 10/20, Train Loss: 0.000358, Val Loss: 0.000277\n",
      "Early stopping at epoch 12, Train Loss: 0.000358, Val Loss: 0.000277\n",
      "    Final - Train Loss: 0.000358, Val Loss: 0.000277\n",
      "[Saved] NN5_w5_2021Q4.pth\n",
      "    Training data size: 267586 samples\n",
      "  Quarter 2022Q1: Training models with expanding data\n",
      "    Added 3153 samples from 2021Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000360, Val Loss: 0.000282\n",
      "    Epoch 5/20, Train Loss: 0.000360, Val Loss: 0.000282\n",
      "Early stopping at epoch 8, Train Loss: 0.000360, Val Loss: 0.000283\n",
      "    Final - Train Loss: 0.000360, Val Loss: 0.000283\n",
      "[Saved] NN1_w5_2022Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000360, Val Loss: 0.000280\n",
      "    Epoch 5/20, Train Loss: 0.000360, Val Loss: 0.000280\n",
      "Early stopping at epoch 9, Train Loss: 0.000360, Val Loss: 0.000281\n",
      "    Final - Train Loss: 0.000360, Val Loss: 0.000281\n",
      "[Saved] NN2_w5_2022Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000358, Val Loss: 0.000280\n",
      "    Epoch 5/20, Train Loss: 0.000357, Val Loss: 0.000281\n",
      "Early stopping at epoch 6, Train Loss: 0.000358, Val Loss: 0.000280\n",
      "    Final - Train Loss: 0.000358, Val Loss: 0.000280\n",
      "[Saved] NN3_w5_2022Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000357, Val Loss: 0.000280\n",
      "    Epoch 5/20, Train Loss: 0.000357, Val Loss: 0.000280\n",
      "    Epoch 10/20, Train Loss: 0.000357, Val Loss: 0.000280\n",
      "    Epoch 15/20, Train Loss: 0.000356, Val Loss: 0.000280\n",
      "Early stopping at epoch 19, Train Loss: 0.000357, Val Loss: 0.000280\n",
      "    Final - Train Loss: 0.000357, Val Loss: 0.000280\n",
      "[Saved] NN4_w5_2022Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000356, Val Loss: 0.000281\n",
      "    Epoch 5/20, Train Loss: 0.000356, Val Loss: 0.000280\n",
      "Early stopping at epoch 7, Train Loss: 0.000356, Val Loss: 0.000281\n",
      "    Final - Train Loss: 0.000356, Val Loss: 0.000281\n",
      "[Saved] NN5_w5_2022Q1.pth\n",
      "    Training data size: 270739 samples\n",
      "  Quarter 2022Q2: Training models with expanding data\n",
      "    Added 3029 samples from 2022Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000358, Val Loss: 0.000295\n",
      "    Epoch 5/20, Train Loss: 0.000358, Val Loss: 0.000296\n",
      "Early stopping at epoch 6, Train Loss: 0.000358, Val Loss: 0.000295\n",
      "    Final - Train Loss: 0.000358, Val Loss: 0.000295\n",
      "[Saved] NN1_w5_2022Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000358, Val Loss: 0.000293\n",
      "    Epoch 5/20, Train Loss: 0.000358, Val Loss: 0.000293\n",
      "Early stopping at epoch 9, Train Loss: 0.000358, Val Loss: 0.000293\n",
      "    Final - Train Loss: 0.000358, Val Loss: 0.000293\n",
      "[Saved] NN2_w5_2022Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000355, Val Loss: 0.000293\n",
      "    Epoch 5/20, Train Loss: 0.000355, Val Loss: 0.000293\n",
      "    Epoch 10/20, Train Loss: 0.000356, Val Loss: 0.000293\n",
      "Early stopping at epoch 13, Train Loss: 0.000355, Val Loss: 0.000293\n",
      "    Final - Train Loss: 0.000355, Val Loss: 0.000293\n",
      "[Saved] NN3_w5_2022Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000354, Val Loss: 0.000293\n",
      "    Epoch 5/20, Train Loss: 0.000354, Val Loss: 0.000293\n",
      "Early stopping at epoch 9, Train Loss: 0.000354, Val Loss: 0.000294\n",
      "    Final - Train Loss: 0.000354, Val Loss: 0.000294\n",
      "[Saved] NN4_w5_2022Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000354, Val Loss: 0.000293\n",
      "    Epoch 5/20, Train Loss: 0.000354, Val Loss: 0.000293\n",
      "Early stopping at epoch 8, Train Loss: 0.000354, Val Loss: 0.000294\n",
      "    Final - Train Loss: 0.000354, Val Loss: 0.000294\n",
      "[Saved] NN5_w5_2022Q2.pth\n",
      "    Training data size: 273768 samples\n",
      "  Quarter 2022Q3: Training models with expanding data\n",
      "    Added 2969 samples from 2022Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000356, Val Loss: 0.000310\n",
      "    Epoch 5/20, Train Loss: 0.000356, Val Loss: 0.000311\n",
      "Early stopping at epoch 6, Train Loss: 0.000356, Val Loss: 0.000311\n",
      "    Final - Train Loss: 0.000356, Val Loss: 0.000311\n",
      "[Saved] NN1_w5_2022Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000356, Val Loss: 0.000309\n",
      "    Epoch 5/20, Train Loss: 0.000356, Val Loss: 0.000309\n",
      "Early stopping at epoch 7, Train Loss: 0.000356, Val Loss: 0.000309\n",
      "    Final - Train Loss: 0.000356, Val Loss: 0.000309\n",
      "[Saved] NN2_w5_2022Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000354, Val Loss: 0.000309\n",
      "    Epoch 5/20, Train Loss: 0.000354, Val Loss: 0.000309\n",
      "    Epoch 10/20, Train Loss: 0.000354, Val Loss: 0.000309\n",
      "Early stopping at epoch 12, Train Loss: 0.000353, Val Loss: 0.000309\n",
      "    Final - Train Loss: 0.000353, Val Loss: 0.000309\n",
      "[Saved] NN3_w5_2022Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000353, Val Loss: 0.000309\n",
      "    Epoch 5/20, Train Loss: 0.000352, Val Loss: 0.000309\n",
      "Early stopping at epoch 7, Train Loss: 0.000353, Val Loss: 0.000309\n",
      "    Final - Train Loss: 0.000353, Val Loss: 0.000309\n",
      "[Saved] NN4_w5_2022Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000352, Val Loss: 0.000310\n",
      "    Epoch 5/20, Train Loss: 0.000352, Val Loss: 0.000310\n",
      "    Epoch 10/20, Train Loss: 0.000352, Val Loss: 0.000310\n",
      "Early stopping at epoch 11, Train Loss: 0.000352, Val Loss: 0.000310\n",
      "    Final - Train Loss: 0.000352, Val Loss: 0.000310\n",
      "[Saved] NN5_w5_2022Q3.pth\n",
      "    Training data size: 276737 samples\n",
      "  Quarter 2022Q4: Training models with expanding data\n",
      "    Added 3152 samples from 2022Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000355, Val Loss: 0.000317\n",
      "    Epoch 5/20, Train Loss: 0.000355, Val Loss: 0.000317\n",
      "Early stopping at epoch 9, Train Loss: 0.000355, Val Loss: 0.000318\n",
      "    Final - Train Loss: 0.000355, Val Loss: 0.000318\n",
      "[Saved] NN1_w5_2022Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000355, Val Loss: 0.000316\n",
      "    Epoch 5/20, Train Loss: 0.000355, Val Loss: 0.000316\n",
      "Early stopping at epoch 9, Train Loss: 0.000355, Val Loss: 0.000316\n",
      "    Final - Train Loss: 0.000355, Val Loss: 0.000316\n",
      "[Saved] NN2_w5_2022Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000352, Val Loss: 0.000316\n",
      "    Epoch 5/20, Train Loss: 0.000352, Val Loss: 0.000316\n",
      "    Epoch 10/20, Train Loss: 0.000352, Val Loss: 0.000317\n",
      "Early stopping at epoch 12, Train Loss: 0.000353, Val Loss: 0.000316\n",
      "    Final - Train Loss: 0.000353, Val Loss: 0.000316\n",
      "[Saved] NN3_w5_2022Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000352, Val Loss: 0.000317\n",
      "    Epoch 5/20, Train Loss: 0.000352, Val Loss: 0.000317\n",
      "Early stopping at epoch 6, Train Loss: 0.000352, Val Loss: 0.000317\n",
      "    Final - Train Loss: 0.000352, Val Loss: 0.000317\n",
      "[Saved] NN4_w5_2022Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000351, Val Loss: 0.000317\n",
      "    Epoch 5/20, Train Loss: 0.000351, Val Loss: 0.000317\n",
      "Early stopping at epoch 6, Train Loss: 0.000351, Val Loss: 0.000317\n",
      "    Final - Train Loss: 0.000351, Val Loss: 0.000317\n",
      "[Saved] NN5_w5_2022Q4.pth\n",
      "    Training data size: 279889 samples\n",
      "  Quarter 2023Q1: Training models with expanding data\n",
      "    Added 3070 samples from 2022Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000354, Val Loss: 0.000325\n",
      "    Epoch 5/20, Train Loss: 0.000354, Val Loss: 0.000325\n",
      "Early stopping at epoch 8, Train Loss: 0.000353, Val Loss: 0.000325\n",
      "    Final - Train Loss: 0.000353, Val Loss: 0.000325\n",
      "[Saved] NN1_w5_2023Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000353, Val Loss: 0.000322\n",
      "    Epoch 5/20, Train Loss: 0.000354, Val Loss: 0.000322\n",
      "Early stopping at epoch 9, Train Loss: 0.000353, Val Loss: 0.000322\n",
      "    Final - Train Loss: 0.000353, Val Loss: 0.000322\n",
      "[Saved] NN2_w5_2023Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000351, Val Loss: 0.000322\n",
      "    Epoch 5/20, Train Loss: 0.000351, Val Loss: 0.000322\n",
      "Early stopping at epoch 9, Train Loss: 0.000351, Val Loss: 0.000323\n",
      "    Final - Train Loss: 0.000351, Val Loss: 0.000323\n",
      "[Saved] NN3_w5_2023Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000350, Val Loss: 0.000322\n",
      "    Epoch 5/20, Train Loss: 0.000350, Val Loss: 0.000322\n",
      "    Epoch 10/20, Train Loss: 0.000350, Val Loss: 0.000322\n",
      "Early stopping at epoch 11, Train Loss: 0.000350, Val Loss: 0.000322\n",
      "    Final - Train Loss: 0.000350, Val Loss: 0.000322\n",
      "[Saved] NN4_w5_2023Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000349, Val Loss: 0.000322\n",
      "    Epoch 5/20, Train Loss: 0.000350, Val Loss: 0.000323\n",
      "    Epoch 10/20, Train Loss: 0.000349, Val Loss: 0.000323\n",
      "Early stopping at epoch 12, Train Loss: 0.000349, Val Loss: 0.000323\n",
      "    Final - Train Loss: 0.000349, Val Loss: 0.000323\n",
      "[Saved] NN5_w5_2023Q1.pth\n",
      "    Training data size: 282959 samples\n",
      "  Quarter 2023Q2: Training models with expanding data\n",
      "    Added 3064 samples from 2023Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000351, Val Loss: 0.000333\n",
      "    Epoch 5/20, Train Loss: 0.000351, Val Loss: 0.000335\n",
      "Early stopping at epoch 7, Train Loss: 0.000351, Val Loss: 0.000335\n",
      "    Final - Train Loss: 0.000351, Val Loss: 0.000335\n",
      "[Saved] NN1_w5_2023Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000351, Val Loss: 0.000328\n",
      "    Epoch 5/20, Train Loss: 0.000351, Val Loss: 0.000328\n",
      "    Epoch 10/20, Train Loss: 0.000352, Val Loss: 0.000328\n",
      "Early stopping at epoch 14, Train Loss: 0.000352, Val Loss: 0.000328\n",
      "    Final - Train Loss: 0.000352, Val Loss: 0.000328\n",
      "[Saved] NN2_w5_2023Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000349, Val Loss: 0.000327\n",
      "    Epoch 5/20, Train Loss: 0.000349, Val Loss: 0.000328\n",
      "Early stopping at epoch 6, Train Loss: 0.000349, Val Loss: 0.000327\n",
      "    Final - Train Loss: 0.000349, Val Loss: 0.000327\n",
      "[Saved] NN3_w5_2023Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000348, Val Loss: 0.000328\n",
      "    Epoch 5/20, Train Loss: 0.000348, Val Loss: 0.000328\n",
      "Early stopping at epoch 9, Train Loss: 0.000348, Val Loss: 0.000328\n",
      "    Final - Train Loss: 0.000348, Val Loss: 0.000328\n",
      "[Saved] NN4_w5_2023Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000347, Val Loss: 0.000328\n",
      "    Epoch 5/20, Train Loss: 0.000347, Val Loss: 0.000328\n",
      "    Epoch 10/20, Train Loss: 0.000347, Val Loss: 0.000328\n",
      "Early stopping at epoch 10, Train Loss: 0.000347, Val Loss: 0.000328\n",
      "    Final - Train Loss: 0.000347, Val Loss: 0.000328\n",
      "[Saved] NN5_w5_2023Q2.pth\n",
      "    Training data size: 286023 samples\n",
      "  Quarter 2023Q3: Training models with expanding data\n",
      "    Added 3069 samples from 2023Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000349, Val Loss: 0.000339\n",
      "    Epoch 5/20, Train Loss: 0.000349, Val Loss: 0.000338\n",
      "Early stopping at epoch 9, Train Loss: 0.000349, Val Loss: 0.000338\n",
      "    Final - Train Loss: 0.000349, Val Loss: 0.000338\n",
      "[Saved] NN1_w5_2023Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000349, Val Loss: 0.000331\n",
      "    Epoch 5/20, Train Loss: 0.000349, Val Loss: 0.000331\n",
      "Early stopping at epoch 7, Train Loss: 0.000349, Val Loss: 0.000331\n",
      "    Final - Train Loss: 0.000349, Val Loss: 0.000331\n",
      "[Saved] NN2_w5_2023Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000347, Val Loss: 0.000332\n",
      "    Epoch 5/20, Train Loss: 0.000347, Val Loss: 0.000331\n",
      "Early stopping at epoch 7, Train Loss: 0.000347, Val Loss: 0.000332\n",
      "    Final - Train Loss: 0.000347, Val Loss: 0.000332\n",
      "[Saved] NN3_w5_2023Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000346, Val Loss: 0.000332\n",
      "    Epoch 5/20, Train Loss: 0.000346, Val Loss: 0.000332\n",
      "Early stopping at epoch 7, Train Loss: 0.000346, Val Loss: 0.000332\n",
      "    Final - Train Loss: 0.000346, Val Loss: 0.000332\n",
      "[Saved] NN4_w5_2023Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000345, Val Loss: 0.000332\n",
      "    Epoch 5/20, Train Loss: 0.000345, Val Loss: 0.000332\n",
      "Early stopping at epoch 9, Train Loss: 0.000345, Val Loss: 0.000332\n",
      "    Final - Train Loss: 0.000345, Val Loss: 0.000332\n",
      "[Saved] NN5_w5_2023Q3.pth\n",
      "    Training data size: 289092 samples\n",
      "  Quarter 2023Q4: Training models with expanding data\n",
      "    Added 3121 samples from 2023Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000349, Val Loss: 0.000324\n",
      "    Epoch 5/20, Train Loss: 0.000349, Val Loss: 0.000324\n",
      "Early stopping at epoch 8, Train Loss: 0.000349, Val Loss: 0.000324\n",
      "    Final - Train Loss: 0.000349, Val Loss: 0.000324\n",
      "[Saved] NN1_w5_2023Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000349, Val Loss: 0.000323\n",
      "    Epoch 5/20, Train Loss: 0.000349, Val Loss: 0.000323\n",
      "Early stopping at epoch 7, Train Loss: 0.000349, Val Loss: 0.000323\n",
      "    Final - Train Loss: 0.000349, Val Loss: 0.000323\n",
      "[Saved] NN2_w5_2023Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000347, Val Loss: 0.000323\n",
      "    Epoch 5/20, Train Loss: 0.000347, Val Loss: 0.000323\n",
      "    Epoch 10/20, Train Loss: 0.000347, Val Loss: 0.000323\n",
      "Early stopping at epoch 11, Train Loss: 0.000347, Val Loss: 0.000323\n",
      "    Final - Train Loss: 0.000347, Val Loss: 0.000323\n",
      "[Saved] NN3_w5_2023Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000346, Val Loss: 0.000323\n",
      "    Epoch 5/20, Train Loss: 0.000345, Val Loss: 0.000323\n",
      "Early stopping at epoch 7, Train Loss: 0.000346, Val Loss: 0.000323\n",
      "    Final - Train Loss: 0.000346, Val Loss: 0.000323\n",
      "[Saved] NN4_w5_2023Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000345, Val Loss: 0.000324\n",
      "    Epoch 5/20, Train Loss: 0.000345, Val Loss: 0.000324\n",
      "Early stopping at epoch 8, Train Loss: 0.000345, Val Loss: 0.000323\n",
      "    Final - Train Loss: 0.000345, Val Loss: 0.000323\n",
      "[Saved] NN5_w5_2023Q4.pth\n",
      "    Training data size: 292213 samples\n",
      "  Quarter 2024Q1: Training models with expanding data\n",
      "    Added 3113 samples from 2023Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000348, Val Loss: 0.000327\n",
      "    Epoch 5/20, Train Loss: 0.000348, Val Loss: 0.000326\n",
      "    Epoch 10/20, Train Loss: 0.000348, Val Loss: 0.000326\n",
      "Early stopping at epoch 10, Train Loss: 0.000348, Val Loss: 0.000326\n",
      "    Final - Train Loss: 0.000348, Val Loss: 0.000326\n",
      "[Saved] NN1_w5_2024Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000348, Val Loss: 0.000324\n",
      "    Epoch 5/20, Train Loss: 0.000348, Val Loss: 0.000324\n",
      "    Epoch 10/20, Train Loss: 0.000348, Val Loss: 0.000324\n",
      "Early stopping at epoch 12, Train Loss: 0.000348, Val Loss: 0.000324\n",
      "    Final - Train Loss: 0.000348, Val Loss: 0.000324\n",
      "[Saved] NN2_w5_2024Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000346, Val Loss: 0.000324\n",
      "    Epoch 5/20, Train Loss: 0.000346, Val Loss: 0.000324\n",
      "    Epoch 10/20, Train Loss: 0.000346, Val Loss: 0.000324\n",
      "Early stopping at epoch 10, Train Loss: 0.000346, Val Loss: 0.000324\n",
      "    Final - Train Loss: 0.000346, Val Loss: 0.000324\n",
      "[Saved] NN3_w5_2024Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000345, Val Loss: 0.000323\n",
      "    Epoch 5/20, Train Loss: 0.000345, Val Loss: 0.000323\n",
      "    Epoch 10/20, Train Loss: 0.000344, Val Loss: 0.000323\n",
      "Early stopping at epoch 11, Train Loss: 0.000345, Val Loss: 0.000323\n",
      "    Final - Train Loss: 0.000345, Val Loss: 0.000323\n",
      "[Saved] NN4_w5_2024Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000344, Val Loss: 0.000323\n",
      "    Epoch 5/20, Train Loss: 0.000344, Val Loss: 0.000324\n",
      "    Epoch 10/20, Train Loss: 0.000344, Val Loss: 0.000324\n",
      "Early stopping at epoch 11, Train Loss: 0.000344, Val Loss: 0.000324\n",
      "    Final - Train Loss: 0.000344, Val Loss: 0.000324\n",
      "[Saved] NN5_w5_2024Q1.pth\n",
      "    Training data size: 295326 samples\n",
      "  Quarter 2024Q2: Training models with expanding data\n",
      "    Added 3026 samples from 2024Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000346, Val Loss: 0.000330\n",
      "    Epoch 5/20, Train Loss: 0.000346, Val Loss: 0.000329\n",
      "Early stopping at epoch 7, Train Loss: 0.000346, Val Loss: 0.000331\n",
      "    Final - Train Loss: 0.000346, Val Loss: 0.000331\n",
      "[Saved] NN1_w5_2024Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000346, Val Loss: 0.000324\n",
      "    Epoch 5/20, Train Loss: 0.000346, Val Loss: 0.000324\n",
      "Early stopping at epoch 6, Train Loss: 0.000346, Val Loss: 0.000324\n",
      "    Final - Train Loss: 0.000346, Val Loss: 0.000324\n",
      "[Saved] NN2_w5_2024Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000344, Val Loss: 0.000324\n",
      "    Epoch 5/20, Train Loss: 0.000344, Val Loss: 0.000324\n",
      "    Epoch 10/20, Train Loss: 0.000344, Val Loss: 0.000324\n",
      "Early stopping at epoch 14, Train Loss: 0.000344, Val Loss: 0.000324\n",
      "    Final - Train Loss: 0.000344, Val Loss: 0.000324\n",
      "[Saved] NN3_w5_2024Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000343, Val Loss: 0.000324\n",
      "    Epoch 5/20, Train Loss: 0.000343, Val Loss: 0.000325\n",
      "    Epoch 10/20, Train Loss: 0.000343, Val Loss: 0.000325\n",
      "Early stopping at epoch 12, Train Loss: 0.000343, Val Loss: 0.000325\n",
      "    Final - Train Loss: 0.000343, Val Loss: 0.000325\n",
      "[Saved] NN4_w5_2024Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000342, Val Loss: 0.000325\n",
      "    Epoch 5/20, Train Loss: 0.000342, Val Loss: 0.000325\n",
      "    Epoch 10/20, Train Loss: 0.000341, Val Loss: 0.000325\n",
      "Early stopping at epoch 10, Train Loss: 0.000341, Val Loss: 0.000325\n",
      "    Final - Train Loss: 0.000341, Val Loss: 0.000325\n",
      "[Saved] NN5_w5_2024Q2.pth\n",
      "    Training data size: 298352 samples\n",
      "  Quarter 2024Q3: Training models with expanding data\n",
      "    Added 3112 samples from 2024Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000344, Val Loss: 0.000330\n",
      "    Epoch 5/20, Train Loss: 0.000344, Val Loss: 0.000331\n",
      "Early stopping at epoch 6, Train Loss: 0.000344, Val Loss: 0.000331\n",
      "    Final - Train Loss: 0.000344, Val Loss: 0.000331\n",
      "[Saved] NN1_w5_2024Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000345, Val Loss: 0.000325\n",
      "    Epoch 5/20, Train Loss: 0.000345, Val Loss: 0.000325\n",
      "Early stopping at epoch 8, Train Loss: 0.000345, Val Loss: 0.000325\n",
      "    Final - Train Loss: 0.000345, Val Loss: 0.000325\n",
      "[Saved] NN2_w5_2024Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000342, Val Loss: 0.000325\n",
      "    Epoch 5/20, Train Loss: 0.000342, Val Loss: 0.000325\n",
      "Early stopping at epoch 6, Train Loss: 0.000342, Val Loss: 0.000325\n",
      "    Final - Train Loss: 0.000342, Val Loss: 0.000325\n",
      "[Saved] NN3_w5_2024Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000341, Val Loss: 0.000326\n",
      "    Epoch 5/20, Train Loss: 0.000341, Val Loss: 0.000325\n",
      "    Epoch 10/20, Train Loss: 0.000340, Val Loss: 0.000325\n",
      "    Epoch 15/20, Train Loss: 0.000341, Val Loss: 0.000325\n",
      "Early stopping at epoch 18, Train Loss: 0.000340, Val Loss: 0.000326\n",
      "    Final - Train Loss: 0.000340, Val Loss: 0.000326\n",
      "[Saved] NN4_w5_2024Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000340, Val Loss: 0.000326\n",
      "    Epoch 5/20, Train Loss: 0.000340, Val Loss: 0.000325\n",
      "    Epoch 10/20, Train Loss: 0.000340, Val Loss: 0.000326\n",
      "Early stopping at epoch 10, Train Loss: 0.000340, Val Loss: 0.000326\n",
      "    Final - Train Loss: 0.000340, Val Loss: 0.000326\n",
      "[Saved] NN5_w5_2024Q3.pth\n",
      "    Training data size: 301464 samples\n",
      "Processing window size: 21\n",
      "  Quarter 2015Q4: Training models with expanding data\n",
      "    [Re-tuning] NN1 for 2015Q4\n",
      "[Hyper] NN1: best_MSE=0.000285, params={'learning_rate': 0.0006496380037338187, 'batch_size': 64, 'dropout_rate': 0.4391665462247476}\n",
      "    [Structure Change] dropout_rate changed: 0.1 -> 0.4391665462247476\n",
      "    Rebuilding NN1 model due to structure change\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 0.139486, Val Loss: 0.000233\n",
      "    Epoch 5/50, Train Loss: 0.000465, Val Loss: 0.000168\n",
      "    Epoch 10/50, Train Loss: 0.000437, Val Loss: 0.000167\n",
      "    Epoch 15/50, Train Loss: 0.000435, Val Loss: 0.000166\n",
      "    Epoch 20/50, Train Loss: 0.000435, Val Loss: 0.000166\n",
      "Early stopping at epoch 22, Train Loss: 0.000434, Val Loss: 0.000167\n",
      "    Final - Train Loss: 0.000434, Val Loss: 0.000167\n",
      "[Saved] NN1_w21_2015Q4.pth\n",
      "    Creating new NN2 model\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 0.129768, Val Loss: 0.000203\n",
      "    Epoch 5/50, Train Loss: 0.000463, Val Loss: 0.000165\n",
      "    Epoch 10/50, Train Loss: 0.000436, Val Loss: 0.000166\n",
      "Early stopping at epoch 10, Train Loss: 0.000436, Val Loss: 0.000166\n",
      "    Final - Train Loss: 0.000436, Val Loss: 0.000166\n",
      "[Saved] NN2_w21_2015Q4.pth\n",
      "    Creating new NN3 model\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 0.105512, Val Loss: 0.000191\n",
      "    Epoch 5/50, Train Loss: 0.000460, Val Loss: 0.000164\n",
      "    Epoch 10/50, Train Loss: 0.000436, Val Loss: 0.000166\n",
      "Early stopping at epoch 10, Train Loss: 0.000436, Val Loss: 0.000166\n",
      "    Final - Train Loss: 0.000436, Val Loss: 0.000166\n",
      "[Saved] NN3_w21_2015Q4.pth\n",
      "    Creating new NN4 model\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 0.084401, Val Loss: 0.000286\n",
      "    Epoch 5/50, Train Loss: 0.000454, Val Loss: 0.000165\n",
      "Early stopping at epoch 9, Train Loss: 0.000436, Val Loss: 0.000166\n",
      "    Final - Train Loss: 0.000436, Val Loss: 0.000166\n",
      "[Saved] NN4_w21_2015Q4.pth\n",
      "    Creating new NN5 model\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 0.132794, Val Loss: 0.000937\n",
      "    Epoch 5/50, Train Loss: 0.000454, Val Loss: 0.000167\n",
      "Early stopping at epoch 8, Train Loss: 0.000437, Val Loss: 0.000166\n",
      "    Final - Train Loss: 0.000437, Val Loss: 0.000166\n",
      "[Saved] NN5_w21_2015Q4.pth\n",
      "    Training data size: 196120 samples\n",
      "  Quarter 2016Q1: Training models with expanding data\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000435, Val Loss: 0.000166\n",
      "    Epoch 5/20, Train Loss: 0.000434, Val Loss: 0.000167\n",
      "Early stopping at epoch 6, Train Loss: 0.000434, Val Loss: 0.000167\n",
      "    Final - Train Loss: 0.000434, Val Loss: 0.000167\n",
      "[Saved] NN1_w21_2016Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000448, Val Loss: 0.000167\n",
      "    Epoch 5/20, Train Loss: 0.000436, Val Loss: 0.000166\n",
      "Early stopping at epoch 8, Train Loss: 0.000436, Val Loss: 0.000166\n",
      "    Final - Train Loss: 0.000436, Val Loss: 0.000166\n",
      "[Saved] NN2_w21_2016Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000445, Val Loss: 0.000166\n",
      "    Epoch 5/20, Train Loss: 0.000436, Val Loss: 0.000166\n",
      "Early stopping at epoch 9, Train Loss: 0.000435, Val Loss: 0.000166\n",
      "    Final - Train Loss: 0.000435, Val Loss: 0.000166\n",
      "[Saved] NN3_w21_2016Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000444, Val Loss: 0.000167\n",
      "    Epoch 5/20, Train Loss: 0.000436, Val Loss: 0.000166\n",
      "Early stopping at epoch 8, Train Loss: 0.000436, Val Loss: 0.000167\n",
      "    Final - Train Loss: 0.000436, Val Loss: 0.000167\n",
      "[Saved] NN4_w21_2016Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000445, Val Loss: 0.000166\n",
      "    Epoch 5/20, Train Loss: 0.000436, Val Loss: 0.000167\n",
      "Early stopping at epoch 7, Train Loss: 0.000435, Val Loss: 0.000166\n",
      "    Final - Train Loss: 0.000435, Val Loss: 0.000166\n",
      "[Saved] NN5_w21_2016Q1.pth\n",
      "    Training data size: 196120 samples\n",
      "  Quarter 2016Q2: Training models with expanding data\n",
      "    Added 2956 samples from 2016Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000431, Val Loss: 0.000173\n",
      "    Epoch 5/20, Train Loss: 0.000430, Val Loss: 0.000173\n",
      "Early stopping at epoch 6, Train Loss: 0.000430, Val Loss: 0.000173\n",
      "    Final - Train Loss: 0.000430, Val Loss: 0.000173\n",
      "[Saved] NN1_w21_2016Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000432, Val Loss: 0.000172\n",
      "    Epoch 5/20, Train Loss: 0.000432, Val Loss: 0.000172\n",
      "    Epoch 10/20, Train Loss: 0.000432, Val Loss: 0.000172\n",
      "Early stopping at epoch 13, Train Loss: 0.000431, Val Loss: 0.000172\n",
      "    Final - Train Loss: 0.000431, Val Loss: 0.000172\n",
      "[Saved] NN2_w21_2016Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000432, Val Loss: 0.000171\n",
      "    Epoch 5/20, Train Loss: 0.000431, Val Loss: 0.000172\n",
      "    Epoch 10/20, Train Loss: 0.000431, Val Loss: 0.000172\n",
      "Early stopping at epoch 12, Train Loss: 0.000430, Val Loss: 0.000171\n",
      "    Final - Train Loss: 0.000430, Val Loss: 0.000171\n",
      "[Saved] NN3_w21_2016Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000432, Val Loss: 0.000171\n",
      "    Epoch 5/20, Train Loss: 0.000431, Val Loss: 0.000172\n",
      "    Epoch 10/20, Train Loss: 0.000431, Val Loss: 0.000171\n",
      "Early stopping at epoch 13, Train Loss: 0.000430, Val Loss: 0.000171\n",
      "    Final - Train Loss: 0.000430, Val Loss: 0.000171\n",
      "[Saved] NN4_w21_2016Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000432, Val Loss: 0.000172\n",
      "    Epoch 5/20, Train Loss: 0.000431, Val Loss: 0.000172\n",
      "Early stopping at epoch 9, Train Loss: 0.000430, Val Loss: 0.000172\n",
      "    Final - Train Loss: 0.000430, Val Loss: 0.000172\n",
      "[Saved] NN5_w21_2016Q2.pth\n",
      "    Training data size: 199076 samples\n",
      "  Quarter 2016Q3: Training models with expanding data\n",
      "    Added 3170 samples from 2016Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000426, Val Loss: 0.000178\n",
      "    Epoch 5/20, Train Loss: 0.000426, Val Loss: 0.000179\n",
      "Early stopping at epoch 7, Train Loss: 0.000425, Val Loss: 0.000179\n",
      "    Final - Train Loss: 0.000425, Val Loss: 0.000179\n",
      "[Saved] NN1_w21_2016Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000427, Val Loss: 0.000174\n",
      "    Epoch 5/20, Train Loss: 0.000427, Val Loss: 0.000174\n",
      "Early stopping at epoch 6, Train Loss: 0.000427, Val Loss: 0.000174\n",
      "    Final - Train Loss: 0.000427, Val Loss: 0.000174\n",
      "[Saved] NN2_w21_2016Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000427, Val Loss: 0.000174\n",
      "    Epoch 5/20, Train Loss: 0.000426, Val Loss: 0.000174\n",
      "    Epoch 10/20, Train Loss: 0.000425, Val Loss: 0.000174\n",
      "Early stopping at epoch 11, Train Loss: 0.000425, Val Loss: 0.000174\n",
      "    Final - Train Loss: 0.000425, Val Loss: 0.000174\n",
      "[Saved] NN3_w21_2016Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000426, Val Loss: 0.000174\n",
      "    Epoch 5/20, Train Loss: 0.000425, Val Loss: 0.000174\n",
      "    Epoch 10/20, Train Loss: 0.000425, Val Loss: 0.000174\n",
      "    Epoch 15/20, Train Loss: 0.000422, Val Loss: 0.000174\n",
      "    Epoch 20/20, Train Loss: 0.000421, Val Loss: 0.000174\n",
      "    Final - Train Loss: 0.000421, Val Loss: 0.000174\n",
      "[Saved] NN4_w21_2016Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000427, Val Loss: 0.000174\n",
      "    Epoch 5/20, Train Loss: 0.000425, Val Loss: 0.000174\n",
      "Early stopping at epoch 8, Train Loss: 0.000425, Val Loss: 0.000174\n",
      "    Final - Train Loss: 0.000425, Val Loss: 0.000174\n",
      "[Saved] NN5_w21_2016Q3.pth\n",
      "    Training data size: 202246 samples\n",
      "  Quarter 2016Q4: Training models with expanding data\n",
      "    Added 3176 samples from 2016Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000422, Val Loss: 0.000171\n",
      "    Epoch 5/20, Train Loss: 0.000422, Val Loss: 0.000172\n",
      "Early stopping at epoch 8, Train Loss: 0.000422, Val Loss: 0.000172\n",
      "    Final - Train Loss: 0.000422, Val Loss: 0.000172\n",
      "[Saved] NN1_w21_2016Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000423, Val Loss: 0.000170\n",
      "    Epoch 5/20, Train Loss: 0.000423, Val Loss: 0.000170\n",
      "Early stopping at epoch 7, Train Loss: 0.000423, Val Loss: 0.000170\n",
      "    Final - Train Loss: 0.000423, Val Loss: 0.000170\n",
      "[Saved] NN2_w21_2016Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000423, Val Loss: 0.000170\n",
      "    Epoch 5/20, Train Loss: 0.000422, Val Loss: 0.000170\n",
      "    Epoch 10/20, Train Loss: 0.000421, Val Loss: 0.000170\n",
      "Early stopping at epoch 12, Train Loss: 0.000421, Val Loss: 0.000170\n",
      "    Final - Train Loss: 0.000421, Val Loss: 0.000170\n",
      "[Saved] NN3_w21_2016Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000419, Val Loss: 0.000170\n",
      "    Epoch 5/20, Train Loss: 0.000419, Val Loss: 0.000170\n",
      "    Epoch 10/20, Train Loss: 0.000419, Val Loss: 0.000170\n",
      "Early stopping at epoch 10, Train Loss: 0.000419, Val Loss: 0.000170\n",
      "    Final - Train Loss: 0.000419, Val Loss: 0.000170\n",
      "[Saved] NN4_w21_2016Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000422, Val Loss: 0.000170\n",
      "    Epoch 5/20, Train Loss: 0.000421, Val Loss: 0.000170\n",
      "    Epoch 10/20, Train Loss: 0.000420, Val Loss: 0.000170\n",
      "    Epoch 15/20, Train Loss: 0.000417, Val Loss: 0.000170\n",
      "Early stopping at epoch 17, Train Loss: 0.000417, Val Loss: 0.000170\n",
      "    Final - Train Loss: 0.000417, Val Loss: 0.000170\n",
      "[Saved] NN5_w21_2016Q4.pth\n",
      "    Training data size: 205422 samples\n",
      "  Quarter 2017Q1: Training models with expanding data\n",
      "    Added 3123 samples from 2016Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000418, Val Loss: 0.000176\n",
      "    Epoch 5/20, Train Loss: 0.000418, Val Loss: 0.000178\n",
      "Early stopping at epoch 6, Train Loss: 0.000417, Val Loss: 0.000178\n",
      "    Final - Train Loss: 0.000417, Val Loss: 0.000178\n",
      "[Saved] NN1_w21_2017Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000419, Val Loss: 0.000172\n",
      "    Epoch 5/20, Train Loss: 0.000419, Val Loss: 0.000172\n",
      "Early stopping at epoch 6, Train Loss: 0.000419, Val Loss: 0.000172\n",
      "    Final - Train Loss: 0.000419, Val Loss: 0.000172\n",
      "[Saved] NN2_w21_2017Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000418, Val Loss: 0.000172\n",
      "    Epoch 5/20, Train Loss: 0.000417, Val Loss: 0.000173\n",
      "    Epoch 10/20, Train Loss: 0.000416, Val Loss: 0.000173\n",
      "Early stopping at epoch 11, Train Loss: 0.000417, Val Loss: 0.000172\n",
      "    Final - Train Loss: 0.000417, Val Loss: 0.000172\n",
      "[Saved] NN3_w21_2017Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000415, Val Loss: 0.000173\n",
      "    Epoch 5/20, Train Loss: 0.000415, Val Loss: 0.000173\n",
      "Early stopping at epoch 9, Train Loss: 0.000415, Val Loss: 0.000173\n",
      "    Final - Train Loss: 0.000415, Val Loss: 0.000173\n",
      "[Saved] NN4_w21_2017Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000415, Val Loss: 0.000173\n",
      "    Epoch 5/20, Train Loss: 0.000416, Val Loss: 0.000173\n",
      "    Epoch 10/20, Train Loss: 0.000414, Val Loss: 0.000173\n",
      "Early stopping at epoch 10, Train Loss: 0.000414, Val Loss: 0.000173\n",
      "    Final - Train Loss: 0.000414, Val Loss: 0.000173\n",
      "[Saved] NN5_w21_2017Q1.pth\n",
      "    Training data size: 208545 samples\n",
      "  Quarter 2017Q2: Training models with expanding data\n",
      "    Added 3083 samples from 2017Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000414, Val Loss: 0.000172\n",
      "    Epoch 5/20, Train Loss: 0.000414, Val Loss: 0.000173\n",
      "Early stopping at epoch 6, Train Loss: 0.000413, Val Loss: 0.000173\n",
      "    Final - Train Loss: 0.000413, Val Loss: 0.000173\n",
      "[Saved] NN1_w21_2017Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000415, Val Loss: 0.000170\n",
      "    Epoch 5/20, Train Loss: 0.000415, Val Loss: 0.000170\n",
      "Early stopping at epoch 6, Train Loss: 0.000415, Val Loss: 0.000170\n",
      "    Final - Train Loss: 0.000415, Val Loss: 0.000170\n",
      "[Saved] NN2_w21_2017Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000413, Val Loss: 0.000170\n",
      "    Epoch 5/20, Train Loss: 0.000412, Val Loss: 0.000170\n",
      "Early stopping at epoch 8, Train Loss: 0.000412, Val Loss: 0.000170\n",
      "    Final - Train Loss: 0.000412, Val Loss: 0.000170\n",
      "[Saved] NN3_w21_2017Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000410, Val Loss: 0.000170\n",
      "    Epoch 5/20, Train Loss: 0.000410, Val Loss: 0.000170\n",
      "    Epoch 10/20, Train Loss: 0.000410, Val Loss: 0.000170\n",
      "Early stopping at epoch 14, Train Loss: 0.000409, Val Loss: 0.000170\n",
      "    Final - Train Loss: 0.000409, Val Loss: 0.000170\n",
      "[Saved] NN4_w21_2017Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000411, Val Loss: 0.000170\n",
      "    Epoch 5/20, Train Loss: 0.000410, Val Loss: 0.000170\n",
      "Early stopping at epoch 7, Train Loss: 0.000410, Val Loss: 0.000170\n",
      "    Final - Train Loss: 0.000410, Val Loss: 0.000170\n",
      "[Saved] NN5_w21_2017Q2.pth\n",
      "    Training data size: 211628 samples\n",
      "  Quarter 2017Q3: Training models with expanding data\n",
      "    Added 3122 samples from 2017Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000409, Val Loss: 0.000175\n",
      "    Epoch 5/20, Train Loss: 0.000409, Val Loss: 0.000176\n",
      "Early stopping at epoch 7, Train Loss: 0.000410, Val Loss: 0.000175\n",
      "    Final - Train Loss: 0.000410, Val Loss: 0.000175\n",
      "[Saved] NN1_w21_2017Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000411, Val Loss: 0.000170\n",
      "    Epoch 5/20, Train Loss: 0.000411, Val Loss: 0.000170\n",
      "Early stopping at epoch 6, Train Loss: 0.000411, Val Loss: 0.000170\n",
      "    Final - Train Loss: 0.000411, Val Loss: 0.000170\n",
      "[Saved] NN2_w21_2017Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000409, Val Loss: 0.000170\n",
      "    Epoch 5/20, Train Loss: 0.000409, Val Loss: 0.000170\n",
      "    Epoch 10/20, Train Loss: 0.000408, Val Loss: 0.000170\n",
      "Early stopping at epoch 12, Train Loss: 0.000408, Val Loss: 0.000170\n",
      "    Final - Train Loss: 0.000408, Val Loss: 0.000170\n",
      "[Saved] NN3_w21_2017Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000407, Val Loss: 0.000170\n",
      "    Epoch 5/20, Train Loss: 0.000405, Val Loss: 0.000170\n",
      "    Epoch 10/20, Train Loss: 0.000405, Val Loss: 0.000170\n",
      "    Epoch 15/20, Train Loss: 0.000402, Val Loss: 0.000170\n",
      "Early stopping at epoch 19, Train Loss: 0.000402, Val Loss: 0.000170\n",
      "    Final - Train Loss: 0.000402, Val Loss: 0.000170\n",
      "[Saved] NN4_w21_2017Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000407, Val Loss: 0.000170\n",
      "    Epoch 5/20, Train Loss: 0.000407, Val Loss: 0.000170\n",
      "    Epoch 10/20, Train Loss: 0.000406, Val Loss: 0.000170\n",
      "Early stopping at epoch 10, Train Loss: 0.000406, Val Loss: 0.000170\n",
      "    Final - Train Loss: 0.000406, Val Loss: 0.000170\n",
      "[Saved] NN5_w21_2017Q3.pth\n",
      "    Training data size: 214750 samples\n",
      "  Quarter 2017Q4: Training models with expanding data\n",
      "    Added 3114 samples from 2017Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000406, Val Loss: 0.000174\n",
      "    Epoch 5/20, Train Loss: 0.000406, Val Loss: 0.000175\n",
      "    Epoch 10/20, Train Loss: 0.000406, Val Loss: 0.000175\n",
      "Early stopping at epoch 14, Train Loss: 0.000406, Val Loss: 0.000175\n",
      "    Final - Train Loss: 0.000406, Val Loss: 0.000175\n",
      "[Saved] NN1_w21_2017Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000408, Val Loss: 0.000169\n",
      "    Epoch 5/20, Train Loss: 0.000408, Val Loss: 0.000169\n",
      "Early stopping at epoch 6, Train Loss: 0.000408, Val Loss: 0.000169\n",
      "    Final - Train Loss: 0.000408, Val Loss: 0.000169\n",
      "[Saved] NN2_w21_2017Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000405, Val Loss: 0.000169\n",
      "    Epoch 5/20, Train Loss: 0.000405, Val Loss: 0.000169\n",
      "    Epoch 10/20, Train Loss: 0.000403, Val Loss: 0.000169\n",
      "Early stopping at epoch 13, Train Loss: 0.000403, Val Loss: 0.000169\n",
      "    Final - Train Loss: 0.000403, Val Loss: 0.000169\n",
      "[Saved] NN3_w21_2017Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000401, Val Loss: 0.000169\n",
      "    Epoch 5/20, Train Loss: 0.000401, Val Loss: 0.000169\n",
      "Early stopping at epoch 6, Train Loss: 0.000401, Val Loss: 0.000169\n",
      "    Final - Train Loss: 0.000401, Val Loss: 0.000169\n",
      "[Saved] NN4_w21_2017Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000403, Val Loss: 0.000169\n",
      "    Epoch 5/20, Train Loss: 0.000403, Val Loss: 0.000169\n",
      "Early stopping at epoch 6, Train Loss: 0.000402, Val Loss: 0.000169\n",
      "    Final - Train Loss: 0.000402, Val Loss: 0.000169\n",
      "[Saved] NN5_w21_2017Q4.pth\n",
      "    Training data size: 217864 samples\n",
      "  Quarter 2018Q1: Training models with expanding data\n",
      "    Added 3115 samples from 2017Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000402, Val Loss: 0.000183\n",
      "    Epoch 5/20, Train Loss: 0.000402, Val Loss: 0.000183\n",
      "Early stopping at epoch 7, Train Loss: 0.000402, Val Loss: 0.000188\n",
      "    Final - Train Loss: 0.000402, Val Loss: 0.000188\n",
      "[Saved] NN1_w21_2018Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000404, Val Loss: 0.000172\n",
      "    Epoch 5/20, Train Loss: 0.000404, Val Loss: 0.000172\n",
      "    Epoch 10/20, Train Loss: 0.000402, Val Loss: 0.000172\n",
      "    Epoch 15/20, Train Loss: 0.000402, Val Loss: 0.000172\n",
      "Early stopping at epoch 19, Train Loss: 0.000402, Val Loss: 0.000172\n",
      "    Final - Train Loss: 0.000402, Val Loss: 0.000172\n",
      "[Saved] NN2_w21_2018Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000401, Val Loss: 0.000172\n",
      "    Epoch 5/20, Train Loss: 0.000400, Val Loss: 0.000172\n",
      "Early stopping at epoch 7, Train Loss: 0.000400, Val Loss: 0.000172\n",
      "    Final - Train Loss: 0.000400, Val Loss: 0.000172\n",
      "[Saved] NN3_w21_2018Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000396, Val Loss: 0.000172\n",
      "    Epoch 5/20, Train Loss: 0.000397, Val Loss: 0.000172\n",
      "    Epoch 10/20, Train Loss: 0.000396, Val Loss: 0.000172\n",
      "Early stopping at epoch 13, Train Loss: 0.000398, Val Loss: 0.000172\n",
      "    Final - Train Loss: 0.000398, Val Loss: 0.000172\n",
      "[Saved] NN4_w21_2018Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000400, Val Loss: 0.000172\n",
      "    Epoch 5/20, Train Loss: 0.000399, Val Loss: 0.000172\n",
      "    Epoch 10/20, Train Loss: 0.000398, Val Loss: 0.000172\n",
      "    Epoch 15/20, Train Loss: 0.000395, Val Loss: 0.000172\n",
      "Early stopping at epoch 16, Train Loss: 0.000395, Val Loss: 0.000172\n",
      "    Final - Train Loss: 0.000395, Val Loss: 0.000172\n",
      "[Saved] NN5_w21_2018Q1.pth\n",
      "    Training data size: 220979 samples\n",
      "  Quarter 2018Q2: Training models with expanding data\n",
      "    Added 2996 samples from 2018Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000398, Val Loss: 0.000187\n",
      "    Epoch 5/20, Train Loss: 0.000398, Val Loss: 0.000187\n",
      "    Epoch 10/20, Train Loss: 0.000398, Val Loss: 0.000189\n",
      "Early stopping at epoch 10, Train Loss: 0.000398, Val Loss: 0.000189\n",
      "    Final - Train Loss: 0.000398, Val Loss: 0.000189\n",
      "[Saved] NN1_w21_2018Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000400, Val Loss: 0.000181\n",
      "    Epoch 5/20, Train Loss: 0.000400, Val Loss: 0.000181\n",
      "Early stopping at epoch 9, Train Loss: 0.000400, Val Loss: 0.000181\n",
      "    Final - Train Loss: 0.000400, Val Loss: 0.000181\n",
      "[Saved] NN2_w21_2018Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000397, Val Loss: 0.000181\n",
      "    Epoch 5/20, Train Loss: 0.000397, Val Loss: 0.000180\n",
      "    Epoch 10/20, Train Loss: 0.000394, Val Loss: 0.000181\n",
      "Early stopping at epoch 10, Train Loss: 0.000394, Val Loss: 0.000181\n",
      "    Final - Train Loss: 0.000394, Val Loss: 0.000181\n",
      "[Saved] NN3_w21_2018Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000393, Val Loss: 0.000181\n",
      "    Epoch 5/20, Train Loss: 0.000393, Val Loss: 0.000181\n",
      "Early stopping at epoch 6, Train Loss: 0.000393, Val Loss: 0.000181\n",
      "    Final - Train Loss: 0.000393, Val Loss: 0.000181\n",
      "[Saved] NN4_w21_2018Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000394, Val Loss: 0.000181\n",
      "    Epoch 5/20, Train Loss: 0.000393, Val Loss: 0.000181\n",
      "Early stopping at epoch 8, Train Loss: 0.000391, Val Loss: 0.000181\n",
      "    Final - Train Loss: 0.000391, Val Loss: 0.000181\n",
      "[Saved] NN5_w21_2018Q2.pth\n",
      "    Training data size: 223975 samples\n",
      "  Quarter 2018Q3: Training models with expanding data\n",
      "    Added 3160 samples from 2018Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000395, Val Loss: 0.000186\n",
      "    Epoch 5/20, Train Loss: 0.000395, Val Loss: 0.000188\n",
      "Early stopping at epoch 6, Train Loss: 0.000394, Val Loss: 0.000187\n",
      "    Final - Train Loss: 0.000394, Val Loss: 0.000187\n",
      "[Saved] NN1_w21_2018Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000396, Val Loss: 0.000182\n",
      "    Epoch 5/20, Train Loss: 0.000397, Val Loss: 0.000182\n",
      "    Epoch 10/20, Train Loss: 0.000396, Val Loss: 0.000182\n",
      "Early stopping at epoch 10, Train Loss: 0.000396, Val Loss: 0.000182\n",
      "    Final - Train Loss: 0.000396, Val Loss: 0.000182\n",
      "[Saved] NN2_w21_2018Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000394, Val Loss: 0.000182\n",
      "    Epoch 5/20, Train Loss: 0.000393, Val Loss: 0.000182\n",
      "Early stopping at epoch 6, Train Loss: 0.000393, Val Loss: 0.000182\n",
      "    Final - Train Loss: 0.000393, Val Loss: 0.000182\n",
      "[Saved] NN3_w21_2018Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000390, Val Loss: 0.000182\n",
      "    Epoch 5/20, Train Loss: 0.000389, Val Loss: 0.000182\n",
      "Early stopping at epoch 6, Train Loss: 0.000389, Val Loss: 0.000182\n",
      "    Final - Train Loss: 0.000389, Val Loss: 0.000182\n",
      "[Saved] NN4_w21_2018Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000390, Val Loss: 0.000183\n",
      "    Epoch 5/20, Train Loss: 0.000390, Val Loss: 0.000182\n",
      "    Epoch 10/20, Train Loss: 0.000388, Val Loss: 0.000182\n",
      "    Epoch 15/20, Train Loss: 0.000388, Val Loss: 0.000183\n",
      "Early stopping at epoch 16, Train Loss: 0.000386, Val Loss: 0.000182\n",
      "    Final - Train Loss: 0.000386, Val Loss: 0.000182\n",
      "[Saved] NN5_w21_2018Q3.pth\n",
      "    Training data size: 227135 samples\n",
      "  Quarter 2018Q4: Training models with expanding data\n",
      "    Added 3125 samples from 2018Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000391, Val Loss: 0.000193\n",
      "    Epoch 5/20, Train Loss: 0.000392, Val Loss: 0.000197\n",
      "Early stopping at epoch 8, Train Loss: 0.000391, Val Loss: 0.000196\n",
      "    Final - Train Loss: 0.000391, Val Loss: 0.000196\n",
      "[Saved] NN1_w21_2018Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000393, Val Loss: 0.000181\n",
      "    Epoch 5/20, Train Loss: 0.000393, Val Loss: 0.000182\n",
      "Early stopping at epoch 7, Train Loss: 0.000393, Val Loss: 0.000182\n",
      "    Final - Train Loss: 0.000393, Val Loss: 0.000182\n",
      "[Saved] NN2_w21_2018Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000390, Val Loss: 0.000182\n",
      "    Epoch 5/20, Train Loss: 0.000390, Val Loss: 0.000182\n",
      "Early stopping at epoch 7, Train Loss: 0.000390, Val Loss: 0.000182\n",
      "    Final - Train Loss: 0.000390, Val Loss: 0.000182\n",
      "[Saved] NN3_w21_2018Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000386, Val Loss: 0.000182\n",
      "    Epoch 5/20, Train Loss: 0.000386, Val Loss: 0.000182\n",
      "    Epoch 10/20, Train Loss: 0.000386, Val Loss: 0.000182\n",
      "Early stopping at epoch 13, Train Loss: 0.000385, Val Loss: 0.000182\n",
      "    Final - Train Loss: 0.000385, Val Loss: 0.000182\n",
      "[Saved] NN4_w21_2018Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000385, Val Loss: 0.000182\n",
      "    Epoch 5/20, Train Loss: 0.000386, Val Loss: 0.000182\n",
      "Early stopping at epoch 7, Train Loss: 0.000385, Val Loss: 0.000182\n",
      "    Final - Train Loss: 0.000385, Val Loss: 0.000182\n",
      "[Saved] NN5_w21_2018Q4.pth\n",
      "    Training data size: 230260 samples\n",
      "  Quarter 2019Q1: Training models with expanding data\n",
      "    Added 3045 samples from 2018Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000388, Val Loss: 0.000213\n",
      "    Epoch 5/20, Train Loss: 0.000389, Val Loss: 0.000213\n",
      "Early stopping at epoch 7, Train Loss: 0.000389, Val Loss: 0.000218\n",
      "    Final - Train Loss: 0.000389, Val Loss: 0.000218\n",
      "[Saved] NN1_w21_2019Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000390, Val Loss: 0.000194\n",
      "    Epoch 5/20, Train Loss: 0.000390, Val Loss: 0.000194\n",
      "    Epoch 10/20, Train Loss: 0.000388, Val Loss: 0.000194\n",
      "Early stopping at epoch 13, Train Loss: 0.000388, Val Loss: 0.000194\n",
      "    Final - Train Loss: 0.000388, Val Loss: 0.000194\n",
      "[Saved] NN2_w21_2019Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000387, Val Loss: 0.000194\n",
      "    Epoch 5/20, Train Loss: 0.000387, Val Loss: 0.000194\n",
      "Early stopping at epoch 6, Train Loss: 0.000386, Val Loss: 0.000194\n",
      "    Final - Train Loss: 0.000386, Val Loss: 0.000194\n",
      "[Saved] NN3_w21_2019Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000383, Val Loss: 0.000194\n",
      "    Epoch 5/20, Train Loss: 0.000384, Val Loss: 0.000194\n",
      "Early stopping at epoch 9, Train Loss: 0.000382, Val Loss: 0.000194\n",
      "    Final - Train Loss: 0.000382, Val Loss: 0.000194\n",
      "[Saved] NN4_w21_2019Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000383, Val Loss: 0.000194\n",
      "    Epoch 5/20, Train Loss: 0.000382, Val Loss: 0.000194\n",
      "Early stopping at epoch 7, Train Loss: 0.000382, Val Loss: 0.000194\n",
      "    Final - Train Loss: 0.000382, Val Loss: 0.000194\n",
      "[Saved] NN5_w21_2019Q1.pth\n",
      "    Training data size: 233305 samples\n",
      "  Quarter 2019Q2: Training models with expanding data\n",
      "    Added 3022 samples from 2019Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000385, Val Loss: 0.000223\n",
      "    Epoch 5/20, Train Loss: 0.000385, Val Loss: 0.000221\n",
      "    Epoch 10/20, Train Loss: 0.000385, Val Loss: 0.000224\n",
      "Early stopping at epoch 11, Train Loss: 0.000385, Val Loss: 0.000223\n",
      "    Final - Train Loss: 0.000385, Val Loss: 0.000223\n",
      "[Saved] NN1_w21_2019Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000386, Val Loss: 0.000198\n",
      "    Epoch 5/20, Train Loss: 0.000387, Val Loss: 0.000198\n",
      "Early stopping at epoch 7, Train Loss: 0.000386, Val Loss: 0.000198\n",
      "    Final - Train Loss: 0.000386, Val Loss: 0.000198\n",
      "[Saved] NN2_w21_2019Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000383, Val Loss: 0.000198\n",
      "    Epoch 5/20, Train Loss: 0.000384, Val Loss: 0.000198\n",
      "Early stopping at epoch 6, Train Loss: 0.000383, Val Loss: 0.000198\n",
      "    Final - Train Loss: 0.000383, Val Loss: 0.000198\n",
      "[Saved] NN3_w21_2019Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000380, Val Loss: 0.000199\n",
      "    Epoch 5/20, Train Loss: 0.000379, Val Loss: 0.000199\n",
      "Early stopping at epoch 7, Train Loss: 0.000379, Val Loss: 0.000198\n",
      "    Final - Train Loss: 0.000379, Val Loss: 0.000198\n",
      "[Saved] NN4_w21_2019Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000378, Val Loss: 0.000198\n",
      "    Epoch 5/20, Train Loss: 0.000379, Val Loss: 0.000198\n",
      "    Epoch 10/20, Train Loss: 0.000376, Val Loss: 0.000198\n",
      "Early stopping at epoch 13, Train Loss: 0.000376, Val Loss: 0.000198\n",
      "    Final - Train Loss: 0.000376, Val Loss: 0.000198\n",
      "[Saved] NN5_w21_2019Q2.pth\n",
      "    Training data size: 236327 samples\n",
      "  Quarter 2019Q3: Training models with expanding data\n",
      "    Added 3120 samples from 2019Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000382, Val Loss: 0.000209\n",
      "    Epoch 5/20, Train Loss: 0.000382, Val Loss: 0.000209\n",
      "Early stopping at epoch 7, Train Loss: 0.000382, Val Loss: 0.000210\n",
      "    Final - Train Loss: 0.000382, Val Loss: 0.000210\n",
      "[Saved] NN1_w21_2019Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000384, Val Loss: 0.000198\n",
      "    Epoch 5/20, Train Loss: 0.000384, Val Loss: 0.000198\n",
      "Early stopping at epoch 7, Train Loss: 0.000383, Val Loss: 0.000198\n",
      "    Final - Train Loss: 0.000383, Val Loss: 0.000198\n",
      "[Saved] NN2_w21_2019Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000381, Val Loss: 0.000198\n",
      "    Epoch 5/20, Train Loss: 0.000381, Val Loss: 0.000198\n",
      "Early stopping at epoch 8, Train Loss: 0.000379, Val Loss: 0.000198\n",
      "    Final - Train Loss: 0.000379, Val Loss: 0.000198\n",
      "[Saved] NN3_w21_2019Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000375, Val Loss: 0.000198\n",
      "    Epoch 5/20, Train Loss: 0.000376, Val Loss: 0.000198\n",
      "    Epoch 10/20, Train Loss: 0.000375, Val Loss: 0.000198\n",
      "Early stopping at epoch 10, Train Loss: 0.000375, Val Loss: 0.000198\n",
      "    Final - Train Loss: 0.000375, Val Loss: 0.000198\n",
      "[Saved] NN4_w21_2019Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000375, Val Loss: 0.000198\n",
      "    Epoch 5/20, Train Loss: 0.000375, Val Loss: 0.000198\n",
      "Early stopping at epoch 6, Train Loss: 0.000375, Val Loss: 0.000198\n",
      "    Final - Train Loss: 0.000375, Val Loss: 0.000198\n",
      "[Saved] NN5_w21_2019Q3.pth\n",
      "    Training data size: 239447 samples\n",
      "  Quarter 2019Q4: Training models with expanding data\n",
      "    Added 3167 samples from 2019Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000380, Val Loss: 0.000202\n",
      "    Epoch 5/20, Train Loss: 0.000381, Val Loss: 0.000202\n",
      "    Epoch 10/20, Train Loss: 0.000381, Val Loss: 0.000202\n",
      "Early stopping at epoch 14, Train Loss: 0.000381, Val Loss: 0.000202\n",
      "    Final - Train Loss: 0.000381, Val Loss: 0.000202\n",
      "[Saved] NN1_w21_2019Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000382, Val Loss: 0.000199\n",
      "    Epoch 5/20, Train Loss: 0.000382, Val Loss: 0.000199\n",
      "    Epoch 10/20, Train Loss: 0.000380, Val Loss: 0.000198\n",
      "    Epoch 15/20, Train Loss: 0.000380, Val Loss: 0.000198\n",
      "Early stopping at epoch 15, Train Loss: 0.000380, Val Loss: 0.000198\n",
      "    Final - Train Loss: 0.000380, Val Loss: 0.000198\n",
      "[Saved] NN2_w21_2019Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000379, Val Loss: 0.000198\n",
      "    Epoch 5/20, Train Loss: 0.000379, Val Loss: 0.000198\n",
      "Early stopping at epoch 7, Train Loss: 0.000379, Val Loss: 0.000199\n",
      "    Final - Train Loss: 0.000379, Val Loss: 0.000199\n",
      "[Saved] NN3_w21_2019Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000374, Val Loss: 0.000198\n",
      "    Epoch 5/20, Train Loss: 0.000374, Val Loss: 0.000199\n",
      "Early stopping at epoch 9, Train Loss: 0.000374, Val Loss: 0.000198\n",
      "    Final - Train Loss: 0.000374, Val Loss: 0.000198\n",
      "[Saved] NN4_w21_2019Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000374, Val Loss: 0.000199\n",
      "    Epoch 5/20, Train Loss: 0.000374, Val Loss: 0.000199\n",
      "    Epoch 10/20, Train Loss: 0.000374, Val Loss: 0.000198\n",
      "    Epoch 15/20, Train Loss: 0.000372, Val Loss: 0.000198\n",
      "Early stopping at epoch 16, Train Loss: 0.000373, Val Loss: 0.000198\n",
      "    Final - Train Loss: 0.000373, Val Loss: 0.000198\n",
      "[Saved] NN5_w21_2019Q4.pth\n",
      "    Training data size: 242614 samples\n",
      "  Quarter 2020Q1: Training models with expanding data\n",
      "    Added 3179 samples from 2019Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000379, Val Loss: 0.000202\n",
      "    Epoch 5/20, Train Loss: 0.000380, Val Loss: 0.000201\n",
      "    Epoch 10/20, Train Loss: 0.000379, Val Loss: 0.000203\n",
      "Early stopping at epoch 10, Train Loss: 0.000379, Val Loss: 0.000203\n",
      "    Final - Train Loss: 0.000379, Val Loss: 0.000203\n",
      "[Saved] NN1_w21_2020Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000380, Val Loss: 0.000198\n",
      "    Epoch 5/20, Train Loss: 0.000380, Val Loss: 0.000198\n",
      "    Epoch 10/20, Train Loss: 0.000379, Val Loss: 0.000194\n",
      "    Epoch 15/20, Train Loss: 0.000379, Val Loss: 0.000194\n",
      "Early stopping at epoch 16, Train Loss: 0.000378, Val Loss: 0.000194\n",
      "    Final - Train Loss: 0.000378, Val Loss: 0.000194\n",
      "[Saved] NN2_w21_2020Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000378, Val Loss: 0.000199\n",
      "    Epoch 5/20, Train Loss: 0.000378, Val Loss: 0.000198\n",
      "    Epoch 10/20, Train Loss: 0.000378, Val Loss: 0.000198\n",
      "    Epoch 15/20, Train Loss: 0.000377, Val Loss: 0.000198\n",
      "Early stopping at epoch 15, Train Loss: 0.000377, Val Loss: 0.000198\n",
      "    Final - Train Loss: 0.000377, Val Loss: 0.000198\n",
      "[Saved] NN3_w21_2020Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000372, Val Loss: 0.000200\n",
      "    Epoch 5/20, Train Loss: 0.000373, Val Loss: 0.000199\n",
      "    Epoch 10/20, Train Loss: 0.000372, Val Loss: 0.000199\n",
      "Early stopping at epoch 14, Train Loss: 0.000372, Val Loss: 0.000199\n",
      "    Final - Train Loss: 0.000372, Val Loss: 0.000199\n",
      "[Saved] NN4_w21_2020Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000371, Val Loss: 0.000201\n",
      "    Epoch 5/20, Train Loss: 0.000371, Val Loss: 0.000199\n",
      "    Epoch 10/20, Train Loss: 0.000371, Val Loss: 0.000198\n",
      "Early stopping at epoch 14, Train Loss: 0.000370, Val Loss: 0.000200\n",
      "    Final - Train Loss: 0.000370, Val Loss: 0.000200\n",
      "[Saved] NN5_w21_2020Q1.pth\n",
      "    Training data size: 245793 samples\n",
      "  Quarter 2020Q2: Training models with expanding data\n",
      "    Added 2595 samples from 2020Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000379, Val Loss: 0.000214\n",
      "    Epoch 5/20, Train Loss: 0.000378, Val Loss: 0.000214\n",
      "    Epoch 10/20, Train Loss: 0.000378, Val Loss: 0.000214\n",
      "Early stopping at epoch 14, Train Loss: 0.000378, Val Loss: 0.000213\n",
      "    Final - Train Loss: 0.000378, Val Loss: 0.000213\n",
      "[Saved] NN1_w21_2020Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000379, Val Loss: 0.000211\n",
      "    Epoch 5/20, Train Loss: 0.000379, Val Loss: 0.000211\n",
      "Early stopping at epoch 7, Train Loss: 0.000380, Val Loss: 0.000211\n",
      "    Final - Train Loss: 0.000380, Val Loss: 0.000211\n",
      "[Saved] NN2_w21_2020Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000376, Val Loss: 0.000209\n",
      "    Epoch 5/20, Train Loss: 0.000375, Val Loss: 0.000211\n",
      "Early stopping at epoch 7, Train Loss: 0.000375, Val Loss: 0.000210\n",
      "    Final - Train Loss: 0.000375, Val Loss: 0.000210\n",
      "[Saved] NN3_w21_2020Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000372, Val Loss: 0.000209\n",
      "    Epoch 5/20, Train Loss: 0.000369, Val Loss: 0.000209\n",
      "    Epoch 10/20, Train Loss: 0.000371, Val Loss: 0.000211\n",
      "Early stopping at epoch 11, Train Loss: 0.000371, Val Loss: 0.000209\n",
      "    Final - Train Loss: 0.000371, Val Loss: 0.000209\n",
      "[Saved] NN4_w21_2020Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000368, Val Loss: 0.000212\n",
      "    Epoch 5/20, Train Loss: 0.000370, Val Loss: 0.000211\n",
      "    Epoch 10/20, Train Loss: 0.000369, Val Loss: 0.000209\n",
      "    Epoch 15/20, Train Loss: 0.000369, Val Loss: 0.000210\n",
      "Early stopping at epoch 15, Train Loss: 0.000369, Val Loss: 0.000210\n",
      "    Final - Train Loss: 0.000369, Val Loss: 0.000210\n",
      "[Saved] NN5_w21_2020Q2.pth\n",
      "    Training data size: 248388 samples\n",
      "  Quarter 2020Q3: Training models with expanding data\n",
      "    Added 2839 samples from 2020Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000376, Val Loss: 0.000334\n",
      "    Epoch 5/20, Train Loss: 0.000376, Val Loss: 0.000340\n",
      "Early stopping at epoch 8, Train Loss: 0.000375, Val Loss: 0.000343\n",
      "    Final - Train Loss: 0.000375, Val Loss: 0.000343\n",
      "[Saved] NN1_w21_2020Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000377, Val Loss: 0.000236\n",
      "    Epoch 5/20, Train Loss: 0.000377, Val Loss: 0.000237\n",
      "    Epoch 10/20, Train Loss: 0.000377, Val Loss: 0.000237\n",
      "Early stopping at epoch 11, Train Loss: 0.000377, Val Loss: 0.000236\n",
      "    Final - Train Loss: 0.000377, Val Loss: 0.000236\n",
      "[Saved] NN2_w21_2020Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000373, Val Loss: 0.000238\n",
      "    Epoch 5/20, Train Loss: 0.000373, Val Loss: 0.000237\n",
      "Early stopping at epoch 7, Train Loss: 0.000374, Val Loss: 0.000238\n",
      "    Final - Train Loss: 0.000374, Val Loss: 0.000238\n",
      "[Saved] NN3_w21_2020Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000368, Val Loss: 0.000238\n",
      "    Epoch 5/20, Train Loss: 0.000368, Val Loss: 0.000239\n",
      "    Epoch 10/20, Train Loss: 0.000367, Val Loss: 0.000240\n",
      "Early stopping at epoch 12, Train Loss: 0.000367, Val Loss: 0.000241\n",
      "    Final - Train Loss: 0.000367, Val Loss: 0.000241\n",
      "[Saved] NN4_w21_2020Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000366, Val Loss: 0.000240\n",
      "    Epoch 5/20, Train Loss: 0.000366, Val Loss: 0.000238\n",
      "Early stopping at epoch 9, Train Loss: 0.000365, Val Loss: 0.000238\n",
      "    Final - Train Loss: 0.000365, Val Loss: 0.000238\n",
      "[Saved] NN5_w21_2020Q3.pth\n",
      "    Training data size: 251227 samples\n",
      "  Quarter 2020Q4: Training models with expanding data\n",
      "    Added 3151 samples from 2020Q3 to training set\n",
      "    [Re-tuning] NN1 for 2020Q4\n",
      "[Hyper] NN1: best_MSE=0.000274, params={'learning_rate': 0.0009866052276181587, 'batch_size': 128, 'dropout_rate': 0.08964864669300382}\n",
      "    [Structure Change] dropout_rate changed: 0.4391665462247476 -> 0.08964864669300382\n",
      "    Rebuilding NN1 model due to structure change\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 0.067705, Val Loss: 0.001312\n",
      "    Epoch 5/50, Train Loss: 0.000401, Val Loss: 0.000252\n",
      "    Epoch 10/50, Train Loss: 0.000379, Val Loss: 0.000254\n",
      "Early stopping at epoch 10, Train Loss: 0.000379, Val Loss: 0.000254\n",
      "    Final - Train Loss: 0.000379, Val Loss: 0.000254\n",
      "[Saved] NN1_w21_2020Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000374, Val Loss: 0.000245\n",
      "    Epoch 5/20, Train Loss: 0.000374, Val Loss: 0.000245\n",
      "Early stopping at epoch 6, Train Loss: 0.000374, Val Loss: 0.000245\n",
      "    Final - Train Loss: 0.000374, Val Loss: 0.000245\n",
      "[Saved] NN2_w21_2020Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000373, Val Loss: 0.000245\n",
      "    Epoch 5/20, Train Loss: 0.000372, Val Loss: 0.000245\n",
      "    Epoch 10/20, Train Loss: 0.000371, Val Loss: 0.000245\n",
      "Early stopping at epoch 11, Train Loss: 0.000371, Val Loss: 0.000244\n",
      "    Final - Train Loss: 0.000371, Val Loss: 0.000244\n",
      "[Saved] NN3_w21_2020Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000373, Val Loss: 0.000245\n",
      "    Epoch 5/20, Train Loss: 0.000371, Val Loss: 0.000244\n",
      "    Epoch 10/20, Train Loss: 0.000370, Val Loss: 0.000245\n",
      "Early stopping at epoch 10, Train Loss: 0.000370, Val Loss: 0.000245\n",
      "    Final - Train Loss: 0.000370, Val Loss: 0.000245\n",
      "[Saved] NN4_w21_2020Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000370, Val Loss: 0.000245\n",
      "    Epoch 5/20, Train Loss: 0.000368, Val Loss: 0.000245\n",
      "    Epoch 10/20, Train Loss: 0.000366, Val Loss: 0.000244\n",
      "    Epoch 15/20, Train Loss: 0.000365, Val Loss: 0.000244\n",
      "Early stopping at epoch 15, Train Loss: 0.000365, Val Loss: 0.000244\n",
      "    Final - Train Loss: 0.000365, Val Loss: 0.000244\n",
      "[Saved] NN5_w21_2020Q4.pth\n",
      "    Training data size: 254378 samples\n",
      "  Quarter 2021Q1: Training models with expanding data\n",
      "    Added 3114 samples from 2020Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000388, Val Loss: 0.000271\n",
      "    Epoch 5/20, Train Loss: 0.000371, Val Loss: 0.000263\n",
      "Early stopping at epoch 9, Train Loss: 0.000369, Val Loss: 0.000274\n",
      "    Final - Train Loss: 0.000369, Val Loss: 0.000274\n",
      "[Saved] NN1_w21_2021Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000371, Val Loss: 0.000260\n",
      "    Epoch 5/20, Train Loss: 0.000371, Val Loss: 0.000259\n",
      "Early stopping at epoch 9, Train Loss: 0.000371, Val Loss: 0.000259\n",
      "    Final - Train Loss: 0.000371, Val Loss: 0.000259\n",
      "[Saved] NN2_w21_2021Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000368, Val Loss: 0.000259\n",
      "    Epoch 5/20, Train Loss: 0.000368, Val Loss: 0.000260\n",
      "Early stopping at epoch 6, Train Loss: 0.000368, Val Loss: 0.000259\n",
      "    Final - Train Loss: 0.000368, Val Loss: 0.000259\n",
      "[Saved] NN3_w21_2021Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000367, Val Loss: 0.000259\n",
      "    Epoch 5/20, Train Loss: 0.000365, Val Loss: 0.000259\n",
      "    Epoch 10/20, Train Loss: 0.000365, Val Loss: 0.000260\n",
      "Early stopping at epoch 11, Train Loss: 0.000365, Val Loss: 0.000259\n",
      "    Final - Train Loss: 0.000365, Val Loss: 0.000259\n",
      "[Saved] NN4_w21_2021Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000364, Val Loss: 0.000259\n",
      "    Epoch 5/20, Train Loss: 0.000364, Val Loss: 0.000259\n",
      "Early stopping at epoch 7, Train Loss: 0.000364, Val Loss: 0.000260\n",
      "    Final - Train Loss: 0.000364, Val Loss: 0.000260\n",
      "[Saved] NN5_w21_2021Q1.pth\n",
      "    Training data size: 257492 samples\n",
      "  Quarter 2021Q2: Training models with expanding data\n",
      "    Added 2989 samples from 2021Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000369, Val Loss: 0.000273\n",
      "    Epoch 5/20, Train Loss: 0.000367, Val Loss: 0.000280\n",
      "Early stopping at epoch 7, Train Loss: 0.000365, Val Loss: 0.000281\n",
      "    Final - Train Loss: 0.000365, Val Loss: 0.000281\n",
      "[Saved] NN1_w21_2021Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000369, Val Loss: 0.000268\n",
      "    Epoch 5/20, Train Loss: 0.000369, Val Loss: 0.000268\n",
      "    Epoch 10/20, Train Loss: 0.000369, Val Loss: 0.000268\n",
      "Early stopping at epoch 11, Train Loss: 0.000369, Val Loss: 0.000268\n",
      "    Final - Train Loss: 0.000369, Val Loss: 0.000268\n",
      "[Saved] NN2_w21_2021Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000366, Val Loss: 0.000268\n",
      "    Epoch 5/20, Train Loss: 0.000366, Val Loss: 0.000268\n",
      "    Epoch 10/20, Train Loss: 0.000366, Val Loss: 0.000268\n",
      "Early stopping at epoch 10, Train Loss: 0.000366, Val Loss: 0.000268\n",
      "    Final - Train Loss: 0.000366, Val Loss: 0.000268\n",
      "[Saved] NN3_w21_2021Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000363, Val Loss: 0.000269\n",
      "    Epoch 5/20, Train Loss: 0.000364, Val Loss: 0.000269\n",
      "Early stopping at epoch 8, Train Loss: 0.000363, Val Loss: 0.000268\n",
      "    Final - Train Loss: 0.000363, Val Loss: 0.000268\n",
      "[Saved] NN4_w21_2021Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000361, Val Loss: 0.000268\n",
      "    Epoch 5/20, Train Loss: 0.000362, Val Loss: 0.000268\n",
      "Early stopping at epoch 7, Train Loss: 0.000362, Val Loss: 0.000268\n",
      "    Final - Train Loss: 0.000362, Val Loss: 0.000268\n",
      "[Saved] NN5_w21_2021Q2.pth\n",
      "    Training data size: 260481 samples\n",
      "  Quarter 2021Q3: Training models with expanding data\n",
      "    Added 3132 samples from 2021Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000365, Val Loss: 0.000280\n",
      "    Epoch 5/20, Train Loss: 0.000363, Val Loss: 0.000290\n",
      "Early stopping at epoch 6, Train Loss: 0.000362, Val Loss: 0.000297\n",
      "    Final - Train Loss: 0.000362, Val Loss: 0.000297\n",
      "[Saved] NN1_w21_2021Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000366, Val Loss: 0.000271\n",
      "    Epoch 5/20, Train Loss: 0.000366, Val Loss: 0.000271\n",
      "Early stopping at epoch 6, Train Loss: 0.000366, Val Loss: 0.000271\n",
      "    Final - Train Loss: 0.000366, Val Loss: 0.000271\n",
      "[Saved] NN2_w21_2021Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000363, Val Loss: 0.000271\n",
      "    Epoch 5/20, Train Loss: 0.000363, Val Loss: 0.000271\n",
      "Early stopping at epoch 7, Train Loss: 0.000363, Val Loss: 0.000271\n",
      "    Final - Train Loss: 0.000363, Val Loss: 0.000271\n",
      "[Saved] NN3_w21_2021Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000360, Val Loss: 0.000271\n",
      "    Epoch 5/20, Train Loss: 0.000360, Val Loss: 0.000271\n",
      "    Epoch 10/20, Train Loss: 0.000359, Val Loss: 0.000271\n",
      "Early stopping at epoch 13, Train Loss: 0.000357, Val Loss: 0.000271\n",
      "    Final - Train Loss: 0.000357, Val Loss: 0.000271\n",
      "[Saved] NN4_w21_2021Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000359, Val Loss: 0.000271\n",
      "    Epoch 5/20, Train Loss: 0.000358, Val Loss: 0.000271\n",
      "Early stopping at epoch 6, Train Loss: 0.000359, Val Loss: 0.000271\n",
      "    Final - Train Loss: 0.000359, Val Loss: 0.000271\n",
      "[Saved] NN5_w21_2021Q3.pth\n",
      "    Training data size: 263613 samples\n",
      "  Quarter 2021Q4: Training models with expanding data\n",
      "    Added 3173 samples from 2021Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000362, Val Loss: 0.000299\n",
      "    Epoch 5/20, Train Loss: 0.000360, Val Loss: 0.000311\n",
      "Early stopping at epoch 6, Train Loss: 0.000359, Val Loss: 0.000322\n",
      "    Final - Train Loss: 0.000359, Val Loss: 0.000322\n",
      "[Saved] NN1_w21_2021Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000363, Val Loss: 0.000276\n",
      "    Epoch 5/20, Train Loss: 0.000363, Val Loss: 0.000278\n",
      "Early stopping at epoch 6, Train Loss: 0.000363, Val Loss: 0.000277\n",
      "    Final - Train Loss: 0.000363, Val Loss: 0.000277\n",
      "[Saved] NN2_w21_2021Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000360, Val Loss: 0.000276\n",
      "    Epoch 5/20, Train Loss: 0.000360, Val Loss: 0.000276\n",
      "Early stopping at epoch 6, Train Loss: 0.000360, Val Loss: 0.000276\n",
      "    Final - Train Loss: 0.000360, Val Loss: 0.000276\n",
      "[Saved] NN3_w21_2021Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000357, Val Loss: 0.000276\n",
      "    Epoch 5/20, Train Loss: 0.000356, Val Loss: 0.000276\n",
      "Early stopping at epoch 6, Train Loss: 0.000356, Val Loss: 0.000276\n",
      "    Final - Train Loss: 0.000356, Val Loss: 0.000276\n",
      "[Saved] NN4_w21_2021Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000356, Val Loss: 0.000276\n",
      "    Epoch 5/20, Train Loss: 0.000356, Val Loss: 0.000276\n",
      "Early stopping at epoch 8, Train Loss: 0.000355, Val Loss: 0.000276\n",
      "    Final - Train Loss: 0.000355, Val Loss: 0.000276\n",
      "[Saved] NN5_w21_2021Q4.pth\n",
      "    Training data size: 266786 samples\n",
      "  Quarter 2022Q1: Training models with expanding data\n",
      "    Added 3153 samples from 2021Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000358, Val Loss: 0.000291\n",
      "    Epoch 5/20, Train Loss: 0.000357, Val Loss: 0.000294\n",
      "Early stopping at epoch 6, Train Loss: 0.000357, Val Loss: 0.000298\n",
      "    Final - Train Loss: 0.000357, Val Loss: 0.000298\n",
      "[Saved] NN1_w21_2022Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000360, Val Loss: 0.000281\n",
      "    Epoch 5/20, Train Loss: 0.000361, Val Loss: 0.000281\n",
      "Early stopping at epoch 6, Train Loss: 0.000361, Val Loss: 0.000281\n",
      "    Final - Train Loss: 0.000361, Val Loss: 0.000281\n",
      "[Saved] NN2_w21_2022Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000357, Val Loss: 0.000281\n",
      "    Epoch 5/20, Train Loss: 0.000357, Val Loss: 0.000281\n",
      "Early stopping at epoch 9, Train Loss: 0.000357, Val Loss: 0.000281\n",
      "    Final - Train Loss: 0.000357, Val Loss: 0.000281\n",
      "[Saved] NN3_w21_2022Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000355, Val Loss: 0.000281\n",
      "    Epoch 5/20, Train Loss: 0.000354, Val Loss: 0.000281\n",
      "Early stopping at epoch 6, Train Loss: 0.000353, Val Loss: 0.000281\n",
      "    Final - Train Loss: 0.000353, Val Loss: 0.000281\n",
      "[Saved] NN4_w21_2022Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000353, Val Loss: 0.000281\n",
      "    Epoch 5/20, Train Loss: 0.000353, Val Loss: 0.000281\n",
      "    Epoch 10/20, Train Loss: 0.000352, Val Loss: 0.000281\n",
      "Early stopping at epoch 12, Train Loss: 0.000352, Val Loss: 0.000281\n",
      "    Final - Train Loss: 0.000352, Val Loss: 0.000281\n",
      "[Saved] NN5_w21_2022Q1.pth\n",
      "    Training data size: 269939 samples\n",
      "  Quarter 2022Q2: Training models with expanding data\n",
      "    Added 3029 samples from 2022Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000356, Val Loss: 0.000303\n",
      "    Epoch 5/20, Train Loss: 0.000355, Val Loss: 0.000311\n",
      "Early stopping at epoch 6, Train Loss: 0.000355, Val Loss: 0.000312\n",
      "    Final - Train Loss: 0.000355, Val Loss: 0.000312\n",
      "[Saved] NN1_w21_2022Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000358, Val Loss: 0.000294\n",
      "    Epoch 5/20, Train Loss: 0.000358, Val Loss: 0.000294\n",
      "    Epoch 10/20, Train Loss: 0.000358, Val Loss: 0.000294\n",
      "Early stopping at epoch 12, Train Loss: 0.000358, Val Loss: 0.000294\n",
      "    Final - Train Loss: 0.000358, Val Loss: 0.000294\n",
      "[Saved] NN2_w21_2022Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000355, Val Loss: 0.000294\n",
      "    Epoch 5/20, Train Loss: 0.000355, Val Loss: 0.000294\n",
      "Early stopping at epoch 7, Train Loss: 0.000355, Val Loss: 0.000294\n",
      "    Final - Train Loss: 0.000355, Val Loss: 0.000294\n",
      "[Saved] NN3_w21_2022Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000351, Val Loss: 0.000294\n",
      "    Epoch 5/20, Train Loss: 0.000352, Val Loss: 0.000294\n",
      "    Epoch 10/20, Train Loss: 0.000350, Val Loss: 0.000293\n",
      "Early stopping at epoch 14, Train Loss: 0.000349, Val Loss: 0.000293\n",
      "    Final - Train Loss: 0.000349, Val Loss: 0.000293\n",
      "[Saved] NN4_w21_2022Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000351, Val Loss: 0.000294\n",
      "    Epoch 5/20, Train Loss: 0.000350, Val Loss: 0.000294\n",
      "Early stopping at epoch 8, Train Loss: 0.000350, Val Loss: 0.000294\n",
      "    Final - Train Loss: 0.000350, Val Loss: 0.000294\n",
      "[Saved] NN5_w21_2022Q2.pth\n",
      "    Training data size: 272968 samples\n",
      "  Quarter 2022Q3: Training models with expanding data\n",
      "    Added 2969 samples from 2022Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000354, Val Loss: 0.000318\n",
      "    Epoch 5/20, Train Loss: 0.000353, Val Loss: 0.000320\n",
      "Early stopping at epoch 8, Train Loss: 0.000352, Val Loss: 0.000324\n",
      "    Final - Train Loss: 0.000352, Val Loss: 0.000324\n",
      "[Saved] NN1_w21_2022Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000356, Val Loss: 0.000311\n",
      "    Epoch 5/20, Train Loss: 0.000356, Val Loss: 0.000311\n",
      "Early stopping at epoch 9, Train Loss: 0.000357, Val Loss: 0.000311\n",
      "    Final - Train Loss: 0.000357, Val Loss: 0.000311\n",
      "[Saved] NN2_w21_2022Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000353, Val Loss: 0.000311\n",
      "    Epoch 5/20, Train Loss: 0.000354, Val Loss: 0.000311\n",
      "Early stopping at epoch 7, Train Loss: 0.000354, Val Loss: 0.000311\n",
      "    Final - Train Loss: 0.000354, Val Loss: 0.000311\n",
      "[Saved] NN3_w21_2022Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000349, Val Loss: 0.000310\n",
      "    Epoch 5/20, Train Loss: 0.000349, Val Loss: 0.000310\n",
      "Early stopping at epoch 6, Train Loss: 0.000348, Val Loss: 0.000311\n",
      "    Final - Train Loss: 0.000348, Val Loss: 0.000311\n",
      "[Saved] NN4_w21_2022Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000349, Val Loss: 0.000311\n",
      "    Epoch 5/20, Train Loss: 0.000348, Val Loss: 0.000310\n",
      "Early stopping at epoch 7, Train Loss: 0.000349, Val Loss: 0.000310\n",
      "    Final - Train Loss: 0.000349, Val Loss: 0.000310\n",
      "[Saved] NN5_w21_2022Q3.pth\n",
      "    Training data size: 275937 samples\n",
      "  Quarter 2022Q4: Training models with expanding data\n",
      "    Added 3152 samples from 2022Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000352, Val Loss: 0.000320\n",
      "    Epoch 5/20, Train Loss: 0.000351, Val Loss: 0.000322\n",
      "Early stopping at epoch 6, Train Loss: 0.000351, Val Loss: 0.000323\n",
      "    Final - Train Loss: 0.000351, Val Loss: 0.000323\n",
      "[Saved] NN1_w21_2022Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000355, Val Loss: 0.000314\n",
      "    Epoch 5/20, Train Loss: 0.000355, Val Loss: 0.000314\n",
      "    Epoch 10/20, Train Loss: 0.000355, Val Loss: 0.000314\n",
      "Early stopping at epoch 10, Train Loss: 0.000355, Val Loss: 0.000314\n",
      "    Final - Train Loss: 0.000355, Val Loss: 0.000314\n",
      "[Saved] NN2_w21_2022Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000352, Val Loss: 0.000314\n",
      "    Epoch 5/20, Train Loss: 0.000352, Val Loss: 0.000314\n",
      "    Epoch 10/20, Train Loss: 0.000350, Val Loss: 0.000315\n",
      "Early stopping at epoch 11, Train Loss: 0.000350, Val Loss: 0.000315\n",
      "    Final - Train Loss: 0.000350, Val Loss: 0.000315\n",
      "[Saved] NN3_w21_2022Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000348, Val Loss: 0.000314\n",
      "    Epoch 5/20, Train Loss: 0.000348, Val Loss: 0.000314\n",
      "    Epoch 10/20, Train Loss: 0.000347, Val Loss: 0.000314\n",
      "Early stopping at epoch 10, Train Loss: 0.000347, Val Loss: 0.000314\n",
      "    Final - Train Loss: 0.000347, Val Loss: 0.000314\n",
      "[Saved] NN4_w21_2022Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000348, Val Loss: 0.000315\n",
      "    Epoch 5/20, Train Loss: 0.000347, Val Loss: 0.000314\n",
      "Early stopping at epoch 7, Train Loss: 0.000347, Val Loss: 0.000315\n",
      "    Final - Train Loss: 0.000347, Val Loss: 0.000315\n",
      "[Saved] NN5_w21_2022Q4.pth\n",
      "    Training data size: 279089 samples\n",
      "  Quarter 2023Q1: Training models with expanding data\n",
      "    Added 3070 samples from 2022Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000350, Val Loss: 0.000330\n",
      "    Epoch 5/20, Train Loss: 0.000350, Val Loss: 0.000332\n",
      "Early stopping at epoch 6, Train Loss: 0.000350, Val Loss: 0.000334\n",
      "    Final - Train Loss: 0.000350, Val Loss: 0.000334\n",
      "[Saved] NN1_w21_2023Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000354, Val Loss: 0.000323\n",
      "    Epoch 5/20, Train Loss: 0.000354, Val Loss: 0.000323\n",
      "Early stopping at epoch 6, Train Loss: 0.000354, Val Loss: 0.000323\n",
      "    Final - Train Loss: 0.000354, Val Loss: 0.000323\n",
      "[Saved] NN2_w21_2023Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000351, Val Loss: 0.000324\n",
      "    Epoch 5/20, Train Loss: 0.000350, Val Loss: 0.000324\n",
      "    Epoch 10/20, Train Loss: 0.000351, Val Loss: 0.000323\n",
      "Early stopping at epoch 11, Train Loss: 0.000350, Val Loss: 0.000324\n",
      "    Final - Train Loss: 0.000350, Val Loss: 0.000324\n",
      "[Saved] NN3_w21_2023Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000347, Val Loss: 0.000323\n",
      "    Epoch 5/20, Train Loss: 0.000346, Val Loss: 0.000323\n",
      "Early stopping at epoch 9, Train Loss: 0.000347, Val Loss: 0.000323\n",
      "    Final - Train Loss: 0.000347, Val Loss: 0.000323\n",
      "[Saved] NN4_w21_2023Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000345, Val Loss: 0.000324\n",
      "    Epoch 5/20, Train Loss: 0.000346, Val Loss: 0.000325\n",
      "Early stopping at epoch 6, Train Loss: 0.000346, Val Loss: 0.000324\n",
      "    Final - Train Loss: 0.000346, Val Loss: 0.000324\n",
      "[Saved] NN5_w21_2023Q1.pth\n",
      "    Training data size: 282159 samples\n",
      "  Quarter 2023Q2: Training models with expanding data\n",
      "    Added 3064 samples from 2023Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000348, Val Loss: 0.000351\n",
      "    Epoch 5/20, Train Loss: 0.000348, Val Loss: 0.000356\n",
      "Early stopping at epoch 8, Train Loss: 0.000348, Val Loss: 0.000361\n",
      "    Final - Train Loss: 0.000348, Val Loss: 0.000361\n",
      "[Saved] NN1_w21_2023Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000352, Val Loss: 0.000329\n",
      "    Epoch 5/20, Train Loss: 0.000352, Val Loss: 0.000329\n",
      "    Epoch 10/20, Train Loss: 0.000352, Val Loss: 0.000329\n",
      "    Epoch 15/20, Train Loss: 0.000352, Val Loss: 0.000329\n",
      "Early stopping at epoch 15, Train Loss: 0.000352, Val Loss: 0.000329\n",
      "    Final - Train Loss: 0.000352, Val Loss: 0.000329\n",
      "[Saved] NN2_w21_2023Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000348, Val Loss: 0.000329\n",
      "    Epoch 5/20, Train Loss: 0.000348, Val Loss: 0.000329\n",
      "Early stopping at epoch 7, Train Loss: 0.000348, Val Loss: 0.000329\n",
      "    Final - Train Loss: 0.000348, Val Loss: 0.000329\n",
      "[Saved] NN3_w21_2023Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000345, Val Loss: 0.000329\n",
      "    Epoch 5/20, Train Loss: 0.000344, Val Loss: 0.000329\n",
      "    Epoch 10/20, Train Loss: 0.000345, Val Loss: 0.000329\n",
      "Early stopping at epoch 11, Train Loss: 0.000344, Val Loss: 0.000329\n",
      "    Final - Train Loss: 0.000344, Val Loss: 0.000329\n",
      "[Saved] NN4_w21_2023Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000344, Val Loss: 0.000329\n",
      "    Epoch 5/20, Train Loss: 0.000343, Val Loss: 0.000330\n",
      "Early stopping at epoch 7, Train Loss: 0.000344, Val Loss: 0.000330\n",
      "    Final - Train Loss: 0.000344, Val Loss: 0.000330\n",
      "[Saved] NN5_w21_2023Q2.pth\n",
      "    Training data size: 285223 samples\n",
      "  Quarter 2023Q3: Training models with expanding data\n",
      "    Added 3069 samples from 2023Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000346, Val Loss: 0.000361\n",
      "    Epoch 5/20, Train Loss: 0.000346, Val Loss: 0.000366\n",
      "Early stopping at epoch 8, Train Loss: 0.000345, Val Loss: 0.000368\n",
      "    Final - Train Loss: 0.000345, Val Loss: 0.000368\n",
      "[Saved] NN1_w21_2023Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000349, Val Loss: 0.000331\n",
      "    Epoch 5/20, Train Loss: 0.000350, Val Loss: 0.000331\n",
      "Early stopping at epoch 6, Train Loss: 0.000349, Val Loss: 0.000331\n",
      "    Final - Train Loss: 0.000349, Val Loss: 0.000331\n",
      "[Saved] NN2_w21_2023Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000347, Val Loss: 0.000331\n",
      "    Epoch 5/20, Train Loss: 0.000346, Val Loss: 0.000331\n",
      "Early stopping at epoch 7, Train Loss: 0.000346, Val Loss: 0.000331\n",
      "    Final - Train Loss: 0.000346, Val Loss: 0.000331\n",
      "[Saved] NN3_w21_2023Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000342, Val Loss: 0.000331\n",
      "    Epoch 5/20, Train Loss: 0.000341, Val Loss: 0.000331\n",
      "Early stopping at epoch 6, Train Loss: 0.000343, Val Loss: 0.000331\n",
      "    Final - Train Loss: 0.000343, Val Loss: 0.000331\n",
      "[Saved] NN4_w21_2023Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000342, Val Loss: 0.000331\n",
      "    Epoch 5/20, Train Loss: 0.000341, Val Loss: 0.000331\n",
      "    Epoch 10/20, Train Loss: 0.000340, Val Loss: 0.000331\n",
      "Early stopping at epoch 13, Train Loss: 0.000339, Val Loss: 0.000331\n",
      "    Final - Train Loss: 0.000339, Val Loss: 0.000331\n",
      "[Saved] NN5_w21_2023Q3.pth\n",
      "    Training data size: 288292 samples\n",
      "  Quarter 2023Q4: Training models with expanding data\n",
      "    Added 3121 samples from 2023Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000345, Val Loss: 0.000335\n",
      "    Epoch 5/20, Train Loss: 0.000346, Val Loss: 0.000336\n",
      "Early stopping at epoch 7, Train Loss: 0.000345, Val Loss: 0.000337\n",
      "    Final - Train Loss: 0.000345, Val Loss: 0.000337\n",
      "[Saved] NN1_w21_2023Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000350, Val Loss: 0.000328\n",
      "    Epoch 5/20, Train Loss: 0.000350, Val Loss: 0.000327\n",
      "Early stopping at epoch 7, Train Loss: 0.000350, Val Loss: 0.000328\n",
      "    Final - Train Loss: 0.000350, Val Loss: 0.000328\n",
      "[Saved] NN2_w21_2023Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000346, Val Loss: 0.000327\n",
      "    Epoch 5/20, Train Loss: 0.000346, Val Loss: 0.000328\n",
      "    Epoch 10/20, Train Loss: 0.000346, Val Loss: 0.000328\n",
      "Early stopping at epoch 11, Train Loss: 0.000346, Val Loss: 0.000329\n",
      "    Final - Train Loss: 0.000346, Val Loss: 0.000329\n",
      "[Saved] NN3_w21_2023Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000342, Val Loss: 0.000328\n",
      "    Epoch 5/20, Train Loss: 0.000341, Val Loss: 0.000329\n",
      "Early stopping at epoch 6, Train Loss: 0.000342, Val Loss: 0.000330\n",
      "    Final - Train Loss: 0.000342, Val Loss: 0.000330\n",
      "[Saved] NN4_w21_2023Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000341, Val Loss: 0.000333\n",
      "    Epoch 5/20, Train Loss: 0.000341, Val Loss: 0.000329\n",
      "Early stopping at epoch 8, Train Loss: 0.000340, Val Loss: 0.000331\n",
      "    Final - Train Loss: 0.000340, Val Loss: 0.000331\n",
      "[Saved] NN5_w21_2023Q4.pth\n",
      "    Training data size: 291413 samples\n",
      "  Quarter 2024Q1: Training models with expanding data\n",
      "    Added 3113 samples from 2023Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000344, Val Loss: 0.000341\n",
      "    Epoch 5/20, Train Loss: 0.000344, Val Loss: 0.000343\n",
      "Early stopping at epoch 7, Train Loss: 0.000344, Val Loss: 0.000344\n",
      "    Final - Train Loss: 0.000344, Val Loss: 0.000344\n",
      "[Saved] NN1_w21_2024Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000348, Val Loss: 0.000325\n",
      "    Epoch 5/20, Train Loss: 0.000348, Val Loss: 0.000325\n",
      "Early stopping at epoch 6, Train Loss: 0.000348, Val Loss: 0.000325\n",
      "    Final - Train Loss: 0.000348, Val Loss: 0.000325\n",
      "[Saved] NN2_w21_2024Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000345, Val Loss: 0.000324\n",
      "    Epoch 5/20, Train Loss: 0.000345, Val Loss: 0.000324\n",
      "Early stopping at epoch 7, Train Loss: 0.000345, Val Loss: 0.000325\n",
      "    Final - Train Loss: 0.000345, Val Loss: 0.000325\n",
      "[Saved] NN3_w21_2024Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000340, Val Loss: 0.000325\n",
      "    Epoch 5/20, Train Loss: 0.000341, Val Loss: 0.000324\n",
      "    Epoch 10/20, Train Loss: 0.000341, Val Loss: 0.000325\n",
      "Early stopping at epoch 10, Train Loss: 0.000341, Val Loss: 0.000325\n",
      "    Final - Train Loss: 0.000341, Val Loss: 0.000325\n",
      "[Saved] NN4_w21_2024Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000340, Val Loss: 0.000325\n",
      "    Epoch 5/20, Train Loss: 0.000339, Val Loss: 0.000325\n",
      "Early stopping at epoch 7, Train Loss: 0.000339, Val Loss: 0.000325\n",
      "    Final - Train Loss: 0.000339, Val Loss: 0.000325\n",
      "[Saved] NN5_w21_2024Q1.pth\n",
      "    Training data size: 294526 samples\n",
      "  Quarter 2024Q2: Training models with expanding data\n",
      "    Added 3026 samples from 2024Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000342, Val Loss: 0.000359\n",
      "    Epoch 5/20, Train Loss: 0.000342, Val Loss: 0.000377\n",
      "Early stopping at epoch 6, Train Loss: 0.000342, Val Loss: 0.000392\n",
      "    Final - Train Loss: 0.000342, Val Loss: 0.000392\n",
      "[Saved] NN1_w21_2024Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000346, Val Loss: 0.000325\n",
      "    Epoch 5/20, Train Loss: 0.000347, Val Loss: 0.000325\n",
      "Early stopping at epoch 8, Train Loss: 0.000347, Val Loss: 0.000325\n",
      "    Final - Train Loss: 0.000347, Val Loss: 0.000325\n",
      "[Saved] NN2_w21_2024Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000343, Val Loss: 0.000325\n",
      "    Epoch 5/20, Train Loss: 0.000343, Val Loss: 0.000325\n",
      "Early stopping at epoch 7, Train Loss: 0.000343, Val Loss: 0.000325\n",
      "    Final - Train Loss: 0.000343, Val Loss: 0.000325\n",
      "[Saved] NN3_w21_2024Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000338, Val Loss: 0.000325\n",
      "    Epoch 5/20, Train Loss: 0.000339, Val Loss: 0.000325\n",
      "    Epoch 10/20, Train Loss: 0.000338, Val Loss: 0.000325\n",
      "    Epoch 15/20, Train Loss: 0.000338, Val Loss: 0.000325\n",
      "Early stopping at epoch 15, Train Loss: 0.000338, Val Loss: 0.000325\n",
      "    Final - Train Loss: 0.000338, Val Loss: 0.000325\n",
      "[Saved] NN4_w21_2024Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000338, Val Loss: 0.000326\n",
      "    Epoch 5/20, Train Loss: 0.000338, Val Loss: 0.000326\n",
      "    Epoch 10/20, Train Loss: 0.000339, Val Loss: 0.000325\n",
      "Early stopping at epoch 13, Train Loss: 0.000338, Val Loss: 0.000325\n",
      "    Final - Train Loss: 0.000338, Val Loss: 0.000325\n",
      "[Saved] NN5_w21_2024Q2.pth\n",
      "    Training data size: 297552 samples\n",
      "  Quarter 2024Q3: Training models with expanding data\n",
      "    Added 3112 samples from 2024Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000340, Val Loss: 0.000367\n",
      "    Epoch 5/20, Train Loss: 0.000340, Val Loss: 0.000385\n",
      "Early stopping at epoch 6, Train Loss: 0.000340, Val Loss: 0.000374\n",
      "    Final - Train Loss: 0.000340, Val Loss: 0.000374\n",
      "[Saved] NN1_w21_2024Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000344, Val Loss: 0.000326\n",
      "    Epoch 5/20, Train Loss: 0.000345, Val Loss: 0.000325\n",
      "    Epoch 10/20, Train Loss: 0.000343, Val Loss: 0.000325\n",
      "Early stopping at epoch 14, Train Loss: 0.000342, Val Loss: 0.000326\n",
      "    Final - Train Loss: 0.000342, Val Loss: 0.000326\n",
      "[Saved] NN2_w21_2024Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000341, Val Loss: 0.000325\n",
      "    Epoch 5/20, Train Loss: 0.000341, Val Loss: 0.000325\n",
      "    Epoch 10/20, Train Loss: 0.000340, Val Loss: 0.000325\n",
      "    Epoch 15/20, Train Loss: 0.000339, Val Loss: 0.000325\n",
      "Early stopping at epoch 18, Train Loss: 0.000338, Val Loss: 0.000325\n",
      "    Final - Train Loss: 0.000338, Val Loss: 0.000325\n",
      "[Saved] NN3_w21_2024Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000337, Val Loss: 0.000325\n",
      "    Epoch 5/20, Train Loss: 0.000337, Val Loss: 0.000325\n",
      "Early stopping at epoch 8, Train Loss: 0.000336, Val Loss: 0.000325\n",
      "    Final - Train Loss: 0.000336, Val Loss: 0.000325\n",
      "[Saved] NN4_w21_2024Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000336, Val Loss: 0.000326\n",
      "    Epoch 5/20, Train Loss: 0.000335, Val Loss: 0.000326\n",
      "    Epoch 10/20, Train Loss: 0.000335, Val Loss: 0.000325\n",
      "Early stopping at epoch 14, Train Loss: 0.000335, Val Loss: 0.000325\n",
      "    Final - Train Loss: 0.000335, Val Loss: 0.000325\n",
      "[Saved] NN5_w21_2024Q3.pth\n",
      "    Training data size: 300664 samples\n",
      "Processing window size: 252\n",
      "  Quarter 2015Q4: Training models with expanding data\n",
      "    [Re-tuning] NN1 for 2015Q4\n",
      "[Hyper] NN1: best_MSE=0.000286, params={'learning_rate': 0.0004916521937820545, 'batch_size': 128, 'dropout_rate': 0.416505293673187}\n",
      "    [Structure Change] dropout_rate changed: 0.1 -> 0.416505293673187\n",
      "    Rebuilding NN1 model due to structure change\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 0.262117, Val Loss: 0.006429\n",
      "    Epoch 5/50, Train Loss: 0.000398, Val Loss: 0.000168\n",
      "Early stopping at epoch 8, Train Loss: 0.000407, Val Loss: 0.000165\n",
      "    Final - Train Loss: 0.000407, Val Loss: 0.000165\n",
      "[Saved] NN1_w252_2015Q4.pth\n",
      "    Creating new NN2 model\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 0.397387, Val Loss: 0.006971\n",
      "    Epoch 5/50, Train Loss: 0.000395, Val Loss: 0.000165\n",
      "Early stopping at epoch 9, Train Loss: 0.000404, Val Loss: 0.000165\n",
      "    Final - Train Loss: 0.000404, Val Loss: 0.000165\n",
      "[Saved] NN2_w252_2015Q4.pth\n",
      "    Creating new NN3 model\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 0.223625, Val Loss: 0.001368\n",
      "    Epoch 5/50, Train Loss: 0.000402, Val Loss: 0.000165\n",
      "    Epoch 10/50, Train Loss: 0.000403, Val Loss: 0.000165\n",
      "Early stopping at epoch 13, Train Loss: 0.000400, Val Loss: 0.000166\n",
      "    Final - Train Loss: 0.000400, Val Loss: 0.000166\n",
      "[Saved] NN3_w252_2015Q4.pth\n",
      "    Creating new NN4 model\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 0.131605, Val Loss: 0.000447\n",
      "    Epoch 5/50, Train Loss: 0.000414, Val Loss: 0.000170\n",
      "    Epoch 10/50, Train Loss: 0.000401, Val Loss: 0.000166\n",
      "    Epoch 15/50, Train Loss: 0.000392, Val Loss: 0.000164\n",
      "    Epoch 20/50, Train Loss: 0.000390, Val Loss: 0.000164\n",
      "Early stopping at epoch 21, Train Loss: 0.000389, Val Loss: 0.000164\n",
      "    Final - Train Loss: 0.000389, Val Loss: 0.000164\n",
      "[Saved] NN4_w252_2015Q4.pth\n",
      "    Creating new NN5 model\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 0.152972, Val Loss: 0.000257\n",
      "    Epoch 5/50, Train Loss: 0.000455, Val Loss: 0.000166\n",
      "    Epoch 10/50, Train Loss: 0.000396, Val Loss: 0.000165\n",
      "    Epoch 15/50, Train Loss: 0.000391, Val Loss: 0.000164\n",
      "Early stopping at epoch 19, Train Loss: 0.000390, Val Loss: 0.000164\n",
      "    Final - Train Loss: 0.000390, Val Loss: 0.000164\n",
      "[Saved] NN5_w252_2015Q4.pth\n",
      "    Training data size: 184570 samples\n",
      "  Quarter 2016Q1: Training models with expanding data\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000405, Val Loss: 0.000166\n",
      "    Epoch 5/20, Train Loss: 0.000390, Val Loss: 0.000165\n",
      "Early stopping at epoch 9, Train Loss: 0.000382, Val Loss: 0.000165\n",
      "    Final - Train Loss: 0.000382, Val Loss: 0.000165\n",
      "[Saved] NN1_w252_2016Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000403, Val Loss: 0.000165\n",
      "    Epoch 5/20, Train Loss: 0.000391, Val Loss: 0.000164\n",
      "Early stopping at epoch 9, Train Loss: 0.000387, Val Loss: 0.000164\n",
      "    Final - Train Loss: 0.000387, Val Loss: 0.000164\n",
      "[Saved] NN2_w252_2016Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000402, Val Loss: 0.000167\n",
      "    Epoch 5/20, Train Loss: 0.000390, Val Loss: 0.000164\n",
      "Early stopping at epoch 8, Train Loss: 0.000387, Val Loss: 0.000164\n",
      "    Final - Train Loss: 0.000387, Val Loss: 0.000164\n",
      "[Saved] NN3_w252_2016Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000391, Val Loss: 0.000164\n",
      "    Epoch 5/20, Train Loss: 0.000390, Val Loss: 0.000164\n",
      "Early stopping at epoch 7, Train Loss: 0.000388, Val Loss: 0.000164\n",
      "    Final - Train Loss: 0.000388, Val Loss: 0.000164\n",
      "[Saved] NN4_w252_2016Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000391, Val Loss: 0.000164\n",
      "    Epoch 5/20, Train Loss: 0.000389, Val Loss: 0.000164\n",
      "Early stopping at epoch 6, Train Loss: 0.000389, Val Loss: 0.000164\n",
      "    Final - Train Loss: 0.000389, Val Loss: 0.000164\n",
      "[Saved] NN5_w252_2016Q1.pth\n",
      "    Training data size: 184570 samples\n",
      "  Quarter 2016Q2: Training models with expanding data\n",
      "    Added 2956 samples from 2016Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000386, Val Loss: 0.000174\n",
      "    Epoch 5/20, Train Loss: 0.000379, Val Loss: 0.000174\n",
      "    Epoch 10/20, Train Loss: 0.000374, Val Loss: 0.000175\n",
      "Early stopping at epoch 11, Train Loss: 0.000373, Val Loss: 0.000175\n",
      "    Final - Train Loss: 0.000373, Val Loss: 0.000175\n",
      "[Saved] NN1_w252_2016Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000387, Val Loss: 0.000173\n",
      "    Epoch 5/20, Train Loss: 0.000384, Val Loss: 0.000173\n",
      "Early stopping at epoch 6, Train Loss: 0.000383, Val Loss: 0.000173\n",
      "    Final - Train Loss: 0.000383, Val Loss: 0.000173\n",
      "[Saved] NN2_w252_2016Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000387, Val Loss: 0.000174\n",
      "    Epoch 5/20, Train Loss: 0.000384, Val Loss: 0.000174\n",
      "Early stopping at epoch 7, Train Loss: 0.000382, Val Loss: 0.000174\n",
      "    Final - Train Loss: 0.000382, Val Loss: 0.000174\n",
      "[Saved] NN3_w252_2016Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000386, Val Loss: 0.000173\n",
      "    Epoch 5/20, Train Loss: 0.000384, Val Loss: 0.000173\n",
      "    Epoch 10/20, Train Loss: 0.000378, Val Loss: 0.000173\n",
      "Early stopping at epoch 14, Train Loss: 0.000373, Val Loss: 0.000173\n",
      "    Final - Train Loss: 0.000373, Val Loss: 0.000173\n",
      "[Saved] NN4_w252_2016Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000387, Val Loss: 0.000173\n",
      "    Epoch 5/20, Train Loss: 0.000385, Val Loss: 0.000175\n",
      "Early stopping at epoch 6, Train Loss: 0.000384, Val Loss: 0.000175\n",
      "    Final - Train Loss: 0.000384, Val Loss: 0.000175\n",
      "[Saved] NN5_w252_2016Q2.pth\n",
      "    Training data size: 187526 samples\n",
      "  Quarter 2016Q3: Training models with expanding data\n",
      "    Added 3170 samples from 2016Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000374, Val Loss: 0.000174\n",
      "    Epoch 5/20, Train Loss: 0.000370, Val Loss: 0.000175\n",
      "Early stopping at epoch 8, Train Loss: 0.000368, Val Loss: 0.000175\n",
      "    Final - Train Loss: 0.000368, Val Loss: 0.000175\n",
      "[Saved] NN1_w252_2016Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000382, Val Loss: 0.000173\n",
      "    Epoch 5/20, Train Loss: 0.000380, Val Loss: 0.000173\n",
      "Early stopping at epoch 7, Train Loss: 0.000379, Val Loss: 0.000173\n",
      "    Final - Train Loss: 0.000379, Val Loss: 0.000173\n",
      "[Saved] NN2_w252_2016Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000382, Val Loss: 0.000174\n",
      "    Epoch 5/20, Train Loss: 0.000378, Val Loss: 0.000174\n",
      "Early stopping at epoch 7, Train Loss: 0.000378, Val Loss: 0.000174\n",
      "    Final - Train Loss: 0.000378, Val Loss: 0.000174\n",
      "[Saved] NN3_w252_2016Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000376, Val Loss: 0.000173\n",
      "    Epoch 5/20, Train Loss: 0.000371, Val Loss: 0.000173\n",
      "    Epoch 10/20, Train Loss: 0.000366, Val Loss: 0.000173\n",
      "Early stopping at epoch 10, Train Loss: 0.000366, Val Loss: 0.000173\n",
      "    Final - Train Loss: 0.000366, Val Loss: 0.000173\n",
      "[Saved] NN4_w252_2016Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000383, Val Loss: 0.000174\n",
      "    Epoch 5/20, Train Loss: 0.000381, Val Loss: 0.000174\n",
      "Early stopping at epoch 6, Train Loss: 0.000380, Val Loss: 0.000175\n",
      "    Final - Train Loss: 0.000380, Val Loss: 0.000175\n",
      "[Saved] NN5_w252_2016Q3.pth\n",
      "    Training data size: 190696 samples\n",
      "  Quarter 2016Q4: Training models with expanding data\n",
      "    Added 3176 samples from 2016Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000368, Val Loss: 0.000173\n",
      "    Epoch 5/20, Train Loss: 0.000365, Val Loss: 0.000173\n",
      "Early stopping at epoch 6, Train Loss: 0.000364, Val Loss: 0.000173\n",
      "    Final - Train Loss: 0.000364, Val Loss: 0.000173\n",
      "[Saved] NN1_w252_2016Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000377, Val Loss: 0.000172\n",
      "    Epoch 5/20, Train Loss: 0.000375, Val Loss: 0.000172\n",
      "    Epoch 10/20, Train Loss: 0.000373, Val Loss: 0.000172\n",
      "Early stopping at epoch 10, Train Loss: 0.000373, Val Loss: 0.000172\n",
      "    Final - Train Loss: 0.000373, Val Loss: 0.000172\n",
      "[Saved] NN2_w252_2016Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000377, Val Loss: 0.000172\n",
      "    Epoch 5/20, Train Loss: 0.000374, Val Loss: 0.000172\n",
      "Early stopping at epoch 6, Train Loss: 0.000373, Val Loss: 0.000172\n",
      "    Final - Train Loss: 0.000373, Val Loss: 0.000172\n",
      "[Saved] NN3_w252_2016Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000366, Val Loss: 0.000172\n",
      "    Epoch 5/20, Train Loss: 0.000361, Val Loss: 0.000172\n",
      "Early stopping at epoch 7, Train Loss: 0.000359, Val Loss: 0.000172\n",
      "    Final - Train Loss: 0.000359, Val Loss: 0.000172\n",
      "[Saved] NN4_w252_2016Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000379, Val Loss: 0.000172\n",
      "    Epoch 5/20, Train Loss: 0.000376, Val Loss: 0.000172\n",
      "Early stopping at epoch 6, Train Loss: 0.000375, Val Loss: 0.000173\n",
      "    Final - Train Loss: 0.000375, Val Loss: 0.000173\n",
      "[Saved] NN5_w252_2016Q4.pth\n",
      "    Training data size: 193872 samples\n",
      "  Quarter 2017Q1: Training models with expanding data\n",
      "    Added 3123 samples from 2016Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000364, Val Loss: 0.000175\n",
      "    Epoch 5/20, Train Loss: 0.000360, Val Loss: 0.000175\n",
      "Early stopping at epoch 7, Train Loss: 0.000360, Val Loss: 0.000175\n",
      "    Final - Train Loss: 0.000360, Val Loss: 0.000175\n",
      "[Saved] NN1_w252_2017Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000371, Val Loss: 0.000173\n",
      "    Epoch 5/20, Train Loss: 0.000369, Val Loss: 0.000173\n",
      "Early stopping at epoch 9, Train Loss: 0.000367, Val Loss: 0.000173\n",
      "    Final - Train Loss: 0.000367, Val Loss: 0.000173\n",
      "[Saved] NN2_w252_2017Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000372, Val Loss: 0.000174\n",
      "    Epoch 5/20, Train Loss: 0.000369, Val Loss: 0.000174\n",
      "Early stopping at epoch 6, Train Loss: 0.000369, Val Loss: 0.000174\n",
      "    Final - Train Loss: 0.000369, Val Loss: 0.000174\n",
      "[Saved] NN3_w252_2017Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000361, Val Loss: 0.000173\n",
      "    Epoch 5/20, Train Loss: 0.000356, Val Loss: 0.000173\n",
      "Early stopping at epoch 6, Train Loss: 0.000354, Val Loss: 0.000173\n",
      "    Final - Train Loss: 0.000354, Val Loss: 0.000173\n",
      "[Saved] NN4_w252_2017Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000375, Val Loss: 0.000173\n",
      "    Epoch 5/20, Train Loss: 0.000371, Val Loss: 0.000174\n",
      "Early stopping at epoch 6, Train Loss: 0.000370, Val Loss: 0.000174\n",
      "    Final - Train Loss: 0.000370, Val Loss: 0.000174\n",
      "[Saved] NN5_w252_2017Q1.pth\n",
      "    Training data size: 196995 samples\n",
      "  Quarter 2017Q2: Training models with expanding data\n",
      "    Added 3083 samples from 2017Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000360, Val Loss: 0.000173\n",
      "    Epoch 5/20, Train Loss: 0.000356, Val Loss: 0.000173\n",
      "Early stopping at epoch 9, Train Loss: 0.000353, Val Loss: 0.000174\n",
      "    Final - Train Loss: 0.000353, Val Loss: 0.000174\n",
      "[Saved] NN1_w252_2017Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000366, Val Loss: 0.000172\n",
      "    Epoch 5/20, Train Loss: 0.000364, Val Loss: 0.000172\n",
      "Early stopping at epoch 7, Train Loss: 0.000364, Val Loss: 0.000172\n",
      "    Final - Train Loss: 0.000364, Val Loss: 0.000172\n",
      "[Saved] NN2_w252_2017Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000368, Val Loss: 0.000172\n",
      "    Epoch 5/20, Train Loss: 0.000365, Val Loss: 0.000172\n",
      "Early stopping at epoch 8, Train Loss: 0.000363, Val Loss: 0.000172\n",
      "    Final - Train Loss: 0.000363, Val Loss: 0.000172\n",
      "[Saved] NN3_w252_2017Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000356, Val Loss: 0.000171\n",
      "    Epoch 5/20, Train Loss: 0.000352, Val Loss: 0.000171\n",
      "Early stopping at epoch 7, Train Loss: 0.000350, Val Loss: 0.000171\n",
      "    Final - Train Loss: 0.000350, Val Loss: 0.000171\n",
      "[Saved] NN4_w252_2017Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000371, Val Loss: 0.000172\n",
      "    Epoch 5/20, Train Loss: 0.000366, Val Loss: 0.000173\n",
      "Early stopping at epoch 6, Train Loss: 0.000365, Val Loss: 0.000173\n",
      "    Final - Train Loss: 0.000365, Val Loss: 0.000173\n",
      "[Saved] NN5_w252_2017Q2.pth\n",
      "    Training data size: 200078 samples\n",
      "  Quarter 2017Q3: Training models with expanding data\n",
      "    Added 3122 samples from 2017Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000354, Val Loss: 0.000171\n",
      "    Epoch 5/20, Train Loss: 0.000351, Val Loss: 0.000172\n",
      "Early stopping at epoch 6, Train Loss: 0.000350, Val Loss: 0.000172\n",
      "    Final - Train Loss: 0.000350, Val Loss: 0.000172\n",
      "[Saved] NN1_w252_2017Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000361, Val Loss: 0.000170\n",
      "    Epoch 5/20, Train Loss: 0.000359, Val Loss: 0.000170\n",
      "Early stopping at epoch 7, Train Loss: 0.000360, Val Loss: 0.000170\n",
      "    Final - Train Loss: 0.000360, Val Loss: 0.000170\n",
      "[Saved] NN2_w252_2017Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000363, Val Loss: 0.000170\n",
      "    Epoch 5/20, Train Loss: 0.000359, Val Loss: 0.000170\n",
      "Early stopping at epoch 9, Train Loss: 0.000357, Val Loss: 0.000170\n",
      "    Final - Train Loss: 0.000357, Val Loss: 0.000170\n",
      "[Saved] NN3_w252_2017Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000351, Val Loss: 0.000170\n",
      "    Epoch 5/20, Train Loss: 0.000347, Val Loss: 0.000170\n",
      "Early stopping at epoch 6, Train Loss: 0.000346, Val Loss: 0.000170\n",
      "    Final - Train Loss: 0.000346, Val Loss: 0.000170\n",
      "[Saved] NN4_w252_2017Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000366, Val Loss: 0.000170\n",
      "    Epoch 5/20, Train Loss: 0.000361, Val Loss: 0.000170\n",
      "Early stopping at epoch 7, Train Loss: 0.000360, Val Loss: 0.000170\n",
      "    Final - Train Loss: 0.000360, Val Loss: 0.000170\n",
      "[Saved] NN5_w252_2017Q3.pth\n",
      "    Training data size: 203200 samples\n",
      "  Quarter 2017Q4: Training models with expanding data\n",
      "    Added 3114 samples from 2017Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000350, Val Loss: 0.000172\n",
      "    Epoch 5/20, Train Loss: 0.000347, Val Loss: 0.000173\n",
      "Early stopping at epoch 6, Train Loss: 0.000347, Val Loss: 0.000173\n",
      "    Final - Train Loss: 0.000347, Val Loss: 0.000173\n",
      "[Saved] NN1_w252_2017Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000358, Val Loss: 0.000171\n",
      "    Epoch 5/20, Train Loss: 0.000356, Val Loss: 0.000171\n",
      "Early stopping at epoch 6, Train Loss: 0.000356, Val Loss: 0.000171\n",
      "    Final - Train Loss: 0.000356, Val Loss: 0.000171\n",
      "[Saved] NN2_w252_2017Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000357, Val Loss: 0.000171\n",
      "    Epoch 5/20, Train Loss: 0.000354, Val Loss: 0.000171\n",
      "Early stopping at epoch 9, Train Loss: 0.000351, Val Loss: 0.000171\n",
      "    Final - Train Loss: 0.000351, Val Loss: 0.000171\n",
      "[Saved] NN3_w252_2017Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000347, Val Loss: 0.000171\n",
      "    Epoch 5/20, Train Loss: 0.000343, Val Loss: 0.000171\n",
      "Early stopping at epoch 9, Train Loss: 0.000339, Val Loss: 0.000171\n",
      "    Final - Train Loss: 0.000339, Val Loss: 0.000171\n",
      "[Saved] NN4_w252_2017Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000361, Val Loss: 0.000170\n",
      "    Epoch 5/20, Train Loss: 0.000356, Val Loss: 0.000170\n",
      "Early stopping at epoch 6, Train Loss: 0.000355, Val Loss: 0.000171\n",
      "    Final - Train Loss: 0.000355, Val Loss: 0.000171\n",
      "[Saved] NN5_w252_2017Q4.pth\n",
      "    Training data size: 206314 samples\n",
      "  Quarter 2018Q1: Training models with expanding data\n",
      "    Added 3115 samples from 2017Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000346, Val Loss: 0.000176\n",
      "    Epoch 5/20, Train Loss: 0.000344, Val Loss: 0.000177\n",
      "Early stopping at epoch 6, Train Loss: 0.000343, Val Loss: 0.000178\n",
      "    Final - Train Loss: 0.000343, Val Loss: 0.000178\n",
      "[Saved] NN1_w252_2018Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000354, Val Loss: 0.000174\n",
      "    Epoch 5/20, Train Loss: 0.000352, Val Loss: 0.000174\n",
      "Early stopping at epoch 6, Train Loss: 0.000352, Val Loss: 0.000174\n",
      "    Final - Train Loss: 0.000352, Val Loss: 0.000174\n",
      "[Saved] NN2_w252_2018Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000350, Val Loss: 0.000174\n",
      "    Epoch 5/20, Train Loss: 0.000348, Val Loss: 0.000174\n",
      "Early stopping at epoch 6, Train Loss: 0.000347, Val Loss: 0.000174\n",
      "    Final - Train Loss: 0.000347, Val Loss: 0.000174\n",
      "[Saved] NN3_w252_2018Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000339, Val Loss: 0.000174\n",
      "    Epoch 5/20, Train Loss: 0.000336, Val Loss: 0.000174\n",
      "Early stopping at epoch 8, Train Loss: 0.000334, Val Loss: 0.000174\n",
      "    Final - Train Loss: 0.000334, Val Loss: 0.000174\n",
      "[Saved] NN4_w252_2018Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000356, Val Loss: 0.000174\n",
      "    Epoch 5/20, Train Loss: 0.000350, Val Loss: 0.000174\n",
      "Early stopping at epoch 7, Train Loss: 0.000348, Val Loss: 0.000174\n",
      "    Final - Train Loss: 0.000348, Val Loss: 0.000174\n",
      "[Saved] NN5_w252_2018Q1.pth\n",
      "    Training data size: 209429 samples\n",
      "  Quarter 2018Q2: Training models with expanding data\n",
      "    Added 2996 samples from 2018Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000343, Val Loss: 0.000183\n",
      "    Epoch 5/20, Train Loss: 0.000340, Val Loss: 0.000185\n",
      "Early stopping at epoch 6, Train Loss: 0.000340, Val Loss: 0.000185\n",
      "    Final - Train Loss: 0.000340, Val Loss: 0.000185\n",
      "[Saved] NN1_w252_2018Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000351, Val Loss: 0.000181\n",
      "    Epoch 5/20, Train Loss: 0.000349, Val Loss: 0.000181\n",
      "Early stopping at epoch 7, Train Loss: 0.000348, Val Loss: 0.000181\n",
      "    Final - Train Loss: 0.000348, Val Loss: 0.000181\n",
      "[Saved] NN2_w252_2018Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000347, Val Loss: 0.000181\n",
      "    Epoch 5/20, Train Loss: 0.000345, Val Loss: 0.000181\n",
      "    Epoch 10/20, Train Loss: 0.000341, Val Loss: 0.000181\n",
      "Early stopping at epoch 11, Train Loss: 0.000342, Val Loss: 0.000181\n",
      "    Final - Train Loss: 0.000342, Val Loss: 0.000181\n",
      "[Saved] NN3_w252_2018Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000335, Val Loss: 0.000181\n",
      "    Epoch 5/20, Train Loss: 0.000332, Val Loss: 0.000182\n",
      "Early stopping at epoch 6, Train Loss: 0.000331, Val Loss: 0.000182\n",
      "    Final - Train Loss: 0.000331, Val Loss: 0.000182\n",
      "[Saved] NN4_w252_2018Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000351, Val Loss: 0.000181\n",
      "    Epoch 5/20, Train Loss: 0.000345, Val Loss: 0.000182\n",
      "Early stopping at epoch 7, Train Loss: 0.000343, Val Loss: 0.000182\n",
      "    Final - Train Loss: 0.000343, Val Loss: 0.000182\n",
      "[Saved] NN5_w252_2018Q2.pth\n",
      "    Training data size: 212425 samples\n",
      "  Quarter 2018Q3: Training models with expanding data\n",
      "    Added 3160 samples from 2018Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000340, Val Loss: 0.000188\n",
      "    Epoch 5/20, Train Loss: 0.000338, Val Loss: 0.000190\n",
      "Early stopping at epoch 6, Train Loss: 0.000336, Val Loss: 0.000189\n",
      "    Final - Train Loss: 0.000336, Val Loss: 0.000189\n",
      "[Saved] NN1_w252_2018Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000346, Val Loss: 0.000186\n",
      "    Epoch 5/20, Train Loss: 0.000345, Val Loss: 0.000186\n",
      "Early stopping at epoch 8, Train Loss: 0.000345, Val Loss: 0.000186\n",
      "    Final - Train Loss: 0.000345, Val Loss: 0.000186\n",
      "[Saved] NN2_w252_2018Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000339, Val Loss: 0.000186\n",
      "    Epoch 5/20, Train Loss: 0.000339, Val Loss: 0.000186\n",
      "    Epoch 10/20, Train Loss: 0.000334, Val Loss: 0.000187\n",
      "Early stopping at epoch 10, Train Loss: 0.000334, Val Loss: 0.000187\n",
      "    Final - Train Loss: 0.000334, Val Loss: 0.000187\n",
      "[Saved] NN3_w252_2018Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000331, Val Loss: 0.000186\n",
      "    Epoch 5/20, Train Loss: 0.000329, Val Loss: 0.000185\n",
      "Early stopping at epoch 8, Train Loss: 0.000327, Val Loss: 0.000186\n",
      "    Final - Train Loss: 0.000327, Val Loss: 0.000186\n",
      "[Saved] NN4_w252_2018Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000345, Val Loss: 0.000188\n",
      "    Epoch 5/20, Train Loss: 0.000340, Val Loss: 0.000188\n",
      "Early stopping at epoch 7, Train Loss: 0.000337, Val Loss: 0.000188\n",
      "    Final - Train Loss: 0.000337, Val Loss: 0.000188\n",
      "[Saved] NN5_w252_2018Q3.pth\n",
      "    Training data size: 215585 samples\n",
      "  Quarter 2018Q4: Training models with expanding data\n",
      "    Added 3125 samples from 2018Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000336, Val Loss: 0.000187\n",
      "    Epoch 5/20, Train Loss: 0.000334, Val Loss: 0.000189\n",
      "Early stopping at epoch 6, Train Loss: 0.000334, Val Loss: 0.000190\n",
      "    Final - Train Loss: 0.000334, Val Loss: 0.000190\n",
      "[Saved] NN1_w252_2018Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000343, Val Loss: 0.000184\n",
      "    Epoch 5/20, Train Loss: 0.000342, Val Loss: 0.000184\n",
      "Early stopping at epoch 7, Train Loss: 0.000341, Val Loss: 0.000184\n",
      "    Final - Train Loss: 0.000341, Val Loss: 0.000184\n",
      "[Saved] NN2_w252_2018Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000334, Val Loss: 0.000184\n",
      "    Epoch 5/20, Train Loss: 0.000333, Val Loss: 0.000185\n",
      "Early stopping at epoch 9, Train Loss: 0.000330, Val Loss: 0.000184\n",
      "    Final - Train Loss: 0.000330, Val Loss: 0.000184\n",
      "[Saved] NN3_w252_2018Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000326, Val Loss: 0.000183\n",
      "    Epoch 5/20, Train Loss: 0.000324, Val Loss: 0.000183\n",
      "Early stopping at epoch 6, Train Loss: 0.000322, Val Loss: 0.000183\n",
      "    Final - Train Loss: 0.000322, Val Loss: 0.000183\n",
      "[Saved] NN4_w252_2018Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000340, Val Loss: 0.000185\n",
      "    Epoch 5/20, Train Loss: 0.000335, Val Loss: 0.000185\n",
      "Early stopping at epoch 7, Train Loss: 0.000332, Val Loss: 0.000185\n",
      "    Final - Train Loss: 0.000332, Val Loss: 0.000185\n",
      "[Saved] NN5_w252_2018Q4.pth\n",
      "    Training data size: 218710 samples\n",
      "  Quarter 2019Q1: Training models with expanding data\n",
      "    Added 3045 samples from 2018Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000333, Val Loss: 0.000202\n",
      "    Epoch 5/20, Train Loss: 0.000331, Val Loss: 0.000203\n",
      "Early stopping at epoch 6, Train Loss: 0.000330, Val Loss: 0.000204\n",
      "    Final - Train Loss: 0.000330, Val Loss: 0.000204\n",
      "[Saved] NN1_w252_2019Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000340, Val Loss: 0.000197\n",
      "    Epoch 5/20, Train Loss: 0.000338, Val Loss: 0.000197\n",
      "    Epoch 10/20, Train Loss: 0.000337, Val Loss: 0.000198\n",
      "Early stopping at epoch 13, Train Loss: 0.000336, Val Loss: 0.000198\n",
      "    Final - Train Loss: 0.000336, Val Loss: 0.000198\n",
      "[Saved] NN2_w252_2019Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000330, Val Loss: 0.000198\n",
      "    Epoch 5/20, Train Loss: 0.000329, Val Loss: 0.000198\n",
      "Early stopping at epoch 7, Train Loss: 0.000328, Val Loss: 0.000199\n",
      "    Final - Train Loss: 0.000328, Val Loss: 0.000199\n",
      "[Saved] NN3_w252_2019Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000322, Val Loss: 0.000199\n",
      "    Epoch 5/20, Train Loss: 0.000319, Val Loss: 0.000198\n",
      "    Epoch 10/20, Train Loss: 0.000316, Val Loss: 0.000199\n",
      "Early stopping at epoch 13, Train Loss: 0.000314, Val Loss: 0.000200\n",
      "    Final - Train Loss: 0.000314, Val Loss: 0.000200\n",
      "[Saved] NN4_w252_2019Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000335, Val Loss: 0.000197\n",
      "    Epoch 5/20, Train Loss: 0.000331, Val Loss: 0.000198\n",
      "Early stopping at epoch 7, Train Loss: 0.000328, Val Loss: 0.000198\n",
      "    Final - Train Loss: 0.000328, Val Loss: 0.000198\n",
      "[Saved] NN5_w252_2019Q1.pth\n",
      "    Training data size: 221755 samples\n",
      "  Quarter 2019Q2: Training models with expanding data\n",
      "    Added 3022 samples from 2019Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000331, Val Loss: 0.000205\n",
      "    Epoch 5/20, Train Loss: 0.000329, Val Loss: 0.000207\n",
      "Early stopping at epoch 6, Train Loss: 0.000328, Val Loss: 0.000207\n",
      "    Final - Train Loss: 0.000328, Val Loss: 0.000207\n",
      "[Saved] NN1_w252_2019Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000334, Val Loss: 0.000200\n",
      "    Epoch 5/20, Train Loss: 0.000333, Val Loss: 0.000200\n",
      "Early stopping at epoch 7, Train Loss: 0.000332, Val Loss: 0.000200\n",
      "    Final - Train Loss: 0.000332, Val Loss: 0.000200\n",
      "[Saved] NN2_w252_2019Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000326, Val Loss: 0.000200\n",
      "    Epoch 5/20, Train Loss: 0.000325, Val Loss: 0.000200\n",
      "Early stopping at epoch 8, Train Loss: 0.000323, Val Loss: 0.000201\n",
      "    Final - Train Loss: 0.000323, Val Loss: 0.000201\n",
      "[Saved] NN3_w252_2019Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000315, Val Loss: 0.000201\n",
      "    Epoch 5/20, Train Loss: 0.000312, Val Loss: 0.000202\n",
      "Early stopping at epoch 7, Train Loss: 0.000311, Val Loss: 0.000202\n",
      "    Final - Train Loss: 0.000311, Val Loss: 0.000202\n",
      "[Saved] NN4_w252_2019Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000329, Val Loss: 0.000200\n",
      "    Epoch 5/20, Train Loss: 0.000325, Val Loss: 0.000201\n",
      "Early stopping at epoch 6, Train Loss: 0.000325, Val Loss: 0.000201\n",
      "    Final - Train Loss: 0.000325, Val Loss: 0.000201\n",
      "[Saved] NN5_w252_2019Q2.pth\n",
      "    Training data size: 224777 samples\n",
      "  Quarter 2019Q3: Training models with expanding data\n",
      "    Added 3120 samples from 2019Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000329, Val Loss: 0.000197\n",
      "    Epoch 5/20, Train Loss: 0.000327, Val Loss: 0.000198\n",
      "Early stopping at epoch 6, Train Loss: 0.000328, Val Loss: 0.000199\n",
      "    Final - Train Loss: 0.000328, Val Loss: 0.000199\n",
      "[Saved] NN1_w252_2019Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000333, Val Loss: 0.000194\n",
      "    Epoch 5/20, Train Loss: 0.000332, Val Loss: 0.000194\n",
      "    Epoch 10/20, Train Loss: 0.000330, Val Loss: 0.000194\n",
      "Early stopping at epoch 11, Train Loss: 0.000330, Val Loss: 0.000194\n",
      "    Final - Train Loss: 0.000330, Val Loss: 0.000194\n",
      "[Saved] NN2_w252_2019Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000324, Val Loss: 0.000194\n",
      "    Epoch 5/20, Train Loss: 0.000323, Val Loss: 0.000194\n",
      "Early stopping at epoch 6, Train Loss: 0.000322, Val Loss: 0.000194\n",
      "    Final - Train Loss: 0.000322, Val Loss: 0.000194\n",
      "[Saved] NN3_w252_2019Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000313, Val Loss: 0.000194\n",
      "    Epoch 5/20, Train Loss: 0.000310, Val Loss: 0.000196\n",
      "Early stopping at epoch 6, Train Loss: 0.000310, Val Loss: 0.000195\n",
      "    Final - Train Loss: 0.000310, Val Loss: 0.000195\n",
      "[Saved] NN4_w252_2019Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000327, Val Loss: 0.000194\n",
      "    Epoch 5/20, Train Loss: 0.000324, Val Loss: 0.000194\n",
      "Early stopping at epoch 6, Train Loss: 0.000323, Val Loss: 0.000195\n",
      "    Final - Train Loss: 0.000323, Val Loss: 0.000195\n",
      "[Saved] NN5_w252_2019Q3.pth\n",
      "    Training data size: 227897 samples\n",
      "  Quarter 2019Q4: Training models with expanding data\n",
      "    Added 3167 samples from 2019Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000328, Val Loss: 0.000199\n",
      "    Epoch 5/20, Train Loss: 0.000327, Val Loss: 0.000200\n",
      "Early stopping at epoch 6, Train Loss: 0.000326, Val Loss: 0.000201\n",
      "    Final - Train Loss: 0.000326, Val Loss: 0.000201\n",
      "[Saved] NN1_w252_2019Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000330, Val Loss: 0.000197\n",
      "    Epoch 5/20, Train Loss: 0.000328, Val Loss: 0.000197\n",
      "Early stopping at epoch 9, Train Loss: 0.000328, Val Loss: 0.000197\n",
      "    Final - Train Loss: 0.000328, Val Loss: 0.000197\n",
      "[Saved] NN2_w252_2019Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000323, Val Loss: 0.000197\n",
      "    Epoch 5/20, Train Loss: 0.000321, Val Loss: 0.000197\n",
      "Early stopping at epoch 6, Train Loss: 0.000321, Val Loss: 0.000197\n",
      "    Final - Train Loss: 0.000321, Val Loss: 0.000197\n",
      "[Saved] NN3_w252_2019Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000310, Val Loss: 0.000197\n",
      "    Epoch 5/20, Train Loss: 0.000309, Val Loss: 0.000198\n",
      "Early stopping at epoch 6, Train Loss: 0.000308, Val Loss: 0.000199\n",
      "    Final - Train Loss: 0.000308, Val Loss: 0.000199\n",
      "[Saved] NN4_w252_2019Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000326, Val Loss: 0.000197\n",
      "    Epoch 5/20, Train Loss: 0.000322, Val Loss: 0.000197\n",
      "Early stopping at epoch 6, Train Loss: 0.000320, Val Loss: 0.000197\n",
      "    Final - Train Loss: 0.000320, Val Loss: 0.000197\n",
      "[Saved] NN5_w252_2019Q4.pth\n",
      "    Training data size: 231064 samples\n",
      "  Quarter 2020Q1: Training models with expanding data\n",
      "    Added 3179 samples from 2019Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000326, Val Loss: 0.000192\n",
      "    Epoch 5/20, Train Loss: 0.000324, Val Loss: 0.000192\n",
      "Early stopping at epoch 6, Train Loss: 0.000324, Val Loss: 0.000192\n",
      "    Final - Train Loss: 0.000324, Val Loss: 0.000192\n",
      "[Saved] NN1_w252_2020Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000328, Val Loss: 0.000190\n",
      "    Epoch 5/20, Train Loss: 0.000327, Val Loss: 0.000190\n",
      "Early stopping at epoch 6, Train Loss: 0.000326, Val Loss: 0.000190\n",
      "    Final - Train Loss: 0.000326, Val Loss: 0.000190\n",
      "[Saved] NN2_w252_2020Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000322, Val Loss: 0.000190\n",
      "    Epoch 5/20, Train Loss: 0.000321, Val Loss: 0.000191\n",
      "Early stopping at epoch 6, Train Loss: 0.000320, Val Loss: 0.000191\n",
      "    Final - Train Loss: 0.000320, Val Loss: 0.000191\n",
      "[Saved] NN3_w252_2020Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000310, Val Loss: 0.000190\n",
      "    Epoch 5/20, Train Loss: 0.000306, Val Loss: 0.000191\n",
      "Early stopping at epoch 7, Train Loss: 0.000306, Val Loss: 0.000190\n",
      "    Final - Train Loss: 0.000306, Val Loss: 0.000190\n",
      "[Saved] NN4_w252_2020Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000324, Val Loss: 0.000192\n",
      "    Epoch 5/20, Train Loss: 0.000319, Val Loss: 0.000194\n",
      "Early stopping at epoch 6, Train Loss: 0.000319, Val Loss: 0.000193\n",
      "    Final - Train Loss: 0.000319, Val Loss: 0.000193\n",
      "[Saved] NN5_w252_2020Q1.pth\n",
      "    Training data size: 234243 samples\n",
      "  Quarter 2020Q2: Training models with expanding data\n",
      "    Added 2595 samples from 2020Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000324, Val Loss: 0.000216\n",
      "    Epoch 5/20, Train Loss: 0.000323, Val Loss: 0.000218\n",
      "Early stopping at epoch 6, Train Loss: 0.000323, Val Loss: 0.000219\n",
      "    Final - Train Loss: 0.000323, Val Loss: 0.000219\n",
      "[Saved] NN1_w252_2020Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000325, Val Loss: 0.000212\n",
      "    Epoch 5/20, Train Loss: 0.000324, Val Loss: 0.000212\n",
      "Early stopping at epoch 7, Train Loss: 0.000323, Val Loss: 0.000212\n",
      "    Final - Train Loss: 0.000323, Val Loss: 0.000212\n",
      "[Saved] NN2_w252_2020Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000320, Val Loss: 0.000211\n",
      "    Epoch 5/20, Train Loss: 0.000317, Val Loss: 0.000211\n",
      "Early stopping at epoch 6, Train Loss: 0.000317, Val Loss: 0.000212\n",
      "    Final - Train Loss: 0.000317, Val Loss: 0.000212\n",
      "[Saved] NN3_w252_2020Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000306, Val Loss: 0.000212\n",
      "    Epoch 5/20, Train Loss: 0.000306, Val Loss: 0.000213\n",
      "Early stopping at epoch 6, Train Loss: 0.000305, Val Loss: 0.000214\n",
      "    Final - Train Loss: 0.000305, Val Loss: 0.000214\n",
      "[Saved] NN4_w252_2020Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000321, Val Loss: 0.000212\n",
      "    Epoch 5/20, Train Loss: 0.000317, Val Loss: 0.000212\n",
      "Early stopping at epoch 8, Train Loss: 0.000313, Val Loss: 0.000214\n",
      "    Final - Train Loss: 0.000313, Val Loss: 0.000214\n",
      "[Saved] NN5_w252_2020Q2.pth\n",
      "    Training data size: 236838 samples\n",
      "  Quarter 2020Q3: Training models with expanding data\n",
      "    Added 2839 samples from 2020Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000322, Val Loss: 0.000239\n",
      "    Epoch 5/20, Train Loss: 0.000321, Val Loss: 0.000240\n",
      "Early stopping at epoch 6, Train Loss: 0.000321, Val Loss: 0.000241\n",
      "    Final - Train Loss: 0.000321, Val Loss: 0.000241\n",
      "[Saved] NN1_w252_2020Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000323, Val Loss: 0.000236\n",
      "    Epoch 5/20, Train Loss: 0.000322, Val Loss: 0.000236\n",
      "Early stopping at epoch 6, Train Loss: 0.000320, Val Loss: 0.000236\n",
      "    Final - Train Loss: 0.000320, Val Loss: 0.000236\n",
      "[Saved] NN2_w252_2020Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000317, Val Loss: 0.000237\n",
      "    Epoch 5/20, Train Loss: 0.000316, Val Loss: 0.000237\n",
      "    Epoch 10/20, Train Loss: 0.000314, Val Loss: 0.000237\n",
      "Early stopping at epoch 11, Train Loss: 0.000313, Val Loss: 0.000238\n",
      "    Final - Train Loss: 0.000313, Val Loss: 0.000238\n",
      "[Saved] NN3_w252_2020Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000304, Val Loss: 0.000237\n",
      "    Epoch 5/20, Train Loss: 0.000303, Val Loss: 0.000236\n",
      "Early stopping at epoch 8, Train Loss: 0.000302, Val Loss: 0.000237\n",
      "    Final - Train Loss: 0.000302, Val Loss: 0.000237\n",
      "[Saved] NN4_w252_2020Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000315, Val Loss: 0.000240\n",
      "    Epoch 5/20, Train Loss: 0.000312, Val Loss: 0.000238\n",
      "Early stopping at epoch 7, Train Loss: 0.000310, Val Loss: 0.000238\n",
      "    Final - Train Loss: 0.000310, Val Loss: 0.000238\n",
      "[Saved] NN5_w252_2020Q3.pth\n",
      "    Training data size: 239677 samples\n",
      "  Quarter 2020Q4: Training models with expanding data\n",
      "    Added 3151 samples from 2020Q3 to training set\n",
      "    [Re-tuning] NN1 for 2020Q4\n",
      "[Hyper] NN1: best_MSE=0.000275, params={'learning_rate': 0.0007016683098186692, 'batch_size': 128, 'dropout_rate': 0.407960852804087}\n",
      "    [Structure Change] dropout_rate changed: 0.416505293673187 -> 0.407960852804087\n",
      "    Rebuilding NN1 model due to structure change\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 0.162846, Val Loss: 0.001031\n",
      "    Epoch 5/50, Train Loss: 0.000353, Val Loss: 0.000252\n",
      "Early stopping at epoch 9, Train Loss: 0.000353, Val Loss: 0.000252\n",
      "    Final - Train Loss: 0.000353, Val Loss: 0.000252\n",
      "[Saved] NN1_w252_2020Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000321, Val Loss: 0.000250\n",
      "    Epoch 5/20, Train Loss: 0.000321, Val Loss: 0.000250\n",
      "Early stopping at epoch 9, Train Loss: 0.000320, Val Loss: 0.000250\n",
      "    Final - Train Loss: 0.000320, Val Loss: 0.000250\n",
      "[Saved] NN2_w252_2020Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000313, Val Loss: 0.000250\n",
      "    Epoch 5/20, Train Loss: 0.000313, Val Loss: 0.000250\n",
      "    Epoch 10/20, Train Loss: 0.000312, Val Loss: 0.000250\n",
      "Early stopping at epoch 11, Train Loss: 0.000311, Val Loss: 0.000250\n",
      "    Final - Train Loss: 0.000311, Val Loss: 0.000250\n",
      "[Saved] NN3_w252_2020Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000301, Val Loss: 0.000251\n",
      "    Epoch 5/20, Train Loss: 0.000302, Val Loss: 0.000251\n",
      "Early stopping at epoch 6, Train Loss: 0.000303, Val Loss: 0.000252\n",
      "    Final - Train Loss: 0.000303, Val Loss: 0.000252\n",
      "[Saved] NN4_w252_2020Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000313, Val Loss: 0.000250\n",
      "    Epoch 5/20, Train Loss: 0.000311, Val Loss: 0.000250\n",
      "Early stopping at epoch 9, Train Loss: 0.000307, Val Loss: 0.000250\n",
      "    Final - Train Loss: 0.000307, Val Loss: 0.000250\n",
      "[Saved] NN5_w252_2020Q4.pth\n",
      "    Training data size: 242828 samples\n",
      "  Quarter 2021Q1: Training models with expanding data\n",
      "    Added 3114 samples from 2020Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000349, Val Loss: 0.000265\n",
      "    Epoch 5/20, Train Loss: 0.000333, Val Loss: 0.000262\n",
      "    Epoch 10/20, Train Loss: 0.000326, Val Loss: 0.000263\n",
      "Early stopping at epoch 11, Train Loss: 0.000325, Val Loss: 0.000265\n",
      "    Final - Train Loss: 0.000325, Val Loss: 0.000265\n",
      "[Saved] NN1_w252_2021Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000319, Val Loss: 0.000260\n",
      "    Epoch 5/20, Train Loss: 0.000318, Val Loss: 0.000260\n",
      "Early stopping at epoch 8, Train Loss: 0.000317, Val Loss: 0.000260\n",
      "    Final - Train Loss: 0.000317, Val Loss: 0.000260\n",
      "[Saved] NN2_w252_2021Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000310, Val Loss: 0.000261\n",
      "    Epoch 5/20, Train Loss: 0.000310, Val Loss: 0.000262\n",
      "Early stopping at epoch 7, Train Loss: 0.000309, Val Loss: 0.000262\n",
      "    Final - Train Loss: 0.000309, Val Loss: 0.000262\n",
      "[Saved] NN3_w252_2021Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000300, Val Loss: 0.000260\n",
      "    Epoch 5/20, Train Loss: 0.000302, Val Loss: 0.000260\n",
      "    Epoch 10/20, Train Loss: 0.000300, Val Loss: 0.000260\n",
      "Early stopping at epoch 10, Train Loss: 0.000300, Val Loss: 0.000260\n",
      "    Final - Train Loss: 0.000300, Val Loss: 0.000260\n",
      "[Saved] NN4_w252_2021Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000309, Val Loss: 0.000261\n",
      "    Epoch 5/20, Train Loss: 0.000306, Val Loss: 0.000262\n",
      "Early stopping at epoch 9, Train Loss: 0.000302, Val Loss: 0.000262\n",
      "    Final - Train Loss: 0.000302, Val Loss: 0.000262\n",
      "[Saved] NN5_w252_2021Q1.pth\n",
      "    Training data size: 245942 samples\n",
      "  Quarter 2021Q2: Training models with expanding data\n",
      "    Added 2989 samples from 2021Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000327, Val Loss: 0.000276\n",
      "    Epoch 5/20, Train Loss: 0.000323, Val Loss: 0.000281\n",
      "Early stopping at epoch 6, Train Loss: 0.000323, Val Loss: 0.000281\n",
      "    Final - Train Loss: 0.000323, Val Loss: 0.000281\n",
      "[Saved] NN1_w252_2021Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000315, Val Loss: 0.000274\n",
      "    Epoch 5/20, Train Loss: 0.000315, Val Loss: 0.000274\n",
      "Early stopping at epoch 8, Train Loss: 0.000314, Val Loss: 0.000274\n",
      "    Final - Train Loss: 0.000314, Val Loss: 0.000274\n",
      "[Saved] NN2_w252_2021Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000308, Val Loss: 0.000274\n",
      "    Epoch 5/20, Train Loss: 0.000306, Val Loss: 0.000274\n",
      "    Epoch 10/20, Train Loss: 0.000305, Val Loss: 0.000274\n",
      "Early stopping at epoch 13, Train Loss: 0.000301, Val Loss: 0.000274\n",
      "    Final - Train Loss: 0.000301, Val Loss: 0.000274\n",
      "[Saved] NN3_w252_2021Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000299, Val Loss: 0.000274\n",
      "    Epoch 5/20, Train Loss: 0.000297, Val Loss: 0.000274\n",
      "Early stopping at epoch 6, Train Loss: 0.000297, Val Loss: 0.000274\n",
      "    Final - Train Loss: 0.000297, Val Loss: 0.000274\n",
      "[Saved] NN4_w252_2021Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000303, Val Loss: 0.000275\n",
      "    Epoch 5/20, Train Loss: 0.000299, Val Loss: 0.000275\n",
      "Early stopping at epoch 6, Train Loss: 0.000301, Val Loss: 0.000275\n",
      "    Final - Train Loss: 0.000301, Val Loss: 0.000275\n",
      "[Saved] NN5_w252_2021Q2.pth\n",
      "    Training data size: 248931 samples\n",
      "  Quarter 2021Q3: Training models with expanding data\n",
      "    Added 3132 samples from 2021Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000323, Val Loss: 0.000279\n",
      "    Epoch 5/20, Train Loss: 0.000320, Val Loss: 0.000285\n",
      "Early stopping at epoch 6, Train Loss: 0.000320, Val Loss: 0.000285\n",
      "    Final - Train Loss: 0.000320, Val Loss: 0.000285\n",
      "[Saved] NN1_w252_2021Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000311, Val Loss: 0.000278\n",
      "    Epoch 5/20, Train Loss: 0.000311, Val Loss: 0.000279\n",
      "Early stopping at epoch 6, Train Loss: 0.000311, Val Loss: 0.000278\n",
      "    Final - Train Loss: 0.000311, Val Loss: 0.000278\n",
      "[Saved] NN2_w252_2021Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000302, Val Loss: 0.000278\n",
      "    Epoch 5/20, Train Loss: 0.000300, Val Loss: 0.000278\n",
      "Early stopping at epoch 7, Train Loss: 0.000299, Val Loss: 0.000278\n",
      "    Final - Train Loss: 0.000299, Val Loss: 0.000278\n",
      "[Saved] NN3_w252_2021Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000294, Val Loss: 0.000279\n",
      "    Epoch 5/20, Train Loss: 0.000292, Val Loss: 0.000279\n",
      "Early stopping at epoch 9, Train Loss: 0.000291, Val Loss: 0.000280\n",
      "    Final - Train Loss: 0.000291, Val Loss: 0.000280\n",
      "[Saved] NN4_w252_2021Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000300, Val Loss: 0.000278\n",
      "    Epoch 5/20, Train Loss: 0.000297, Val Loss: 0.000278\n",
      "Early stopping at epoch 6, Train Loss: 0.000297, Val Loss: 0.000278\n",
      "    Final - Train Loss: 0.000297, Val Loss: 0.000278\n",
      "[Saved] NN5_w252_2021Q3.pth\n",
      "    Training data size: 252063 samples\n",
      "  Quarter 2021Q4: Training models with expanding data\n",
      "    Added 3173 samples from 2021Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000321, Val Loss: 0.000286\n",
      "    Epoch 5/20, Train Loss: 0.000318, Val Loss: 0.000289\n",
      "Early stopping at epoch 7, Train Loss: 0.000317, Val Loss: 0.000294\n",
      "    Final - Train Loss: 0.000317, Val Loss: 0.000294\n",
      "[Saved] NN1_w252_2021Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000309, Val Loss: 0.000282\n",
      "    Epoch 5/20, Train Loss: 0.000309, Val Loss: 0.000282\n",
      "Early stopping at epoch 6, Train Loss: 0.000308, Val Loss: 0.000283\n",
      "    Final - Train Loss: 0.000308, Val Loss: 0.000283\n",
      "[Saved] NN2_w252_2021Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000299, Val Loss: 0.000282\n",
      "    Epoch 5/20, Train Loss: 0.000299, Val Loss: 0.000282\n",
      "    Epoch 10/20, Train Loss: 0.000296, Val Loss: 0.000281\n",
      "Early stopping at epoch 11, Train Loss: 0.000296, Val Loss: 0.000282\n",
      "    Final - Train Loss: 0.000296, Val Loss: 0.000282\n",
      "[Saved] NN3_w252_2021Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000290, Val Loss: 0.000283\n",
      "    Epoch 5/20, Train Loss: 0.000289, Val Loss: 0.000282\n",
      "Early stopping at epoch 8, Train Loss: 0.000289, Val Loss: 0.000284\n",
      "    Final - Train Loss: 0.000289, Val Loss: 0.000284\n",
      "[Saved] NN4_w252_2021Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000296, Val Loss: 0.000281\n",
      "    Epoch 5/20, Train Loss: 0.000294, Val Loss: 0.000281\n",
      "Early stopping at epoch 6, Train Loss: 0.000293, Val Loss: 0.000281\n",
      "    Final - Train Loss: 0.000293, Val Loss: 0.000281\n",
      "[Saved] NN5_w252_2021Q4.pth\n",
      "    Training data size: 255236 samples\n",
      "  Quarter 2022Q1: Training models with expanding data\n",
      "    Added 3153 samples from 2021Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000317, Val Loss: 0.000291\n",
      "    Epoch 5/20, Train Loss: 0.000315, Val Loss: 0.000299\n",
      "Early stopping at epoch 6, Train Loss: 0.000314, Val Loss: 0.000298\n",
      "    Final - Train Loss: 0.000314, Val Loss: 0.000298\n",
      "[Saved] NN1_w252_2022Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000307, Val Loss: 0.000286\n",
      "    Epoch 5/20, Train Loss: 0.000306, Val Loss: 0.000286\n",
      "Early stopping at epoch 6, Train Loss: 0.000306, Val Loss: 0.000286\n",
      "    Final - Train Loss: 0.000306, Val Loss: 0.000286\n",
      "[Saved] NN2_w252_2022Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000296, Val Loss: 0.000286\n",
      "    Epoch 5/20, Train Loss: 0.000295, Val Loss: 0.000286\n",
      "Early stopping at epoch 8, Train Loss: 0.000293, Val Loss: 0.000286\n",
      "    Final - Train Loss: 0.000293, Val Loss: 0.000286\n",
      "[Saved] NN3_w252_2022Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000287, Val Loss: 0.000286\n",
      "    Epoch 5/20, Train Loss: 0.000287, Val Loss: 0.000286\n",
      "    Epoch 10/20, Train Loss: 0.000285, Val Loss: 0.000286\n",
      "Early stopping at epoch 12, Train Loss: 0.000284, Val Loss: 0.000287\n",
      "    Final - Train Loss: 0.000284, Val Loss: 0.000287\n",
      "[Saved] NN4_w252_2022Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000294, Val Loss: 0.000286\n",
      "    Epoch 5/20, Train Loss: 0.000292, Val Loss: 0.000287\n",
      "Early stopping at epoch 6, Train Loss: 0.000292, Val Loss: 0.000287\n",
      "    Final - Train Loss: 0.000292, Val Loss: 0.000287\n",
      "[Saved] NN5_w252_2022Q1.pth\n",
      "    Training data size: 258389 samples\n",
      "  Quarter 2022Q2: Training models with expanding data\n",
      "    Added 3029 samples from 2022Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000316, Val Loss: 0.000306\n",
      "    Epoch 5/20, Train Loss: 0.000314, Val Loss: 0.000314\n",
      "Early stopping at epoch 6, Train Loss: 0.000313, Val Loss: 0.000317\n",
      "    Final - Train Loss: 0.000313, Val Loss: 0.000317\n",
      "[Saved] NN1_w252_2022Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000306, Val Loss: 0.000299\n",
      "    Epoch 5/20, Train Loss: 0.000306, Val Loss: 0.000299\n",
      "Early stopping at epoch 6, Train Loss: 0.000305, Val Loss: 0.000299\n",
      "    Final - Train Loss: 0.000305, Val Loss: 0.000299\n",
      "[Saved] NN2_w252_2022Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000294, Val Loss: 0.000299\n",
      "    Epoch 5/20, Train Loss: 0.000293, Val Loss: 0.000300\n",
      "    Epoch 10/20, Train Loss: 0.000290, Val Loss: 0.000299\n",
      "Early stopping at epoch 13, Train Loss: 0.000288, Val Loss: 0.000299\n",
      "    Final - Train Loss: 0.000288, Val Loss: 0.000299\n",
      "[Saved] NN3_w252_2022Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000283, Val Loss: 0.000298\n",
      "    Epoch 5/20, Train Loss: 0.000282, Val Loss: 0.000299\n",
      "Early stopping at epoch 6, Train Loss: 0.000282, Val Loss: 0.000299\n",
      "    Final - Train Loss: 0.000282, Val Loss: 0.000299\n",
      "[Saved] NN4_w252_2022Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000293, Val Loss: 0.000299\n",
      "    Epoch 5/20, Train Loss: 0.000291, Val Loss: 0.000299\n",
      "    Epoch 10/20, Train Loss: 0.000286, Val Loss: 0.000299\n",
      "Early stopping at epoch 14, Train Loss: 0.000284, Val Loss: 0.000300\n",
      "    Final - Train Loss: 0.000284, Val Loss: 0.000300\n",
      "[Saved] NN5_w252_2022Q2.pth\n",
      "    Training data size: 261418 samples\n",
      "  Quarter 2022Q3: Training models with expanding data\n",
      "    Added 2969 samples from 2022Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000314, Val Loss: 0.000320\n",
      "    Epoch 5/20, Train Loss: 0.000312, Val Loss: 0.000324\n",
      "Early stopping at epoch 8, Train Loss: 0.000311, Val Loss: 0.000332\n",
      "    Final - Train Loss: 0.000311, Val Loss: 0.000332\n",
      "[Saved] NN1_w252_2022Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000304, Val Loss: 0.000312\n",
      "    Epoch 5/20, Train Loss: 0.000305, Val Loss: 0.000313\n",
      "Early stopping at epoch 9, Train Loss: 0.000302, Val Loss: 0.000313\n",
      "    Final - Train Loss: 0.000302, Val Loss: 0.000313\n",
      "[Saved] NN2_w252_2022Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000290, Val Loss: 0.000312\n",
      "    Epoch 5/20, Train Loss: 0.000289, Val Loss: 0.000312\n",
      "Early stopping at epoch 8, Train Loss: 0.000289, Val Loss: 0.000312\n",
      "    Final - Train Loss: 0.000289, Val Loss: 0.000312\n",
      "[Saved] NN3_w252_2022Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000282, Val Loss: 0.000312\n",
      "    Epoch 5/20, Train Loss: 0.000281, Val Loss: 0.000312\n",
      "Early stopping at epoch 7, Train Loss: 0.000281, Val Loss: 0.000312\n",
      "    Final - Train Loss: 0.000281, Val Loss: 0.000312\n",
      "[Saved] NN4_w252_2022Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000285, Val Loss: 0.000312\n",
      "    Epoch 5/20, Train Loss: 0.000283, Val Loss: 0.000312\n",
      "    Epoch 10/20, Train Loss: 0.000280, Val Loss: 0.000313\n",
      "Early stopping at epoch 12, Train Loss: 0.000278, Val Loss: 0.000312\n",
      "    Final - Train Loss: 0.000278, Val Loss: 0.000312\n",
      "[Saved] NN5_w252_2022Q3.pth\n",
      "    Training data size: 264387 samples\n",
      "  Quarter 2022Q4: Training models with expanding data\n",
      "    Added 3152 samples from 2022Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000312, Val Loss: 0.000324\n",
      "    Epoch 5/20, Train Loss: 0.000310, Val Loss: 0.000331\n",
      "Early stopping at epoch 6, Train Loss: 0.000309, Val Loss: 0.000337\n",
      "    Final - Train Loss: 0.000309, Val Loss: 0.000337\n",
      "[Saved] NN1_w252_2022Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000303, Val Loss: 0.000319\n",
      "    Epoch 5/20, Train Loss: 0.000302, Val Loss: 0.000319\n",
      "Early stopping at epoch 6, Train Loss: 0.000303, Val Loss: 0.000319\n",
      "    Final - Train Loss: 0.000303, Val Loss: 0.000319\n",
      "[Saved] NN2_w252_2022Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000289, Val Loss: 0.000318\n",
      "    Epoch 5/20, Train Loss: 0.000287, Val Loss: 0.000318\n",
      "    Epoch 10/20, Train Loss: 0.000287, Val Loss: 0.000318\n",
      "Early stopping at epoch 10, Train Loss: 0.000287, Val Loss: 0.000318\n",
      "    Final - Train Loss: 0.000287, Val Loss: 0.000318\n",
      "[Saved] NN3_w252_2022Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000280, Val Loss: 0.000319\n",
      "    Epoch 5/20, Train Loss: 0.000279, Val Loss: 0.000319\n",
      "Early stopping at epoch 6, Train Loss: 0.000279, Val Loss: 0.000319\n",
      "    Final - Train Loss: 0.000279, Val Loss: 0.000319\n",
      "[Saved] NN4_w252_2022Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000280, Val Loss: 0.000319\n",
      "    Epoch 5/20, Train Loss: 0.000277, Val Loss: 0.000318\n",
      "Early stopping at epoch 9, Train Loss: 0.000275, Val Loss: 0.000319\n",
      "    Final - Train Loss: 0.000275, Val Loss: 0.000319\n",
      "[Saved] NN5_w252_2022Q4.pth\n",
      "    Training data size: 267539 samples\n",
      "  Quarter 2023Q1: Training models with expanding data\n",
      "    Added 3070 samples from 2022Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000309, Val Loss: 0.000339\n",
      "    Epoch 5/20, Train Loss: 0.000308, Val Loss: 0.000348\n",
      "Early stopping at epoch 6, Train Loss: 0.000308, Val Loss: 0.000356\n",
      "    Final - Train Loss: 0.000308, Val Loss: 0.000356\n",
      "[Saved] NN1_w252_2023Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000301, Val Loss: 0.000332\n",
      "    Epoch 5/20, Train Loss: 0.000300, Val Loss: 0.000332\n",
      "Early stopping at epoch 6, Train Loss: 0.000300, Val Loss: 0.000332\n",
      "    Final - Train Loss: 0.000300, Val Loss: 0.000332\n",
      "[Saved] NN2_w252_2023Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000286, Val Loss: 0.000333\n",
      "    Epoch 5/20, Train Loss: 0.000285, Val Loss: 0.000334\n",
      "Early stopping at epoch 7, Train Loss: 0.000284, Val Loss: 0.000334\n",
      "    Final - Train Loss: 0.000284, Val Loss: 0.000334\n",
      "[Saved] NN3_w252_2023Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000278, Val Loss: 0.000331\n",
      "    Epoch 5/20, Train Loss: 0.000278, Val Loss: 0.000332\n",
      "Early stopping at epoch 6, Train Loss: 0.000277, Val Loss: 0.000331\n",
      "    Final - Train Loss: 0.000277, Val Loss: 0.000331\n",
      "[Saved] NN4_w252_2023Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000275, Val Loss: 0.000334\n",
      "    Epoch 5/20, Train Loss: 0.000275, Val Loss: 0.000333\n",
      "    Epoch 10/20, Train Loss: 0.000270, Val Loss: 0.000334\n",
      "Early stopping at epoch 10, Train Loss: 0.000270, Val Loss: 0.000334\n",
      "    Final - Train Loss: 0.000270, Val Loss: 0.000334\n",
      "[Saved] NN5_w252_2023Q1.pth\n",
      "    Training data size: 270609 samples\n",
      "  Quarter 2023Q2: Training models with expanding data\n",
      "    Added 3064 samples from 2023Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000307, Val Loss: 0.000348\n",
      "    Epoch 5/20, Train Loss: 0.000306, Val Loss: 0.000357\n",
      "Early stopping at epoch 6, Train Loss: 0.000305, Val Loss: 0.000367\n",
      "    Final - Train Loss: 0.000305, Val Loss: 0.000367\n",
      "[Saved] NN1_w252_2023Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000298, Val Loss: 0.000336\n",
      "    Epoch 5/20, Train Loss: 0.000299, Val Loss: 0.000336\n",
      "    Epoch 10/20, Train Loss: 0.000298, Val Loss: 0.000336\n",
      "Early stopping at epoch 12, Train Loss: 0.000298, Val Loss: 0.000336\n",
      "    Final - Train Loss: 0.000298, Val Loss: 0.000336\n",
      "[Saved] NN2_w252_2023Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000285, Val Loss: 0.000335\n",
      "    Epoch 5/20, Train Loss: 0.000284, Val Loss: 0.000335\n",
      "Early stopping at epoch 6, Train Loss: 0.000284, Val Loss: 0.000335\n",
      "    Final - Train Loss: 0.000284, Val Loss: 0.000335\n",
      "[Saved] NN3_w252_2023Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000277, Val Loss: 0.000335\n",
      "    Epoch 5/20, Train Loss: 0.000277, Val Loss: 0.000336\n",
      "    Epoch 10/20, Train Loss: 0.000273, Val Loss: 0.000335\n",
      "Early stopping at epoch 12, Train Loss: 0.000271, Val Loss: 0.000336\n",
      "    Final - Train Loss: 0.000271, Val Loss: 0.000336\n",
      "[Saved] NN4_w252_2023Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000271, Val Loss: 0.000337\n",
      "    Epoch 5/20, Train Loss: 0.000269, Val Loss: 0.000338\n",
      "Early stopping at epoch 9, Train Loss: 0.000266, Val Loss: 0.000338\n",
      "    Final - Train Loss: 0.000266, Val Loss: 0.000338\n",
      "[Saved] NN5_w252_2023Q2.pth\n",
      "    Training data size: 273673 samples\n",
      "  Quarter 2023Q3: Training models with expanding data\n",
      "    Added 3069 samples from 2023Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000308, Val Loss: 0.000343\n",
      "    Epoch 5/20, Train Loss: 0.000305, Val Loss: 0.000352\n",
      "Early stopping at epoch 6, Train Loss: 0.000305, Val Loss: 0.000355\n",
      "    Final - Train Loss: 0.000305, Val Loss: 0.000355\n",
      "[Saved] NN1_w252_2023Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000298, Val Loss: 0.000332\n",
      "    Epoch 5/20, Train Loss: 0.000298, Val Loss: 0.000332\n",
      "Early stopping at epoch 9, Train Loss: 0.000297, Val Loss: 0.000332\n",
      "    Final - Train Loss: 0.000297, Val Loss: 0.000332\n",
      "[Saved] NN2_w252_2023Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000284, Val Loss: 0.000331\n",
      "    Epoch 5/20, Train Loss: 0.000285, Val Loss: 0.000331\n",
      "Early stopping at epoch 9, Train Loss: 0.000284, Val Loss: 0.000331\n",
      "    Final - Train Loss: 0.000284, Val Loss: 0.000331\n",
      "[Saved] NN3_w252_2023Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000276, Val Loss: 0.000334\n",
      "    Epoch 5/20, Train Loss: 0.000275, Val Loss: 0.000333\n",
      "Early stopping at epoch 7, Train Loss: 0.000274, Val Loss: 0.000334\n",
      "    Final - Train Loss: 0.000274, Val Loss: 0.000334\n",
      "[Saved] NN4_w252_2023Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000270, Val Loss: 0.000331\n",
      "    Epoch 5/20, Train Loss: 0.000268, Val Loss: 0.000331\n",
      "    Epoch 10/20, Train Loss: 0.000264, Val Loss: 0.000331\n",
      "Early stopping at epoch 12, Train Loss: 0.000263, Val Loss: 0.000331\n",
      "    Final - Train Loss: 0.000263, Val Loss: 0.000331\n",
      "[Saved] NN5_w252_2023Q3.pth\n",
      "    Training data size: 276742 samples\n",
      "  Quarter 2023Q4: Training models with expanding data\n",
      "    Added 3121 samples from 2023Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000306, Val Loss: 0.000339\n",
      "    Epoch 5/20, Train Loss: 0.000305, Val Loss: 0.000346\n",
      "Early stopping at epoch 6, Train Loss: 0.000304, Val Loss: 0.000346\n",
      "    Final - Train Loss: 0.000304, Val Loss: 0.000346\n",
      "[Saved] NN1_w252_2023Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000296, Val Loss: 0.000327\n",
      "    Epoch 5/20, Train Loss: 0.000296, Val Loss: 0.000327\n",
      "Early stopping at epoch 9, Train Loss: 0.000294, Val Loss: 0.000327\n",
      "    Final - Train Loss: 0.000294, Val Loss: 0.000327\n",
      "[Saved] NN2_w252_2023Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000283, Val Loss: 0.000330\n",
      "    Epoch 5/20, Train Loss: 0.000283, Val Loss: 0.000329\n",
      "Early stopping at epoch 8, Train Loss: 0.000283, Val Loss: 0.000332\n",
      "    Final - Train Loss: 0.000283, Val Loss: 0.000332\n",
      "[Saved] NN3_w252_2023Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000275, Val Loss: 0.000328\n",
      "    Epoch 5/20, Train Loss: 0.000275, Val Loss: 0.000327\n",
      "    Epoch 10/20, Train Loss: 0.000271, Val Loss: 0.000328\n",
      "Early stopping at epoch 13, Train Loss: 0.000271, Val Loss: 0.000328\n",
      "    Final - Train Loss: 0.000271, Val Loss: 0.000328\n",
      "[Saved] NN4_w252_2023Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000265, Val Loss: 0.000334\n",
      "    Epoch 5/20, Train Loss: 0.000264, Val Loss: 0.000332\n",
      "    Epoch 10/20, Train Loss: 0.000261, Val Loss: 0.000333\n",
      "Early stopping at epoch 10, Train Loss: 0.000261, Val Loss: 0.000333\n",
      "    Final - Train Loss: 0.000261, Val Loss: 0.000333\n",
      "[Saved] NN5_w252_2023Q4.pth\n",
      "    Training data size: 279863 samples\n",
      "  Quarter 2024Q1: Training models with expanding data\n",
      "    Added 3113 samples from 2023Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000304, Val Loss: 0.000343\n",
      "    Epoch 5/20, Train Loss: 0.000303, Val Loss: 0.000351\n",
      "Early stopping at epoch 6, Train Loss: 0.000303, Val Loss: 0.000351\n",
      "    Final - Train Loss: 0.000303, Val Loss: 0.000351\n",
      "[Saved] NN1_w252_2024Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000294, Val Loss: 0.000330\n",
      "    Epoch 5/20, Train Loss: 0.000294, Val Loss: 0.000331\n",
      "Early stopping at epoch 7, Train Loss: 0.000293, Val Loss: 0.000331\n",
      "    Final - Train Loss: 0.000293, Val Loss: 0.000331\n",
      "[Saved] NN2_w252_2024Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000282, Val Loss: 0.000331\n",
      "    Epoch 5/20, Train Loss: 0.000281, Val Loss: 0.000332\n",
      "Early stopping at epoch 7, Train Loss: 0.000280, Val Loss: 0.000331\n",
      "    Final - Train Loss: 0.000280, Val Loss: 0.000331\n",
      "[Saved] NN3_w252_2024Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000270, Val Loss: 0.000331\n",
      "    Epoch 5/20, Train Loss: 0.000269, Val Loss: 0.000331\n",
      "Early stopping at epoch 8, Train Loss: 0.000269, Val Loss: 0.000332\n",
      "    Final - Train Loss: 0.000269, Val Loss: 0.000332\n",
      "[Saved] NN4_w252_2024Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000262, Val Loss: 0.000330\n",
      "    Epoch 5/20, Train Loss: 0.000260, Val Loss: 0.000332\n",
      "Early stopping at epoch 6, Train Loss: 0.000259, Val Loss: 0.000332\n",
      "    Final - Train Loss: 0.000259, Val Loss: 0.000332\n",
      "[Saved] NN5_w252_2024Q1.pth\n",
      "    Training data size: 282976 samples\n",
      "  Quarter 2024Q2: Training models with expanding data\n",
      "    Added 3026 samples from 2024Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000303, Val Loss: 0.000347\n",
      "    Epoch 5/20, Train Loss: 0.000301, Val Loss: 0.000356\n",
      "Early stopping at epoch 7, Train Loss: 0.000301, Val Loss: 0.000361\n",
      "    Final - Train Loss: 0.000301, Val Loss: 0.000361\n",
      "[Saved] NN1_w252_2024Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000292, Val Loss: 0.000332\n",
      "    Epoch 5/20, Train Loss: 0.000292, Val Loss: 0.000331\n",
      "Early stopping at epoch 7, Train Loss: 0.000292, Val Loss: 0.000332\n",
      "    Final - Train Loss: 0.000292, Val Loss: 0.000332\n",
      "[Saved] NN2_w252_2024Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000280, Val Loss: 0.000331\n",
      "    Epoch 5/20, Train Loss: 0.000279, Val Loss: 0.000331\n",
      "Early stopping at epoch 9, Train Loss: 0.000278, Val Loss: 0.000331\n",
      "    Final - Train Loss: 0.000278, Val Loss: 0.000331\n",
      "[Saved] NN3_w252_2024Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000268, Val Loss: 0.000331\n",
      "    Epoch 5/20, Train Loss: 0.000267, Val Loss: 0.000332\n",
      "Early stopping at epoch 6, Train Loss: 0.000268, Val Loss: 0.000331\n",
      "    Final - Train Loss: 0.000268, Val Loss: 0.000331\n",
      "[Saved] NN4_w252_2024Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000260, Val Loss: 0.000332\n",
      "    Epoch 5/20, Train Loss: 0.000259, Val Loss: 0.000334\n",
      "Early stopping at epoch 6, Train Loss: 0.000258, Val Loss: 0.000334\n",
      "    Final - Train Loss: 0.000258, Val Loss: 0.000334\n",
      "[Saved] NN5_w252_2024Q2.pth\n",
      "    Training data size: 286002 samples\n",
      "  Quarter 2024Q3: Training models with expanding data\n",
      "    Added 3112 samples from 2024Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000303, Val Loss: 0.000341\n",
      "    Epoch 5/20, Train Loss: 0.000301, Val Loss: 0.000350\n",
      "Early stopping at epoch 6, Train Loss: 0.000301, Val Loss: 0.000348\n",
      "    Final - Train Loss: 0.000301, Val Loss: 0.000348\n",
      "[Saved] NN1_w252_2024Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000293, Val Loss: 0.000327\n",
      "    Epoch 5/20, Train Loss: 0.000292, Val Loss: 0.000327\n",
      "Early stopping at epoch 7, Train Loss: 0.000291, Val Loss: 0.000327\n",
      "    Final - Train Loss: 0.000291, Val Loss: 0.000327\n",
      "[Saved] NN2_w252_2024Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000279, Val Loss: 0.000326\n",
      "    Epoch 5/20, Train Loss: 0.000280, Val Loss: 0.000326\n",
      "Early stopping at epoch 7, Train Loss: 0.000279, Val Loss: 0.000326\n",
      "    Final - Train Loss: 0.000279, Val Loss: 0.000326\n",
      "[Saved] NN3_w252_2024Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000269, Val Loss: 0.000327\n",
      "    Epoch 5/20, Train Loss: 0.000267, Val Loss: 0.000328\n",
      "Early stopping at epoch 7, Train Loss: 0.000266, Val Loss: 0.000327\n",
      "    Final - Train Loss: 0.000266, Val Loss: 0.000327\n",
      "[Saved] NN4_w252_2024Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000260, Val Loss: 0.000326\n",
      "    Epoch 5/20, Train Loss: 0.000257, Val Loss: 0.000328\n",
      "Early stopping at epoch 6, Train Loss: 0.000257, Val Loss: 0.000328\n",
      "    Final - Train Loss: 0.000257, Val Loss: 0.000328\n",
      "[Saved] NN5_w252_2024Q3.pth\n",
      "    Training data size: 289114 samples\n",
      "Processing window size: 512\n",
      "  Quarter 2015Q4: Training models with expanding data\n",
      "    [Re-tuning] NN1 for 2015Q4\n",
      "[Hyper] NN1: best_MSE=0.000289, params={'learning_rate': 0.00020268689002984831, 'batch_size': 128, 'dropout_rate': 0.3616572716810112}\n",
      "    [Structure Change] dropout_rate changed: 0.1 -> 0.3616572716810112\n",
      "    Rebuilding NN1 model due to structure change\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 0.436126, Val Loss: 0.035954\n",
      "    Epoch 5/50, Train Loss: 0.000675, Val Loss: 0.000177\n",
      "    Epoch 10/50, Train Loss: 0.000361, Val Loss: 0.000166\n",
      "    Epoch 15/50, Train Loss: 0.000361, Val Loss: 0.000166\n",
      "    Epoch 20/50, Train Loss: 0.000355, Val Loss: 0.000167\n",
      "Early stopping at epoch 21, Train Loss: 0.000352, Val Loss: 0.000167\n",
      "    Final - Train Loss: 0.000352, Val Loss: 0.000167\n",
      "[Saved] NN1_w512_2015Q4.pth\n",
      "    Creating new NN2 model\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 0.483813, Val Loss: 0.020023\n",
      "    Epoch 5/50, Train Loss: 0.001038, Val Loss: 0.000214\n",
      "    Epoch 10/50, Train Loss: 0.000361, Val Loss: 0.000166\n",
      "Early stopping at epoch 14, Train Loss: 0.000361, Val Loss: 0.000166\n",
      "    Final - Train Loss: 0.000361, Val Loss: 0.000166\n",
      "[Saved] NN2_w512_2015Q4.pth\n",
      "    Creating new NN3 model\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 0.505728, Val Loss: 0.012234\n",
      "    Epoch 5/50, Train Loss: 0.001776, Val Loss: 0.000307\n",
      "    Epoch 10/50, Train Loss: 0.000360, Val Loss: 0.000166\n",
      "    Epoch 15/50, Train Loss: 0.000360, Val Loss: 0.000166\n",
      "    Epoch 20/50, Train Loss: 0.000356, Val Loss: 0.000167\n",
      "Early stopping at epoch 21, Train Loss: 0.000356, Val Loss: 0.000167\n",
      "    Final - Train Loss: 0.000356, Val Loss: 0.000167\n",
      "[Saved] NN3_w512_2015Q4.pth\n",
      "    Creating new NN4 model\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 0.381762, Val Loss: 0.005139\n",
      "    Epoch 5/50, Train Loss: 0.001846, Val Loss: 0.000225\n",
      "    Epoch 10/50, Train Loss: 0.000365, Val Loss: 0.000167\n",
      "    Epoch 15/50, Train Loss: 0.000359, Val Loss: 0.000166\n",
      "Early stopping at epoch 17, Train Loss: 0.000359, Val Loss: 0.000167\n",
      "    Final - Train Loss: 0.000359, Val Loss: 0.000167\n",
      "[Saved] NN4_w512_2015Q4.pth\n",
      "    Creating new NN5 model\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 0.516015, Val Loss: 0.002248\n",
      "    Epoch 5/50, Train Loss: 0.003146, Val Loss: 0.000265\n",
      "    Epoch 10/50, Train Loss: 0.000376, Val Loss: 0.000166\n",
      "    Epoch 15/50, Train Loss: 0.000358, Val Loss: 0.000167\n",
      "Early stopping at epoch 15, Train Loss: 0.000358, Val Loss: 0.000167\n",
      "    Final - Train Loss: 0.000358, Val Loss: 0.000167\n",
      "[Saved] NN5_w512_2015Q4.pth\n",
      "    Training data size: 171570 samples\n",
      "  Quarter 2016Q1: Training models with expanding data\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000359, Val Loss: 0.000166\n",
      "    Epoch 5/20, Train Loss: 0.000351, Val Loss: 0.000167\n",
      "Early stopping at epoch 6, Train Loss: 0.000349, Val Loss: 0.000167\n",
      "    Final - Train Loss: 0.000349, Val Loss: 0.000167\n",
      "[Saved] NN1_w512_2016Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000361, Val Loss: 0.000166\n",
      "    Epoch 5/20, Train Loss: 0.000355, Val Loss: 0.000167\n",
      "Early stopping at epoch 6, Train Loss: 0.000354, Val Loss: 0.000167\n",
      "    Final - Train Loss: 0.000354, Val Loss: 0.000167\n",
      "[Saved] NN2_w512_2016Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000359, Val Loss: 0.000167\n",
      "    Epoch 5/20, Train Loss: 0.000355, Val Loss: 0.000168\n",
      "Early stopping at epoch 7, Train Loss: 0.000353, Val Loss: 0.000168\n",
      "    Final - Train Loss: 0.000353, Val Loss: 0.000168\n",
      "[Saved] NN3_w512_2016Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000359, Val Loss: 0.000167\n",
      "    Epoch 5/20, Train Loss: 0.000355, Val Loss: 0.000168\n",
      "Early stopping at epoch 6, Train Loss: 0.000353, Val Loss: 0.000168\n",
      "    Final - Train Loss: 0.000353, Val Loss: 0.000168\n",
      "[Saved] NN4_w512_2016Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000359, Val Loss: 0.000168\n",
      "    Epoch 5/20, Train Loss: 0.000355, Val Loss: 0.000167\n",
      "    Epoch 10/20, Train Loss: 0.000341, Val Loss: 0.000172\n",
      "Early stopping at epoch 11, Train Loss: 0.000338, Val Loss: 0.000170\n",
      "    Final - Train Loss: 0.000338, Val Loss: 0.000170\n",
      "[Saved] NN5_w512_2016Q1.pth\n",
      "    Training data size: 171570 samples\n",
      "  Quarter 2016Q2: Training models with expanding data\n",
      "    Added 2956 samples from 2016Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000355, Val Loss: 0.000173\n",
      "    Epoch 5/20, Train Loss: 0.000346, Val Loss: 0.000173\n",
      "Early stopping at epoch 8, Train Loss: 0.000341, Val Loss: 0.000173\n",
      "    Final - Train Loss: 0.000341, Val Loss: 0.000173\n",
      "[Saved] NN1_w512_2016Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000357, Val Loss: 0.000172\n",
      "    Epoch 5/20, Train Loss: 0.000350, Val Loss: 0.000172\n",
      "Early stopping at epoch 6, Train Loss: 0.000349, Val Loss: 0.000172\n",
      "    Final - Train Loss: 0.000349, Val Loss: 0.000172\n",
      "[Saved] NN2_w512_2016Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000355, Val Loss: 0.000172\n",
      "    Epoch 5/20, Train Loss: 0.000350, Val Loss: 0.000173\n",
      "Early stopping at epoch 6, Train Loss: 0.000348, Val Loss: 0.000173\n",
      "    Final - Train Loss: 0.000348, Val Loss: 0.000173\n",
      "[Saved] NN3_w512_2016Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000356, Val Loss: 0.000172\n",
      "    Epoch 5/20, Train Loss: 0.000349, Val Loss: 0.000174\n",
      "Early stopping at epoch 6, Train Loss: 0.000347, Val Loss: 0.000174\n",
      "    Final - Train Loss: 0.000347, Val Loss: 0.000174\n",
      "[Saved] NN4_w512_2016Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000346, Val Loss: 0.000173\n",
      "    Epoch 5/20, Train Loss: 0.000335, Val Loss: 0.000176\n",
      "Early stopping at epoch 6, Train Loss: 0.000334, Val Loss: 0.000175\n",
      "    Final - Train Loss: 0.000334, Val Loss: 0.000175\n",
      "[Saved] NN5_w512_2016Q2.pth\n",
      "    Training data size: 174526 samples\n",
      "  Quarter 2016Q3: Training models with expanding data\n",
      "    Added 3170 samples from 2016Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000345, Val Loss: 0.000176\n",
      "    Epoch 5/20, Train Loss: 0.000338, Val Loss: 0.000176\n",
      "Early stopping at epoch 8, Train Loss: 0.000334, Val Loss: 0.000176\n",
      "    Final - Train Loss: 0.000334, Val Loss: 0.000176\n",
      "[Saved] NN1_w512_2016Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000352, Val Loss: 0.000176\n",
      "    Epoch 5/20, Train Loss: 0.000346, Val Loss: 0.000176\n",
      "Early stopping at epoch 7, Train Loss: 0.000343, Val Loss: 0.000176\n",
      "    Final - Train Loss: 0.000343, Val Loss: 0.000176\n",
      "[Saved] NN2_w512_2016Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000350, Val Loss: 0.000176\n",
      "    Epoch 5/20, Train Loss: 0.000345, Val Loss: 0.000176\n",
      "Early stopping at epoch 7, Train Loss: 0.000342, Val Loss: 0.000176\n",
      "    Final - Train Loss: 0.000342, Val Loss: 0.000176\n",
      "[Saved] NN3_w512_2016Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000351, Val Loss: 0.000175\n",
      "    Epoch 5/20, Train Loss: 0.000343, Val Loss: 0.000176\n",
      "Early stopping at epoch 6, Train Loss: 0.000341, Val Loss: 0.000177\n",
      "    Final - Train Loss: 0.000341, Val Loss: 0.000177\n",
      "[Saved] NN4_w512_2016Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000340, Val Loss: 0.000178\n",
      "    Epoch 5/20, Train Loss: 0.000330, Val Loss: 0.000177\n",
      "    Epoch 10/20, Train Loss: 0.000320, Val Loss: 0.000178\n",
      "Early stopping at epoch 10, Train Loss: 0.000320, Val Loss: 0.000178\n",
      "    Final - Train Loss: 0.000320, Val Loss: 0.000178\n",
      "[Saved] NN5_w512_2016Q3.pth\n",
      "    Training data size: 177696 samples\n",
      "  Quarter 2016Q4: Training models with expanding data\n",
      "    Added 3176 samples from 2016Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000336, Val Loss: 0.000174\n",
      "    Epoch 5/20, Train Loss: 0.000331, Val Loss: 0.000174\n",
      "Early stopping at epoch 8, Train Loss: 0.000328, Val Loss: 0.000174\n",
      "    Final - Train Loss: 0.000328, Val Loss: 0.000174\n",
      "[Saved] NN1_w512_2016Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000346, Val Loss: 0.000173\n",
      "    Epoch 5/20, Train Loss: 0.000340, Val Loss: 0.000173\n",
      "Early stopping at epoch 6, Train Loss: 0.000339, Val Loss: 0.000173\n",
      "    Final - Train Loss: 0.000339, Val Loss: 0.000173\n",
      "[Saved] NN2_w512_2016Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000345, Val Loss: 0.000174\n",
      "    Epoch 5/20, Train Loss: 0.000340, Val Loss: 0.000174\n",
      "Early stopping at epoch 6, Train Loss: 0.000338, Val Loss: 0.000174\n",
      "    Final - Train Loss: 0.000338, Val Loss: 0.000174\n",
      "[Saved] NN3_w512_2016Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000346, Val Loss: 0.000173\n",
      "    Epoch 5/20, Train Loss: 0.000338, Val Loss: 0.000175\n",
      "Early stopping at epoch 6, Train Loss: 0.000335, Val Loss: 0.000176\n",
      "    Final - Train Loss: 0.000335, Val Loss: 0.000176\n",
      "[Saved] NN4_w512_2016Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000325, Val Loss: 0.000178\n",
      "    Epoch 5/20, Train Loss: 0.000317, Val Loss: 0.000177\n",
      "Early stopping at epoch 8, Train Loss: 0.000312, Val Loss: 0.000178\n",
      "    Final - Train Loss: 0.000312, Val Loss: 0.000178\n",
      "[Saved] NN5_w512_2016Q4.pth\n",
      "    Training data size: 180872 samples\n",
      "  Quarter 2017Q1: Training models with expanding data\n",
      "    Added 3123 samples from 2016Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000329, Val Loss: 0.000176\n",
      "    Epoch 5/20, Train Loss: 0.000324, Val Loss: 0.000177\n",
      "Early stopping at epoch 9, Train Loss: 0.000322, Val Loss: 0.000177\n",
      "    Final - Train Loss: 0.000322, Val Loss: 0.000177\n",
      "[Saved] NN1_w512_2017Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000341, Val Loss: 0.000176\n",
      "    Epoch 5/20, Train Loss: 0.000336, Val Loss: 0.000176\n",
      "Early stopping at epoch 6, Train Loss: 0.000335, Val Loss: 0.000176\n",
      "    Final - Train Loss: 0.000335, Val Loss: 0.000176\n",
      "[Saved] NN2_w512_2017Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000340, Val Loss: 0.000176\n",
      "    Epoch 5/20, Train Loss: 0.000334, Val Loss: 0.000177\n",
      "Early stopping at epoch 6, Train Loss: 0.000334, Val Loss: 0.000177\n",
      "    Final - Train Loss: 0.000334, Val Loss: 0.000177\n",
      "[Saved] NN3_w512_2017Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000341, Val Loss: 0.000176\n",
      "    Epoch 5/20, Train Loss: 0.000332, Val Loss: 0.000177\n",
      "Early stopping at epoch 6, Train Loss: 0.000330, Val Loss: 0.000178\n",
      "    Final - Train Loss: 0.000330, Val Loss: 0.000178\n",
      "[Saved] NN4_w512_2017Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000316, Val Loss: 0.000179\n",
      "    Epoch 5/20, Train Loss: 0.000309, Val Loss: 0.000177\n",
      "Early stopping at epoch 9, Train Loss: 0.000302, Val Loss: 0.000177\n",
      "    Final - Train Loss: 0.000302, Val Loss: 0.000177\n",
      "[Saved] NN5_w512_2017Q1.pth\n",
      "    Training data size: 183995 samples\n",
      "  Quarter 2017Q2: Training models with expanding data\n",
      "    Added 3083 samples from 2017Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000322, Val Loss: 0.000172\n",
      "    Epoch 5/20, Train Loss: 0.000320, Val Loss: 0.000172\n",
      "Early stopping at epoch 7, Train Loss: 0.000318, Val Loss: 0.000173\n",
      "    Final - Train Loss: 0.000318, Val Loss: 0.000173\n",
      "[Saved] NN1_w512_2017Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000336, Val Loss: 0.000171\n",
      "    Epoch 5/20, Train Loss: 0.000332, Val Loss: 0.000172\n",
      "Early stopping at epoch 6, Train Loss: 0.000331, Val Loss: 0.000172\n",
      "    Final - Train Loss: 0.000331, Val Loss: 0.000172\n",
      "[Saved] NN2_w512_2017Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000335, Val Loss: 0.000172\n",
      "    Epoch 5/20, Train Loss: 0.000331, Val Loss: 0.000172\n",
      "Early stopping at epoch 8, Train Loss: 0.000328, Val Loss: 0.000172\n",
      "    Final - Train Loss: 0.000328, Val Loss: 0.000172\n",
      "[Saved] NN3_w512_2017Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000337, Val Loss: 0.000171\n",
      "    Epoch 5/20, Train Loss: 0.000328, Val Loss: 0.000172\n",
      "Early stopping at epoch 6, Train Loss: 0.000326, Val Loss: 0.000172\n",
      "    Final - Train Loss: 0.000326, Val Loss: 0.000172\n",
      "[Saved] NN4_w512_2017Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000306, Val Loss: 0.000173\n",
      "    Epoch 5/20, Train Loss: 0.000300, Val Loss: 0.000174\n",
      "Early stopping at epoch 9, Train Loss: 0.000291, Val Loss: 0.000173\n",
      "    Final - Train Loss: 0.000291, Val Loss: 0.000173\n",
      "[Saved] NN5_w512_2017Q2.pth\n",
      "    Training data size: 187078 samples\n",
      "  Quarter 2017Q3: Training models with expanding data\n",
      "    Added 3122 samples from 2017Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000318, Val Loss: 0.000172\n",
      "    Epoch 5/20, Train Loss: 0.000316, Val Loss: 0.000173\n",
      "Early stopping at epoch 7, Train Loss: 0.000314, Val Loss: 0.000173\n",
      "    Final - Train Loss: 0.000314, Val Loss: 0.000173\n",
      "[Saved] NN1_w512_2017Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000332, Val Loss: 0.000171\n",
      "    Epoch 5/20, Train Loss: 0.000328, Val Loss: 0.000171\n",
      "    Epoch 10/20, Train Loss: 0.000323, Val Loss: 0.000171\n",
      "Early stopping at epoch 10, Train Loss: 0.000323, Val Loss: 0.000171\n",
      "    Final - Train Loss: 0.000323, Val Loss: 0.000171\n",
      "[Saved] NN2_w512_2017Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000329, Val Loss: 0.000172\n",
      "    Epoch 5/20, Train Loss: 0.000325, Val Loss: 0.000172\n",
      "Early stopping at epoch 8, Train Loss: 0.000323, Val Loss: 0.000172\n",
      "    Final - Train Loss: 0.000323, Val Loss: 0.000172\n",
      "[Saved] NN3_w512_2017Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000332, Val Loss: 0.000171\n",
      "    Epoch 5/20, Train Loss: 0.000323, Val Loss: 0.000172\n",
      "Early stopping at epoch 6, Train Loss: 0.000321, Val Loss: 0.000172\n",
      "    Final - Train Loss: 0.000321, Val Loss: 0.000172\n",
      "[Saved] NN4_w512_2017Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000296, Val Loss: 0.000172\n",
      "    Epoch 5/20, Train Loss: 0.000289, Val Loss: 0.000173\n",
      "Early stopping at epoch 6, Train Loss: 0.000286, Val Loss: 0.000172\n",
      "    Final - Train Loss: 0.000286, Val Loss: 0.000172\n",
      "[Saved] NN5_w512_2017Q3.pth\n",
      "    Training data size: 190200 samples\n",
      "  Quarter 2017Q4: Training models with expanding data\n",
      "    Added 3114 samples from 2017Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000313, Val Loss: 0.000175\n",
      "    Epoch 5/20, Train Loss: 0.000311, Val Loss: 0.000175\n",
      "Early stopping at epoch 7, Train Loss: 0.000310, Val Loss: 0.000175\n",
      "    Final - Train Loss: 0.000310, Val Loss: 0.000175\n",
      "[Saved] NN1_w512_2017Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000324, Val Loss: 0.000174\n",
      "    Epoch 5/20, Train Loss: 0.000320, Val Loss: 0.000174\n",
      "    Epoch 10/20, Train Loss: 0.000317, Val Loss: 0.000174\n",
      "Early stopping at epoch 11, Train Loss: 0.000316, Val Loss: 0.000174\n",
      "    Final - Train Loss: 0.000316, Val Loss: 0.000174\n",
      "[Saved] NN2_w512_2017Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000323, Val Loss: 0.000174\n",
      "    Epoch 5/20, Train Loss: 0.000319, Val Loss: 0.000175\n",
      "Early stopping at epoch 7, Train Loss: 0.000316, Val Loss: 0.000175\n",
      "    Final - Train Loss: 0.000316, Val Loss: 0.000175\n",
      "[Saved] NN3_w512_2017Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000325, Val Loss: 0.000175\n",
      "    Epoch 5/20, Train Loss: 0.000319, Val Loss: 0.000175\n",
      "Early stopping at epoch 8, Train Loss: 0.000314, Val Loss: 0.000175\n",
      "    Final - Train Loss: 0.000314, Val Loss: 0.000175\n",
      "[Saved] NN4_w512_2017Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000292, Val Loss: 0.000175\n",
      "    Epoch 5/20, Train Loss: 0.000284, Val Loss: 0.000176\n",
      "Early stopping at epoch 8, Train Loss: 0.000279, Val Loss: 0.000178\n",
      "    Final - Train Loss: 0.000279, Val Loss: 0.000178\n",
      "[Saved] NN5_w512_2017Q4.pth\n",
      "    Training data size: 193314 samples\n",
      "  Quarter 2018Q1: Training models with expanding data\n",
      "    Added 3115 samples from 2017Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000310, Val Loss: 0.000176\n",
      "    Epoch 5/20, Train Loss: 0.000308, Val Loss: 0.000176\n",
      "Early stopping at epoch 6, Train Loss: 0.000308, Val Loss: 0.000176\n",
      "    Final - Train Loss: 0.000308, Val Loss: 0.000176\n",
      "[Saved] NN1_w512_2018Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000316, Val Loss: 0.000175\n",
      "    Epoch 5/20, Train Loss: 0.000314, Val Loss: 0.000175\n",
      "Early stopping at epoch 9, Train Loss: 0.000312, Val Loss: 0.000175\n",
      "    Final - Train Loss: 0.000312, Val Loss: 0.000175\n",
      "[Saved] NN2_w512_2018Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000318, Val Loss: 0.000175\n",
      "    Epoch 5/20, Train Loss: 0.000315, Val Loss: 0.000176\n",
      "Early stopping at epoch 6, Train Loss: 0.000313, Val Loss: 0.000176\n",
      "    Final - Train Loss: 0.000313, Val Loss: 0.000176\n",
      "[Saved] NN3_w512_2018Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000317, Val Loss: 0.000175\n",
      "    Epoch 5/20, Train Loss: 0.000312, Val Loss: 0.000176\n",
      "Early stopping at epoch 6, Train Loss: 0.000310, Val Loss: 0.000176\n",
      "    Final - Train Loss: 0.000310, Val Loss: 0.000176\n",
      "[Saved] NN4_w512_2018Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000284, Val Loss: 0.000176\n",
      "    Epoch 5/20, Train Loss: 0.000277, Val Loss: 0.000176\n",
      "Early stopping at epoch 6, Train Loss: 0.000275, Val Loss: 0.000176\n",
      "    Final - Train Loss: 0.000275, Val Loss: 0.000176\n",
      "[Saved] NN5_w512_2018Q1.pth\n",
      "    Training data size: 196429 samples\n",
      "  Quarter 2018Q2: Training models with expanding data\n",
      "    Added 2996 samples from 2018Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000308, Val Loss: 0.000186\n",
      "    Epoch 5/20, Train Loss: 0.000305, Val Loss: 0.000186\n",
      "    Epoch 10/20, Train Loss: 0.000304, Val Loss: 0.000187\n",
      "Early stopping at epoch 10, Train Loss: 0.000304, Val Loss: 0.000187\n",
      "    Final - Train Loss: 0.000304, Val Loss: 0.000187\n",
      "[Saved] NN1_w512_2018Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000312, Val Loss: 0.000185\n",
      "    Epoch 5/20, Train Loss: 0.000310, Val Loss: 0.000185\n",
      "Early stopping at epoch 8, Train Loss: 0.000308, Val Loss: 0.000185\n",
      "    Final - Train Loss: 0.000308, Val Loss: 0.000185\n",
      "[Saved] NN2_w512_2018Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000314, Val Loss: 0.000185\n",
      "    Epoch 5/20, Train Loss: 0.000311, Val Loss: 0.000185\n",
      "Early stopping at epoch 6, Train Loss: 0.000310, Val Loss: 0.000186\n",
      "    Final - Train Loss: 0.000310, Val Loss: 0.000186\n",
      "[Saved] NN3_w512_2018Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000313, Val Loss: 0.000185\n",
      "    Epoch 5/20, Train Loss: 0.000308, Val Loss: 0.000185\n",
      "Early stopping at epoch 6, Train Loss: 0.000306, Val Loss: 0.000186\n",
      "    Final - Train Loss: 0.000306, Val Loss: 0.000186\n",
      "[Saved] NN4_w512_2018Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000280, Val Loss: 0.000185\n",
      "    Epoch 5/20, Train Loss: 0.000273, Val Loss: 0.000186\n",
      "Early stopping at epoch 6, Train Loss: 0.000271, Val Loss: 0.000186\n",
      "    Final - Train Loss: 0.000271, Val Loss: 0.000186\n",
      "[Saved] NN5_w512_2018Q2.pth\n",
      "    Training data size: 199425 samples\n",
      "  Quarter 2018Q3: Training models with expanding data\n",
      "    Added 3160 samples from 2018Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000303, Val Loss: 0.000188\n",
      "    Epoch 5/20, Train Loss: 0.000301, Val Loss: 0.000188\n",
      "Early stopping at epoch 6, Train Loss: 0.000300, Val Loss: 0.000188\n",
      "    Final - Train Loss: 0.000300, Val Loss: 0.000188\n",
      "[Saved] NN1_w512_2018Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000308, Val Loss: 0.000187\n",
      "    Epoch 5/20, Train Loss: 0.000306, Val Loss: 0.000187\n",
      "Early stopping at epoch 6, Train Loss: 0.000306, Val Loss: 0.000187\n",
      "    Final - Train Loss: 0.000306, Val Loss: 0.000187\n",
      "[Saved] NN2_w512_2018Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000311, Val Loss: 0.000188\n",
      "    Epoch 5/20, Train Loss: 0.000307, Val Loss: 0.000188\n",
      "Early stopping at epoch 7, Train Loss: 0.000305, Val Loss: 0.000188\n",
      "    Final - Train Loss: 0.000305, Val Loss: 0.000188\n",
      "[Saved] NN3_w512_2018Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000309, Val Loss: 0.000188\n",
      "    Epoch 5/20, Train Loss: 0.000304, Val Loss: 0.000188\n",
      "    Epoch 10/20, Train Loss: 0.000297, Val Loss: 0.000188\n",
      "Early stopping at epoch 12, Train Loss: 0.000295, Val Loss: 0.000188\n",
      "    Final - Train Loss: 0.000295, Val Loss: 0.000188\n",
      "[Saved] NN4_w512_2018Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000277, Val Loss: 0.000188\n",
      "    Epoch 5/20, Train Loss: 0.000271, Val Loss: 0.000188\n",
      "Early stopping at epoch 7, Train Loss: 0.000267, Val Loss: 0.000189\n",
      "    Final - Train Loss: 0.000267, Val Loss: 0.000189\n",
      "[Saved] NN5_w512_2018Q3.pth\n",
      "    Training data size: 202585 samples\n",
      "  Quarter 2018Q4: Training models with expanding data\n",
      "    Added 3125 samples from 2018Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000300, Val Loss: 0.000188\n",
      "    Epoch 5/20, Train Loss: 0.000298, Val Loss: 0.000188\n",
      "Early stopping at epoch 6, Train Loss: 0.000297, Val Loss: 0.000188\n",
      "    Final - Train Loss: 0.000297, Val Loss: 0.000188\n",
      "[Saved] NN1_w512_2018Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000305, Val Loss: 0.000187\n",
      "    Epoch 5/20, Train Loss: 0.000303, Val Loss: 0.000186\n",
      "    Epoch 10/20, Train Loss: 0.000301, Val Loss: 0.000186\n",
      "Early stopping at epoch 14, Train Loss: 0.000299, Val Loss: 0.000187\n",
      "    Final - Train Loss: 0.000299, Val Loss: 0.000187\n",
      "[Saved] NN2_w512_2018Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000306, Val Loss: 0.000187\n",
      "    Epoch 5/20, Train Loss: 0.000302, Val Loss: 0.000187\n",
      "Early stopping at epoch 9, Train Loss: 0.000299, Val Loss: 0.000187\n",
      "    Final - Train Loss: 0.000299, Val Loss: 0.000187\n",
      "[Saved] NN3_w512_2018Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000298, Val Loss: 0.000187\n",
      "    Epoch 5/20, Train Loss: 0.000292, Val Loss: 0.000188\n",
      "Early stopping at epoch 9, Train Loss: 0.000288, Val Loss: 0.000188\n",
      "    Final - Train Loss: 0.000288, Val Loss: 0.000188\n",
      "[Saved] NN4_w512_2018Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000271, Val Loss: 0.000187\n",
      "    Epoch 5/20, Train Loss: 0.000264, Val Loss: 0.000188\n",
      "Early stopping at epoch 6, Train Loss: 0.000263, Val Loss: 0.000188\n",
      "    Final - Train Loss: 0.000263, Val Loss: 0.000188\n",
      "[Saved] NN5_w512_2018Q4.pth\n",
      "    Training data size: 205710 samples\n",
      "  Quarter 2019Q1: Training models with expanding data\n",
      "    Added 3045 samples from 2018Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000297, Val Loss: 0.000200\n",
      "    Epoch 5/20, Train Loss: 0.000296, Val Loss: 0.000200\n",
      "Early stopping at epoch 6, Train Loss: 0.000295, Val Loss: 0.000200\n",
      "    Final - Train Loss: 0.000295, Val Loss: 0.000200\n",
      "[Saved] NN1_w512_2019Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000299, Val Loss: 0.000199\n",
      "    Epoch 5/20, Train Loss: 0.000298, Val Loss: 0.000199\n",
      "Early stopping at epoch 6, Train Loss: 0.000296, Val Loss: 0.000199\n",
      "    Final - Train Loss: 0.000296, Val Loss: 0.000199\n",
      "[Saved] NN2_w512_2019Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000300, Val Loss: 0.000199\n",
      "    Epoch 5/20, Train Loss: 0.000297, Val Loss: 0.000199\n",
      "Early stopping at epoch 7, Train Loss: 0.000295, Val Loss: 0.000200\n",
      "    Final - Train Loss: 0.000295, Val Loss: 0.000200\n",
      "[Saved] NN3_w512_2019Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000290, Val Loss: 0.000200\n",
      "    Epoch 5/20, Train Loss: 0.000286, Val Loss: 0.000200\n",
      "Early stopping at epoch 6, Train Loss: 0.000285, Val Loss: 0.000200\n",
      "    Final - Train Loss: 0.000285, Val Loss: 0.000200\n",
      "[Saved] NN4_w512_2019Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000267, Val Loss: 0.000200\n",
      "    Epoch 5/20, Train Loss: 0.000261, Val Loss: 0.000201\n",
      "Early stopping at epoch 6, Train Loss: 0.000259, Val Loss: 0.000201\n",
      "    Final - Train Loss: 0.000259, Val Loss: 0.000201\n",
      "[Saved] NN5_w512_2019Q1.pth\n",
      "    Training data size: 208755 samples\n",
      "  Quarter 2019Q2: Training models with expanding data\n",
      "    Added 3022 samples from 2019Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000297, Val Loss: 0.000195\n",
      "    Epoch 5/20, Train Loss: 0.000295, Val Loss: 0.000195\n",
      "Early stopping at epoch 8, Train Loss: 0.000295, Val Loss: 0.000196\n",
      "    Final - Train Loss: 0.000295, Val Loss: 0.000196\n",
      "[Saved] NN1_w512_2019Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000298, Val Loss: 0.000194\n",
      "    Epoch 5/20, Train Loss: 0.000296, Val Loss: 0.000194\n",
      "    Epoch 10/20, Train Loss: 0.000295, Val Loss: 0.000194\n",
      "Early stopping at epoch 14, Train Loss: 0.000292, Val Loss: 0.000194\n",
      "    Final - Train Loss: 0.000292, Val Loss: 0.000194\n",
      "[Saved] NN2_w512_2019Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000299, Val Loss: 0.000194\n",
      "    Epoch 5/20, Train Loss: 0.000295, Val Loss: 0.000194\n",
      "Early stopping at epoch 8, Train Loss: 0.000292, Val Loss: 0.000195\n",
      "    Final - Train Loss: 0.000292, Val Loss: 0.000195\n",
      "[Saved] NN3_w512_2019Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000289, Val Loss: 0.000194\n",
      "    Epoch 5/20, Train Loss: 0.000284, Val Loss: 0.000195\n",
      "Early stopping at epoch 6, Train Loss: 0.000284, Val Loss: 0.000195\n",
      "    Final - Train Loss: 0.000284, Val Loss: 0.000195\n",
      "[Saved] NN4_w512_2019Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000266, Val Loss: 0.000195\n",
      "    Epoch 5/20, Train Loss: 0.000260, Val Loss: 0.000196\n",
      "Early stopping at epoch 6, Train Loss: 0.000259, Val Loss: 0.000196\n",
      "    Final - Train Loss: 0.000259, Val Loss: 0.000196\n",
      "[Saved] NN5_w512_2019Q2.pth\n",
      "    Training data size: 211777 samples\n",
      "  Quarter 2019Q3: Training models with expanding data\n",
      "    Added 3120 samples from 2019Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000295, Val Loss: 0.000192\n",
      "    Epoch 5/20, Train Loss: 0.000294, Val Loss: 0.000192\n",
      "Early stopping at epoch 9, Train Loss: 0.000296, Val Loss: 0.000194\n",
      "    Final - Train Loss: 0.000296, Val Loss: 0.000194\n",
      "[Saved] NN1_w512_2019Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000295, Val Loss: 0.000191\n",
      "    Epoch 5/20, Train Loss: 0.000292, Val Loss: 0.000191\n",
      "Early stopping at epoch 6, Train Loss: 0.000291, Val Loss: 0.000191\n",
      "    Final - Train Loss: 0.000291, Val Loss: 0.000191\n",
      "[Saved] NN2_w512_2019Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000295, Val Loss: 0.000191\n",
      "    Epoch 5/20, Train Loss: 0.000292, Val Loss: 0.000191\n",
      "Early stopping at epoch 6, Train Loss: 0.000290, Val Loss: 0.000191\n",
      "    Final - Train Loss: 0.000290, Val Loss: 0.000191\n",
      "[Saved] NN3_w512_2019Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000287, Val Loss: 0.000191\n",
      "    Epoch 5/20, Train Loss: 0.000283, Val Loss: 0.000192\n",
      "Early stopping at epoch 6, Train Loss: 0.000281, Val Loss: 0.000192\n",
      "    Final - Train Loss: 0.000281, Val Loss: 0.000192\n",
      "[Saved] NN4_w512_2019Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000263, Val Loss: 0.000191\n",
      "    Epoch 5/20, Train Loss: 0.000257, Val Loss: 0.000192\n",
      "Early stopping at epoch 6, Train Loss: 0.000255, Val Loss: 0.000193\n",
      "    Final - Train Loss: 0.000255, Val Loss: 0.000193\n",
      "[Saved] NN5_w512_2019Q3.pth\n",
      "    Training data size: 214897 samples\n",
      "  Quarter 2019Q4: Training models with expanding data\n",
      "    Added 3167 samples from 2019Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000294, Val Loss: 0.000193\n",
      "    Epoch 5/20, Train Loss: 0.000292, Val Loss: 0.000194\n",
      "Early stopping at epoch 7, Train Loss: 0.000292, Val Loss: 0.000194\n",
      "    Final - Train Loss: 0.000292, Val Loss: 0.000194\n",
      "[Saved] NN1_w512_2019Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000293, Val Loss: 0.000192\n",
      "    Epoch 5/20, Train Loss: 0.000291, Val Loss: 0.000192\n",
      "Early stopping at epoch 6, Train Loss: 0.000292, Val Loss: 0.000192\n",
      "    Final - Train Loss: 0.000292, Val Loss: 0.000192\n",
      "[Saved] NN2_w512_2019Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000293, Val Loss: 0.000193\n",
      "    Epoch 5/20, Train Loss: 0.000290, Val Loss: 0.000193\n",
      "Early stopping at epoch 6, Train Loss: 0.000289, Val Loss: 0.000194\n",
      "    Final - Train Loss: 0.000289, Val Loss: 0.000194\n",
      "[Saved] NN3_w512_2019Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000285, Val Loss: 0.000193\n",
      "    Epoch 5/20, Train Loss: 0.000281, Val Loss: 0.000193\n",
      "Early stopping at epoch 6, Train Loss: 0.000280, Val Loss: 0.000193\n",
      "    Final - Train Loss: 0.000280, Val Loss: 0.000193\n",
      "[Saved] NN4_w512_2019Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000261, Val Loss: 0.000193\n",
      "    Epoch 5/20, Train Loss: 0.000256, Val Loss: 0.000195\n",
      "Early stopping at epoch 6, Train Loss: 0.000254, Val Loss: 0.000195\n",
      "    Final - Train Loss: 0.000254, Val Loss: 0.000195\n",
      "[Saved] NN5_w512_2019Q4.pth\n",
      "    Training data size: 218064 samples\n",
      "  Quarter 2020Q1: Training models with expanding data\n",
      "    Added 3179 samples from 2019Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000291, Val Loss: 0.000193\n",
      "    Epoch 5/20, Train Loss: 0.000290, Val Loss: 0.000193\n",
      "Early stopping at epoch 8, Train Loss: 0.000289, Val Loss: 0.000193\n",
      "    Final - Train Loss: 0.000289, Val Loss: 0.000193\n",
      "[Saved] NN1_w512_2020Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000291, Val Loss: 0.000192\n",
      "    Epoch 5/20, Train Loss: 0.000288, Val Loss: 0.000192\n",
      "Early stopping at epoch 7, Train Loss: 0.000288, Val Loss: 0.000192\n",
      "    Final - Train Loss: 0.000288, Val Loss: 0.000192\n",
      "[Saved] NN2_w512_2020Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000291, Val Loss: 0.000192\n",
      "    Epoch 5/20, Train Loss: 0.000287, Val Loss: 0.000192\n",
      "Early stopping at epoch 8, Train Loss: 0.000285, Val Loss: 0.000193\n",
      "    Final - Train Loss: 0.000285, Val Loss: 0.000193\n",
      "[Saved] NN3_w512_2020Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000282, Val Loss: 0.000192\n",
      "    Epoch 5/20, Train Loss: 0.000279, Val Loss: 0.000193\n",
      "Early stopping at epoch 6, Train Loss: 0.000277, Val Loss: 0.000193\n",
      "    Final - Train Loss: 0.000277, Val Loss: 0.000193\n",
      "[Saved] NN4_w512_2020Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000259, Val Loss: 0.000192\n",
      "    Epoch 5/20, Train Loss: 0.000253, Val Loss: 0.000193\n",
      "Early stopping at epoch 6, Train Loss: 0.000251, Val Loss: 0.000194\n",
      "    Final - Train Loss: 0.000251, Val Loss: 0.000194\n",
      "[Saved] NN5_w512_2020Q1.pth\n",
      "    Training data size: 221243 samples\n",
      "  Quarter 2020Q2: Training models with expanding data\n",
      "    Added 2595 samples from 2020Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000290, Val Loss: 0.000212\n",
      "    Epoch 5/20, Train Loss: 0.000288, Val Loss: 0.000213\n",
      "Early stopping at epoch 6, Train Loss: 0.000287, Val Loss: 0.000213\n",
      "    Final - Train Loss: 0.000287, Val Loss: 0.000213\n",
      "[Saved] NN1_w512_2020Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000288, Val Loss: 0.000211\n",
      "    Epoch 5/20, Train Loss: 0.000287, Val Loss: 0.000211\n",
      "Early stopping at epoch 6, Train Loss: 0.000287, Val Loss: 0.000211\n",
      "    Final - Train Loss: 0.000287, Val Loss: 0.000211\n",
      "[Saved] NN2_w512_2020Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000287, Val Loss: 0.000212\n",
      "    Epoch 5/20, Train Loss: 0.000284, Val Loss: 0.000212\n",
      "Early stopping at epoch 9, Train Loss: 0.000281, Val Loss: 0.000213\n",
      "    Final - Train Loss: 0.000281, Val Loss: 0.000213\n",
      "[Saved] NN3_w512_2020Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000280, Val Loss: 0.000211\n",
      "    Epoch 5/20, Train Loss: 0.000276, Val Loss: 0.000213\n",
      "Early stopping at epoch 6, Train Loss: 0.000275, Val Loss: 0.000213\n",
      "    Final - Train Loss: 0.000275, Val Loss: 0.000213\n",
      "[Saved] NN4_w512_2020Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000256, Val Loss: 0.000213\n",
      "    Epoch 5/20, Train Loss: 0.000251, Val Loss: 0.000214\n",
      "Early stopping at epoch 8, Train Loss: 0.000246, Val Loss: 0.000214\n",
      "    Final - Train Loss: 0.000246, Val Loss: 0.000214\n",
      "[Saved] NN5_w512_2020Q2.pth\n",
      "    Training data size: 223838 samples\n",
      "  Quarter 2020Q3: Training models with expanding data\n",
      "    Added 2839 samples from 2020Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000288, Val Loss: 0.000243\n",
      "    Epoch 5/20, Train Loss: 0.000285, Val Loss: 0.000243\n",
      "Early stopping at epoch 6, Train Loss: 0.000285, Val Loss: 0.000243\n",
      "    Final - Train Loss: 0.000285, Val Loss: 0.000243\n",
      "[Saved] NN1_w512_2020Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000286, Val Loss: 0.000242\n",
      "    Epoch 5/20, Train Loss: 0.000283, Val Loss: 0.000242\n",
      "Early stopping at epoch 8, Train Loss: 0.000283, Val Loss: 0.000242\n",
      "    Final - Train Loss: 0.000283, Val Loss: 0.000242\n",
      "[Saved] NN2_w512_2020Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000282, Val Loss: 0.000242\n",
      "    Epoch 5/20, Train Loss: 0.000278, Val Loss: 0.000243\n",
      "Early stopping at epoch 6, Train Loss: 0.000277, Val Loss: 0.000243\n",
      "    Final - Train Loss: 0.000277, Val Loss: 0.000243\n",
      "[Saved] NN3_w512_2020Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000277, Val Loss: 0.000242\n",
      "    Epoch 5/20, Train Loss: 0.000274, Val Loss: 0.000242\n",
      "Early stopping at epoch 6, Train Loss: 0.000272, Val Loss: 0.000242\n",
      "    Final - Train Loss: 0.000272, Val Loss: 0.000242\n",
      "[Saved] NN4_w512_2020Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000250, Val Loss: 0.000242\n",
      "    Epoch 5/20, Train Loss: 0.000244, Val Loss: 0.000243\n",
      "Early stopping at epoch 6, Train Loss: 0.000243, Val Loss: 0.000244\n",
      "    Final - Train Loss: 0.000243, Val Loss: 0.000244\n",
      "[Saved] NN5_w512_2020Q3.pth\n",
      "    Training data size: 226677 samples\n",
      "  Quarter 2020Q4: Training models with expanding data\n",
      "    Added 3151 samples from 2020Q3 to training set\n",
      "    [Re-tuning] NN1 for 2020Q4\n",
      "[Hyper] NN1: best_MSE=0.000277, params={'learning_rate': 0.0004548860533183753, 'batch_size': 64, 'dropout_rate': 0.31926471943819873}\n",
      "    [Structure Change] dropout_rate changed: 0.3616572716810112 -> 0.31926471943819873\n",
      "    Rebuilding NN1 model due to structure change\n",
      "    Training for 50 epochs (full training) on mps\n",
      "    Epoch 1/50, Train Loss: 0.130644, Val Loss: 0.000516\n",
      "    Epoch 5/50, Train Loss: 0.000327, Val Loss: 0.000270\n",
      "    Epoch 10/50, Train Loss: 0.000303, Val Loss: 0.000257\n",
      "Early stopping at epoch 12, Train Loss: 0.000298, Val Loss: 0.000258\n",
      "    Final - Train Loss: 0.000298, Val Loss: 0.000258\n",
      "[Saved] NN1_w512_2020Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000291, Val Loss: 0.000255\n",
      "    Epoch 5/20, Train Loss: 0.000289, Val Loss: 0.000255\n",
      "Early stopping at epoch 6, Train Loss: 0.000288, Val Loss: 0.000255\n",
      "    Final - Train Loss: 0.000288, Val Loss: 0.000255\n",
      "[Saved] NN2_w512_2020Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000292, Val Loss: 0.000255\n",
      "    Epoch 5/20, Train Loss: 0.000287, Val Loss: 0.000256\n",
      "Early stopping at epoch 6, Train Loss: 0.000285, Val Loss: 0.000256\n",
      "    Final - Train Loss: 0.000285, Val Loss: 0.000256\n",
      "[Saved] NN3_w512_2020Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000293, Val Loss: 0.000255\n",
      "    Epoch 5/20, Train Loss: 0.000284, Val Loss: 0.000255\n",
      "Early stopping at epoch 9, Train Loss: 0.000276, Val Loss: 0.000258\n",
      "    Final - Train Loss: 0.000276, Val Loss: 0.000258\n",
      "[Saved] NN4_w512_2020Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000280, Val Loss: 0.000255\n",
      "    Epoch 5/20, Train Loss: 0.000269, Val Loss: 0.000257\n",
      "Early stopping at epoch 6, Train Loss: 0.000266, Val Loss: 0.000257\n",
      "    Final - Train Loss: 0.000266, Val Loss: 0.000257\n",
      "[Saved] NN5_w512_2020Q4.pth\n",
      "    Training data size: 229828 samples\n",
      "  Quarter 2021Q1: Training models with expanding data\n",
      "    Added 3114 samples from 2020Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000308, Val Loss: 0.000269\n",
      "    Epoch 5/20, Train Loss: 0.000297, Val Loss: 0.000271\n",
      "Early stopping at epoch 7, Train Loss: 0.000293, Val Loss: 0.000273\n",
      "    Final - Train Loss: 0.000293, Val Loss: 0.000273\n",
      "[Saved] NN1_w512_2021Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000288, Val Loss: 0.000268\n",
      "    Epoch 5/20, Train Loss: 0.000287, Val Loss: 0.000268\n",
      "    Epoch 10/20, Train Loss: 0.000282, Val Loss: 0.000268\n",
      "Early stopping at epoch 12, Train Loss: 0.000280, Val Loss: 0.000268\n",
      "    Final - Train Loss: 0.000280, Val Loss: 0.000268\n",
      "[Saved] NN2_w512_2021Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000289, Val Loss: 0.000268\n",
      "    Epoch 5/20, Train Loss: 0.000282, Val Loss: 0.000268\n",
      "Early stopping at epoch 6, Train Loss: 0.000281, Val Loss: 0.000268\n",
      "    Final - Train Loss: 0.000281, Val Loss: 0.000268\n",
      "[Saved] NN3_w512_2021Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000282, Val Loss: 0.000268\n",
      "    Epoch 5/20, Train Loss: 0.000274, Val Loss: 0.000269\n",
      "Early stopping at epoch 6, Train Loss: 0.000272, Val Loss: 0.000269\n",
      "    Final - Train Loss: 0.000272, Val Loss: 0.000269\n",
      "[Saved] NN4_w512_2021Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000276, Val Loss: 0.000268\n",
      "    Epoch 5/20, Train Loss: 0.000264, Val Loss: 0.000268\n",
      "Early stopping at epoch 7, Train Loss: 0.000257, Val Loss: 0.000269\n",
      "    Final - Train Loss: 0.000257, Val Loss: 0.000269\n",
      "[Saved] NN5_w512_2021Q1.pth\n",
      "    Training data size: 232942 samples\n",
      "  Quarter 2021Q2: Training models with expanding data\n",
      "    Added 2989 samples from 2021Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000299, Val Loss: 0.000285\n",
      "    Epoch 5/20, Train Loss: 0.000291, Val Loss: 0.000289\n",
      "Early stopping at epoch 6, Train Loss: 0.000289, Val Loss: 0.000289\n",
      "    Final - Train Loss: 0.000289, Val Loss: 0.000289\n",
      "[Saved] NN1_w512_2021Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000281, Val Loss: 0.000283\n",
      "    Epoch 5/20, Train Loss: 0.000277, Val Loss: 0.000283\n",
      "Early stopping at epoch 6, Train Loss: 0.000277, Val Loss: 0.000283\n",
      "    Final - Train Loss: 0.000277, Val Loss: 0.000283\n",
      "[Saved] NN2_w512_2021Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000284, Val Loss: 0.000283\n",
      "    Epoch 5/20, Train Loss: 0.000279, Val Loss: 0.000283\n",
      "Early stopping at epoch 7, Train Loss: 0.000276, Val Loss: 0.000283\n",
      "    Final - Train Loss: 0.000276, Val Loss: 0.000283\n",
      "[Saved] NN3_w512_2021Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000277, Val Loss: 0.000283\n",
      "    Epoch 5/20, Train Loss: 0.000270, Val Loss: 0.000283\n",
      "    Epoch 10/20, Train Loss: 0.000255, Val Loss: 0.000284\n",
      "Early stopping at epoch 12, Train Loss: 0.000252, Val Loss: 0.000284\n",
      "    Final - Train Loss: 0.000252, Val Loss: 0.000284\n",
      "[Saved] NN4_w512_2021Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000267, Val Loss: 0.000283\n",
      "    Epoch 5/20, Train Loss: 0.000258, Val Loss: 0.000283\n",
      "Early stopping at epoch 8, Train Loss: 0.000248, Val Loss: 0.000283\n",
      "    Final - Train Loss: 0.000248, Val Loss: 0.000283\n",
      "[Saved] NN5_w512_2021Q2.pth\n",
      "    Training data size: 235931 samples\n",
      "  Quarter 2021Q3: Training models with expanding data\n",
      "    Added 3132 samples from 2021Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000295, Val Loss: 0.000287\n",
      "    Epoch 5/20, Train Loss: 0.000288, Val Loss: 0.000293\n",
      "Early stopping at epoch 6, Train Loss: 0.000287, Val Loss: 0.000295\n",
      "    Final - Train Loss: 0.000287, Val Loss: 0.000295\n",
      "[Saved] NN1_w512_2021Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000277, Val Loss: 0.000285\n",
      "    Epoch 5/20, Train Loss: 0.000276, Val Loss: 0.000285\n",
      "    Epoch 10/20, Train Loss: 0.000270, Val Loss: 0.000285\n",
      "Early stopping at epoch 11, Train Loss: 0.000269, Val Loss: 0.000285\n",
      "    Final - Train Loss: 0.000269, Val Loss: 0.000285\n",
      "[Saved] NN2_w512_2021Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000279, Val Loss: 0.000285\n",
      "    Epoch 5/20, Train Loss: 0.000274, Val Loss: 0.000285\n",
      "    Epoch 10/20, Train Loss: 0.000266, Val Loss: 0.000285\n",
      "Early stopping at epoch 12, Train Loss: 0.000260, Val Loss: 0.000285\n",
      "    Final - Train Loss: 0.000260, Val Loss: 0.000285\n",
      "[Saved] NN3_w512_2021Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000261, Val Loss: 0.000285\n",
      "    Epoch 5/20, Train Loss: 0.000256, Val Loss: 0.000285\n",
      "Early stopping at epoch 8, Train Loss: 0.000251, Val Loss: 0.000285\n",
      "    Final - Train Loss: 0.000251, Val Loss: 0.000285\n",
      "[Saved] NN4_w512_2021Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000257, Val Loss: 0.000285\n",
      "    Epoch 5/20, Train Loss: 0.000246, Val Loss: 0.000285\n",
      "Early stopping at epoch 6, Train Loss: 0.000244, Val Loss: 0.000286\n",
      "    Final - Train Loss: 0.000244, Val Loss: 0.000286\n",
      "[Saved] NN5_w512_2021Q3.pth\n",
      "    Training data size: 239063 samples\n",
      "  Quarter 2021Q4: Training models with expanding data\n",
      "    Added 3173 samples from 2021Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000292, Val Loss: 0.000291\n",
      "    Epoch 5/20, Train Loss: 0.000285, Val Loss: 0.000301\n",
      "Early stopping at epoch 6, Train Loss: 0.000284, Val Loss: 0.000304\n",
      "    Final - Train Loss: 0.000284, Val Loss: 0.000304\n",
      "[Saved] NN1_w512_2021Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000271, Val Loss: 0.000288\n",
      "    Epoch 5/20, Train Loss: 0.000268, Val Loss: 0.000288\n",
      "Early stopping at epoch 7, Train Loss: 0.000267, Val Loss: 0.000289\n",
      "    Final - Train Loss: 0.000267, Val Loss: 0.000289\n",
      "[Saved] NN2_w512_2021Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000267, Val Loss: 0.000288\n",
      "    Epoch 5/20, Train Loss: 0.000262, Val Loss: 0.000289\n",
      "Early stopping at epoch 6, Train Loss: 0.000262, Val Loss: 0.000290\n",
      "    Final - Train Loss: 0.000262, Val Loss: 0.000290\n",
      "[Saved] NN3_w512_2021Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000256, Val Loss: 0.000290\n",
      "    Epoch 5/20, Train Loss: 0.000250, Val Loss: 0.000289\n",
      "Early stopping at epoch 8, Train Loss: 0.000246, Val Loss: 0.000290\n",
      "    Final - Train Loss: 0.000246, Val Loss: 0.000290\n",
      "[Saved] NN4_w512_2021Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000251, Val Loss: 0.000288\n",
      "    Epoch 5/20, Train Loss: 0.000243, Val Loss: 0.000290\n",
      "Early stopping at epoch 6, Train Loss: 0.000241, Val Loss: 0.000289\n",
      "    Final - Train Loss: 0.000241, Val Loss: 0.000289\n",
      "[Saved] NN5_w512_2021Q4.pth\n",
      "    Training data size: 242236 samples\n",
      "  Quarter 2022Q1: Training models with expanding data\n",
      "    Added 3153 samples from 2021Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000289, Val Loss: 0.000298\n",
      "    Epoch 5/20, Train Loss: 0.000283, Val Loss: 0.000314\n",
      "Early stopping at epoch 6, Train Loss: 0.000281, Val Loss: 0.000325\n",
      "    Final - Train Loss: 0.000281, Val Loss: 0.000325\n",
      "[Saved] NN1_w512_2022Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000267, Val Loss: 0.000294\n",
      "    Epoch 5/20, Train Loss: 0.000265, Val Loss: 0.000294\n",
      "Early stopping at epoch 7, Train Loss: 0.000264, Val Loss: 0.000294\n",
      "    Final - Train Loss: 0.000264, Val Loss: 0.000294\n",
      "[Saved] NN2_w512_2022Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000265, Val Loss: 0.000294\n",
      "    Epoch 5/20, Train Loss: 0.000262, Val Loss: 0.000294\n",
      "Early stopping at epoch 8, Train Loss: 0.000256, Val Loss: 0.000295\n",
      "    Final - Train Loss: 0.000256, Val Loss: 0.000295\n",
      "[Saved] NN3_w512_2022Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000249, Val Loss: 0.000295\n",
      "    Epoch 5/20, Train Loss: 0.000245, Val Loss: 0.000297\n",
      "Early stopping at epoch 8, Train Loss: 0.000240, Val Loss: 0.000298\n",
      "    Final - Train Loss: 0.000240, Val Loss: 0.000298\n",
      "[Saved] NN4_w512_2022Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000247, Val Loss: 0.000294\n",
      "    Epoch 5/20, Train Loss: 0.000239, Val Loss: 0.000296\n",
      "Early stopping at epoch 6, Train Loss: 0.000236, Val Loss: 0.000297\n",
      "    Final - Train Loss: 0.000236, Val Loss: 0.000297\n",
      "[Saved] NN5_w512_2022Q1.pth\n",
      "    Training data size: 245389 samples\n",
      "  Quarter 2022Q2: Training models with expanding data\n",
      "    Added 3029 samples from 2022Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000287, Val Loss: 0.000309\n",
      "    Epoch 5/20, Train Loss: 0.000282, Val Loss: 0.000334\n",
      "Early stopping at epoch 6, Train Loss: 0.000281, Val Loss: 0.000332\n",
      "    Final - Train Loss: 0.000281, Val Loss: 0.000332\n",
      "[Saved] NN1_w512_2022Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000266, Val Loss: 0.000300\n",
      "    Epoch 5/20, Train Loss: 0.000262, Val Loss: 0.000300\n",
      "Early stopping at epoch 7, Train Loss: 0.000261, Val Loss: 0.000301\n",
      "    Final - Train Loss: 0.000261, Val Loss: 0.000301\n",
      "[Saved] NN2_w512_2022Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000260, Val Loss: 0.000300\n",
      "    Epoch 5/20, Train Loss: 0.000257, Val Loss: 0.000300\n",
      "    Epoch 10/20, Train Loss: 0.000252, Val Loss: 0.000300\n",
      "Early stopping at epoch 11, Train Loss: 0.000251, Val Loss: 0.000300\n",
      "    Final - Train Loss: 0.000251, Val Loss: 0.000300\n",
      "[Saved] NN3_w512_2022Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000247, Val Loss: 0.000300\n",
      "    Epoch 5/20, Train Loss: 0.000242, Val Loss: 0.000301\n",
      "Early stopping at epoch 6, Train Loss: 0.000241, Val Loss: 0.000301\n",
      "    Final - Train Loss: 0.000241, Val Loss: 0.000301\n",
      "[Saved] NN4_w512_2022Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000244, Val Loss: 0.000300\n",
      "    Epoch 5/20, Train Loss: 0.000237, Val Loss: 0.000300\n",
      "Early stopping at epoch 9, Train Loss: 0.000226, Val Loss: 0.000300\n",
      "    Final - Train Loss: 0.000226, Val Loss: 0.000300\n",
      "[Saved] NN5_w512_2022Q2.pth\n",
      "    Training data size: 248418 samples\n",
      "  Quarter 2022Q3: Training models with expanding data\n",
      "    Added 2969 samples from 2022Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000285, Val Loss: 0.000329\n",
      "    Epoch 5/20, Train Loss: 0.000280, Val Loss: 0.000352\n",
      "Early stopping at epoch 6, Train Loss: 0.000279, Val Loss: 0.000361\n",
      "    Final - Train Loss: 0.000279, Val Loss: 0.000361\n",
      "[Saved] NN1_w512_2022Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000263, Val Loss: 0.000316\n",
      "    Epoch 5/20, Train Loss: 0.000262, Val Loss: 0.000316\n",
      "Early stopping at epoch 8, Train Loss: 0.000258, Val Loss: 0.000317\n",
      "    Final - Train Loss: 0.000258, Val Loss: 0.000317\n",
      "[Saved] NN2_w512_2022Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000254, Val Loss: 0.000316\n",
      "    Epoch 5/20, Train Loss: 0.000251, Val Loss: 0.000316\n",
      "Early stopping at epoch 6, Train Loss: 0.000250, Val Loss: 0.000316\n",
      "    Final - Train Loss: 0.000250, Val Loss: 0.000316\n",
      "[Saved] NN3_w512_2022Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000245, Val Loss: 0.000317\n",
      "    Epoch 5/20, Train Loss: 0.000241, Val Loss: 0.000319\n",
      "Early stopping at epoch 6, Train Loss: 0.000239, Val Loss: 0.000322\n",
      "    Final - Train Loss: 0.000239, Val Loss: 0.000322\n",
      "[Saved] NN4_w512_2022Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000235, Val Loss: 0.000316\n",
      "    Epoch 5/20, Train Loss: 0.000228, Val Loss: 0.000319\n",
      "Early stopping at epoch 6, Train Loss: 0.000225, Val Loss: 0.000321\n",
      "    Final - Train Loss: 0.000225, Val Loss: 0.000321\n",
      "[Saved] NN5_w512_2022Q3.pth\n",
      "    Training data size: 251387 samples\n",
      "  Quarter 2022Q4: Training models with expanding data\n",
      "    Added 3152 samples from 2022Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000282, Val Loss: 0.000345\n",
      "    Epoch 5/20, Train Loss: 0.000278, Val Loss: 0.000365\n",
      "Early stopping at epoch 6, Train Loss: 0.000278, Val Loss: 0.000374\n",
      "    Final - Train Loss: 0.000278, Val Loss: 0.000374\n",
      "[Saved] NN1_w512_2022Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000260, Val Loss: 0.000328\n",
      "    Epoch 5/20, Train Loss: 0.000258, Val Loss: 0.000328\n",
      "    Epoch 10/20, Train Loss: 0.000252, Val Loss: 0.000329\n",
      "Early stopping at epoch 10, Train Loss: 0.000252, Val Loss: 0.000329\n",
      "    Final - Train Loss: 0.000252, Val Loss: 0.000329\n",
      "[Saved] NN2_w512_2022Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000251, Val Loss: 0.000328\n",
      "    Epoch 5/20, Train Loss: 0.000250, Val Loss: 0.000328\n",
      "Early stopping at epoch 6, Train Loss: 0.000247, Val Loss: 0.000328\n",
      "    Final - Train Loss: 0.000247, Val Loss: 0.000328\n",
      "[Saved] NN3_w512_2022Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000242, Val Loss: 0.000328\n",
      "    Epoch 5/20, Train Loss: 0.000238, Val Loss: 0.000330\n",
      "Early stopping at epoch 6, Train Loss: 0.000238, Val Loss: 0.000336\n",
      "    Final - Train Loss: 0.000238, Val Loss: 0.000336\n",
      "[Saved] NN4_w512_2022Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000233, Val Loss: 0.000327\n",
      "    Epoch 5/20, Train Loss: 0.000226, Val Loss: 0.000329\n",
      "Early stopping at epoch 6, Train Loss: 0.000224, Val Loss: 0.000330\n",
      "    Final - Train Loss: 0.000224, Val Loss: 0.000330\n",
      "[Saved] NN5_w512_2022Q4.pth\n",
      "    Training data size: 254539 samples\n",
      "  Quarter 2023Q1: Training models with expanding data\n",
      "    Added 3070 samples from 2022Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000280, Val Loss: 0.000355\n",
      "    Epoch 5/20, Train Loss: 0.000275, Val Loss: 0.000389\n",
      "Early stopping at epoch 6, Train Loss: 0.000275, Val Loss: 0.000402\n",
      "    Final - Train Loss: 0.000275, Val Loss: 0.000402\n",
      "[Saved] NN1_w512_2023Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000255, Val Loss: 0.000338\n",
      "    Epoch 5/20, Train Loss: 0.000252, Val Loss: 0.000339\n",
      "Early stopping at epoch 6, Train Loss: 0.000251, Val Loss: 0.000338\n",
      "    Final - Train Loss: 0.000251, Val Loss: 0.000338\n",
      "[Saved] NN2_w512_2023Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000248, Val Loss: 0.000338\n",
      "    Epoch 5/20, Train Loss: 0.000248, Val Loss: 0.000338\n",
      "Early stopping at epoch 6, Train Loss: 0.000247, Val Loss: 0.000338\n",
      "    Final - Train Loss: 0.000247, Val Loss: 0.000338\n",
      "[Saved] NN3_w512_2023Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000240, Val Loss: 0.000338\n",
      "    Epoch 5/20, Train Loss: 0.000237, Val Loss: 0.000339\n",
      "Early stopping at epoch 6, Train Loss: 0.000235, Val Loss: 0.000339\n",
      "    Final - Train Loss: 0.000235, Val Loss: 0.000339\n",
      "[Saved] NN4_w512_2023Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000229, Val Loss: 0.000338\n",
      "    Epoch 5/20, Train Loss: 0.000225, Val Loss: 0.000338\n",
      "Early stopping at epoch 9, Train Loss: 0.000216, Val Loss: 0.000338\n",
      "    Final - Train Loss: 0.000216, Val Loss: 0.000338\n",
      "[Saved] NN5_w512_2023Q1.pth\n",
      "    Training data size: 257609 samples\n",
      "  Quarter 2023Q2: Training models with expanding data\n",
      "    Added 3064 samples from 2023Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000279, Val Loss: 0.000352\n",
      "    Epoch 5/20, Train Loss: 0.000277, Val Loss: 0.000381\n",
      "Early stopping at epoch 6, Train Loss: 0.000275, Val Loss: 0.000385\n",
      "    Final - Train Loss: 0.000275, Val Loss: 0.000385\n",
      "[Saved] NN1_w512_2023Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000254, Val Loss: 0.000337\n",
      "    Epoch 5/20, Train Loss: 0.000252, Val Loss: 0.000337\n",
      "Early stopping at epoch 9, Train Loss: 0.000250, Val Loss: 0.000337\n",
      "    Final - Train Loss: 0.000250, Val Loss: 0.000337\n",
      "[Saved] NN2_w512_2023Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000250, Val Loss: 0.000338\n",
      "    Epoch 5/20, Train Loss: 0.000248, Val Loss: 0.000337\n",
      "    Epoch 10/20, Train Loss: 0.000243, Val Loss: 0.000336\n",
      "    Epoch 15/20, Train Loss: 0.000240, Val Loss: 0.000335\n",
      "    Epoch 20/20, Train Loss: 0.000236, Val Loss: 0.000336\n",
      "    Final - Train Loss: 0.000236, Val Loss: 0.000336\n",
      "[Saved] NN3_w512_2023Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000239, Val Loss: 0.000335\n",
      "    Epoch 5/20, Train Loss: 0.000238, Val Loss: 0.000334\n",
      "    Epoch 10/20, Train Loss: 0.000232, Val Loss: 0.000336\n",
      "Early stopping at epoch 11, Train Loss: 0.000230, Val Loss: 0.000338\n",
      "    Final - Train Loss: 0.000230, Val Loss: 0.000338\n",
      "[Saved] NN4_w512_2023Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000223, Val Loss: 0.000337\n",
      "    Epoch 5/20, Train Loss: 0.000216, Val Loss: 0.000339\n",
      "Early stopping at epoch 7, Train Loss: 0.000214, Val Loss: 0.000340\n",
      "    Final - Train Loss: 0.000214, Val Loss: 0.000340\n",
      "[Saved] NN5_w512_2023Q2.pth\n",
      "    Training data size: 260673 samples\n",
      "  Quarter 2023Q3: Training models with expanding data\n",
      "    Added 3069 samples from 2023Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000278, Val Loss: 0.000351\n",
      "    Epoch 5/20, Train Loss: 0.000276, Val Loss: 0.000376\n",
      "Early stopping at epoch 6, Train Loss: 0.000275, Val Loss: 0.000392\n",
      "    Final - Train Loss: 0.000275, Val Loss: 0.000392\n",
      "[Saved] NN1_w512_2023Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000251, Val Loss: 0.000338\n",
      "    Epoch 5/20, Train Loss: 0.000251, Val Loss: 0.000339\n",
      "Early stopping at epoch 6, Train Loss: 0.000249, Val Loss: 0.000340\n",
      "    Final - Train Loss: 0.000249, Val Loss: 0.000340\n",
      "[Saved] NN2_w512_2023Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000235, Val Loss: 0.000346\n",
      "    Epoch 5/20, Train Loss: 0.000233, Val Loss: 0.000353\n",
      "Early stopping at epoch 6, Train Loss: 0.000234, Val Loss: 0.000347\n",
      "    Final - Train Loss: 0.000234, Val Loss: 0.000347\n",
      "[Saved] NN3_w512_2023Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000234, Val Loss: 0.000342\n",
      "    Epoch 5/20, Train Loss: 0.000231, Val Loss: 0.000352\n",
      "Early stopping at epoch 6, Train Loss: 0.000230, Val Loss: 0.000357\n",
      "    Final - Train Loss: 0.000230, Val Loss: 0.000357\n",
      "[Saved] NN4_w512_2023Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000218, Val Loss: 0.000345\n",
      "    Epoch 5/20, Train Loss: 0.000213, Val Loss: 0.000348\n",
      "Early stopping at epoch 8, Train Loss: 0.000210, Val Loss: 0.000346\n",
      "    Final - Train Loss: 0.000210, Val Loss: 0.000346\n",
      "[Saved] NN5_w512_2023Q3.pth\n",
      "    Training data size: 263742 samples\n",
      "  Quarter 2023Q4: Training models with expanding data\n",
      "    Added 3121 samples from 2023Q3 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000276, Val Loss: 0.000354\n",
      "    Epoch 5/20, Train Loss: 0.000273, Val Loss: 0.000382\n",
      "Early stopping at epoch 6, Train Loss: 0.000273, Val Loss: 0.000397\n",
      "    Final - Train Loss: 0.000273, Val Loss: 0.000397\n",
      "[Saved] NN1_w512_2023Q4.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000250, Val Loss: 0.000335\n",
      "    Epoch 5/20, Train Loss: 0.000250, Val Loss: 0.000335\n",
      "Early stopping at epoch 7, Train Loss: 0.000247, Val Loss: 0.000335\n",
      "    Final - Train Loss: 0.000247, Val Loss: 0.000335\n",
      "[Saved] NN2_w512_2023Q4.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000234, Val Loss: 0.000334\n",
      "    Epoch 5/20, Train Loss: 0.000234, Val Loss: 0.000335\n",
      "Early stopping at epoch 6, Train Loss: 0.000232, Val Loss: 0.000337\n",
      "    Final - Train Loss: 0.000232, Val Loss: 0.000337\n",
      "[Saved] NN3_w512_2023Q4.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000232, Val Loss: 0.000334\n",
      "    Epoch 5/20, Train Loss: 0.000228, Val Loss: 0.000337\n",
      "Early stopping at epoch 6, Train Loss: 0.000228, Val Loss: 0.000338\n",
      "    Final - Train Loss: 0.000228, Val Loss: 0.000338\n",
      "[Saved] NN4_w512_2023Q4.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000214, Val Loss: 0.000334\n",
      "    Epoch 5/20, Train Loss: 0.000209, Val Loss: 0.000337\n",
      "Early stopping at epoch 6, Train Loss: 0.000207, Val Loss: 0.000337\n",
      "    Final - Train Loss: 0.000207, Val Loss: 0.000337\n",
      "[Saved] NN5_w512_2023Q4.pth\n",
      "    Training data size: 266863 samples\n",
      "  Quarter 2024Q1: Training models with expanding data\n",
      "    Added 3113 samples from 2023Q4 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000274, Val Loss: 0.000365\n",
      "    Epoch 5/20, Train Loss: 0.000272, Val Loss: 0.000390\n",
      "Early stopping at epoch 6, Train Loss: 0.000271, Val Loss: 0.000396\n",
      "    Final - Train Loss: 0.000271, Val Loss: 0.000396\n",
      "[Saved] NN1_w512_2024Q1.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000249, Val Loss: 0.000341\n",
      "    Epoch 5/20, Train Loss: 0.000247, Val Loss: 0.000342\n",
      "Early stopping at epoch 7, Train Loss: 0.000246, Val Loss: 0.000342\n",
      "    Final - Train Loss: 0.000246, Val Loss: 0.000342\n",
      "[Saved] NN2_w512_2024Q1.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000233, Val Loss: 0.000342\n",
      "    Epoch 5/20, Train Loss: 0.000231, Val Loss: 0.000347\n",
      "Early stopping at epoch 7, Train Loss: 0.000231, Val Loss: 0.000346\n",
      "    Final - Train Loss: 0.000231, Val Loss: 0.000346\n",
      "[Saved] NN3_w512_2024Q1.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000230, Val Loss: 0.000342\n",
      "    Epoch 5/20, Train Loss: 0.000227, Val Loss: 0.000347\n",
      "Early stopping at epoch 6, Train Loss: 0.000227, Val Loss: 0.000348\n",
      "    Final - Train Loss: 0.000227, Val Loss: 0.000348\n",
      "[Saved] NN4_w512_2024Q1.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000211, Val Loss: 0.000346\n",
      "    Epoch 5/20, Train Loss: 0.000207, Val Loss: 0.000345\n",
      "    Epoch 10/20, Train Loss: 0.000200, Val Loss: 0.000349\n",
      "Early stopping at epoch 11, Train Loss: 0.000198, Val Loss: 0.000348\n",
      "    Final - Train Loss: 0.000198, Val Loss: 0.000348\n",
      "[Saved] NN5_w512_2024Q1.pth\n",
      "    Training data size: 269976 samples\n",
      "  Quarter 2024Q2: Training models with expanding data\n",
      "    Added 3026 samples from 2024Q1 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000274, Val Loss: 0.000352\n",
      "    Epoch 5/20, Train Loss: 0.000272, Val Loss: 0.000394\n",
      "Early stopping at epoch 6, Train Loss: 0.000271, Val Loss: 0.000405\n",
      "    Final - Train Loss: 0.000271, Val Loss: 0.000405\n",
      "[Saved] NN1_w512_2024Q2.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000247, Val Loss: 0.000332\n",
      "    Epoch 5/20, Train Loss: 0.000247, Val Loss: 0.000333\n",
      "Early stopping at epoch 6, Train Loss: 0.000245, Val Loss: 0.000333\n",
      "    Final - Train Loss: 0.000245, Val Loss: 0.000333\n",
      "[Saved] NN2_w512_2024Q2.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000233, Val Loss: 0.000333\n",
      "    Epoch 5/20, Train Loss: 0.000231, Val Loss: 0.000337\n",
      "Early stopping at epoch 6, Train Loss: 0.000231, Val Loss: 0.000340\n",
      "    Final - Train Loss: 0.000231, Val Loss: 0.000340\n",
      "[Saved] NN3_w512_2024Q2.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000230, Val Loss: 0.000333\n",
      "    Epoch 5/20, Train Loss: 0.000228, Val Loss: 0.000334\n",
      "Early stopping at epoch 7, Train Loss: 0.000225, Val Loss: 0.000334\n",
      "    Final - Train Loss: 0.000225, Val Loss: 0.000334\n",
      "[Saved] NN4_w512_2024Q2.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000203, Val Loss: 0.000334\n",
      "    Epoch 5/20, Train Loss: 0.000200, Val Loss: 0.000336\n",
      "Early stopping at epoch 7, Train Loss: 0.000196, Val Loss: 0.000337\n",
      "    Final - Train Loss: 0.000196, Val Loss: 0.000337\n",
      "[Saved] NN5_w512_2024Q2.pth\n",
      "    Training data size: 273002 samples\n",
      "  Quarter 2024Q3: Training models with expanding data\n",
      "    Added 3112 samples from 2024Q2 to training set\n",
      "    Using warm-start for NN1 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000272, Val Loss: 0.000357\n",
      "    Epoch 5/20, Train Loss: 0.000270, Val Loss: 0.000392\n",
      "Early stopping at epoch 6, Train Loss: 0.000269, Val Loss: 0.000392\n",
      "    Final - Train Loss: 0.000269, Val Loss: 0.000392\n",
      "[Saved] NN1_w512_2024Q3.pth\n",
      "    Using warm-start for NN2 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000246, Val Loss: 0.000335\n",
      "    Epoch 5/20, Train Loss: 0.000245, Val Loss: 0.000337\n",
      "Early stopping at epoch 6, Train Loss: 0.000244, Val Loss: 0.000337\n",
      "    Final - Train Loss: 0.000244, Val Loss: 0.000337\n",
      "[Saved] NN2_w512_2024Q3.pth\n",
      "    Using warm-start for NN3 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000232, Val Loss: 0.000334\n",
      "    Epoch 5/20, Train Loss: 0.000230, Val Loss: 0.000336\n",
      "Early stopping at epoch 6, Train Loss: 0.000231, Val Loss: 0.000336\n",
      "    Final - Train Loss: 0.000231, Val Loss: 0.000336\n",
      "[Saved] NN3_w512_2024Q3.pth\n",
      "    Using warm-start for NN4 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000228, Val Loss: 0.000337\n",
      "    Epoch 5/20, Train Loss: 0.000225, Val Loss: 0.000340\n",
      "Early stopping at epoch 7, Train Loss: 0.000225, Val Loss: 0.000339\n",
      "    Final - Train Loss: 0.000225, Val Loss: 0.000339\n",
      "[Saved] NN4_w512_2024Q3.pth\n",
      "    Using warm-start for NN5 model (retaining previous weights)\n",
      "    [Warm Start] Retaining model weights, continuing training...\n",
      "    Training for 20 epochs (warm-start fine-tuning) on mps\n",
      "    Epoch 1/20, Train Loss: 0.000200, Val Loss: 0.000336\n",
      "    Epoch 5/20, Train Loss: 0.000196, Val Loss: 0.000341\n",
      "Early stopping at epoch 6, Train Loss: 0.000196, Val Loss: 0.000347\n",
      "    Final - Train Loss: 0.000196, Val Loss: 0.000347\n",
      "[Saved] NN5_w512_2024Q3.pth\n",
      "    Training data size: 276114 samples\n",
      "MLP Quarterly Expanding Window Training Completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NN1_w5': {'learning_rate': 0.0003202493137065723,\n",
       "  'batch_size': 128,\n",
       "  'dropout_rate': 0.498601860763653},\n",
       " 'NN2_w5': {'learning_rate': 0.0003202493137065723,\n",
       "  'batch_size': 128,\n",
       "  'dropout_rate': 0.498601860763653},\n",
       " 'NN3_w5': {'learning_rate': 0.0003202493137065723,\n",
       "  'batch_size': 128,\n",
       "  'dropout_rate': 0.498601860763653},\n",
       " 'NN4_w5': {'learning_rate': 0.0003202493137065723,\n",
       "  'batch_size': 128,\n",
       "  'dropout_rate': 0.498601860763653},\n",
       " 'NN5_w5': {'learning_rate': 0.0003202493137065723,\n",
       "  'batch_size': 128,\n",
       "  'dropout_rate': 0.498601860763653},\n",
       " 'NN1_w21': {'learning_rate': 0.0009866052276181587,\n",
       "  'batch_size': 128,\n",
       "  'dropout_rate': 0.08964864669300382},\n",
       " 'NN2_w21': {'learning_rate': 0.0009866052276181587,\n",
       "  'batch_size': 128,\n",
       "  'dropout_rate': 0.08964864669300382},\n",
       " 'NN3_w21': {'learning_rate': 0.0009866052276181587,\n",
       "  'batch_size': 128,\n",
       "  'dropout_rate': 0.08964864669300382},\n",
       " 'NN4_w21': {'learning_rate': 0.0009866052276181587,\n",
       "  'batch_size': 128,\n",
       "  'dropout_rate': 0.08964864669300382},\n",
       " 'NN5_w21': {'learning_rate': 0.0009866052276181587,\n",
       "  'batch_size': 128,\n",
       "  'dropout_rate': 0.08964864669300382},\n",
       " 'NN1_w252': {'learning_rate': 0.0007016683098186692,\n",
       "  'batch_size': 128,\n",
       "  'dropout_rate': 0.407960852804087},\n",
       " 'NN2_w252': {'learning_rate': 0.0007016683098186692,\n",
       "  'batch_size': 128,\n",
       "  'dropout_rate': 0.407960852804087},\n",
       " 'NN3_w252': {'learning_rate': 0.0007016683098186692,\n",
       "  'batch_size': 128,\n",
       "  'dropout_rate': 0.407960852804087},\n",
       " 'NN4_w252': {'learning_rate': 0.0007016683098186692,\n",
       "  'batch_size': 128,\n",
       "  'dropout_rate': 0.407960852804087},\n",
       " 'NN5_w252': {'learning_rate': 0.0007016683098186692,\n",
       "  'batch_size': 128,\n",
       "  'dropout_rate': 0.407960852804087},\n",
       " 'NN1_w512': {'learning_rate': 0.0004548860533183753,\n",
       "  'batch_size': 64,\n",
       "  'dropout_rate': 0.31926471943819873},\n",
       " 'NN2_w512': {'learning_rate': 0.0004548860533183753,\n",
       "  'batch_size': 64,\n",
       "  'dropout_rate': 0.31926471943819873},\n",
       " 'NN3_w512': {'learning_rate': 0.0004548860533183753,\n",
       "  'batch_size': 64,\n",
       "  'dropout_rate': 0.31926471943819873},\n",
       " 'NN4_w512': {'learning_rate': 0.0004548860533183753,\n",
       "  'batch_size': 64,\n",
       "  'dropout_rate': 0.31926471943819873},\n",
       " 'NN5_w512': {'learning_rate': 0.0004548860533183753,\n",
       "  'batch_size': 64,\n",
       "  'dropout_rate': 0.31926471943819873}}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mlp_models_expanding_quarterly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Daily Rebalance Portfolio Backtesting Simulation\n",
      "Processing window size: 5\n",
      "  Model: NN1, Scheme: VW\n",
      "[Create] New metrics file created with NN1 w=5\n",
      "  Model: NN1, Scheme: EW\n",
      "  Model: NN2, Scheme: VW\n",
      "[Update] Metrics updated for NN2 w=5\n",
      "  Model: NN2, Scheme: EW\n",
      "  Model: NN3, Scheme: VW\n",
      "[Update] Metrics updated for NN3 w=5\n",
      "  Model: NN3, Scheme: EW\n",
      "  Model: NN4, Scheme: VW\n",
      "[Update] Metrics updated for NN4 w=5\n",
      "  Model: NN4, Scheme: EW\n",
      "  Model: NN5, Scheme: VW\n",
      "[Update] Metrics updated for NN5 w=5\n",
      "  Model: NN5, Scheme: EW\n",
      "Processing window size: 21\n",
      "  Model: NN1, Scheme: VW\n",
      "[Update] Metrics updated for NN1 w=21\n",
      "  Model: NN1, Scheme: EW\n",
      "  Model: NN2, Scheme: VW\n",
      "[Update] Metrics updated for NN2 w=21\n",
      "  Model: NN2, Scheme: EW\n",
      "  Model: NN3, Scheme: VW\n",
      "[Update] Metrics updated for NN3 w=21\n",
      "  Model: NN3, Scheme: EW\n",
      "  Model: NN4, Scheme: VW\n",
      "[Update] Metrics updated for NN4 w=21\n",
      "  Model: NN4, Scheme: EW\n",
      "  Model: NN5, Scheme: VW\n",
      "[Update] Metrics updated for NN5 w=21\n",
      "  Model: NN5, Scheme: EW\n",
      "Processing window size: 252\n",
      "  Model: NN1, Scheme: VW\n",
      "[Update] Metrics updated for NN1 w=252\n",
      "  Model: NN1, Scheme: EW\n",
      "  Model: NN2, Scheme: VW\n",
      "[Update] Metrics updated for NN2 w=252\n",
      "  Model: NN2, Scheme: EW\n",
      "  Model: NN3, Scheme: VW\n",
      "[Update] Metrics updated for NN3 w=252\n",
      "  Model: NN3, Scheme: EW\n",
      "  Model: NN4, Scheme: VW\n",
      "[Update] Metrics updated for NN4 w=252\n",
      "  Model: NN4, Scheme: EW\n",
      "  Model: NN5, Scheme: VW\n",
      "[Update] Metrics updated for NN5 w=252\n",
      "  Model: NN5, Scheme: EW\n",
      "Processing window size: 512\n",
      "  Model: NN1, Scheme: VW\n",
      "[Update] Metrics updated for NN1 w=512\n",
      "  Model: NN1, Scheme: EW\n",
      "  Model: NN2, Scheme: VW\n",
      "[Update] Metrics updated for NN2 w=512\n",
      "  Model: NN2, Scheme: EW\n",
      "  Model: NN3, Scheme: VW\n",
      "[Update] Metrics updated for NN3 w=512\n",
      "  Model: NN3, Scheme: EW\n",
      "  Model: NN4, Scheme: VW\n",
      "[Update] Metrics updated for NN4 w=512\n",
      "  Model: NN4, Scheme: EW\n",
      "  Model: NN5, Scheme: VW\n",
      "[Update] Metrics updated for NN5 w=512\n",
      "  Model: NN5, Scheme: EW\n",
      "VW results saved to portfolio_results_daily_rebalance_VW.csv\n",
      "EW results saved to portfolio_results_daily_rebalance_EW.csv\n",
      "VW results saved to portfolio_daily_series_VW.csv\n",
      "EW results saved to portfolio_daily_series_EW.csv\n",
      "Saved 2217000 prediction rows to predictions_daily.csv\n",
      "Generated 120 portfolio summary records\n",
      "Generated 258360 daily series records\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(    scheme model  window portfolio_type  annual_return  annual_vol    sharpe  \\\n",
       " 0       VW   NN1       5      long_only       0.215464    0.220818  0.975755   \n",
       " 1       VW   NN1       5     short_only      -0.133895    0.197423 -0.678214   \n",
       " 2       VW   NN1       5     long_short       0.081569    0.226536  0.360070   \n",
       " 3       EW   NN1       5      long_only       0.184907    0.232151  0.796495   \n",
       " 4       EW   NN1       5     short_only      -0.125858    0.199443 -0.631047   \n",
       " ..     ...   ...     ...            ...            ...         ...       ...   \n",
       " 115     VW   NN5     512     short_only      -0.147199    0.211902 -0.694659   \n",
       " 116     VW   NN5     512     long_short       0.086642    0.211171  0.410294   \n",
       " 117     EW   NN5     512      long_only       0.211579    0.201951  1.047674   \n",
       " 118     EW   NN5     512     short_only      -0.102692    0.225955 -0.454481   \n",
       " 119     EW   NN5     512     long_short       0.108887    0.190952  0.570230   \n",
       " \n",
       "      max_drawdown  max_1d_loss  avg_turnover  ...  tc20_sharpe  \\\n",
       " 0        0.232350    -0.077703      1.508420  ...    -2.456921   \n",
       " 1        0.789265    -0.059438      1.542290  ...    -4.601217   \n",
       " 2        0.456647    -0.079553      3.050555  ...    -6.356516   \n",
       " 3        0.372872    -0.079536      1.296748  ...    -2.014589   \n",
       " 4        0.757040    -0.062838      1.422630  ...    -4.211465   \n",
       " ..            ...          ...           ...  ...          ...   \n",
       " 115      0.795814    -0.057247      1.772735  ...    -4.903409   \n",
       " 116      0.573236    -0.058811      3.541814  ...    -7.983887   \n",
       " 117      0.212266    -0.048165      1.648710  ...    -3.058925   \n",
       " 118      0.790533    -0.058931      1.601935  ...    -4.029688   \n",
       " 119      0.306135    -0.057046      3.250256  ...    -7.978109   \n",
       " \n",
       "      tc20_max_drawdown  tc30_annual_return  tc30_annual_vol  tc30_sharpe  \\\n",
       " 0            -0.992387           -0.924902         0.222656    -4.153940   \n",
       " 1            -0.999667           -1.299866         0.198915    -6.534785   \n",
       " 2            -0.999997           -2.224650         0.231287    -9.618573   \n",
       " 3            -0.985435           -0.795434         0.233187    -3.411138   \n",
       " 4            -0.999420           -1.201366         0.200841    -5.981672   \n",
       " ..                 ...                 ...              ...          ...   \n",
       " 115          -0.999892           -1.487387         0.212642    -6.994779   \n",
       " 116          -1.000000           -2.590970         0.214061   -12.103872   \n",
       " 117          -0.995805           -1.034846         0.202940    -5.099259   \n",
       " 118          -0.999682           -1.313755         0.225978    -5.813630   \n",
       " 119          -0.999998           -2.348307         0.192475   -12.200567   \n",
       " \n",
       "      tc30_max_drawdown  tc40_annual_return  tc40_annual_vol  tc40_sharpe  \\\n",
       " 0            -0.999701           -1.305024         0.223886    -5.828952   \n",
       " 1            -0.999988           -1.688524         0.200168    -8.435514   \n",
       " 2            -1.000000           -2.993390         0.234169   -12.783013   \n",
       " 3            -0.999110           -1.122215         0.233955    -4.796713   \n",
       " 4            -0.999972           -1.559869         0.201783    -7.730408   \n",
       " ..                 ...                 ...              ...          ...   \n",
       " 115          -0.999998           -1.934116         0.213216    -9.071147   \n",
       " 116          -1.000000           -3.483507         0.215747   -16.146278   \n",
       " 117          -0.999881           -1.450321         0.203530    -7.125816   \n",
       " 118          -0.999990           -1.717443         0.226246    -7.591047   \n",
       " 119          -1.000000           -3.167372         0.193554   -16.364272   \n",
       " \n",
       "      tc40_max_drawdown  \n",
       " 0            -0.999989  \n",
       " 1            -1.000000  \n",
       " 2            -1.000000  \n",
       " 3            -0.999946  \n",
       " 4            -0.999999  \n",
       " ..                 ...  \n",
       " 115          -1.000000  \n",
       " 116          -1.000000  \n",
       " 117          -0.999997  \n",
       " 118          -1.000000  \n",
       " 119          -1.000000  \n",
       " \n",
       " [120 rows x 32 columns],\n",
       "        scheme model  window portfolio_type                 date    return  \\\n",
       " 0          VW   NN1       5      long_only  2016-01-05 00:00:00 -0.011842   \n",
       " 1          VW   NN1       5      long_only  2016-01-06 00:00:00 -0.007269   \n",
       " 2          VW   NN1       5      long_only  2016-01-07 00:00:00 -0.006282   \n",
       " 3          VW   NN1       5      long_only  2016-01-08 00:00:00 -0.003558   \n",
       " 4          VW   NN1       5      long_only  2016-01-11 00:00:00  0.015054   \n",
       " ...       ...   ...     ...            ...                  ...       ...   \n",
       " 258355     EW   NN5     512     long_short  2024-12-20 00:00:00  0.007478   \n",
       " 258356     EW   NN5     512     long_short  2024-12-23 00:00:00 -0.000828   \n",
       " 258357     EW   NN5     512     long_short  2024-12-24 00:00:00  0.000319   \n",
       " 258358     EW   NN5     512     long_short  2024-12-27 00:00:00 -0.004732   \n",
       " 258359     EW   NN5     512     long_short  2024-12-30 00:00:00  0.000182   \n",
       " \n",
       "         turnover  cumulative  tc5_return  tc5_cumulative  tc10_return  \\\n",
       " 0       1.000000   -0.011912   -0.012342       -0.012418    -0.012842   \n",
       " 1       2.000000   -0.019208   -0.008269       -0.020722    -0.009269   \n",
       " 2       2.000000   -0.025510   -0.007282       -0.028031    -0.008282   \n",
       " 3       2.000000   -0.029074   -0.004558       -0.032599    -0.005558   \n",
       " 4       2.000000   -0.014133    0.014054       -0.018643     0.013054   \n",
       " ...          ...         ...         ...             ...          ...   \n",
       " 258355  2.802665    0.779667    0.006077       -2.715548     0.004676   \n",
       " 258356  3.218174    0.778839   -0.002437       -2.717988    -0.004046   \n",
       " 258357  2.009365    0.779158   -0.000686       -2.718674    -0.001690   \n",
       " 258358  3.207605    0.774414   -0.006336       -2.725030    -0.007940   \n",
       " 258359  2.791075    0.774596   -0.001213       -2.726244    -0.002609   \n",
       " \n",
       "         tc10_cumulative  tc20_return  tc20_cumulative  tc30_return  \\\n",
       " 0             -0.012925    -0.013842        -0.013938    -0.014842   \n",
       " 1             -0.022237    -0.011269        -0.025272    -0.013269   \n",
       " 2             -0.030554    -0.010282        -0.035607    -0.012282   \n",
       " 3             -0.036127    -0.007558        -0.043193    -0.009558   \n",
       " 4             -0.023158     0.011054        -0.032200     0.009054   \n",
       " ...                 ...          ...              ...          ...   \n",
       " 258355        -6.216576     0.001873       -13.236154    -0.000930   \n",
       " 258356        -6.220630    -0.007264       -13.243444    -0.010482   \n",
       " 258357        -6.222322    -0.003700       -13.247151    -0.005709   \n",
       " 258358        -6.230294    -0.011147       -13.258361    -0.014355   \n",
       " 258359        -6.232906    -0.005400       -13.263775    -0.008191   \n",
       " \n",
       "         tc30_cumulative  tc40_return  tc40_cumulative  \n",
       " 0             -0.014953    -0.015842        -0.015968  \n",
       " 1             -0.028311    -0.015269        -0.031356  \n",
       " 2             -0.040669    -0.014282        -0.045740  \n",
       " 3             -0.050273    -0.011558        -0.057365  \n",
       " 4             -0.041260     0.007054        -0.050337  \n",
       " ...                 ...          ...              ...  \n",
       " 258355       -20.279224    -0.003732       -27.345949  \n",
       " 258356       -20.289762    -0.013700       -27.359744  \n",
       " 258357       -20.295487    -0.007718       -27.367492  \n",
       " 258358       -20.309946    -0.017562       -27.385211  \n",
       " 258359       -20.318171    -0.010982       -27.396254  \n",
       " \n",
       " [258360 rows x 18 columns],\n",
       " <__main__.PortfolioBacktester at 0x35df80ec0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_portfolio_simulation_daily_rebalance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] 5_factor_analysis_VW_gross.csv \n",
      "[Saved] 5_factor_analysis_VW_net.csv \n",
      "[Saved] 5_factor_analysis_EW_gross.csv \n",
      "[Saved] 5_factor_analysis_EW_net.csv \n"
     ]
    }
   ],
   "source": [
    "def run_factor_regression(port_ret, factors, use_excess=True):\n",
    "    df = pd.concat([port_ret, factors], axis=1, join='inner').dropna()\n",
    "    df.columns = ['ret'] + list(factors.columns)\n",
    "    \n",
    "    if use_excess:\n",
    "        y = df['ret'].values\n",
    "    else:\n",
    "        y = df['ret'].values - df['rf'].values\n",
    "    \n",
    "    X = df[['mktrf','smb','hml','rmw','cma','umd']].values\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    model = sm.OLS(y, X)\n",
    "    res = model.fit()\n",
    "    alpha = res.params[0]\n",
    "    resid_std = res.resid.std(ddof=1)\n",
    "\n",
    "    ir_daily = alpha / resid_std\n",
    "    ir_annual = ir_daily * np.sqrt(252)\n",
    "\n",
    "    y_hat = np.asarray(res.fittedvalues)\n",
    "    \n",
    "    out = {\n",
    "        'N_obs'            : len(y),\n",
    "        'alpha_daily'      : alpha,\n",
    "        'alpha_annual'     : alpha*252,      \n",
    "        't_alpha'          : res.tvalues[0],\n",
    "        'IR_daily'         : ir_daily,\n",
    "        'IR_annual'        : ir_annual,\n",
    "        'R2_zero'          : r2_zero(y, y_hat),\n",
    "    }\n",
    "    \n",
    "    factor_names = ['MKT','SMB','HML','RMW','CMA','UMD']\n",
    "    for i, fac in enumerate(factor_names, start=1):\n",
    "        out[f'beta_{fac}'] = res.params[i]\n",
    "        out[f't_{fac}']    = res.tvalues[i]\n",
    "    \n",
    "    return out\n",
    "\n",
    "def batch_factor_analysis(\n",
    "    daily_df: pd.DataFrame,\n",
    "    factors_path: str,\n",
    "    scheme: str,\n",
    "    tc_levels=(0, 5, 10, 20, 40),\n",
    "    portfolio_types=('long_only','short_only','long_short'),\n",
    "    model_filter=None,\n",
    "    window_filter=None,\n",
    "    gross_only=False,\n",
    "    out_dir='factor_IR_results',\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a CSV file containing IR results.\n",
    "    If gross_only=True, only tc=0 is calculated; if False, all tc_levels are included.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    fac = (pd.read_csv(factors_path, parse_dates=['date'])\n",
    "             .set_index('date')\n",
    "             .sort_index())\n",
    "\n",
    "    sub = daily_df[daily_df['scheme'] == scheme].copy()\n",
    "    if model_filter is not None:\n",
    "        sub = sub[sub['model'].isin(model_filter)]\n",
    "    if window_filter is not None:\n",
    "        sub = sub[sub['window'].isin(window_filter)]\n",
    "\n",
    "    tc_iter = (0,) if gross_only else tc_levels\n",
    "    results = []\n",
    "\n",
    "    for (model, win, ptype), g in sub.groupby(['model','window','portfolio_type']):\n",
    "        g = g.sort_values('date').set_index(pd.to_datetime(g['date']))\n",
    "\n",
    "        for tc in tc_iter:\n",
    "            col = 'return' if tc == 0 else f'tc{tc}_return'\n",
    "            if col not in g.columns:\n",
    "                continue\n",
    "            port_ret = g[col]\n",
    "            stats = run_factor_regression(port_ret, fac, use_excess=True)\n",
    "            stats.update({\n",
    "                'scheme'        : scheme,\n",
    "                'model'         : model,\n",
    "                'window'        : win,\n",
    "                'portfolio_type': ptype,\n",
    "                'tc_bps'        : tc,\n",
    "            })\n",
    "            results.append(stats)\n",
    "\n",
    "    df_out = pd.DataFrame(results)[[\n",
    "        'scheme','model','window','portfolio_type','tc_bps','N_obs',\n",
    "        'alpha_daily','alpha_annual','t_alpha',\n",
    "        'IR_daily','IR_annual','R2_zero',\n",
    "        'beta_MKT','t_MKT','beta_SMB','t_SMB',\n",
    "        'beta_HML','t_HML','beta_RMW','t_RMW',\n",
    "        'beta_CMA','t_CMA','beta_UMD','t_UMD'\n",
    "    ]]\n",
    "\n",
    "    tag = 'gross' if gross_only else 'net'\n",
    "    fname = f'5_factor_analysis_{scheme}_{tag}.csv'\n",
    "    df_out.to_csv(os.path.join(out_dir, fname), index=False)\n",
    "    print(f'[Saved] {fname}')\n",
    "    return df_out\n",
    "\n",
    "\n",
    "\n",
    "def run_all_factor_tests(vw_csv=\"portfolio_daily_series_VW.csv\",\n",
    "                         ew_csv=\"portfolio_daily_series_EW.csv\",\n",
    "                         factor_csv=\"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/5_Factors_Plus_Momentum.csv\",\n",
    "                         save_dir=\"results\",\n",
    "                         y_is_excess=True,\n",
    "                         hac_lags=5,\n",
    "                         save_txt=True):\n",
    "    vw_df = pd.read_csv(vw_csv)\n",
    "    ew_df = pd.read_csv(ew_csv)\n",
    "\n",
    "    vw_gross = batch_factor_analysis(\n",
    "        vw_df, factor_csv, scheme='VW', gross_only=True)\n",
    "    vw_net   = batch_factor_analysis(\n",
    "        vw_df, factor_csv, scheme='VW', gross_only=False)\n",
    "\n",
    "    ew_gross = batch_factor_analysis(\n",
    "        ew_df, factor_csv, scheme='EW', gross_only=True)\n",
    "    ew_net   = batch_factor_analysis(\n",
    "        ew_df, factor_csv, scheme='EW', gross_only=False)\n",
    "\n",
    "    return vw_gross, vw_net, ew_gross, ew_net\n",
    "    \n",
    "\n",
    "vw_gross, vw_net, ew_gross, ew_net = run_all_factor_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish: portfolio_daily_series_VW_with_rf.csv\n",
      "Finish: portfolio_daily_series_EW_with_rf.csv\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "rf_file = \"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/CRSP_2016_2024_top50_with_exret.csv\"\n",
    "vw_file = \"portfolio_daily_series_VW.csv\"\n",
    "ew_file = \"portfolio_daily_series_EW.csv\"\n",
    "\n",
    "# Load risk-free rate data\n",
    "rf_df = pd.read_csv(rf_file, usecols=[\"date\", \"rf\"])\n",
    "rf_df[\"date\"] = pd.to_datetime(rf_df[\"date\"])\n",
    "rf_dict = dict(zip(rf_df[\"date\"], rf_df[\"rf\"]))\n",
    "\n",
    "\n",
    "def adjust_returns_with_rf_grouped(file_path, output_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], format='mixed', dayfirst=True)\n",
    "\n",
    "    return_cols = [col for col in df.columns if \"return\" in col and \"cumul\" not in col]\n",
    "\n",
    "    order = [\"long_only\", \"short_only\", \"long_short\"]\n",
    "    df[\"portfolio_type\"] = pd.Categorical(df[\"portfolio_type\"], categories=order, ordered=True)\n",
    "\n",
    "    df_list = []\n",
    "    for _, group in df.groupby([\"scheme\", \"model\", \"window\", \"portfolio_type\"], sort=False):\n",
    "        group = group.sort_values(\"date\").copy()\n",
    "        for col in return_cols:\n",
    "            group[col] = group.apply(lambda row: row[col] + rf_dict.get(row[\"date\"], 0), axis=1)\n",
    "            cum_col = col.replace(\"return\", \"cumulative\")\n",
    "            group[cum_col] = np.log1p(group[col]).cumsum()\n",
    "        df_list.append(group)\n",
    "\n",
    "    df_new = pd.concat(df_list).sort_values([\"scheme\", \"model\", \"window\", \"portfolio_type\", \"date\"])\n",
    "    df_new.to_csv(output_path, index=False)\n",
    "    print(f\"Finished: {output_path}\")\n",
    "\n",
    "adjust_returns_with_rf_grouped(vw_file, \"portfolio_daily_series_VW_with_rf.csv\")\n",
    "adjust_returns_with_rf_grouped(ew_file, \"portfolio_daily_series_EW_with_rf.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All figures have been generated and saved to: Baseline_Portfolio/\n"
     ]
    }
   ],
   "source": [
    "# ======== Download S&P500 (2016-2024) ========\n",
    "sp500 = yf.download(\"^GSPC\", start=\"2016-01-01\", end=\"2024-12-31\")\n",
    "price_col = \"Adj Close\" if \"Adj Close\" in sp500.columns else \"Close\"\n",
    "sp500[\"daily_return\"] = sp500[price_col].pct_change().fillna(0)\n",
    "# Cumulative log return\n",
    "sp500[\"cum_return\"] = np.cumsum(np.log1p(sp500[\"daily_return\"]))\n",
    "sp500 = sp500[[\"cum_return\"]]\n",
    "sp500.index = pd.to_datetime(sp500.index)\n",
    "\n",
    "# ======== Configuration ========\n",
    "files = [\n",
    "    (\"VW\", \"portfolio_daily_series_VW_with_rf.csv\"),\n",
    "    (\"EW\", \"portfolio_daily_series_EW_with_rf.csv\")\n",
    "]\n",
    "tc_levels = [0, 5, 10, 20, 40]      # Transaction cost (bps)\n",
    "windows = [5, 21, 252, 512]         # Window size\n",
    "strategies = [\"long_only\", \"short_only\", \"long_short\"]\n",
    "\n",
    "output_dir = \"Baseline_Portfolio\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Economic event periods (for shading)\n",
    "crisis_periods = [\n",
    "    (datetime(2018, 6, 1), datetime(2019, 1, 1), \"US-China Trade War\"),\n",
    "    (datetime(2020, 2, 1), datetime(2020, 7, 1), \"COVID-19\"),\n",
    "    (datetime(2022, 2, 1), datetime(2022, 6, 1), \"Russia-Ukraine War\"),\n",
    "    (datetime(2023, 1, 1), datetime(2023, 4, 1), \"US Bank Crisis\"),\n",
    "]\n",
    "\n",
    "def plot_comparison_styled(df, scheme, tc, window):\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    model_names = df[\"model\"].unique()\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(model_names)))\n",
    "\n",
    "    offset_step = 0.02\n",
    "\n",
    "    for i, strat in enumerate(strategies, 1):\n",
    "        ax = plt.subplot(3, 1, i)\n",
    "\n",
    "        plt.plot(sp500.index, sp500[\"cum_return\"],\n",
    "                 color=\"black\", lw=2.5, label=\"S&P500 (Total Return)\", zorder=10)\n",
    "\n",
    "        for idx, model_name in enumerate(model_names):\n",
    "            sub = df[(df[\"window\"] == window) &\n",
    "                     (df[\"portfolio_type\"] == strat) &\n",
    "                     (df[\"model\"] == model_name)].sort_values(\"date\")\n",
    "            if sub.empty:\n",
    "                continue\n",
    "\n",
    "            if tc == 0:\n",
    "                ret_col = \"return\"\n",
    "            else:\n",
    "                ret_col = f\"tc{tc}_return\"\n",
    "\n",
    "            if ret_col not in sub.columns:\n",
    "                continue\n",
    "\n",
    "            log_cum = np.cumsum(np.log1p(sub[ret_col].values))\n",
    "\n",
    "            y_shift = idx * offset_step\n",
    "            plt.plot(sub[\"date\"], log_cum + y_shift,\n",
    "                     label=f\"{model_name} ({strat.replace('_',' ').title()})\",\n",
    "                     lw=2, color=colors[idx], alpha=0.9)\n",
    "\n",
    "        for start, end, label in crisis_periods:\n",
    "            ax.axvspan(start, end, color='grey', alpha=0.3)\n",
    "            ax.text(start + pd.Timedelta(days=10),\n",
    "                    ax.get_ylim()[1]*0.92, label, fontsize=8, color='grey')\n",
    "        ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "        ax.set_ylabel(\"Cumulative log return (start = 0)\")\n",
    "        ax.set_title(f\"{scheme} | Window={window} | Strategy={strat} | TC={tc} bps\")\n",
    "        ax.grid(alpha=0.3)\n",
    "        plt.xticks(rotation=30)\n",
    "        plt.legend(bbox_to_anchor=(1.04, 1), loc='upper left', fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fname = f\"{scheme}_window{window}_TC{tc}_logreturn_offset.png\"\n",
    "    plt.savefig(os.path.join(output_dir, fname), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Main loop to generate all figures\n",
    "for scheme, file_path in files:\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    for tc in tc_levels:\n",
    "        for window in windows:\n",
    "            plot_comparison_styled(df, scheme, tc, window)\n",
    "\n",
    "print(f\"All figures have been generated and saved to: {output_dir}/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Update] ΔSharpe has been written to portfolio_results_daily_rebalance_VW.csv\n",
      "[Update] ΔSharpe has been written to portfolio_results_daily_rebalance_EW.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load R²_zero from portfolio_metrics.csv\n",
    "metrics_df = pd.read_csv(\"portfolio_metrics.csv\")[[\"Model\", \"Window\", \"R²_zero\"]]\n",
    "metrics_df.rename(columns={\"Model\": \"model\", \"Window\": \"window\"}, inplace=True)\n",
    "\n",
    "# Process VW/EW result files\n",
    "for fname in [\"portfolio_results_daily_rebalance_VW.csv\", \"portfolio_results_daily_rebalance_EW.csv\"]:\n",
    "    df = pd.read_csv(fname)\n",
    "\n",
    "    # Merge R²_zero by model and window\n",
    "    df = df.merge(metrics_df, on=[\"model\", \"window\"], how=\"left\")\n",
    "\n",
    "    rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        r2 = float(row[\"R²_zero\"]) if not pd.isna(row[\"R²_zero\"]) else 0.0\n",
    "        if row[\"portfolio_type\"] == \"long_only\":\n",
    "            d_sr, sr_star = delta_sharpe(r2, SR_MKT_EX)\n",
    "            row[\"ΔSharpe\"]  = d_sr\n",
    "            row[\"Sharpe*\"]  = sr_star\n",
    "            row[\"baseline\"] = f\"SPX_excess ({SR_MKT_EX:.2f})\"\n",
    "        else:\n",
    "            d_sr, sr_star = delta_sharpe(r2, 0)\n",
    "            row[\"ΔSharpe\"]  = d_sr\n",
    "            row[\"Sharpe*\"]  = sr_star\n",
    "            row[\"baseline\"] = \"cash (0)\"\n",
    "        rows.append(row)\n",
    "\n",
    "    pd.DataFrame(rows).to_csv(fname, index=False)\n",
    "    print(f\"[Update] ΔSharpe has been written to {fname}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-mac)",
   "language": "python",
   "name": "tf-mac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
