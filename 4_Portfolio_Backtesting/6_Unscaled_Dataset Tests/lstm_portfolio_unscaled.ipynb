{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps (Apple Metal GPU)\n",
      " MPS cache cleared\n"
     ]
    }
   ],
   "source": [
    "# Numerical computation & data processing\n",
    "import os\n",
    "import copy\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine learning & deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import optuna\n",
    "\n",
    "# Statistics & finance tools\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import f as f_dist\n",
    "import yfinance as yf\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from typing import Optional\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration - prefer MPS (Mac GPU), then CUDA, then CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(f\"Using device: {device} (Apple Metal GPU)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Using device: {device} (NVIDIA GPU)\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(f\"Using device: {device} (CPU only)\")\n",
    "\n",
    "# MPS specific settings\n",
    "if device.type == 'mps':\n",
    "    torch.mps.empty_cache()\n",
    "    print(\"MPS cache cleared\")\n",
    "\n",
    "def get_device_info():\n",
    "    \"\"\"Get device information\"\"\"\n",
    "    if device.type == 'mps':\n",
    "        return f\"Apple Metal GPU (MPS) - Mac M-series chip\"\n",
    "    elif device.type == 'cuda':\n",
    "        return f\"NVIDIA GPU: {torch.cuda.get_device_name()}\"\n",
    "    else:\n",
    "        return \"CPU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] S&P500 Excess Sharpe (2016–24) = 0.652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ========== Core Function Definitions ==========\n",
    "def r2_zero(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute zero-based R² (baseline is 0)\n",
    "    y_true: array of true values (N,)\n",
    "    y_pred: array of predicted values (N,)\n",
    "    \"\"\"\n",
    "    rss = np.sum((y_true - y_pred)**2)  \n",
    "    tss = np.sum(y_true**2)            \n",
    "    return 1 - rss / tss\n",
    "\n",
    "def calc_ic_daily(df, method='spearman'):\n",
    "    \"\"\"\n",
    "    Calculate daily cross-sectional RankIC.\n",
    "    df: must contain ['signal_date','y_true','y_pred']\n",
    "    \"\"\"\n",
    "    ics = (df.groupby('signal_date')\n",
    "             .apply(lambda g: g['y_pred'].corr(g['y_true'], method=method))\n",
    "             .dropna())\n",
    "    mean_ic = ics.mean()\n",
    "    std_ic  = ics.std(ddof=1)\n",
    "    t_ic    = mean_ic / (std_ic / np.sqrt(len(ics))) if std_ic > 0 else np.nan\n",
    "    pos_ratio = (ics > 0).mean()  # Proportion of days with positive correlation\n",
    "    return mean_ic, t_ic, pos_ratio, ics\n",
    "\n",
    "def annual_sharpe(rets, freq=252):\n",
    "    mu = float(np.mean(rets)) * freq\n",
    "    sd = float(np.std(rets, ddof=1)) * np.sqrt(freq)\n",
    "    return mu / sd if sd > 0 else 0\n",
    "\n",
    "def delta_sharpe(r2_zero: float, sr_base: float):\n",
    "    \"\"\"\n",
    "    If r2_zero <= 0   → ΔSharpe = 0, Sharpe* = sr_base\n",
    "    If r2_zero >= 1   → ΔSharpe = 0, Sharpe* = sr_base (extreme case fallback)\n",
    "    Otherwise, use the original formula\n",
    "    \"\"\"\n",
    "    if (r2_zero <= 0) or (r2_zero >= 1):\n",
    "        return 0.0, sr_base\n",
    "    sr_star = np.sqrt(sr_base ** 2 + r2_zero) / np.sqrt(1 - r2_zero)\n",
    "    return sr_star - sr_base, sr_star\n",
    "\n",
    "# Load risk-free rate & calculate S&P500 Excess Sharpe\n",
    "rf_file = \"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/CRSP_2016_2024_top50_with_exret.csv\"\n",
    "rf_df = pd.read_csv(rf_file, usecols=[\"date\", \"rf\"])\n",
    "rf_df[\"date\"] = pd.to_datetime(rf_df[\"date\"])\n",
    "rf_df = rf_df.drop_duplicates(\"date\").set_index(\"date\").sort_index()\n",
    "rf_series = rf_df[\"rf\"].astype(float)\n",
    "\n",
    "px = yf.download(\"^GSPC\", start=\"2016-01-01\", end=\"2024-12-31\")[\"Close\"]\n",
    "sp_ret = px.pct_change().dropna()\n",
    "rf_align = rf_series.reindex(sp_ret.index).fillna(method=\"ffill\")\n",
    "sp_excess = sp_ret.values - rf_align.values\n",
    "\n",
    "SR_MKT_EX = annual_sharpe(sp_excess)\n",
    "print(f\"[INFO] S&P500 Excess Sharpe (2016–24) = {SR_MKT_EX:.3f}\")\n",
    "\n",
    "def calc_directional_metrics(y_true, y_pred, permnos=None):\n",
    "    \"\"\"\n",
    "    - Sample-level sign prediction\n",
    "    - If grouped by stock, compute Overall, Up, Down for each stock and then average\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    if permnos is None:\n",
    "        s_true = np.sign(y_true)\n",
    "        s_pred = np.sign(y_pred)\n",
    "        mask = s_true != 0\n",
    "        s_true = s_true[mask]\n",
    "        s_pred = s_pred[mask]\n",
    "\n",
    "        overall_acc = np.mean(s_true == s_pred)\n",
    "\n",
    "        up_mask = s_true > 0\n",
    "        down_mask = s_true < 0\n",
    "        up_acc = np.mean(s_true[up_mask] == s_pred[up_mask]) if np.any(up_mask) else 0\n",
    "        down_acc = np.mean(s_true[down_mask] == s_pred[down_mask]) if np.any(down_mask) else 0\n",
    "\n",
    "    else:\n",
    "        df = pd.DataFrame({\"permno\": permnos, \"yt\": y_true, \"yp\": y_pred})\n",
    "        overall_accs = []\n",
    "        up_accs = []\n",
    "        down_accs = []\n",
    "\n",
    "        for _, g in df.groupby(\"permno\"):\n",
    "            s_true = np.sign(g[\"yt\"].values)\n",
    "            s_pred = np.sign(g[\"yp\"].values)\n",
    "            mask = s_true != 0\n",
    "            s_true = s_true[mask]\n",
    "            s_pred = s_pred[mask]\n",
    "            if len(s_true) == 0:\n",
    "                continue\n",
    "            overall_accs.append(np.mean(s_true == s_pred))\n",
    "\n",
    "            up_mask = s_true > 0\n",
    "            down_mask = s_true < 0\n",
    "            up_accs.append(np.mean(s_true[up_mask] == s_pred[up_mask]) if np.any(up_mask) else np.nan)\n",
    "            down_accs.append(np.mean(s_true[down_mask] == s_pred[down_mask]) if np.any(down_mask) else np.nan)\n",
    "\n",
    "        overall_acc = np.nanmean(overall_accs)\n",
    "        up_acc = np.nanmean(up_accs)\n",
    "        down_acc = np.nanmean(down_accs)\n",
    "\n",
    "    return overall_acc, up_acc, down_acc\n",
    "\n",
    "def regression_metrics(y_true, y_pred, k, meta=None, permnos=None):\n",
    "    \"\"\"\n",
    "    Includes:\n",
    "    - Regression metrics\n",
    "    - Pointwise directional accuracy\n",
    "    - Market cap group metrics\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    n = len(y_true)\n",
    "\n",
    "    r2 = r2_zero(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "\n",
    "    dir_acc, up_acc, down_acc = calc_directional_metrics(y_true, y_pred, permnos)\n",
    "\n",
    "    metrics = {\n",
    "        \"R²_zero\": r2,\n",
    "        \"RMSE\": rmse,\n",
    "        \"MAE\": mae,\n",
    "        \"MSE\": mse,\n",
    "        \"Directional Accuracy\": dir_acc,\n",
    "        \"Up_Directional_Acc\": up_acc,\n",
    "        \"Down_Directional_Acc\": down_acc\n",
    "    }\n",
    "\n",
    "    if meta is not None and \"MKTCAP_PERCENTILE\" in meta:\n",
    "        top_mask = meta[\"MKTCAP_PERCENTILE\"] >= 0.75\n",
    "        bottom_mask = meta[\"MKTCAP_PERCENTILE\"] <= 0.25\n",
    "\n",
    "        if np.any(top_mask):\n",
    "            yt_top = y_true[top_mask]\n",
    "            yp_top = y_pred[top_mask]\n",
    "            perm_top = permnos[top_mask] if permnos is not None else None\n",
    "            r2_top = r2_zero(yt_top, yp_top)\n",
    "            rmse_top = np.sqrt(mean_squared_error(yt_top, yp_top))\n",
    "            mae_top = mean_absolute_error(yt_top, yp_top)\n",
    "            mse_top = mean_squared_error(yt_top, yp_top)\n",
    "            dir_top, up_top, down_top = calc_directional_metrics(yt_top, yp_top, perm_top)\n",
    "            metrics.update({\n",
    "                \"Top25_R2_zero\": r2_top,\n",
    "                \"Top25_RMSE\": rmse_top,\n",
    "                \"Top25_MAE\": mae_top,\n",
    "                \"Top25_MSE\": mse_top,\n",
    "                \"Top25_Dir_Acc\": dir_top,\n",
    "                \"Top25_Up_Acc\": up_top,\n",
    "                \"Top25_Down_Acc\": down_top\n",
    "            })\n",
    "\n",
    "        if np.any(bottom_mask):\n",
    "            yt_bot = y_true[bottom_mask]\n",
    "            yp_bot = y_pred[bottom_mask]\n",
    "            perm_bot = permnos[bottom_mask] if permnos is not None else None\n",
    "            r2_bot = r2_zero(yt_bot, yp_bot)\n",
    "            rmse_bot = np.sqrt(mean_squared_error(yt_bot, yp_bot))\n",
    "            mae_bot = mean_absolute_error(yt_bot, yp_bot)\n",
    "            mse_bot = mean_squared_error(yt_bot, yp_bot)\n",
    "            dir_bot, up_bot, down_bot = calc_directional_metrics(yt_bot, yp_bot, perm_bot)\n",
    "            metrics.update({\n",
    "                \"Bottom25_R2_zero\": r2_bot,\n",
    "                \"Bottom25_RMSE\": rmse_bot,\n",
    "                \"Bottom25_MAE\": mae_bot,\n",
    "                \"Bottom25_MSE\": mse_bot,\n",
    "                \"Bottom25_Dir_Acc\": dir_bot,\n",
    "                \"Bottom25_Up_Acc\": up_bot,\n",
    "                \"Bottom25_Down_Acc\": down_bot\n",
    "            })\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def f_statistic(y_true, y_pred, k):\n",
    "    \"\"\"Return F statistic and corresponding p-value\"\"\"\n",
    "    n   = len(y_true)\n",
    "    rss = np.sum((y_true - y_pred) ** 2)\n",
    "    tss = np.sum(y_true ** 2)\n",
    "    r2  = 1 - rss / tss\n",
    "    if (r2 <= 0) or (n <= k):\n",
    "        return 0.0, 1.0\n",
    "    F = (r2 / k) / ((1 - r2) / (n - k))\n",
    "    p = f_dist.sf(F, k, n - k)\n",
    "    return F, p\n",
    "\n",
    "def overall_interval_metrics_method1(y_all, yhat_all, k, permnos_all=None, meta_all=None):\n",
    "    \"\"\"\n",
    "    Method 1: Compute metrics for the entire interval at once (2016-2024, all samples concatenated)\n",
    "    Returns: a dict, can be directly passed to save_metrics()\n",
    "    \"\"\"\n",
    "    base = regression_metrics(\n",
    "        y_true=y_all, \n",
    "        y_pred=yhat_all, \n",
    "        k=k, \n",
    "        meta=meta_all, \n",
    "        permnos=permnos_all\n",
    "    )\n",
    "    F, p = f_statistic(y_all, yhat_all, k)\n",
    "    base[\"F_stat\"]     = F\n",
    "    base[\"F_pvalue\"]   = p\n",
    "    base[\"N_obs\"] = len(y_all)\n",
    "    \n",
    "    delta_cash, sr_star_cash = delta_sharpe(base[\"R²_zero\"], sr_base=0)\n",
    "    base[\"ΔSharpe_cash\"]      = delta_cash\n",
    "    base[\"Sharpe*_cash\"]      = sr_star_cash\n",
    "\n",
    "    delta_mkt , sr_star_mkt  = delta_sharpe(base[\"R²_zero\"], sr_base=SR_MKT_EX)\n",
    "    base[\"ΔSharpe_mkt\"]       = delta_mkt\n",
    "    base[\"Sharpe*_mkt\"]       = sr_star_mkt\n",
    "    \n",
    "    return base\n",
    "\n",
    "def sortino_ratio(rets, freq=252):\n",
    "    \"\"\"Compute Sortino Ratio\"\"\"\n",
    "    downside = rets[rets < 0]\n",
    "    if len(downside) == 0:\n",
    "        return np.inf\n",
    "    mu = rets.mean() * freq\n",
    "    sigma = np.sqrt((downside ** 2).mean()) * np.sqrt(freq)\n",
    "    return mu / sigma\n",
    "\n",
    "def cvar(rets, alpha=0.95):\n",
    "    \"\"\"Compute CVaR\"\"\"\n",
    "    q = np.quantile(rets, 1 - alpha)\n",
    "    return rets[rets <= q].mean()\n",
    "\n",
    "def save_predictions(model_name, window_size, y_true, y_pred, permnos, path=\"predictions/\"):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        \"PERMNO\": permnos,\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred\n",
    "    })\n",
    "\n",
    "    filename = f\"{model_name}_w{window_size}.csv\"\n",
    "    df.to_csv(os.path.join(path, filename), index=False)\n",
    "    print(f\"[Save] {filename}\")\n",
    "\n",
    "def save_metrics(metrics_dict, name, window, path=\"results.csv\"):\n",
    "    \"\"\"Save evaluation metrics\"\"\"\n",
    "    row = pd.DataFrame([metrics_dict])\n",
    "    row.insert(0, \"Model\", name)\n",
    "    row.insert(1, \"Window\", window)\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path)\n",
    "        df = df[~((df[\"Model\"] == name) & (df[\"Window\"] == window))]\n",
    "        df = pd.concat([df, row], ignore_index=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        print(f\"[Update] Metrics updated for {name} w={window}\")\n",
    "    else:\n",
    "        row.to_csv(path, index=False)\n",
    "        print(f\"[Create] New metrics file created with {name} w={window}\")\n",
    "\n",
    "def get_quarter_periods(start_year=2015, end_year=2024):\n",
    "    \"\"\"Generate list of quarters\"\"\"\n",
    "    quarters = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for q in range(1, 5):\n",
    "            quarters.append((year, q))\n",
    "    return quarters\n",
    "\n",
    "def save_model_with_quarter(lstm_wrapper, name, window, year, quarter,\n",
    "                            path=\"models/\"):\n",
    "    \"\"\"\n",
    "    Safely save LSTM model parameters, handle MPS device compatibility and file size optimization\n",
    "    \"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    lstm_wrapper.clear_loss_history()\n",
    "    \n",
    "    original_device = lstm_wrapper.training_device\n",
    "    lstm_wrapper.model.to('cpu')\n",
    "    \n",
    "    pth_file = f\"{name}_w{window}_{year}Q{quarter}.pth\"\n",
    "    torch.save(lstm_wrapper.model.state_dict(),\n",
    "               os.path.join(path, pth_file))\n",
    "    \n",
    "    lstm_wrapper.model.to(original_device)\n",
    "    \n",
    "    print(f\"[Saved] {pth_file}\")\n",
    "    \n",
    "def load_datasets(npz_path):\n",
    "    \"\"\"Load dataset\"\"\"\n",
    "    data = np.load(npz_path, allow_pickle=True) \n",
    "    datasets = {}\n",
    "    for key in data.files:\n",
    "        datasets[key] = data[key]\n",
    "    return datasets\n",
    "\n",
    "def find_coef_step(model):\n",
    "    \"\"\"\n",
    "    Get model coefficients, handle Pipeline and standalone models.\n",
    "    For nonlinear models (e.g., MLP), return None to indicate no coefficients.\n",
    "    \"\"\"\n",
    "    if hasattr(model, 'named_steps'):\n",
    "        for name, est in model.named_steps.items():\n",
    "            if hasattr(est, 'coef_'):\n",
    "                return name, est\n",
    "            if isinstance(est, Pipeline):\n",
    "                for subname, subest in est.named_steps.items():\n",
    "                    if hasattr(subest, 'coef_'):\n",
    "                        return f\"{name}__{subname}\", subest\n",
    "        return None, None\n",
    "    else:\n",
    "        if hasattr(model, 'coef_'):\n",
    "            return 'model', model\n",
    "    \n",
    "    raise ValueError(\"No estimator with coef_ found in model\")\n",
    "    \n",
    "def train_or_skip(model, train_loader, valid_loader, window_size, year, quarter, **train_kwargs):\n",
    "    save_path = f\"models/NN1_w{window_size}_{year}Q{quarter}.pth\"\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"[Skip Training] Model already exists: {save_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"[Training] Start training model: {save_path}\")\n",
    "    train_model(model, train_loader, valid_loader, **train_kwargs)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"[Done] Model saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 0) Utilities (keep the same interface as the original script)\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def get_default_device() -> torch.device:\n",
    "    \"\"\"Automatically select GPU / MPS / CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "    \n",
    "DEVICE_TUNE  = torch.device(\"cpu\")\n",
    "DEVICE_TRAIN = get_default_device()\n",
    "\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    \"\"\"Release GPU / MPS memory, no-op for CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Early Stopping\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Same as the original API, just set default patience to 10\"\"\"\n",
    "\n",
    "    def __init__(self, patience: int = 10, min_delta: float = 0.0, restore_best_weights: bool = True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_loss: Optional[float] = None\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def __call__(self, val_loss: float, model: nn.Module) -> bool:\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self._save(model)\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self._save(model)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "\n",
    "        if self.counter >= self.patience:\n",
    "            if self.restore_best_weights and self.best_weights is not None:\n",
    "                model.load_state_dict(self.best_weights)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _save(self, model: nn.Module):\n",
    "        # Move weights to CPU, replace original copy.deepcopy\n",
    "        self.best_weights = {\n",
    "            k: v.detach().to(\"cpu\")\n",
    "            for k, v in model.state_dict().items()\n",
    "        }\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) LSTM Base Model\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        seq_len: int,\n",
    "        hidden_size: int = 64,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.2,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.features_per_ts = input_size // seq_len\n",
    "        assert (\n",
    "            input_size % seq_len == 0\n",
    "        ), \"input_size must be divisible by seq_len for reshape -> (B, seq_len, feat_per_ts)\"\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.features_per_ts,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_weights(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LSTM):\n",
    "            for name, p in m.named_parameters():\n",
    "                if \"weight\" in name:\n",
    "                    nn.init.xavier_uniform_(p)\n",
    "                else:\n",
    "                    nn.init.constant_(p, 0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, input_size)  OR (B, seq_len, feat_per_ts)\n",
    "        if x.ndim == 2:\n",
    "            bsz = x.size(0)\n",
    "            x = x.view(bsz, self.seq_len, self.features_per_ts)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last = lstm_out[:, -1, :]  # (B, hidden)\n",
    "        last = self.dropout(last)\n",
    "        return self.out(last).squeeze(-1)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) LSTM Wrapper\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "class LSTMWrapper:\n",
    "   \n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        seq_len: int,\n",
    "        hidden_size: int = 64,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.2,\n",
    "        learning_rate: float = 1e-3,\n",
    "        batch_size: int = 256,\n",
    "        max_epochs: int = 40,\n",
    "        warm_start_epochs: int = 10,\n",
    "        training_device: Optional[torch.device] = None,\n",
    "    ):\n",
    "        self.input_size = input_size\n",
    "        self.seq_len = seq_len\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.max_epochs = max_epochs\n",
    "        self.warm_start_epochs = warm_start_epochs\n",
    "        self.training_device = training_device or get_default_device()\n",
    "\n",
    "        self._build_model()\n",
    "        self.is_fitted = False\n",
    "        self.loss_history: dict[str, list[float]] = {}\n",
    "\n",
    "    def _build_model(self):\n",
    "        self.model = LSTMModel(\n",
    "            input_size=self.input_size,\n",
    "            seq_len=self.seq_len,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=self.dropout,\n",
    "        ).to(self.training_device)\n",
    "        self._init_training_components()\n",
    "\n",
    "    def _init_training_components(self):\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode=\"min\", factor=0.5, patience=5, verbose=False)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.early_stopping = EarlyStopping(patience=5)\n",
    "\n",
    "    def _make_loaders(self, X, y, validation_split):\n",
    "        val_size = int(len(X) * validation_split) if validation_split > 0 else max(1, int(len(X) * 0.1))\n",
    "        X_train, X_val = X[:-val_size], X[-val_size:]\n",
    "        y_train, y_val = y[:-val_size], y[-val_size:]\n",
    "\n",
    "        train_ds = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
    "        val_ds = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))\n",
    "        train_loader = DataLoader(train_ds, batch_size=self.batch_size, shuffle=False, drop_last=True)\n",
    "        val_loader = DataLoader(val_ds, batch_size=self.batch_size, shuffle=False)\n",
    "        return train_loader, val_loader\n",
    "\n",
    "    def fit(self, X, y, validation_split: float = 0.1, warm_start: bool = False, verbose: bool = True):\n",
    "        if not warm_start:\n",
    "            self._build_model()\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"    [Warm Start] continue training existing LSTM weights ...\")\n",
    "            self.early_stopping = EarlyStopping(patience=5)\n",
    "\n",
    "        train_loader, val_loader = self._make_loaders(X, y, validation_split)\n",
    "        epochs = self.warm_start_epochs if warm_start else self.max_epochs\n",
    "        if verbose:\n",
    "            print(f\"    Training LSTM for {epochs} epochs on {self.training_device}\")\n",
    "\n",
    "        losses = {\"train\": [], \"val\": []}\n",
    "        for ep in range(epochs):\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            for bx, by in train_loader:\n",
    "                bx, by = bx.to(self.training_device), by.to(self.training_device)\n",
    "                self.optimizer.zero_grad()\n",
    "                preds = self.model(bx)\n",
    "                loss = self.criterion(preds, by)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            train_loss /= len(train_loader)\n",
    "\n",
    "            self.model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for bx, by in val_loader:\n",
    "                    bx, by = bx.to(self.training_device), by.to(self.training_device)\n",
    "                    loss = self.criterion(self.model(bx), by)\n",
    "                    val_loss += loss.item()\n",
    "            val_loss /= len(val_loader)\n",
    "\n",
    "            losses[\"train\"].append(train_loss)\n",
    "            losses[\"val\"].append(val_loss)\n",
    "\n",
    "            if verbose and ((ep + 1) % 5 == 0 or ep == 0):\n",
    "                print(f\"    Epoch {ep+1:3d}/{epochs}: Train={train_loss:.6f}, Val={val_loss:.6f}\")\n",
    "\n",
    "            self.scheduler.step(val_loss)\n",
    "            if self.early_stopping(val_loss, self.model):\n",
    "                if verbose:\n",
    "                    print(f\"    Early-stopped at epoch {ep+1}\")\n",
    "                break\n",
    "\n",
    "            if self.training_device.type == \"mps\" and (ep + 1) % 5 == 0:\n",
    "                torch.mps.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "        self.loss_history = losses\n",
    "        clear_memory()\n",
    "        self.is_fitted = True\n",
    "\n",
    "    def predict(self, X):\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Call fit() before predict()\")\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.FloatTensor(X).to(self.training_device)\n",
    "            preds = self.model(X_tensor).cpu().numpy()\n",
    "        return preds\n",
    "\n",
    "    def partial_fit(self, X_new, y_new, validation_split: float = 0.1, extra_epochs: int = 5, verbose: bool = True):\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Must fit() once before partial_fit()\")\n",
    "        self.early_stopping = EarlyStopping(patience=5)\n",
    "        if verbose:\n",
    "            print(f\"    Partial-fit on {len(X_new)} samples for {extra_epochs} epochs ...\")\n",
    "\n",
    "        train_loader, val_loader = self._make_loaders(X_new, y_new, validation_split)\n",
    "        for ep in range(extra_epochs):\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            for bx, by in train_loader:\n",
    "                bx, by = bx.to(self.training_device), by.to(self.training_device)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.criterion(self.model(bx), by)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            train_loss /= len(train_loader)\n",
    "\n",
    "            self.model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for bx, by in val_loader:\n",
    "                    bx, by = bx.to(self.training_device), by.to(self.training_device)\n",
    "                    val_loss += self.criterion(self.model(bx), by).item()\n",
    "            val_loss /= len(val_loader)\n",
    "\n",
    "            if verbose and (ep % 2 == 0 or ep == extra_epochs - 1):\n",
    "                print(f\"        Epoch {ep+1:2d}/{extra_epochs}: Train={train_loss:.6f}, Val={val_loss:.6f}\")\n",
    "\n",
    "            self.scheduler.step(val_loss)\n",
    "            if self.training_device.type == \"mps\" and (ep + 1) % 5 == 0:\n",
    "                torch.mps.empty_cache()\n",
    "                gc.collect()\n",
    "            if self.early_stopping(val_loss, self.model):\n",
    "                if verbose:\n",
    "                    print(f\"        Early-stopped at epoch {ep+1}\")\n",
    "                break\n",
    "        clear_memory()\n",
    "\n",
    "    def clear_loss_history(self):\n",
    "        if hasattr(self, \"loss_history\"):\n",
    "            del self.loss_history\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for k, v in params.items():\n",
    "            if hasattr(self, k):\n",
    "                setattr(self, k, v)\n",
    "        self._init_training_components()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 1. Only window=5 uses Optuna tuning ===============================\n",
    "TUNED_WINDOW = 5  # Other windows share the best hyperparameters from window=5\n",
    "\n",
    "def tune_lstm_with_optuna(X, y, seq_len, n_trials: int = 10):\n",
    "    \"\"\"LSTM hyperparameter search with time series CV (only called for window=5)\"\"\"\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    total_feat = X.shape[1]\n",
    "    print(\"Start hyperparameter tuning...\")\n",
    "    def objective(trial):\n",
    "        params = dict(\n",
    "            batch_size   = trial.suggest_categorical(\"batch_size\",  [32, 64, 128]),\n",
    "            learning_rate= trial.suggest_float(\"learning_rate\",   1e-4, 1e-2, log=True),\n",
    "            dropout      = trial.suggest_float(\"dropout\",         0.0,  0.3),\n",
    "            hidden_size  = trial.suggest_categorical(\"hidden_size\", [64, 128]),\n",
    "            num_layers   = trial.suggest_int(\"num_layers\",        1, 3),\n",
    "            max_epochs   = 10,          # Use short epochs for tuning speed\n",
    "        )\n",
    "\n",
    "        cv_mse = []\n",
    "        for tr_idx, val_idx in tscv.split(X):\n",
    "            X_tr, X_val = X[tr_idx], X[val_idx]\n",
    "            y_tr, y_val = y[tr_idx], y[val_idx]\n",
    "\n",
    "            model = LSTMWrapper(\n",
    "                input_size   = total_feat,\n",
    "                seq_len      = seq_len,\n",
    "                hidden_size  = params[\"hidden_size\"],\n",
    "                num_layers   = params[\"num_layers\"],\n",
    "                dropout      = params[\"dropout\"],\n",
    "                learning_rate= params[\"learning_rate\"],\n",
    "                batch_size   = params[\"batch_size\"],\n",
    "                max_epochs   = params[\"max_epochs\"],\n",
    "                training_device  = DEVICE_TUNE,\n",
    "            )\n",
    "            model.fit(X_tr, y_tr, validation_split=0.0, warm_start=False)\n",
    "            preds = model.predict(X_val)\n",
    "            cv_mse.append(mean_squared_error(y_val, preds))\n",
    "            del model\n",
    "            clear_memory()\n",
    "        return float(np.mean(cv_mse))\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "    study.optimize(objective, n_trials=n_trials, n_jobs=1)\n",
    "\n",
    "    if study.best_trial is None:\n",
    "        # Fallback default\n",
    "        return dict(batch_size=32, learning_rate=1e-3, dropout=0.2,\n",
    "                    hidden_size=64, num_layers=2, max_epochs=25)\n",
    "\n",
    "    best = study.best_params\n",
    "    best[\"max_epochs\"] = 25          # Epochs for final training\n",
    "    print(f\"[Optuna-LSTM] best_MSE={study.best_value:.6f}, params={best}\")\n",
    "    return best\n",
    "\n",
    "# ========== 2. Model saving / loading ============================\n",
    "\n",
    "def _lstm_save_path(name: str, window: int, year: int, quarter: int | None):\n",
    "    if quarter is None:\n",
    "        return f\"models/{name}_w{window}_{year}.pth\"\n",
    "    return f\"models/{name}_w{window}_{year}Q{quarter}.pth\"\n",
    "\n",
    "\n",
    "def save_lstm_model(wrapper: LSTMWrapper,\n",
    "                    name: str, window: int, year: int, quarter: int | None = None):\n",
    "    \"\"\"\n",
    "    Save model weights and structure hyperparameters\n",
    "    \"\"\"\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    path = _lstm_save_path(name, window, year, quarter)\n",
    "\n",
    "    ckpt = {\n",
    "        \"state_dict\": wrapper.model.state_dict(),\n",
    "        \"hyper_params\": dict(\n",
    "            input_size   = wrapper.input_size,\n",
    "            seq_len      = wrapper.seq_len,\n",
    "            hidden_size  = wrapper.hidden_size,\n",
    "            num_layers   = wrapper.num_layers,\n",
    "            dropout      = wrapper.dropout,\n",
    "            learning_rate= wrapper.learning_rate,\n",
    "            batch_size   = wrapper.batch_size,\n",
    "            max_epochs   = wrapper.max_epochs,\n",
    "        )\n",
    "    }\n",
    "\n",
    "    torch.save(ckpt, path)\n",
    "    print(f\"[Saved] {path}\")\n",
    "\n",
    "\n",
    "def load_lstm_model(name: str, window: int, year: int,\n",
    "                    quarter: int | None = None,\n",
    "                    training_device: torch.device | None = None):\n",
    "    \"\"\"\n",
    "    Load checkpoint from disk and restore LSTMWrapper (including structure hyperparameters)\n",
    "    \"\"\"\n",
    "    path = _lstm_save_path(name, window, year, quarter)\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "\n",
    "    ckpt = torch.load(path, map_location=\"cpu\")\n",
    "\n",
    "    hp = ckpt[\"hyper_params\"]\n",
    "    wrapper = LSTMWrapper(\n",
    "        input_size     = hp[\"input_size\"],\n",
    "        seq_len        = hp[\"seq_len\"],\n",
    "        hidden_size    = hp[\"hidden_size\"],\n",
    "        num_layers     = hp[\"num_layers\"],\n",
    "        dropout        = hp[\"dropout\"],\n",
    "        learning_rate  = hp.get(\"learning_rate\", 1e-3),\n",
    "        batch_size     = hp.get(\"batch_size\", 32),\n",
    "        max_epochs     = hp.get(\"max_epochs\", 25),\n",
    "        training_device= training_device or DEVICE_TRAIN,\n",
    "    )\n",
    "    wrapper.model.load_state_dict(ckpt[\"state_dict\"])\n",
    "    wrapper.is_fitted = True\n",
    "    return wrapper\n",
    "\n",
    "# ========== 3. Quarterly expanding training =================================\n",
    "\n",
    "def train_lstm_models_expanding_quarterly(start_year: int = 2015, end_year: int = 2024,\n",
    "                                          window_sizes: list[int] | None = None,\n",
    "                                          npz_path: str = \"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/all_window_datasets.npz\",\n",
    "                                          n_trials_optuna: int = 10):\n",
    "    \"\"\"Same behavior as MLP version, only replaced with LSTMWrapper\"\"\"\n",
    "    if window_sizes is None:\n",
    "        window_sizes = [5, 21, 252, 512]\n",
    "\n",
    "    print(f\"Starting **LSTM** quarterly expanding training {start_year}-{end_year}\")\n",
    "    data = load_datasets(npz_path)\n",
    "\n",
    "    best_params_cache: dict[str, dict] = {}\n",
    "    quarters_to_tune = {(2020, 4)}\n",
    "\n",
    "    for window in window_sizes:\n",
    "        print(f\"\\n=== Window = {window} ===\")\n",
    "        X_train_init = data[f\"X_train_{window}\"]\n",
    "        y_train_init = data[f\"y_train_{window}\"]\n",
    "        X_test_full  = data[f\"X_test_{window}\"]\n",
    "        y_test_full  = data[f\"y_test_{window}\"]\n",
    "        meta_test    = pd.DataFrame.from_dict(data[f\"meta_test_{window}\"].item())\n",
    "        meta_test[\"ret_date\"] = pd.to_datetime(meta_test[\"ret_date\"])\n",
    "\n",
    "        total_feat = X_train_init.shape[1]\n",
    "        feat_per_ts = total_feat\n",
    "\n",
    "        # Initial tuning (only window=5 / 2015Q4)\n",
    "        cache_key = f\"LSTM_w{window}\"\n",
    "        if window == TUNED_WINDOW:\n",
    "            tuned_model = load_lstm_model(\"LSTM\", window, 2015, 4)\n",
    "            if tuned_model is not None:\n",
    "                hp = dict(\n",
    "                    hidden_size   = tuned_model.hidden_size,\n",
    "                    num_layers    = tuned_model.num_layers,\n",
    "                    dropout       = tuned_model.dropout,\n",
    "                    learning_rate = tuned_model.learning_rate,\n",
    "                    batch_size    = tuned_model.batch_size,\n",
    "                    max_epochs    = tuned_model.max_epochs,\n",
    "                )\n",
    "                print(\"[Skip-Optuna] hyper-params loaded from existing 2015Q4 model\")\n",
    "            else:\n",
    "                print(\"  - Optuna tuning on initial window...\")\n",
    "                hp = tune_lstm_with_optuna(X_train_init, y_train_init, window, n_trials_optuna)\n",
    "        else:\n",
    "            hp = None\n",
    "        best_params_cache[cache_key] = hp\n",
    "        \n",
    "        model_prev = None\n",
    "        # Quarter loop\n",
    "        for year, quarter in get_quarter_periods(start_year, end_year):\n",
    "            model_cur = load_lstm_model(\"LSTM\", window, year, quarter)\n",
    "            if model_cur is not None:\n",
    "                print(f\"[Skip] Model already trained for window={window}, {year}Q{quarter}\")\n",
    "                continue\n",
    "            if (year == start_year and quarter < 4) or (year == end_year and quarter > 3):\n",
    "                continue  \n",
    "\n",
    "            print(f\"\\n[Window {window}] {year}Q{quarter}\")\n",
    "\n",
    "            # Prepare expanding dataset\n",
    "            if not (year == start_year and quarter == 4):\n",
    "                if quarter == 1:\n",
    "                    py, pq = year - 1, 4\n",
    "                else:\n",
    "                    py, pq = year, quarter - 1\n",
    "                mask_prev = (meta_test[\"ret_date\"].dt.year == py) & (meta_test[\"ret_date\"].dt.quarter == pq)\n",
    "                if mask_prev.any():\n",
    "                    X_prev, y_prev = X_test_full[mask_prev], y_test_full[mask_prev]\n",
    "                    X_train_init = np.vstack([X_train_init, X_prev])\n",
    "                    y_train_init = np.hstack([y_train_init, y_prev])\n",
    "                    print(f\"    +{mask_prev.sum()} obs from {py}Q{pq} -> expanding size {len(y_train_init)}\")\n",
    "\n",
    "            # Hyperparameter copy / re-tune\n",
    "            cache_key = f\"LSTM_w{window}\"\n",
    "            hp = best_params_cache.get(cache_key)\n",
    "            if hp is None and window != TUNED_WINDOW:\n",
    "                hp = best_params_cache[f\"LSTM_w{TUNED_WINDOW}\"].copy()\n",
    "                best_params_cache[cache_key] = hp\n",
    "\n",
    "            if (year, quarter) in quarters_to_tune and window == TUNED_WINDOW:\n",
    "                print(\"    Re-tuning via Optuna...\")\n",
    "                hp = tune_lstm_with_optuna(X_train_init, y_train_init, window, n_trials_optuna)\n",
    "                best_params_cache[cache_key] = hp\n",
    "\n",
    "            # Load previous model for warm-start\n",
    "            model_prev = None\n",
    "            if not (year == start_year and quarter == 4):\n",
    "                if quarter == 1:\n",
    "                    prev_year, prev_quarter = year - 1, 4\n",
    "                else:\n",
    "                    prev_year, prev_quarter = year, quarter - 1\n",
    "\n",
    "                model_prev = load_lstm_model(\"LSTM\", window, prev_year, prev_quarter)\n",
    "\n",
    "            if model_prev is not None:\n",
    "                print(\"    Warm-start from last quarter model ...\")\n",
    "                model_prev.partial_fit(X_train_init, y_train_init, validation_split=0.1, extra_epochs=hp.get(\"warm_start_epochs\", 10))\n",
    "                lstm_wrap = model_prev\n",
    "            else:\n",
    "                print(\"    Cold start training ...\")\n",
    "                lstm_wrap = LSTMWrapper(\n",
    "                    input_size   = total_feat,\n",
    "                    seq_len      = window,\n",
    "                    hidden_size  = hp[\"hidden_size\"],\n",
    "                    num_layers   = hp[\"num_layers\"],\n",
    "                    dropout      = hp[\"dropout\"],\n",
    "                    learning_rate= hp[\"learning_rate\"],\n",
    "                    batch_size   = hp[\"batch_size\"],\n",
    "                    max_epochs   = hp[\"max_epochs\"],\n",
    "                    training_device       = DEVICE_TRAIN,\n",
    "                )\n",
    "                lstm_wrap.fit(X_train_init, y_train_init, validation_split=0.1, warm_start=False)\n",
    "\n",
    "            save_lstm_model(lstm_wrap, \"LSTM\", window, year, quarter)\n",
    "            del lstm_wrap\n",
    "            clear_memory()\n",
    "            \n",
    "    print(\"All LSTM quarterly expanding models trained\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Portfolio Core Class =====\n",
    "# ===== Transaction Cost Settings =====\n",
    "TC_GRID = [0.0005, 0.001, 0.002, 0.003, 0.004]  # 5, 10, 20, 30, 40 bps\n",
    "TC_TAG  = {\n",
    "    0.0005: \"tc5\",\n",
    "    0.001:  \"tc10\", \n",
    "    0.002:  \"tc20\",\n",
    "    0.003:  \"tc30\",\n",
    "    0.004:  \"tc40\"\n",
    "}\n",
    "\n",
    "class PortfolioBacktester:\n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "        \n",
    "    def calc_turnover(self, w_t, r_t, w_tp1):\n",
    "        \"\"\"Calculate turnover using the provided standard formula\"\"\"\n",
    "        if w_t is None:\n",
    "            return np.sum(np.abs(w_tp1))\n",
    "        \n",
    "        gross_ret = np.sum(w_t * r_t)\n",
    "        if abs(1 + gross_ret) < 1e-8:\n",
    "            return np.sum(np.abs(w_tp1))\n",
    "        \n",
    "        passive_weight = w_t * (1 + r_t) / (1 + gross_ret)\n",
    "        turnover = np.sum(np.abs(w_tp1 - passive_weight))\n",
    "        return turnover\n",
    "    \n",
    "    def create_portfolios_with_permno_tracking(self, signals, market_caps, permnos, top_pct=0.1, bottom_pct=0.1, weight_scheme=\"VW\"):\n",
    "        \"\"\"\n",
    "        Create portfolio weights based on signals, strictly tracking permno alignment.\n",
    "        weight_scheme: 'VW' for value-weighted, 'EW' for equal-weighted\n",
    "        \"\"\"\n",
    "        n_stocks = len(signals)\n",
    "        top_n    = max(1, int(round(n_stocks * top_pct)))\n",
    "        bottom_n = max(1, int(round(n_stocks * bottom_pct)))\n",
    "        \n",
    "        sorted_idx = np.argsort(signals)[::-1]\n",
    "        \n",
    "        top_idx = sorted_idx[:top_n]\n",
    "        bottom_idx = sorted_idx[-bottom_n:]\n",
    "        \n",
    "        portfolio_data = {}\n",
    "        \n",
    "        long_weights = np.zeros(n_stocks)\n",
    "        if len(top_idx) > 0:\n",
    "            if weight_scheme == \"VW\":\n",
    "                top_market_caps = market_caps[top_idx]\n",
    "                if np.sum(top_market_caps) > 0:\n",
    "                    long_weights[top_idx] = top_market_caps / np.sum(top_market_caps)\n",
    "            else:\n",
    "                long_weights[top_idx] = 1.0 / len(top_idx)\n",
    "        \n",
    "        portfolio_data['long_only'] = {\n",
    "            'weights': long_weights,\n",
    "            'permnos': permnos.copy(),\n",
    "            'selected_permnos': permnos[top_idx] if len(top_idx) > 0 else np.array([])\n",
    "        }\n",
    "        \n",
    "        short_weights = np.zeros(n_stocks)\n",
    "        if len(bottom_idx) > 0:\n",
    "            if weight_scheme == \"VW\":\n",
    "                bottom_market_caps = market_caps[bottom_idx]\n",
    "                if np.sum(bottom_market_caps) > 0:\n",
    "                    short_weights[bottom_idx] = -bottom_market_caps / np.sum(bottom_market_caps)\n",
    "            else:\n",
    "                short_weights[bottom_idx] = -1.0 / len(bottom_idx)\n",
    "        \n",
    "        portfolio_data['short_only'] = {\n",
    "            'weights': short_weights,\n",
    "            'permnos': permnos.copy(),\n",
    "            'selected_permnos': permnos[bottom_idx] if len(bottom_idx) > 0 else np.array([])\n",
    "        }\n",
    "        \n",
    "        # Long-Short portfolio (Top long + Bottom short)\n",
    "        ls_raw = long_weights + short_weights\n",
    "\n",
    "        gross_target = 2.0\n",
    "        current_gross = np.sum(np.abs(long_weights)) + np.sum(np.abs(short_weights))\n",
    "        scale = gross_target / current_gross if current_gross > 1e-8 else 0.0\n",
    "        ls_weights = scale * ls_raw\n",
    "\n",
    "        ls_selected_permnos = np.concatenate([\n",
    "            permnos[top_idx] if len(top_idx) > 0 else np.array([]),\n",
    "            permnos[bottom_idx] if len(bottom_idx) > 0 else np.array([])\n",
    "        ])\n",
    "\n",
    "        portfolio_data['long_short'] = {\n",
    "            'weights': ls_weights,\n",
    "            'permnos': permnos.copy(),\n",
    "            'selected_permnos': ls_selected_permnos\n",
    "        }\n",
    "\n",
    "        return portfolio_data\n",
    "    \n",
    "    def calculate_aligned_portfolio_return(self, portfolio_weights, portfolio_permnos, actual_returns, actual_permnos):\n",
    "        \"\"\"Calculate portfolio return strictly aligned by permno\"\"\"\n",
    "        aligned_returns = np.zeros(len(portfolio_permnos))\n",
    "        \n",
    "        return_dict = dict(zip(actual_permnos, actual_returns))\n",
    "        \n",
    "        for i, permno in enumerate(portfolio_permnos):\n",
    "            if permno in return_dict:\n",
    "                aligned_returns[i] = return_dict[permno]\n",
    "        \n",
    "        portfolio_return = np.sum(portfolio_weights * aligned_returns)\n",
    "        return portfolio_return, aligned_returns\n",
    "\n",
    "    def calculate_metrics(self, returns, turnover_series=None):\n",
    "        \"\"\"Calculate portfolio metrics - only returns summary metrics, not long series\"\"\"\n",
    "        returns = np.array(returns)\n",
    "        \n",
    "        annual_return = np.mean(returns) * 252\n",
    "        annual_vol = np.std(returns, ddof=1) * np.sqrt(252)\n",
    "        sharpe = annual_return / annual_vol if annual_vol > 0 else 0\n",
    "        \n",
    "        log_cum = np.cumsum(np.log1p(returns))\n",
    "        peak_log = np.maximum.accumulate(log_cum)\n",
    "        dd_log = peak_log - log_cum\n",
    "        max_drawdown = 1 - np.exp(-dd_log.max()) \n",
    "        max_1d_loss = np.min(returns) \n",
    "        \n",
    "        avg_turnover = np.mean(turnover_series) if turnover_series is not None else 0\n",
    "        \n",
    "        sortino = sortino_ratio(returns)\n",
    "        cvar95  = cvar(returns, alpha=0.95)\n",
    "\n",
    "        result = {\n",
    "            'annual_return': annual_return,\n",
    "            'annual_vol': annual_vol,\n",
    "            'sharpe': sharpe,\n",
    "            'max_drawdown': max_drawdown,\n",
    "            'max_1d_loss': max_1d_loss,\n",
    "            'avg_turnover': avg_turnover,\n",
    "            'sortino': sortino,\n",
    "            'cvar95': cvar95\n",
    "        }\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Main function for daily prediction and next-day rebalancing portfolio simulation ==========\n",
    "\n",
    "def run_portfolio_simulation_daily_rebalance(start_year=2016, end_year=2024, window_sizes=None, model_names=None,\n",
    "                                           npz_path=\"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/all_window_datasets.npz\"):\n",
    "    \"\"\"\n",
    "Portfolio simulation (daily prediction, next-day rebalancing):\n",
    "    1. Load quarterly models (trained with quarterly expanding window)\n",
    "    2. Daily prediction to daily signals\n",
    "    3. Daily portfolio construction (T+1 rebalancing, strict permno alignment)\n",
    "    4. Separate summary metrics and time series data\n",
    "    \"\"\"\n",
    "    if window_sizes is None:\n",
    "        window_sizes = [5, 21, 252, 512]\n",
    "    if model_names is None:\n",
    "        model_names = [\"LSTM\"]\n",
    "    \n",
    "    print(\"Starting Daily Rebalance Portfolio Backtesting Simulation\")\n",
    "    \n",
    "    backtester = PortfolioBacktester()\n",
    "    datasets = load_datasets(npz_path)\n",
    "    \n",
    "    summary_results = []\n",
    "    daily_series_data = []\n",
    "    pred_rows = []\n",
    "    \n",
    "    WEIGHT_SCHEMES = [\"VW\", \"EW\"]\n",
    "    \n",
    "    for window in window_sizes:\n",
    "        print(f\"Processing window size: {window}\")\n",
    "        \n",
    "        X_test = datasets[f\"X_test_{window}\"]\n",
    "        y_test = datasets[f\"y_test_{window}\"]\n",
    "        input_size = X_test.shape[1]\n",
    "        meta_test_dict = datasets[f\"meta_test_{window}\"].item()\n",
    "        meta_test = pd.DataFrame.from_dict(meta_test_dict)\n",
    "        \n",
    "        permnos_test = meta_test[\"PERMNO\"].values\n",
    "        meta_test[\"signal_date\"]  = pd.to_datetime(meta_test[\"date\"])\n",
    "        meta_test[\"ret_date\"]     = pd.to_datetime(meta_test[\"ret_date\"])\n",
    "        market_caps = meta_test.get(\"MKTCAP\", np.ones(len(permnos_test)))\n",
    "        \n",
    "        meta_test['date'] = pd.to_datetime(meta_test[\"date\"])\n",
    "        dates_test = meta_test['signal_date']\n",
    "        \n",
    "        for model_name in model_names:\n",
    "            for scheme in WEIGHT_SCHEMES:\n",
    "                all_y_true   = []\n",
    "                all_y_pred   = []\n",
    "                all_permnos  = []\n",
    "                all_meta     = []\n",
    "                print(f\"  Model: {model_name}, Scheme: {scheme}\")\n",
    "                \n",
    "                portfolio_daily_data = {\n",
    "                    'long_only': {'returns': [], 'turnovers': [], 'dates': []},\n",
    "                    'short_only': {'returns': [], 'turnovers': [], 'dates': []},\n",
    "                    'long_short': {'returns': [], 'turnovers': [], 'dates': []}\n",
    "                }\n",
    "                \n",
    "                prev_portfolio_data = {'long_only': None, 'short_only': None, 'long_short': None}\n",
    "                \n",
    "                signals_buf = {}\n",
    "                \n",
    "                for year in range(start_year, min(end_year + 1, 2025)):\n",
    "                    for quarter in range(1, 5):\n",
    "                        # Determine model file year and quarter (T+1 logic: use previous quarter's model to predict current quarter)\n",
    "                        if quarter == 1:\n",
    "                            model_file_year, model_file_quarter = year - 1, 4\n",
    "                        else:\n",
    "                            model_file_year, model_file_quarter = year, quarter - 1\n",
    "                            \n",
    "                        pth_path = f\"models/{model_name}_w{window}_{model_file_year}Q{model_file_quarter}.pth\"\n",
    "                        if not os.path.exists(pth_path):\n",
    "                            print(f\"      Skip: Model file not found {pth_path}\")\n",
    "                            continue\n",
    "                        cpu = torch.device('cpu')\n",
    "                        ckpt = torch.load(pth_path, map_location=cpu)\n",
    "\n",
    "                        hp = ckpt.get(\"hyper_params\", {})\n",
    "                        hidden_size  = hp.get(\"hidden_size\", 64)\n",
    "                        num_layers   = hp.get(\"num_layers\", 2)\n",
    "                        dropout      = hp.get(\"dropout\", 0.2)\n",
    "                        batch_size   = hp.get(\"batch_size\", 256)\n",
    "                        learning_rate= hp.get(\"learning_rate\", 1e-3)\n",
    "                        max_epochs   = hp.get(\"max_epochs\", 0)\n",
    "\n",
    "                        lstm = LSTMWrapper(\n",
    "                            input_size=input_size,\n",
    "                            seq_len=window,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            dropout=dropout,\n",
    "                            learning_rate=learning_rate,\n",
    "                            batch_size=batch_size,\n",
    "                            max_epochs=max_epochs,\n",
    "                            training_device=cpu\n",
    "                        )\n",
    "\n",
    "                        if \"state_dict\" in ckpt:\n",
    "                            state_dict = ckpt[\"state_dict\"]\n",
    "                        elif \"model_state_dict\" in ckpt:\n",
    "                            state_dict = ckpt[\"model_state_dict\"]\n",
    "                        else:\n",
    "                            state_dict = ckpt\n",
    "\n",
    "                        lstm.model.load_state_dict(state_dict, strict=False)\n",
    "                        lstm.is_fitted = True\n",
    "                        model = lstm\n",
    "\n",
    "                        quarter_mask = (\n",
    "                            (dates_test.dt.year == year) & \n",
    "                            (dates_test.dt.quarter == quarter)\n",
    "                        )\n",
    "                        if not np.any(quarter_mask):\n",
    "                            continue\n",
    "                        \n",
    "                        X_quarter = X_test[quarter_mask]\n",
    "                        y_quarter = y_test[quarter_mask]\n",
    "                        permnos_quarter = permnos_test[quarter_mask]\n",
    "                        market_caps_quarter = market_caps[quarter_mask]\n",
    "                        dates_quarter = dates_test[quarter_mask]\n",
    "                        ret_dates_quarter = meta_test.loc[quarter_mask, 'ret_date'].values\n",
    "                        \n",
    "                        df_quarter = pd.DataFrame({\n",
    "                            'signal_date': dates_quarter,\n",
    "                            'ret_date': ret_dates_quarter,\n",
    "                            'permno': permnos_quarter,\n",
    "                            'market_cap': market_caps_quarter,\n",
    "                            'actual_return': y_quarter,                 \n",
    "                            'prediction': model.predict(X_quarter)   \n",
    "                        })\n",
    "                        \n",
    "                        if scheme == 'VW':\n",
    "                            df_q_save = df_quarter[['signal_date','ret_date','permno',\n",
    "                                                    'actual_return','prediction','market_cap']].copy()\n",
    "                            df_q_save.rename(columns={'actual_return':'y_true',\n",
    "                                                      'prediction':'y_pred'}, inplace=True)\n",
    "                            df_q_save['model']  = model_name\n",
    "                            df_q_save['window'] = window\n",
    "                            pred_rows.append(df_q_save)\n",
    "                        \n",
    "                        all_y_true.append(df_quarter['actual_return'].values)\n",
    "                        all_y_pred.append(df_quarter['prediction'].values)\n",
    "                        all_permnos.append(df_quarter['permno'].values)\n",
    "                        all_meta.append(meta_test.loc[quarter_mask, :])   \n",
    "\n",
    "                        for signal_date, sig_grp in df_quarter.groupby('signal_date'):\n",
    "                            daily_signals = (\n",
    "                                sig_grp.groupby('permno')['prediction'].mean()\n",
    "                                      .to_frame('prediction')\n",
    "                                      .join(sig_grp.groupby('permno')['market_cap'].mean())\n",
    "                            )\n",
    "                            signals_buf[signal_date] = daily_signals\n",
    "\n",
    "                            prev_date = signal_date - pd.tseries.offsets.BDay(1)\n",
    "                            if prev_date not in signals_buf:\n",
    "                                continue\n",
    "\n",
    "                            sigs = signals_buf.pop(prev_date)\n",
    "                            if prev_date in signals_buf:\n",
    "                                del signals_buf[prev_date]\n",
    "\n",
    "                            ret_grp = df_quarter[df_quarter['ret_date'] == signal_date]\n",
    "                            if len(ret_grp) == 0:\n",
    "                                continue\n",
    "\n",
    "                            daily_actual_returns = (\n",
    "                                ret_grp.groupby('permno')['actual_return']\n",
    "                                       .mean()\n",
    "                                       .reindex(sigs.index, fill_value=0)\n",
    "                                       .values\n",
    "                            )\n",
    "                            daily_permnos = sigs.index.values\n",
    "\n",
    "                            portfolios_data = backtester.create_portfolios_with_permno_tracking(\n",
    "                                signals      = sigs['prediction'].values,\n",
    "                                market_caps  = sigs['market_cap'].values,\n",
    "                                permnos      = daily_permnos,\n",
    "                                weight_scheme= scheme\n",
    "                            )\n",
    "                            \n",
    "                            for portfolio_type in ['long_only', 'short_only', 'long_short']:\n",
    "                                portfolio_info = portfolios_data[portfolio_type]\n",
    "                                \n",
    "                                portfolio_return, aligned_returns = backtester.calculate_aligned_portfolio_return(\n",
    "                                    portfolio_weights=portfolio_info['weights'],\n",
    "                                    portfolio_permnos=portfolio_info['permnos'],\n",
    "                                    actual_returns=daily_actual_returns,\n",
    "                                    actual_permnos=daily_permnos\n",
    "                                )\n",
    "                                \n",
    "                                if prev_portfolio_data[portfolio_type] is not None:\n",
    "                                    prev_w_ser = pd.Series(\n",
    "                                        prev_portfolio_data[portfolio_type]['weights'],\n",
    "                                        index=prev_portfolio_data[portfolio_type]['permnos']\n",
    "                                    )\n",
    "                                    cur_w_ser = pd.Series(\n",
    "                                        portfolio_info['weights'],\n",
    "                                        index=portfolio_info['permnos']\n",
    "                                    )\n",
    "\n",
    "                                    prev_r_ser = pd.Series(\n",
    "                                        prev_portfolio_data[portfolio_type]['aligned_returns'],\n",
    "                                        index=prev_portfolio_data[portfolio_type]['permnos']\n",
    "                                    )\n",
    "\n",
    "                                    aligned_prev_w = prev_w_ser.reindex(cur_w_ser.index, fill_value=0).values\n",
    "                                    aligned_prev_r = prev_r_ser.reindex(cur_w_ser.index, fill_value=0).values\n",
    "\n",
    "                                    aligned_cur_w = cur_w_ser.values\n",
    "\n",
    "                                    turnover = backtester.calc_turnover(\n",
    "                                        w_t  = aligned_prev_w,\n",
    "                                        r_t  = aligned_prev_r,\n",
    "                                        w_tp1= aligned_cur_w\n",
    "                                    )\n",
    "                                else:\n",
    "                                    turnover = np.sum(np.abs(portfolio_info['weights']))\n",
    "                                \n",
    "                                portfolio_daily_data[portfolio_type]['returns'].append(portfolio_return)\n",
    "                                portfolio_daily_data[portfolio_type]['turnovers'].append(turnover)\n",
    "                                portfolio_daily_data[portfolio_type]['dates'].append(signal_date)\n",
    "                                \n",
    "                                prev_portfolio_data[portfolio_type] = {\n",
    "                                    'weights'        : portfolio_info['weights'],\n",
    "                                    'permnos'        : portfolio_info['permnos'],\n",
    "                                    'aligned_returns': aligned_returns      \n",
    "                                }\n",
    "                \n",
    "                for portfolio_type in ['long_only', 'short_only', 'long_short']:\n",
    "                    portfolio_data = portfolio_daily_data[portfolio_type]\n",
    "                    \n",
    "                    if len(portfolio_data['returns']) > 0:\n",
    "                        metrics = backtester.calculate_metrics(\n",
    "                            returns=portfolio_data['returns'],\n",
    "                            turnover_series=portfolio_data['turnovers']\n",
    "                        )\n",
    "                        \n",
    "                        rets = np.array(portfolio_data['returns'])\n",
    "                        tovs = np.array(portfolio_data['turnovers'])\n",
    "\n",
    "                        for tc in TC_GRID:\n",
    "                            tag = TC_TAG[tc]\n",
    "                            adj = rets - tovs * tc\n",
    "\n",
    "                            ann_ret = adj.mean() * 252\n",
    "                            ann_vol = adj.std(ddof=1) * np.sqrt(252)\n",
    "                            sharpe  = ann_ret / ann_vol if ann_vol > 0 else 0\n",
    "\n",
    "                            cum_adj = np.cumprod(1 + adj)\n",
    "                            mdd = ((cum_adj - np.maximum.accumulate(cum_adj)) /\n",
    "                                   np.maximum.accumulate(cum_adj)).min()\n",
    "\n",
    "                            metrics[f'{tag}_annual_return'] = ann_ret\n",
    "                            metrics[f'{tag}_annual_vol']    = ann_vol\n",
    "                            metrics[f'{tag}_sharpe']        = sharpe\n",
    "                            metrics[f'{tag}_max_drawdown']  = mdd\n",
    "                        \n",
    "                        summary_results.append({\n",
    "                            'scheme': scheme,\n",
    "                            'model': model_name,\n",
    "                            'window': window,\n",
    "                            'portfolio_type': portfolio_type,\n",
    "                            **metrics\n",
    "                        })\n",
    "                        \n",
    "                        rets_arr = np.array(portfolio_data['returns'])\n",
    "                        tovs_arr = np.array(portfolio_data['turnovers'])\n",
    "                        cum_no_tc = np.log1p(rets_arr).cumsum()\n",
    "\n",
    "                        tc_ret_dict = {}\n",
    "                        tc_cum_dict = {}\n",
    "                        for tc in TC_GRID:\n",
    "                            tag = TC_TAG[tc]\n",
    "                            r = rets_arr - tovs_arr * tc\n",
    "                            tc_ret_dict[tag] = r\n",
    "                            tc_cum_dict[tag] = np.log1p(r).cumsum()\n",
    "\n",
    "                        for i, date in enumerate(portfolio_data['dates']):\n",
    "                            row = {\n",
    "                                'scheme'        : scheme,\n",
    "                                'model'         : model_name,\n",
    "                                'window'        : window,\n",
    "                                'portfolio_type': portfolio_type,\n",
    "                                'date'          : str(date),\n",
    "                                'return'        : rets_arr[i],\n",
    "                                'turnover'      : tovs_arr[i],\n",
    "                                'cumulative'    : cum_no_tc[i],\n",
    "                            }\n",
    "                            for tag in TC_TAG.values():\n",
    "                                row[f'{tag}_return']     = tc_ret_dict[tag][i]\n",
    "                                row[f'{tag}_cumulative'] = tc_cum_dict[tag][i]\n",
    "\n",
    "                            daily_series_data.append(row)\n",
    "\n",
    "                if scheme == \"VW\" and len(all_y_true) > 0:\n",
    "                    y_all    = np.concatenate(all_y_true)\n",
    "                    yhat_all = np.concatenate(all_y_pred)\n",
    "                    perm_all = np.concatenate(all_permnos)\n",
    "                    meta_all = pd.concat(all_meta, ignore_index=True)\n",
    "\n",
    "                    k = X_test.shape[1]\n",
    "\n",
    "                    m1_metrics = overall_interval_metrics_method1(\n",
    "                        y_all, yhat_all, k,\n",
    "                        permnos_all=perm_all,\n",
    "                        meta_all=meta_all\n",
    "                    )\n",
    "\n",
    "                    full_pred_df = pd.concat(pred_rows, ignore_index=True)\n",
    "                    mean_ic, t_ic, pos_ic, _ = calc_ic_daily(full_pred_df, method='spearman')\n",
    "                    m1_metrics['RankIC_mean']  = mean_ic\n",
    "                    m1_metrics['RankIC_t']     = t_ic\n",
    "                    m1_metrics['RankIC_pos%']  = pos_ic\n",
    "\n",
    "                    save_metrics(m1_metrics, name=model_name, window=window,\n",
    "                        path=\"portfolio_metrics.csv\")\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_results)\n",
    "    daily_df = pd.DataFrame(daily_series_data) if daily_series_data else pd.DataFrame()\n",
    "    \n",
    "    tc_columns = [c for c in summary_df.columns if c.startswith('tc')]\n",
    "    summary_df[tc_columns] = summary_df[tc_columns].fillna(0.0)\n",
    "    \n",
    "    def save_split_by_scheme(df, base_filename):\n",
    "        \"\"\"Helper function to save files split by scheme\"\"\"\n",
    "        if df.empty:\n",
    "            print(f\"Warning: DataFrame is empty, skipping save for {base_filename}\")\n",
    "            return None, None\n",
    "            \n",
    "        vw_df = df[df['scheme'] == 'VW']\n",
    "        ew_df = df[df['scheme'] == 'EW']\n",
    "        \n",
    "        vw_filename = f\"{base_filename}_VW.csv\"\n",
    "        ew_filename = f\"{base_filename}_EW.csv\"\n",
    "        \n",
    "        vw_df.to_csv(vw_filename, index=False)\n",
    "        ew_df.to_csv(ew_filename, index=False)\n",
    "        \n",
    "        print(f\"VW results saved to {vw_filename}\")\n",
    "        print(f\"EW results saved to {ew_filename}\")\n",
    "        \n",
    "        return vw_filename, ew_filename\n",
    "    \n",
    "    save_split_by_scheme(summary_df, \"portfolio_results_daily_rebalance\")\n",
    "    \n",
    "    if not daily_df.empty:\n",
    "        save_split_by_scheme(daily_df, \"portfolio_daily_series\")\n",
    "    \n",
    "    if pred_rows:\n",
    "        pred_df = pd.concat(pred_rows, ignore_index=True)\n",
    "        pred_df.to_csv(\"predictions_daily.csv\", index=False)\n",
    "        print(f\"Saved {len(pred_df)} prediction rows to predictions_daily.csv\")\n",
    "    \n",
    "    print(f\"Generated {len(summary_results)} portfolio summary records\")\n",
    "    print(f\"Generated {len(daily_series_data)} daily series records\")\n",
    "    \n",
    "    return summary_df, daily_df, backtester\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting **LSTM** quarterly expanding training 2015‑2024\n",
      "\n",
      "=== Window = 5 ===\n",
      "[Skip-Optuna] hyper-params loaded from existing 2015Q4 model\n",
      "[Skip] Model already trained for window=5, 2015Q4\n",
      "[Skip] Model already trained for window=5, 2016Q1\n",
      "[Skip] Model already trained for window=5, 2016Q2\n",
      "[Skip] Model already trained for window=5, 2016Q3\n",
      "[Skip] Model already trained for window=5, 2016Q4\n",
      "[Skip] Model already trained for window=5, 2017Q1\n",
      "[Skip] Model already trained for window=5, 2017Q2\n",
      "[Skip] Model already trained for window=5, 2017Q3\n",
      "[Skip] Model already trained for window=5, 2017Q4\n",
      "[Skip] Model already trained for window=5, 2018Q1\n",
      "[Skip] Model already trained for window=5, 2018Q2\n",
      "[Skip] Model already trained for window=5, 2018Q3\n",
      "[Skip] Model already trained for window=5, 2018Q4\n",
      "[Skip] Model already trained for window=5, 2019Q1\n",
      "[Skip] Model already trained for window=5, 2019Q2\n",
      "[Skip] Model already trained for window=5, 2019Q3\n",
      "[Skip] Model already trained for window=5, 2019Q4\n",
      "[Skip] Model already trained for window=5, 2020Q1\n",
      "[Skip] Model already trained for window=5, 2020Q2\n",
      "[Skip] Model already trained for window=5, 2020Q3\n",
      "[Skip] Model already trained for window=5, 2020Q4\n",
      "[Skip] Model already trained for window=5, 2021Q1\n",
      "[Skip] Model already trained for window=5, 2021Q2\n",
      "[Skip] Model already trained for window=5, 2021Q3\n",
      "[Skip] Model already trained for window=5, 2021Q4\n",
      "[Skip] Model already trained for window=5, 2022Q1\n",
      "[Skip] Model already trained for window=5, 2022Q2\n",
      "[Skip] Model already trained for window=5, 2022Q3\n",
      "[Skip] Model already trained for window=5, 2022Q4\n",
      "[Skip] Model already trained for window=5, 2023Q1\n",
      "[Skip] Model already trained for window=5, 2023Q2\n",
      "[Skip] Model already trained for window=5, 2023Q3\n",
      "[Skip] Model already trained for window=5, 2023Q4\n",
      "[Skip] Model already trained for window=5, 2024Q1\n",
      "[Skip] Model already trained for window=5, 2024Q2\n",
      "[Skip] Model already trained for window=5, 2024Q3\n",
      "\n",
      "=== Window = 21 ===\n",
      "[Skip] Model already trained for window=21, 2015Q4\n",
      "[Skip] Model already trained for window=21, 2016Q1\n",
      "[Skip] Model already trained for window=21, 2016Q2\n",
      "[Skip] Model already trained for window=21, 2016Q3\n",
      "[Skip] Model already trained for window=21, 2016Q4\n",
      "[Skip] Model already trained for window=21, 2017Q1\n",
      "[Skip] Model already trained for window=21, 2017Q2\n",
      "[Skip] Model already trained for window=21, 2017Q3\n",
      "[Skip] Model already trained for window=21, 2017Q4\n",
      "[Skip] Model already trained for window=21, 2018Q1\n",
      "[Skip] Model already trained for window=21, 2018Q2\n",
      "[Skip] Model already trained for window=21, 2018Q3\n",
      "[Skip] Model already trained for window=21, 2018Q4\n",
      "[Skip] Model already trained for window=21, 2019Q1\n",
      "[Skip] Model already trained for window=21, 2019Q2\n",
      "[Skip] Model already trained for window=21, 2019Q3\n",
      "[Skip] Model already trained for window=21, 2019Q4\n",
      "[Skip] Model already trained for window=21, 2020Q1\n",
      "[Skip] Model already trained for window=21, 2020Q2\n",
      "[Skip] Model already trained for window=21, 2020Q3\n",
      "[Skip] Model already trained for window=21, 2020Q4\n",
      "[Skip] Model already trained for window=21, 2021Q1\n",
      "[Skip] Model already trained for window=21, 2021Q2\n",
      "[Skip] Model already trained for window=21, 2021Q3\n",
      "[Skip] Model already trained for window=21, 2021Q4\n",
      "[Skip] Model already trained for window=21, 2022Q1\n",
      "[Skip] Model already trained for window=21, 2022Q2\n",
      "[Skip] Model already trained for window=21, 2022Q3\n",
      "[Skip] Model already trained for window=21, 2022Q4\n",
      "[Skip] Model already trained for window=21, 2023Q1\n",
      "[Skip] Model already trained for window=21, 2023Q2\n",
      "[Skip] Model already trained for window=21, 2023Q3\n",
      "[Skip] Model already trained for window=21, 2023Q4\n",
      "[Skip] Model already trained for window=21, 2024Q1\n",
      "[Skip] Model already trained for window=21, 2024Q2\n",
      "[Skip] Model already trained for window=21, 2024Q3\n",
      "\n",
      "=== Window = 252 ===\n",
      "[Skip] Model already trained for window=252, 2015Q4\n",
      "[Skip] Model already trained for window=252, 2016Q1\n",
      "[Skip] Model already trained for window=252, 2016Q2\n",
      "[Skip] Model already trained for window=252, 2016Q3\n",
      "[Skip] Model already trained for window=252, 2016Q4\n",
      "[Skip] Model already trained for window=252, 2017Q1\n",
      "[Skip] Model already trained for window=252, 2017Q2\n",
      "[Skip] Model already trained for window=252, 2017Q3\n",
      "[Skip] Model already trained for window=252, 2017Q4\n",
      "[Skip] Model already trained for window=252, 2018Q1\n",
      "[Skip] Model already trained for window=252, 2018Q2\n",
      "[Skip] Model already trained for window=252, 2018Q3\n",
      "[Skip] Model already trained for window=252, 2018Q4\n",
      "[Skip] Model already trained for window=252, 2019Q1\n",
      "[Skip] Model already trained for window=252, 2019Q2\n",
      "[Skip] Model already trained for window=252, 2019Q3\n",
      "[Skip] Model already trained for window=252, 2019Q4\n",
      "[Skip] Model already trained for window=252, 2020Q1\n",
      "[Skip] Model already trained for window=252, 2020Q2\n",
      "[Skip] Model already trained for window=252, 2020Q3\n",
      "[Skip] Model already trained for window=252, 2020Q4\n",
      "[Skip] Model already trained for window=252, 2021Q1\n",
      "[Skip] Model already trained for window=252, 2021Q2\n",
      "[Skip] Model already trained for window=252, 2021Q3\n",
      "[Skip] Model already trained for window=252, 2021Q4\n",
      "[Skip] Model already trained for window=252, 2022Q1\n",
      "[Skip] Model already trained for window=252, 2022Q2\n",
      "[Skip] Model already trained for window=252, 2022Q3\n",
      "[Skip] Model already trained for window=252, 2022Q4\n",
      "[Skip] Model already trained for window=252, 2023Q1\n",
      "[Skip] Model already trained for window=252, 2023Q2\n",
      "[Skip] Model already trained for window=252, 2023Q3\n",
      "[Skip] Model already trained for window=252, 2023Q4\n",
      "[Skip] Model already trained for window=252, 2024Q1\n",
      "[Skip] Model already trained for window=252, 2024Q2\n",
      "[Skip] Model already trained for window=252, 2024Q3\n",
      "\n",
      "=== Window = 512 ===\n",
      "[Skip] Model already trained for window=512, 2015Q4\n",
      "[Skip] Model already trained for window=512, 2016Q1\n",
      "[Skip] Model already trained for window=512, 2016Q2\n",
      "[Skip] Model already trained for window=512, 2016Q3\n",
      "[Skip] Model already trained for window=512, 2016Q4\n",
      "[Skip] Model already trained for window=512, 2017Q1\n",
      "[Skip] Model already trained for window=512, 2017Q2\n",
      "[Skip] Model already trained for window=512, 2017Q3\n",
      "[Skip] Model already trained for window=512, 2017Q4\n",
      "[Skip] Model already trained for window=512, 2018Q1\n",
      "\n",
      "[Window 512] 2018Q2\n",
      "    +2996 obs from 2018Q1 -> expanding size 174566\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 174566 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.000330, Val=0.000197\n",
      "        Epoch  3/10: Train=0.000330, Val=0.000197\n",
      "        Epoch  5/10: Train=0.000330, Val=0.000197\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w512_2018Q2.pth\n",
      "\n",
      "[Window 512] 2018Q3\n",
      "    +3160 obs from 2018Q2 -> expanding size 177726\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 177726 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.000327, Val=0.000203\n",
      "        Epoch  3/10: Train=0.000327, Val=0.000203\n",
      "        Epoch  5/10: Train=0.000327, Val=0.000203\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w512_2018Q3.pth\n",
      "\n",
      "[Window 512] 2018Q4\n",
      "    +3125 obs from 2018Q3 -> expanding size 180851\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 180851 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.000324, Val=0.000202\n",
      "        Epoch  3/10: Train=0.000324, Val=0.000202\n",
      "        Epoch  5/10: Train=0.000324, Val=0.000202\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w512_2018Q4.pth\n",
      "\n",
      "[Window 512] 2019Q1\n",
      "    +3045 obs from 2018Q4 -> expanding size 183896\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 183896 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.000321, Val=0.000237\n",
      "        Epoch  3/10: Train=0.000321, Val=0.000237\n",
      "        Epoch  5/10: Train=0.000321, Val=0.000237\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w512_2019Q1.pth\n",
      "\n",
      "[Window 512] 2019Q2\n",
      "    +3022 obs from 2019Q1 -> expanding size 186918\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 186918 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.000319, Val=0.000232\n",
      "        Epoch  3/10: Train=0.000320, Val=0.000232\n",
      "        Epoch  5/10: Train=0.000320, Val=0.000232\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w512_2019Q2.pth\n",
      "\n",
      "[Window 512] 2019Q3\n",
      "    +3120 obs from 2019Q2 -> expanding size 190038\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 190038 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.000318, Val=0.000222\n",
      "        Epoch  3/10: Train=0.000318, Val=0.000222\n",
      "        Epoch  5/10: Train=0.000318, Val=0.000222\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w512_2019Q3.pth\n",
      "\n",
      "[Window 512] 2019Q4\n",
      "    +3167 obs from 2019Q3 -> expanding size 193205\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 193205 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.000317, Val=0.000222\n",
      "        Epoch  3/10: Train=0.000317, Val=0.000222\n",
      "        Epoch  5/10: Train=0.000317, Val=0.000222\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w512_2019Q4.pth\n",
      "\n",
      "[Window 512] 2020Q1\n",
      "    +3179 obs from 2019Q4 -> expanding size 196384\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 196384 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.000316, Val=0.000213\n",
      "        Epoch  3/10: Train=0.000316, Val=0.000213\n",
      "        Epoch  5/10: Train=0.000316, Val=0.000213\n",
      "        Epoch  7/10: Train=0.000316, Val=0.000213\n",
      "        Epoch  9/10: Train=0.000316, Val=0.000213\n",
      "        Epoch 10/10: Train=0.000316, Val=0.000213\n",
      "        Early‑stopped at epoch 10\n",
      "[Saved] models/LSTM_w512_2020Q1.pth\n",
      "\n",
      "[Window 512] 2020Q2\n",
      "    +2595 obs from 2020Q1 -> expanding size 198979\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 198979 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.000313, Val=0.000260\n",
      "        Epoch  3/10: Train=0.000314, Val=0.000260\n",
      "        Epoch  5/10: Train=0.000314, Val=0.000260\n",
      "        Epoch  7/10: Train=0.000313, Val=0.000260\n",
      "        Epoch  9/10: Train=0.000313, Val=0.000260\n",
      "        Epoch 10/10: Train=0.000313, Val=0.000260\n",
      "[Saved] models/LSTM_w512_2020Q2.pth\n",
      "\n",
      "[Window 512] 2020Q3\n",
      "    +2839 obs from 2020Q2 -> expanding size 201818\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 201818 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.000311, Val=0.000322\n",
      "        Epoch  3/10: Train=0.000311, Val=0.000322\n",
      "        Epoch  5/10: Train=0.000311, Val=0.000322\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w512_2020Q3.pth\n",
      "\n",
      "[Window 512] 2020Q4\n",
      "    +3151 obs from 2020Q3 -> expanding size 204969\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 204969 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.000312, Val=0.000334\n",
      "        Epoch  3/10: Train=0.000312, Val=0.000335\n",
      "        Epoch  5/10: Train=0.000312, Val=0.000335\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w512_2020Q4.pth\n",
      "\n",
      "[Window 512] 2021Q1\n",
      "    +3114 obs from 2020Q4 -> expanding size 208083\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 208083 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.000310, Val=0.000353\n",
      "        Epoch  3/10: Train=0.000310, Val=0.000353\n",
      "        Epoch  5/10: Train=0.000310, Val=0.000354\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w512_2021Q1.pth\n",
      "\n",
      "[Window 512] 2021Q2\n",
      "    +2989 obs from 2021Q1 -> expanding size 211072\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 211072 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.000308, Val=0.000377\n",
      "        Epoch  3/10: Train=0.000309, Val=0.000377\n",
      "        Epoch  5/10: Train=0.000308, Val=0.000378\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w512_2021Q2.pth\n",
      "\n",
      "[Window 512] 2021Q3\n",
      "    +3132 obs from 2021Q2 -> expanding size 214204\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 214204 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.000308, Val=0.000367\n",
      "        Epoch  3/10: Train=0.000308, Val=0.000368\n",
      "        Epoch  5/10: Train=0.000308, Val=0.000368\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w512_2021Q3.pth\n",
      "\n",
      "[Window 512] 2021Q4\n",
      "    +3173 obs from 2021Q3 -> expanding size 217377\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 217377 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.000306, Val=0.000374\n",
      "        Epoch  3/10: Train=0.000306, Val=0.000374\n",
      "        Epoch  5/10: Train=0.000306, Val=0.000374\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w512_2021Q4.pth\n",
      "\n",
      "[Window 512] 2022Q1\n",
      "    +3153 obs from 2021Q4 -> expanding size 220530\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 220530 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.000305, Val=0.000373\n",
      "        Epoch  3/10: Train=0.000305, Val=0.000373\n",
      "        Epoch  5/10: Train=0.000305, Val=0.000373\n",
      "        Epoch  7/10: Train=0.000305, Val=0.000373\n",
      "        Early‑stopped at epoch 8\n",
      "[Saved] models/LSTM_w512_2022Q1.pth\n",
      "\n",
      "[Window 512] 2022Q2\n",
      "    +3029 obs from 2022Q1 -> expanding size 223559\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 223559 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.000312, Val=0.000326\n",
      "        Epoch  3/10: Train=0.000312, Val=0.000326\n",
      "        Epoch  5/10: Train=0.000312, Val=0.000326\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w512_2022Q2.pth\n",
      "\n",
      "[Window 512] 2022Q3\n",
      "    +2969 obs from 2022Q2 -> expanding size 226528\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 226528 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.000313, Val=0.000347\n",
      "        Epoch  3/10: Train=0.000313, Val=0.000347\n",
      "        Epoch  5/10: Train=0.000313, Val=0.000347\n",
      "        Epoch  7/10: Train=0.000313, Val=0.000348\n",
      "        Early‑stopped at epoch 8\n",
      "[Saved] models/LSTM_w512_2022Q3.pth\n",
      "\n",
      "[Window 512] 2022Q4\n",
      "    +3152 obs from 2022Q3 -> expanding size 229680\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 229680 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.000315, Val=0.000335\n",
      "        Epoch  3/10: Train=0.000315, Val=0.000336\n",
      "        Epoch  5/10: Train=0.000314, Val=0.000336\n",
      "        Epoch  7/10: Train=0.000314, Val=0.000337\n",
      "        Early‑stopped at epoch 7\n",
      "[Saved] models/LSTM_w512_2022Q4.pth\n",
      "\n",
      "[Window 512] 2023Q1\n",
      "    +3070 obs from 2022Q4 -> expanding size 232750\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 232750 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.000314, Val=0.000347\n",
      "        Epoch  3/10: Train=0.000314, Val=0.000347\n",
      "        Epoch  5/10: Train=0.000314, Val=0.000347\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w512_2023Q1.pth\n",
      "\n",
      "[Window 512] 2023Q2\n",
      "    +3064 obs from 2023Q1 -> expanding size 235814\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 235814 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.000314, Val=0.000347\n",
      "        Epoch  3/10: Train=0.000314, Val=0.000347\n",
      "        Epoch  5/10: Train=0.000314, Val=0.000348\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w512_2023Q2.pth\n",
      "\n",
      "[Window 512] 2023Q3\n",
      "    +3069 obs from 2023Q2 -> expanding size 238883\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 238883 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.000313, Val=0.000341\n",
      "        Epoch  3/10: Train=0.000313, Val=0.000341\n",
      "        Epoch  5/10: Train=0.000313, Val=0.000342\n",
      "        Epoch  7/10: Train=0.000313, Val=0.000341\n",
      "        Early‑stopped at epoch 7\n",
      "[Saved] models/LSTM_w512_2023Q3.pth\n",
      "\n",
      "[Window 512] 2023Q4\n",
      "    +3121 obs from 2023Q3 -> expanding size 242004\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 242004 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.000312, Val=0.000336\n",
      "        Epoch  3/10: Train=0.000312, Val=0.000337\n",
      "        Epoch  5/10: Train=0.000312, Val=0.000336\n",
      "        Epoch  7/10: Train=0.000311, Val=0.000337\n",
      "        Epoch  9/10: Train=0.000310, Val=0.000338\n",
      "        Epoch 10/10: Train=0.000310, Val=0.000337\n",
      "[Saved] models/LSTM_w512_2023Q4.pth\n",
      "\n",
      "[Window 512] 2024Q1\n",
      "    +3113 obs from 2023Q4 -> expanding size 245117\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 245117 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.000310, Val=0.000337\n",
      "        Epoch  3/10: Train=0.000311, Val=0.000337\n",
      "        Epoch  5/10: Train=0.000310, Val=0.000337\n",
      "        Early‑stopped at epoch 6\n",
      "[Saved] models/LSTM_w512_2024Q1.pth\n",
      "\n",
      "[Window 512] 2024Q2\n",
      "    +3026 obs from 2024Q1 -> expanding size 248143\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 248143 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.000312, Val=0.000315\n",
      "        Epoch  3/10: Train=0.000312, Val=0.000315\n",
      "        Epoch  5/10: Train=0.000312, Val=0.000316\n",
      "        Epoch  7/10: Train=0.000311, Val=0.000316\n",
      "        Epoch  9/10: Train=0.000310, Val=0.000317\n",
      "        Early‑stopped at epoch 9\n",
      "[Saved] models/LSTM_w512_2024Q2.pth\n",
      "\n",
      "[Window 512] 2024Q3\n",
      "    +3112 obs from 2024Q2 -> expanding size 251255\n",
      "    Warm‑start from last quarter model …\n",
      "    Partial‑fit on 251255 samples for 10 epochs …\n",
      "        Epoch  1/10: Train=0.000313, Val=0.000284\n",
      "        Epoch  3/10: Train=0.000313, Val=0.000284\n",
      "        Epoch  5/10: Train=0.000313, Val=0.000285\n",
      "        Epoch  7/10: Train=0.000313, Val=0.000285\n",
      "        Early‑stopped at epoch 7\n",
      "[Saved] models/LSTM_w512_2024Q3.pth\n",
      "All LSTM quarterly expanding models trained \n"
     ]
    }
   ],
   "source": [
    "train_lstm_models_expanding_quarterly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Daily Rebalance Portfolio Backtesting Simulation\n",
      "Processing window size: 5\n",
      "  Model: LSTM, Scheme: VW\n",
      "[Create] New metrics file created with LSTM w=5\n",
      "  Model: LSTM, Scheme: EW\n",
      "Processing window size: 21\n",
      "  Model: LSTM, Scheme: VW\n",
      "[Update] Metrics updated for LSTM w=21\n",
      "  Model: LSTM, Scheme: EW\n",
      "Processing window size: 252\n",
      "  Model: LSTM, Scheme: VW\n",
      "[Update] Metrics updated for LSTM w=252\n",
      "  Model: LSTM, Scheme: EW\n",
      "Processing window size: 512\n",
      "  Model: LSTM, Scheme: VW\n",
      "[Update] Metrics updated for LSTM w=512\n",
      "  Model: LSTM, Scheme: EW\n",
      "VW results saved to portfolio_results_daily_rebalance_VW.csv\n",
      "EW results saved to portfolio_results_daily_rebalance_EW.csv\n",
      "VW results saved to portfolio_daily_series_VW.csv\n",
      "EW results saved to portfolio_daily_series_EW.csv\n",
      "Saved 443400 prediction rows to predictions_daily.csv\n",
      "Generated 24 portfolio summary records\n",
      "Generated 51672 daily series records\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   scheme model  window portfolio_type  annual_return  annual_vol    sharpe  \\\n",
       " 0      VW  LSTM       5      long_only       0.048606    0.205496  0.236532   \n",
       " 1      VW  LSTM       5     short_only      -0.199673    0.215380 -0.927073   \n",
       " 2      VW  LSTM       5     long_short      -0.151066    0.233871 -0.645938   \n",
       " 3      EW  LSTM       5      long_only       0.029817    0.209240  0.142500   \n",
       " 4      EW  LSTM       5     short_only      -0.181127    0.224357 -0.807316   \n",
       " 5      EW  LSTM       5     long_short      -0.151310    0.218665 -0.691972   \n",
       " 6      VW  LSTM      21      long_only       0.179192    0.232989  0.769103   \n",
       " 7      VW  LSTM      21     short_only      -0.093756    0.177146 -0.529258   \n",
       " 8      VW  LSTM      21     long_short       0.085436    0.232724  0.367114   \n",
       " 9      EW  LSTM      21      long_only       0.168443    0.252802  0.666303   \n",
       " 10     EW  LSTM      21     short_only      -0.089638    0.174367 -0.514074   \n",
       " 11     EW  LSTM      21     long_short       0.078805    0.231058  0.341062   \n",
       " 12     VW  LSTM     252      long_only       0.219921    0.229597  0.957857   \n",
       " 13     VW  LSTM     252     short_only      -0.006989    0.179604 -0.038914   \n",
       " 14     VW  LSTM     252     long_short       0.212932    0.230747  0.922795   \n",
       " 15     EW  LSTM     252      long_only       0.188956    0.247840  0.762413   \n",
       " 16     EW  LSTM     252     short_only      -0.016810    0.172272 -0.097577   \n",
       " 17     EW  LSTM     252     long_short       0.172147    0.226252  0.760861   \n",
       " 18     VW  LSTM     512      long_only       0.211974    0.234859  0.902560   \n",
       " 19     VW  LSTM     512     short_only      -0.053867    0.171086 -0.314855   \n",
       " 20     VW  LSTM     512     long_short       0.158107    0.233851  0.676102   \n",
       " 21     EW  LSTM     512      long_only       0.187519    0.257787  0.727418   \n",
       " 22     EW  LSTM     512     short_only      -0.042471    0.165878 -0.256037   \n",
       " 23     EW  LSTM     512     long_short       0.145048    0.232293  0.624421   \n",
       " \n",
       "     max_drawdown  max_1d_loss  avg_turnover  ...  tc20_sharpe  \\\n",
       " 0       0.322559    -0.058625      1.067128  ...    -2.369088   \n",
       " 1       0.865348    -0.058051      1.082016  ...    -3.438338   \n",
       " 2       0.826677    -0.071338      2.150200  ...    -5.216784   \n",
       " 3       0.379709    -0.056679      0.851093  ...    -1.903624   \n",
       " 4       0.847596    -0.072922      0.840206  ...    -2.694618   \n",
       " 5       0.822708    -0.082151      1.692781  ...    -4.578561   \n",
       " 6       0.351119    -0.068088      0.916602  ...    -1.207670   \n",
       " 7       0.691611    -0.073426      1.067916  ...    -3.542482   \n",
       " 8       0.391086    -0.062976      1.985189  ...    -3.880589   \n",
       " 9       0.379145    -0.070637      0.664988  ...    -0.658040   \n",
       " 10      0.645025    -0.073426      0.878726  ...    -3.042184   \n",
       " 11      0.375955    -0.075986      1.544986  ...    -3.009127   \n",
       " 12      0.272918    -0.053969      0.837929  ...    -0.880738   \n",
       " 13      0.417979    -0.073426      0.928907  ...    -2.626072   \n",
       " 14      0.277686    -0.075921      1.767785  ...    -2.912251   \n",
       " 15      0.356300    -0.059133      0.601618  ...    -0.460818   \n",
       " 16      0.445078    -0.073426      0.751144  ...    -2.291462   \n",
       " 17      0.253908    -0.073359      1.353648  ...    -2.246821   \n",
       " 18      0.358247    -0.061587      0.702499  ...    -0.603734   \n",
       " 19      0.525832    -0.041371      0.780230  ...    -2.603557   \n",
       " 20      0.321279    -0.048289      1.484460  ...    -2.508507   \n",
       " 21      0.349049    -0.070637      0.490139  ...    -0.230397   \n",
       " 22      0.494578    -0.039790      0.628172  ...    -2.160298   \n",
       " 23      0.300896    -0.064516      1.119629  ...    -1.794407   \n",
       " \n",
       "     tc20_max_drawdown  tc30_annual_return  tc30_annual_vol  tc30_sharpe  \\\n",
       " 0           -0.987898           -0.758142         0.207756    -3.649189   \n",
       " 1           -0.998647           -1.017677         0.217970    -4.668876   \n",
       " 2           -0.999981           -1.776617         0.239450    -7.419584   \n",
       " 3           -0.973553           -0.613610         0.210182    -2.919418   \n",
       " 4           -0.995767           -0.816323         0.224645    -3.633827   \n",
       " 5           -0.999859           -1.431052         0.220403    -6.492879   \n",
       " 6           -0.931007           -0.513759         0.235392    -2.182567   \n",
       " 7           -0.996385           -0.901101         0.179840    -5.010580   \n",
       " 8           -0.999686           -1.415367         0.238628    -5.931266   \n",
       " 9           -0.825817           -0.334288         0.253850    -1.316875   \n",
       " 10          -0.991175           -0.753954         0.175733    -4.290349   \n",
       " 11          -0.997949           -1.089204         0.233886    -4.656980   \n",
       " 12          -0.863238           -0.413553         0.230536    -1.793879   \n",
       " 13          -0.985910           -0.709243         0.182449    -3.887343   \n",
       " 14          -0.997546           -1.123513         0.235106    -4.778744   \n",
       " 15          -0.745821           -0.265867         0.248225    -1.071073   \n",
       " 16          -0.970599           -0.584675         0.173041    -3.378827   \n",
       " 17          -0.989627           -0.851212         0.227948    -3.734241   \n",
       " 18          -0.767422           -0.319115         0.236217    -1.350940   \n",
       " 19          -0.982222           -0.643721         0.172886    -3.723381   \n",
       " 20          -0.994865           -0.964145         0.237116    -4.066129   \n",
       " 21          -0.634598           -0.183026         0.258750    -0.707347   \n",
       " 22          -0.962122           -0.517369         0.166690    -3.103781   \n",
       " 23          -0.977654           -0.701391         0.234805    -2.987124   \n",
       " \n",
       "     tc30_max_drawdown  tc40_annual_return  tc40_annual_vol  tc40_sharpe  \\\n",
       " 0           -0.998793           -1.027058         0.209497    -4.902497   \n",
       " 1           -0.999869           -1.290345         0.219684    -5.873631   \n",
       " 2           -1.000000           -2.318468         0.243070    -9.538282   \n",
       " 3           -0.995783           -0.828085         0.210889    -3.926633   \n",
       " 4           -0.999300           -1.028055         0.225100    -4.567106   \n",
       " 5           -0.999996           -1.857633         0.221852    -8.373312   \n",
       " 6           -0.990272           -0.744742         0.237068    -3.141475   \n",
       " 7           -0.999627           -1.170215         0.181802    -6.436771   \n",
       " 8           -0.999996           -1.915635         0.242251    -7.907644   \n",
       " 9           -0.955848           -0.501865         0.254508    -1.971900   \n",
       " 10          -0.998639           -0.975393         0.176652    -5.521554   \n",
       " 11          -0.999927           -1.478541         0.235546    -6.277087   \n",
       " 12          -0.976770           -0.624711         0.231687    -2.696361   \n",
       " 13          -0.998049           -0.943327         0.184505    -5.112746   \n",
       " 14          -0.999946           -1.568995         0.238196    -6.586992   \n",
       " 15          -0.922635           -0.417475         0.248648    -1.678977   \n",
       " 16          -0.994116           -0.773963         0.173768    -4.453994   \n",
       " 17          -0.999442           -1.192331         0.229216    -5.201779   \n",
       " 18          -0.948045           -0.496145         0.237504    -2.088991   \n",
       " 19          -0.996595           -0.840339         0.174590    -4.813221   \n",
       " 20          -0.999788           -1.338229         0.239789    -5.580851   \n",
       " 21          -0.849810           -0.306541         0.259335    -1.182028   \n",
       " 22          -0.989926           -0.675668         0.167371    -4.036947   \n",
       " 23          -0.997967           -0.983537         0.236292    -4.162384   \n",
       " \n",
       "     tc40_max_drawdown  \n",
       " 0           -0.999880  \n",
       " 1           -0.999987  \n",
       " 2           -1.000000  \n",
       " 3           -0.999329  \n",
       " 4           -0.999885  \n",
       " 5           -1.000000  \n",
       " 6           -0.998644  \n",
       " 7           -0.999962  \n",
       " 8           -1.000000  \n",
       " 9           -0.989479  \n",
       " 10          -0.999794  \n",
       " 11          -0.999997  \n",
       " 12          -0.996196  \n",
       " 13          -0.999732  \n",
       " 14          -0.999999  \n",
       " 15          -0.978340  \n",
       " 16          -0.998835  \n",
       " 17          -0.999970  \n",
       " 18          -0.988602  \n",
       " 19          -0.999349  \n",
       " 20          -0.999991  \n",
       " 21          -0.944908  \n",
       " 22          -0.997324  \n",
       " 23          -0.999819  \n",
       " \n",
       " [24 rows x 32 columns],\n",
       "       scheme model  window portfolio_type                 date    return  \\\n",
       " 0         VW  LSTM       5      long_only  2016-01-05 00:00:00  0.023031   \n",
       " 1         VW  LSTM       5      long_only  2016-01-06 00:00:00  0.006125   \n",
       " 2         VW  LSTM       5      long_only  2016-01-07 00:00:00  0.020606   \n",
       " 3         VW  LSTM       5      long_only  2016-01-08 00:00:00 -0.022447   \n",
       " 4         VW  LSTM       5      long_only  2016-01-11 00:00:00  0.012466   \n",
       " ...      ...   ...     ...            ...                  ...       ...   \n",
       " 51667     EW  LSTM     512     long_short  2024-12-20 00:00:00 -0.000329   \n",
       " 51668     EW  LSTM     512     long_short  2024-12-23 00:00:00  0.000534   \n",
       " 51669     EW  LSTM     512     long_short  2024-12-24 00:00:00 -0.001303   \n",
       " 51670     EW  LSTM     512     long_short  2024-12-27 00:00:00  0.006344   \n",
       " 51671     EW  LSTM     512     long_short  2024-12-30 00:00:00 -0.004476   \n",
       " \n",
       "            turnover  cumulative  tc5_return  tc5_cumulative  tc10_return  \\\n",
       " 0      1.000000e+00    0.022770    0.022531        0.022281     0.022031   \n",
       " 1      1.320830e-07    0.028877    0.006125        0.028388     0.006125   \n",
       " 2      6.880713e-08    0.049274    0.020606        0.048785     0.020606   \n",
       " 3      9.125272e-08    0.026570   -0.022447        0.026082    -0.022447   \n",
       " 4      1.580671e-01    0.038960    0.012387        0.038393     0.012308   \n",
       " ...             ...         ...         ...             ...          ...   \n",
       " 51667  8.315226e-01    1.008086   -0.000745       -0.194883    -0.001161   \n",
       " 51668  1.619616e+00    1.008620   -0.000276       -0.195159    -0.001085   \n",
       " 51669  1.203904e+00    1.007316   -0.001905       -0.197066    -0.002507   \n",
       " 51670  8.251582e-01    1.013639    0.005931       -0.191152     0.005518   \n",
       " 51671  1.608175e+00    1.009153   -0.005280       -0.196446    -0.006084   \n",
       " \n",
       "        tc10_cumulative  tc20_return  tc20_cumulative  tc30_return  \\\n",
       " 0             0.021792     0.021031         0.020813     0.020031   \n",
       " 1             0.027899     0.006125         0.026920     0.006125   \n",
       " 2             0.048296     0.020606         0.047317     0.020606   \n",
       " 3             0.025593    -0.022447         0.024614    -0.022447   \n",
       " 4             0.037825     0.012150         0.036690     0.011992   \n",
       " ...                ...          ...              ...          ...   \n",
       " 51667        -1.398692    -0.001992        -3.808835    -0.002824   \n",
       " 51668        -1.399778    -0.002705        -3.811544    -0.004325   \n",
       " 51669        -1.402288    -0.003711        -3.815261    -0.004915   \n",
       " 51670        -1.396785     0.004693        -3.810579     0.003868   \n",
       " 51671        -1.402888    -0.007693        -3.818301    -0.009301   \n",
       " \n",
       "        tc30_cumulative  tc40_return  tc40_cumulative  \n",
       " 0             0.019833     0.019031         0.018852  \n",
       " 1             0.025940     0.006125         0.024959  \n",
       " 2             0.046337     0.020606         0.045356  \n",
       " 3             0.023634    -0.022447         0.022653  \n",
       " 4             0.035554     0.011834         0.034417  \n",
       " ...                ...          ...              ...  \n",
       " 51667        -6.222355    -0.003655        -8.639262  \n",
       " 51668        -6.226689    -0.005944        -8.645224  \n",
       " 51669        -6.231615    -0.006119        -8.651362  \n",
       " 51670        -6.227755     0.003043        -8.648323  \n",
       " 51671        -6.237099    -0.010909        -8.659292  \n",
       " \n",
       " [51672 rows x 18 columns],\n",
       " <__main__.PortfolioBacktester at 0x49fe73cb0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_portfolio_simulation_daily_rebalance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] 5_factor_analysis_VW_gross.csv \n",
      "[Saved] 5_factor_analysis_VW_net.csv \n",
      "[Saved] 5_factor_analysis_EW_gross.csv \n",
      "[Saved] 5_factor_analysis_EW_net.csv \n"
     ]
    }
   ],
   "source": [
    "def run_factor_regression(port_ret, factors, use_excess=True):\n",
    "    df = pd.concat([port_ret, factors], axis=1, join='inner').dropna()\n",
    "    df.columns = ['ret'] + list(factors.columns)\n",
    "    \n",
    "    if use_excess:\n",
    "        y = df['ret'].values\n",
    "    else:\n",
    "        y = df['ret'].values - df['rf'].values\n",
    "    \n",
    "    X = df[['mktrf','smb','hml','rmw','cma','umd']].values\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    model = sm.OLS(y, X)\n",
    "    res = model.fit()\n",
    "    alpha = res.params[0]\n",
    "    resid_std = res.resid.std(ddof=1)\n",
    "\n",
    "    ir_daily = alpha / resid_std\n",
    "    ir_annual = ir_daily * np.sqrt(252)\n",
    "\n",
    "    y_hat = np.asarray(res.fittedvalues)\n",
    "    \n",
    "    out = {\n",
    "        'N_obs'            : len(y),\n",
    "        'alpha_daily'      : alpha,\n",
    "        'alpha_annual'     : alpha*252,      \n",
    "        't_alpha'          : res.tvalues[0],\n",
    "        'IR_daily'         : ir_daily,\n",
    "        'IR_annual'        : ir_annual,\n",
    "        'R2_zero'          : r2_zero(y, y_hat),\n",
    "    }\n",
    "    \n",
    "    factor_names = ['MKT','SMB','HML','RMW','CMA','UMD']\n",
    "    for i, fac in enumerate(factor_names, start=1):\n",
    "        out[f'beta_{fac}'] = res.params[i]\n",
    "        out[f't_{fac}']    = res.tvalues[i]\n",
    "    \n",
    "    return out\n",
    "\n",
    "def batch_factor_analysis(\n",
    "    daily_df: pd.DataFrame,\n",
    "    factors_path: str,\n",
    "    scheme: str,\n",
    "    tc_levels=(0, 5, 10, 20, 40),\n",
    "    portfolio_types=('long_only','short_only','long_short'),\n",
    "    model_filter=None,\n",
    "    window_filter=None,\n",
    "    gross_only=False,\n",
    "    out_dir='factor_IR_results',\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a CSV file containing IR results.\n",
    "    gross_only=True  → only tc=0; False → all tc_levels.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    fac = (pd.read_csv(factors_path, parse_dates=['date'])\n",
    "             .set_index('date')\n",
    "             .sort_index())\n",
    "\n",
    "    sub = daily_df[daily_df['scheme'] == scheme].copy()\n",
    "    if model_filter is not None:\n",
    "        sub = sub[sub['model'].isin(model_filter)]\n",
    "    if window_filter is not None:\n",
    "        sub = sub[sub['window'].isin(window_filter)]\n",
    "\n",
    "    tc_iter = (0,) if gross_only else tc_levels\n",
    "    results = []\n",
    "\n",
    "    for (model, win, ptype), g in sub.groupby(['model','window','portfolio_type']):\n",
    "        g = g.sort_values('date').set_index(pd.to_datetime(g['date']))\n",
    "\n",
    "        for tc in tc_iter:\n",
    "            col = 'return' if tc == 0 else f'tc{tc}_return'\n",
    "            if col not in g.columns:\n",
    "                continue\n",
    "            port_ret = g[col]\n",
    "            stats = run_factor_regression(port_ret, fac, use_excess=True)\n",
    "            stats.update({\n",
    "                'scheme'        : scheme,\n",
    "                'model'         : model,\n",
    "                'window'        : win,\n",
    "                'portfolio_type': ptype,\n",
    "                'tc_bps'        : tc,\n",
    "            })\n",
    "            results.append(stats)\n",
    "\n",
    "    df_out = pd.DataFrame(results)[[\n",
    "        'scheme','model','window','portfolio_type','tc_bps','N_obs',\n",
    "        'alpha_daily','alpha_annual','t_alpha',\n",
    "        'IR_daily','IR_annual','R2_zero',\n",
    "        'beta_MKT','t_MKT','beta_SMB','t_SMB',\n",
    "        'beta_HML','t_HML','beta_RMW','t_RMW',\n",
    "        'beta_CMA','t_CMA','beta_UMD','t_UMD'\n",
    "    ]]\n",
    "\n",
    "    tag = 'gross' if gross_only else 'net'\n",
    "    fname = f'5_factor_analysis_{scheme}_{tag}.csv'\n",
    "    df_out.to_csv(os.path.join(out_dir, fname), index=False)\n",
    "    print(f'[Saved] {fname}')\n",
    "    return df_out\n",
    "\n",
    "\n",
    "\n",
    "def run_all_factor_tests(vw_csv=\"portfolio_daily_series_VW.csv\",\n",
    "                         ew_csv=\"portfolio_daily_series_EW.csv\",\n",
    "                         factor_csv=\"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/5_Factors_Plus_Momentum.csv\",\n",
    "                         save_dir=\"results\",\n",
    "                         y_is_excess=True,\n",
    "                         hac_lags=5,\n",
    "                         save_txt=True):\n",
    "    vw_df = pd.read_csv(vw_csv)\n",
    "    ew_df = pd.read_csv(ew_csv)\n",
    "\n",
    "    vw_gross = batch_factor_analysis(\n",
    "        vw_df, factor_csv, scheme='VW', gross_only=True)\n",
    "    vw_net   = batch_factor_analysis(\n",
    "        vw_df, factor_csv, scheme='VW', gross_only=False)\n",
    "\n",
    "    ew_gross = batch_factor_analysis(\n",
    "        ew_df, factor_csv, scheme='EW', gross_only=True)\n",
    "    ew_net   = batch_factor_analysis(\n",
    "        ew_df, factor_csv, scheme='EW', gross_only=False)\n",
    "\n",
    "    return vw_gross, vw_net, ew_gross, ew_net\n",
    "    \n",
    "\n",
    "vw_gross, vw_net, ew_gross, ew_net = run_all_factor_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish: portfolio_daily_series_VW_with_rf.csv\n",
      "Finish: portfolio_daily_series_EW_with_rf.csv\n"
     ]
    }
   ],
   "source": [
    "# === File paths ===\n",
    "rf_file = \"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/CRSP_2016_2024_top50_with_exret.csv\"\n",
    "vw_file = \"portfolio_daily_series_VW.csv\"\n",
    "ew_file = \"portfolio_daily_series_EW.csv\"\n",
    "\n",
    "# === Load risk-free rate (rf) data ===\n",
    "\n",
    "rf_df = pd.read_csv(rf_file, usecols=[\"date\", \"rf\"])\n",
    "rf_df[\"date\"] = pd.to_datetime(rf_df[\"date\"])\n",
    "rf_dict = dict(zip(rf_df[\"date\"], rf_df[\"rf\"]))\n",
    "\n",
    "\n",
    "def adjust_returns_with_rf_grouped(file_path, output_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Parse date with flexible format\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], format='mixed', dayfirst=True)\n",
    "\n",
    "    # Find all return columns (e.g., tc5_return, tc10_return, etc.)\n",
    "    return_cols = [col for col in df.columns if \"return\" in col and \"cumul\" not in col]\n",
    "\n",
    "    # Set portfolio_type order to avoid groupby sorting issues\n",
    "    order = [\"long_only\", \"short_only\", \"long_short\"]\n",
    "    df[\"portfolio_type\"] = pd.Categorical(df[\"portfolio_type\"], categories=order, ordered=True)\n",
    "\n",
    "    df_list = []\n",
    "    # Group by scheme/model/window/portfolio_type, add rf and recalculate cumulative\n",
    "    for _, group in df.groupby([\"scheme\", \"model\", \"window\", \"portfolio_type\"], sort=False):\n",
    "        group = group.sort_values(\"date\").copy()\n",
    "        for col in return_cols:\n",
    "            # Add rf to daily return\n",
    "            group[col] = group.apply(lambda row: row[col] + rf_dict.get(row[\"date\"], 0), axis=1)\n",
    "\n",
    "            # Find the corresponding cumulative column (same naming as original table)\n",
    "            cum_col = col.replace(\"return\", \"cumulative\")\n",
    "            group[cum_col] = np.log1p(group[col]).cumsum()\n",
    "        df_list.append(group)\n",
    "\n",
    "    # Concatenate results and output in order\n",
    "    df_new = pd.concat(df_list).sort_values([\"scheme\", \"model\", \"window\", \"portfolio_type\", \"date\"])\n",
    "    df_new.to_csv(output_path, index=False)\n",
    "    print(f\"Finished: {output_path}\")\n",
    "\n",
    "# === Process VW and EW files ===\n",
    "adjust_returns_with_rf_grouped(vw_file, \"portfolio_daily_series_VW_with_rf.csv\")\n",
    "adjust_returns_with_rf_grouped(ew_file, \"portfolio_daily_series_EW_with_rf.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All figures have been generated and saved to: Baseline_Portfolio/\n"
     ]
    }
   ],
   "source": [
    "# ======== Download S&P500 (2016-2024) ========\n",
    "sp500 = yf.download(\"^GSPC\", start=\"2016-01-01\", end=\"2024-12-31\")\n",
    "price_col = \"Adj Close\" if \"Adj Close\" in sp500.columns else \"Close\"\n",
    "sp500[\"daily_return\"] = sp500[price_col].pct_change().fillna(0)\n",
    "# Cumulative log return (as in paper)\n",
    "sp500[\"cum_return\"] = np.cumsum(np.log1p(sp500[\"daily_return\"]))\n",
    "sp500 = sp500[[\"cum_return\"]]\n",
    "sp500.index = pd.to_datetime(sp500.index)\n",
    "\n",
    "# ======== Configuration ========\n",
    "files = [\n",
    "    (\"VW\", \"portfolio_daily_series_VW_with_rf.csv\"),\n",
    "    (\"EW\", \"portfolio_daily_series_EW_with_rf.csv\")\n",
    "]\n",
    "tc_levels = [0, 5, 10, 20, 40]      # Transaction cost (bps)\n",
    "windows = [5, 21, 252, 512]         # Window size\n",
    "strategies = [\"long_only\", \"short_only\", \"long_short\"]\n",
    "\n",
    "output_dir = \"Baseline_Portfolio\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Economic event periods (for shading)\n",
    "crisis_periods = [\n",
    "    (datetime(2018, 6, 1), datetime(2019, 1, 1), \"US-China Trade War\"),\n",
    "    (datetime(2020, 2, 1), datetime(2020, 7, 1), \"COVID-19\"),\n",
    "    (datetime(2022, 2, 1), datetime(2022, 6, 1), \"Russia-Ukraine War\"),\n",
    "    (datetime(2023, 1, 1), datetime(2023, 4, 1), \"US Bank Crisis\"),\n",
    "]\n",
    "\n",
    "def plot_comparison_styled(df, scheme, tc, window):\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    model_names = df[\"model\"].unique()\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(model_names)))\n",
    "\n",
    "    offset_step = 0.02\n",
    "\n",
    "    for i, strat in enumerate(strategies, 1):\n",
    "        ax = plt.subplot(3, 1, i)\n",
    "\n",
    "        plt.plot(sp500.index, sp500[\"cum_return\"],\n",
    "                 color=\"black\", lw=2.5, label=\"S&P500 (Total Return)\", zorder=10)\n",
    "\n",
    "        for idx, model_name in enumerate(model_names):\n",
    "            sub = df[(df[\"window\"] == window) &\n",
    "                     (df[\"portfolio_type\"] == strat) &\n",
    "                     (df[\"model\"] == model_name)].sort_values(\"date\")\n",
    "            if sub.empty:\n",
    "                continue\n",
    "\n",
    "            if tc == 0:\n",
    "                ret_col = \"return\"          # Original excess return\n",
    "            else:\n",
    "                ret_col = f\"tc{tc}_return\"  # Return with transaction cost\n",
    "\n",
    "            if ret_col not in sub.columns:\n",
    "                continue\n",
    "\n",
    "            log_cum = np.cumsum(np.log1p(sub[ret_col].values))\n",
    "\n",
    "            y_shift = idx * offset_step\n",
    "            plt.plot(sub[\"date\"], log_cum + y_shift,\n",
    "                     label=f\"{model_name} ({strat.replace('_',' ').title()})\",\n",
    "                     lw=2, color=colors[idx], alpha=0.9)\n",
    "\n",
    "        for start, end, label in crisis_periods:\n",
    "            ax.axvspan(start, end, color='grey', alpha=0.3)\n",
    "            ax.text(start + pd.Timedelta(days=10),\n",
    "                    ax.get_ylim()[1]*0.92, label, fontsize=8, color='grey')\n",
    "        ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "        ax.set_ylabel(\"Cumulative log return (start = 0)\")\n",
    "        ax.set_title(f\"{scheme} | Window={window} | Strategy={strat} | TC={tc} bps\")\n",
    "        ax.grid(alpha=0.3)\n",
    "        plt.xticks(rotation=30)\n",
    "        plt.legend(bbox_to_anchor=(1.04, 1), loc='upper left', fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fname = f\"{scheme}_window{window}_TC{tc}_logreturn_offset.png\"\n",
    "    plt.savefig(os.path.join(output_dir, fname), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ======== Main loop to generate all figures ========\n",
    "for scheme, file_path in files:\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    for tc in tc_levels:\n",
    "        for window in windows:\n",
    "            plot_comparison_styled(df, scheme, tc, window)\n",
    "\n",
    "print(f\"All figures have been generated and saved to: {output_dir}/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Update] ΔSharpe has been written to portfolio_results_daily_rebalance_VW.csv\n",
      "[Update] ΔSharpe has been written to portfolio_results_daily_rebalance_EW.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load R²_zero from portfolio_metrics.csv\n",
    "metrics_df = pd.read_csv(\"portfolio_metrics.csv\")[[\"Model\", \"Window\", \"R²_zero\"]]\n",
    "metrics_df.rename(columns={\"Model\": \"model\", \"Window\": \"window\"}, inplace=True)\n",
    "\n",
    "# Process VW/EW result files\n",
    "for fname in [\"portfolio_results_daily_rebalance_VW.csv\", \"portfolio_results_daily_rebalance_EW.csv\"]:\n",
    "    df = pd.read_csv(fname)\n",
    "\n",
    "    # Merge R²_zero by model and window\n",
    "    df = df.merge(metrics_df, on=[\"model\", \"window\"], how=\"left\")\n",
    "\n",
    "    rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        r2 = float(row[\"R²_zero\"]) if not pd.isna(row[\"R²_zero\"]) else 0.0\n",
    "        if row[\"portfolio_type\"] == \"long_only\":\n",
    "            d_sr, sr_star = delta_sharpe(r2, SR_MKT_EX)\n",
    "            row[\"ΔSharpe\"]  = d_sr\n",
    "            row[\"Sharpe*\"]  = sr_star\n",
    "            row[\"baseline\"] = f\"SPX_excess ({SR_MKT_EX:.2f})\"\n",
    "        else:\n",
    "            d_sr, sr_star = delta_sharpe(r2, 0)\n",
    "            row[\"ΔSharpe\"]  = d_sr\n",
    "            row[\"Sharpe*\"]  = sr_star\n",
    "            row[\"baseline\"] = \"cash (0)\"\n",
    "        rows.append(row)\n",
    "\n",
    "    pd.DataFrame(rows).to_csv(fname, index=False)\n",
    "    print(f\"[Update] ΔSharpe has been written to {fname}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-mac)",
   "language": "python",
   "name": "tf-mac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
