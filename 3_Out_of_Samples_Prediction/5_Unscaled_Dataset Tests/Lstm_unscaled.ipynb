{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optuna is available for hyperparameter tuning\n",
            "Using device: mps\n"
          ]
        }
      ],
      "source": [
        "# ========== Basic Libraries ==========\n",
        "import os\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gc\n",
        "import joblib\n",
        "import random\n",
        "\n",
        "# ========== Evaluation Metrics ==========\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error, mean_absolute_error, r2_score, confusion_matrix\n",
        ")\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "# ========== Hyperparameter Tuning ==========\n",
        "try:\n",
        "    import optuna\n",
        "    OPTUNA_AVAILABLE = True\n",
        "    print(\"Optuna is available for hyperparameter tuning\")\n",
        "except ImportError:\n",
        "    OPTUNA_AVAILABLE = False\n",
        "    print(\"Optuna not available, will use default parameters\")\n",
        "\n",
        "# ========== Global Configuration ==========\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def set_random_seeds(seed=42):\n",
        "    \"\"\"Set all relevant random seeds for reproducibility\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    if torch.backends.mps.is_available():\n",
        "        torch.mps.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_random_seeds(42)\n",
        "\n",
        "device = (\n",
        "    \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "def clear_memory():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    if device == \"mps\":\n",
        "        torch.mps.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkHmGLL_E_dl",
        "outputId": "0eef771b-7434-42ba-ced9-78648d558836"
      },
      "outputs": [],
      "source": [
        "# ========== 2. Evaluation Metrics ==========\n",
        "\n",
        "def r2_zero(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute zero-based R² (baseline is 0)\n",
        "    y_true: true values (N,)\n",
        "    y_pred: predicted values (N,)\n",
        "    \"\"\"\n",
        "    rss = np.sum((y_true - y_pred)**2)  \n",
        "    tss = np.sum(y_true**2)            \n",
        "    return 1 - rss / tss\n",
        "\n",
        "def calc_directional_metrics(y_true, y_pred, permnos=None):\n",
        "    \"\"\"\n",
        "    - Sign prediction at sample level\n",
        "    - If grouped by stock, calculate Overall, Up, Down for each stock and then average\n",
        "    \"\"\"\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "\n",
        "    if permnos is None:\n",
        "        s_true = np.sign(y_true)\n",
        "        s_pred = np.sign(y_pred)\n",
        "        mask = s_true != 0\n",
        "        s_true = s_true[mask]\n",
        "        s_pred = s_pred[mask]\n",
        "\n",
        "        overall_acc = np.mean(s_true == s_pred)\n",
        "\n",
        "        up_mask = s_true > 0\n",
        "        down_mask = s_true < 0\n",
        "        up_acc = np.mean(s_true[up_mask] == s_pred[up_mask]) if np.any(up_mask) else 0\n",
        "        down_acc = np.mean(s_true[down_mask] == s_pred[down_mask]) if np.any(down_mask) else 0\n",
        "\n",
        "    else:\n",
        "        df = pd.DataFrame({\"permno\": permnos, \"yt\": y_true, \"yp\": y_pred})\n",
        "        overall_accs = []\n",
        "        up_accs = []\n",
        "        down_accs = []\n",
        "\n",
        "        for _, g in df.groupby(\"permno\"):\n",
        "            s_true = np.sign(g[\"yt\"].values)\n",
        "            s_pred = np.sign(g[\"yp\"].values)\n",
        "            mask = s_true != 0\n",
        "            s_true = s_true[mask]\n",
        "            s_pred = s_pred[mask]\n",
        "            if len(s_true) == 0:\n",
        "                continue\n",
        "            overall_accs.append(np.mean(s_true == s_pred))\n",
        "\n",
        "            up_mask = s_true > 0\n",
        "            down_mask = s_true < 0\n",
        "            up_accs.append(np.mean(s_true[up_mask] == s_pred[up_mask]) if np.any(up_mask) else np.nan)\n",
        "            down_accs.append(np.mean(s_true[down_mask] == s_pred[down_mask]) if np.any(down_mask) else np.nan)\n",
        "\n",
        "        overall_acc = np.nanmean(overall_accs)\n",
        "        up_acc = np.nanmean(up_accs)\n",
        "        down_acc = np.nanmean(down_accs)\n",
        "\n",
        "    return overall_acc, up_acc, down_acc\n",
        "\n",
        "def regression_metrics(y_true, y_pred, k, meta=None, permnos=None):\n",
        "    \"\"\"\n",
        "    Includes:\n",
        "    - Regression metrics\n",
        "    - Pointwise directional accuracy\n",
        "    - Market cap group metrics\n",
        "    \"\"\"\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    n = len(y_true)\n",
        "\n",
        "    r2 = r2_zero(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "\n",
        "    dir_acc, up_acc, down_acc = calc_directional_metrics(y_true, y_pred, permnos)\n",
        "\n",
        "    metrics = {\n",
        "        \"R2_zero\": r2,\n",
        "        \"RMSE\": rmse,\n",
        "        \"MAE\": mae,\n",
        "        \"MSE\": mse,\n",
        "        \"Directional Accuracy\": dir_acc,\n",
        "        \"Up_Directional_Acc\": up_acc,\n",
        "        \"Down_Directional_Acc\": down_acc\n",
        "    }\n",
        "\n",
        "    if meta is not None and \"MKTCAP_PERCENTILE\" in meta:\n",
        "        top_mask = meta[\"MKTCAP_PERCENTILE\"] >= 0.75\n",
        "        bottom_mask = meta[\"MKTCAP_PERCENTILE\"] <= 0.25\n",
        "\n",
        "        if np.any(top_mask):\n",
        "            yt_top = y_true[top_mask]\n",
        "            yp_top = y_pred[top_mask]\n",
        "            perm_top = permnos[top_mask] if permnos is not None else None\n",
        "            r2_top = r2_zero(yt_top, yp_top)\n",
        "            rmse_top = np.sqrt(mean_squared_error(yt_top, yp_top))\n",
        "            mae_top = mean_absolute_error(yt_top, yp_top)\n",
        "            mse_top = mean_squared_error(yt_top, yp_top)\n",
        "            dir_top, up_top, down_top = calc_directional_metrics(yt_top, yp_top, perm_top)\n",
        "            metrics.update({\n",
        "                \"Top25_R2_zero\": r2_top,\n",
        "                \"Top25_RMSE\": rmse_top,\n",
        "                \"Top25_MAE\": mae_top,\n",
        "                \"Top25_MSE\": mse_top,\n",
        "                \"Top25_Dir_Acc\": dir_top,\n",
        "                \"Top25_Up_Acc\": up_top,\n",
        "                \"Top25_Down_Acc\": down_top\n",
        "            })\n",
        "\n",
        "        if np.any(bottom_mask):\n",
        "            yt_bot = y_true[bottom_mask]\n",
        "            yp_bot = y_pred[bottom_mask]\n",
        "            perm_bot = permnos[bottom_mask] if permnos is not None else None\n",
        "            r2_bot = r2_zero(yt_bot, yp_bot)\n",
        "            rmse_bot = np.sqrt(mean_squared_error(yt_bot, yp_bot))\n",
        "            mae_bot = mean_absolute_error(yt_bot, yp_bot)\n",
        "            mse_bot = mean_squared_error(yt_bot, yp_bot)\n",
        "            dir_bot, up_bot, down_bot = calc_directional_metrics(yt_bot, yp_bot, perm_bot)\n",
        "            metrics.update({\n",
        "                \"Bottom25_R2_zero\": r2_bot,\n",
        "                \"Bottom25_RMSE\": rmse_bot,\n",
        "                \"Bottom25_MAE\": mae_bot,\n",
        "                \"Bottom25_MSE\": mse_bot,\n",
        "                \"Bottom25_Dir_Acc\": dir_bot,\n",
        "                \"Bottom25_Up_Acc\": up_bot,\n",
        "                \"Bottom25_Down_Acc\": down_bot\n",
        "            })\n",
        "\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Fw6M08OE_dm"
      },
      "outputs": [],
      "source": [
        "# ========== 3. Save functions (consistent with Linear_Models) ==========\n",
        "\n",
        "def save_model(model, name, window, path=\"models/\"):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    torch.save(model.state_dict(), os.path.join(path, f\"{name}_w{window}.pth\"))\n",
        "\n",
        "def save_metrics(metrics_dict, name, window, path=\"results.csv\"):\n",
        "    row = pd.DataFrame([metrics_dict])\n",
        "    row.insert(0, \"Model\", name)\n",
        "    row.insert(1, \"Window\", window)\n",
        "\n",
        "    if os.path.exists(path):\n",
        "        df = pd.read_csv(path)\n",
        "        df = df[~((df[\"Model\"] == name) & (df[\"Window\"] == window))]\n",
        "        df = pd.concat([df, row], ignore_index=True)\n",
        "        df.to_csv(path, index=False)\n",
        "        print(f\"[Update] Metrics updated for {name} w={window}\")\n",
        "    else:\n",
        "        row.to_csv(path, index=False)\n",
        "        print(f\"[Create] New metrics file created with {name} w={window}\")\n",
        "\n",
        "def save_predictions(model_name, window_size, y_true, y_pred, permnos, path=\"predictions/\"):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    \n",
        "    df = pd.DataFrame({\n",
        "        \"PERMNO\": permnos,\n",
        "        \"y_true\": y_true,\n",
        "        \"y_pred\": y_pred\n",
        "    })\n",
        "\n",
        "    filename = f\"{model_name}_w{window_size}.csv\"\n",
        "    df.to_csv(os.path.join(path, filename), index=False)\n",
        "    print(f\"[Save] {filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeMzH06WE_dm"
      },
      "outputs": [],
      "source": [
        "# ========== 4. Dataset and Model Definition ==========\n",
        "\n",
        "class StockDataset(Dataset):\n",
        "    def __init__(self, X, y, device):\n",
        "        self.X = torch.FloatTensor(X).to(device)\n",
        "        self.y = torch.FloatTensor(y).to(device)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, dropout=0.2):\n",
        "        super(LSTMModel, self).__init__()\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, input_size)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        # Only use the output of the last time step\n",
        "        last_output = lstm_out[:, -1, :]\n",
        "        last_output = self.dropout(last_output)\n",
        "        out = self.fc(last_output)\n",
        "        return out.squeeze()\n",
        "\n",
        "def prepare_data(X, y, batch_size=32, device=None):\n",
        "    \"\"\"Prepare data loader\"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "    dataset = StockDataset(X, y, device)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1AlUtoL9E_dn",
        "outputId": "f520b088-ef55-46de-c990-8c9866ce19d7"
      },
      "outputs": [],
      "source": [
        "# ========== 5. Training Functions ==========\n",
        "\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for X, y in train_loader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model(X)\n",
        "        loss = criterion(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    \"\"\"Validate the model\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    predictions = []\n",
        "    targets = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for X, y in val_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            output = model(X)\n",
        "            loss = criterion(output, y)\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            predictions.extend(output.cpu().numpy())\n",
        "            targets.extend(y.cpu().numpy())\n",
        "    \n",
        "    return (total_loss / len(val_loader), \n",
        "            np.array(predictions), \n",
        "            np.array(targets))\n",
        "\n",
        "def create_train_val_split_timeseries(X_train, y_train, permnos_train, n_splits=3, test_size_ratio=0.2):\n",
        "    \"\"\"\n",
        "    Create time series cross-validation splits using TimeSeriesSplit to avoid data leakage.\n",
        "    Return the last split as the train-validation split.\n",
        "    \"\"\"\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits, test_size=int(len(X_train) * test_size_ratio))\n",
        "    splits = list(tscv.split(X_train))\n",
        "    train_idx, val_idx = splits[-1]\n",
        "    \n",
        "    X_tr = X_train[train_idx]\n",
        "    X_val = X_train[val_idx]\n",
        "    y_tr = y_train[train_idx]\n",
        "    y_val = y_train[val_idx]\n",
        "    \n",
        "    if permnos_train is not None:\n",
        "        perm_tr = permnos_train[train_idx]\n",
        "        perm_val = permnos_train[val_idx]\n",
        "        return X_tr, X_val, y_tr, y_val, perm_tr, perm_val\n",
        "    else:\n",
        "        return X_tr, X_val, y_tr, y_val, None, None\n",
        "\n",
        "def create_train_val_split(X_train, y_train, permnos_train, val_ratio=0.2):\n",
        "    \"\"\"\n",
        "    Create a validation set from the end of the training set in chronological order.\n",
        "    Take the last val_ratio proportion of the data as the validation set.\n",
        "    \"\"\"\n",
        "    split_idx = int(len(X_train) * (1 - val_ratio))\n",
        "    \n",
        "    X_tr = X_train[:split_idx]\n",
        "    X_val = X_train[split_idx:]\n",
        "    y_tr = y_train[:split_idx]\n",
        "    y_val = y_train[split_idx:]\n",
        "    \n",
        "    if permnos_train is not None:\n",
        "        perm_tr = permnos_train[:split_idx]\n",
        "        perm_val = permnos_train[split_idx:]\n",
        "        return X_tr, X_val, y_tr, y_val, perm_tr, perm_val\n",
        "    else:\n",
        "        return X_tr, X_val, y_tr, y_val, None, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== 6. Hyperparameter Optimization Function ==========\n",
        "\n",
        "def optimize_lstm_hyperparameters(X_train, y_train, permnos_train, window_size, n_trials=15):\n",
        "    \"\"\"\n",
        "    Use Optuna to optimize LSTM hyperparameters (with pruning, some parameters fixed)\n",
        "    \"\"\"\n",
        "    if not OPTUNA_AVAILABLE:\n",
        "        print(\"[Warning] Optuna not available, using default parameters\")\n",
        "        return {\n",
        "            'batch_size': 32,\n",
        "            'learning_rate': 0.0005,\n",
        "            'dropout_rate': 0.1,\n",
        "            'hidden_size': 128,\n",
        "            'num_layers': 2,\n",
        "            'epochs': 50\n",
        "        }\n",
        "    \n",
        "    X_train_reshaped = X_train.reshape(X_train.shape[0], window_size, -1)\n",
        "    input_size = X_train_reshaped.shape[2]\n",
        "    \n",
        "    X_tr, X_val, y_tr, y_val, _, _ = create_train_val_split_timeseries(\n",
        "        X_train_reshaped, y_train, permnos_train, n_splits=4, test_size_ratio=0.2\n",
        "    )\n",
        "    \n",
        "    cpu_device = torch.device(\"cpu\")\n",
        "    \n",
        "    def objective(trial):\n",
        "        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
        "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 5e-3, log=True)\n",
        "        dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.2)\n",
        "        \n",
        "        hidden_size = 128\n",
        "        num_layers = 2\n",
        "        \n",
        "        try:\n",
        "            train_loader = prepare_data(X_tr, y_tr, batch_size=batch_size, device=cpu_device)\n",
        "            val_loader = prepare_data(X_val, y_val, batch_size=batch_size, device=cpu_device)\n",
        "            \n",
        "            model = LSTMModel(\n",
        "                input_size=input_size,\n",
        "                hidden_size=hidden_size,\n",
        "                num_layers=num_layers,\n",
        "                dropout=dropout_rate\n",
        "            ).to(cpu_device)\n",
        "            \n",
        "            criterion = nn.MSELoss()\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "            \n",
        "            best_val_loss = float('inf')\n",
        "            patience = 10\n",
        "            patience_counter = 0\n",
        "            max_epochs = 20\n",
        "            \n",
        "            for epoch in range(max_epochs):\n",
        "                model.train()\n",
        "                train_loss = 0\n",
        "                for X_batch, y_batch in train_loader:\n",
        "                    X_batch, y_batch = X_batch.to(cpu_device), y_batch.to(cpu_device)\n",
        "                    optimizer.zero_grad()\n",
        "                    output = model(X_batch)\n",
        "                    loss = criterion(output, y_batch)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    train_loss += loss.item()\n",
        "                \n",
        "                model.eval()\n",
        "                val_loss = 0\n",
        "                with torch.no_grad():\n",
        "                    for X_batch, y_batch in val_loader:\n",
        "                        X_batch, y_batch = X_batch.to(cpu_device), y_batch.to(cpu_device)\n",
        "                        output = model(X_batch)\n",
        "                        loss = criterion(output, y_batch)\n",
        "                        val_loss += loss.item()\n",
        "                \n",
        "                val_loss /= len(val_loader)\n",
        "                \n",
        "                trial.report(val_loss, epoch)\n",
        "                \n",
        "                if trial.should_prune():\n",
        "                    del model, train_loader, val_loader\n",
        "                    gc.collect()\n",
        "                    raise optuna.exceptions.TrialPruned()\n",
        "                \n",
        "                if val_loss < best_val_loss:\n",
        "                    best_val_loss = val_loss\n",
        "                    patience_counter = 0\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "                    if patience_counter >= patience:\n",
        "                        break\n",
        "            \n",
        "            del model, train_loader, val_loader\n",
        "            gc.collect()\n",
        "            \n",
        "            return best_val_loss\n",
        "            \n",
        "        except optuna.exceptions.TrialPruned:\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            print(f\"Trial failed: {e}\")\n",
        "            return float('inf')\n",
        "    \n",
        "    pruner = optuna.pruners.MedianPruner(\n",
        "        n_startup_trials=3,\n",
        "        n_warmup_steps=5,\n",
        "        interval_steps=2\n",
        "    )\n",
        "    \n",
        "    print(f\"[Optuna] Starting hyperparameter optimization with {n_trials} trials on CPU...\")\n",
        "    print(\"[Optuna] Using MedianPruner for early trial termination\")\n",
        "    print(\"[Optuna] Fixed parameters: hidden_size=128, num_layers=2\")\n",
        "    print(\"[Optuna] Searching: batch_size, learning_rate, dropout_rate\")\n",
        "    \n",
        "    study = optuna.create_study(\n",
        "        direction='minimize', \n",
        "        sampler=optuna.samplers.TPESampler(seed=42),\n",
        "        pruner=pruner\n",
        "    )\n",
        "    \n",
        "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
        "    \n",
        "    best_params = study.best_params\n",
        "    best_params['hidden_size'] = 128\n",
        "    best_params['num_layers'] = 2\n",
        "    best_params['epochs'] = 50\n",
        "    \n",
        "    print(f\"[Optuna] Best parameters: {best_params}\")\n",
        "    print(f\"[Optuna] Best validation loss: {study.best_value:.6f}\")\n",
        "    print(f\"[Optuna] Number of pruned trials: {len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])}\")\n",
        "    print(f\"[Optuna] Number of completed trials: {len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])}\")\n",
        "    \n",
        "    return best_params\n",
        "\n",
        "# ========== 7. Training and Evaluation Main Function ==========\n",
        "\n",
        "def train_and_evaluate_lstm(window_size, X_train, y_train, X_test, y_test,\n",
        "                           permnos_train, permnos_test, meta=None, shared_params=None):\n",
        "    \"\"\"Train and evaluate LSTM model\"\"\"\n",
        "    print(f\"\\nTraining LSTM on Window = {window_size}\")\n",
        "    \n",
        "    X_train_reshaped = X_train.reshape(X_train.shape[0], window_size, -1)\n",
        "    X_test_reshaped = X_test.reshape(X_test.shape[0], window_size, -1)\n",
        "    \n",
        "    input_size = X_train_reshaped.shape[2]\n",
        "    print(f\"[Info] Input shape: {X_train_reshaped.shape}, Input size: {input_size}\")\n",
        "    \n",
        "    X_tr, X_val, y_tr, y_val, perm_tr, perm_val = create_train_val_split_timeseries(\n",
        "        X_train_reshaped, y_train, permnos_train, n_splits=4, test_size_ratio=0.2\n",
        "    )\n",
        "    \n",
        "    if shared_params is not None:\n",
        "        print(f\"[Info] Using shared parameters for LSTM\")\n",
        "        batch_size = shared_params['batch_size']\n",
        "        learning_rate = shared_params['learning_rate']\n",
        "        dropout_rate = shared_params['dropout_rate']\n",
        "        hidden_size = shared_params['hidden_size']\n",
        "        num_layers = shared_params['num_layers']\n",
        "        epochs = shared_params['epochs']\n",
        "    else:\n",
        "        print(f\"[Info] Using default parameters for LSTM\")\n",
        "        batch_size = 32\n",
        "        learning_rate = 0.0005\n",
        "        dropout_rate = 0.1\n",
        "        hidden_size = 128\n",
        "        num_layers = 2\n",
        "        epochs = 50\n",
        "    \n",
        "    train_loader = prepare_data(X_tr, y_tr, batch_size=batch_size, device=device)\n",
        "    val_loader = prepare_data(X_val, y_val, batch_size=batch_size, device=device)\n",
        "    test_loader = prepare_data(X_test_reshaped, y_test, batch_size=batch_size, device=device)\n",
        "    \n",
        "    model = LSTMModel(\n",
        "        input_size=input_size,\n",
        "        hidden_size=hidden_size,\n",
        "        num_layers=num_layers,\n",
        "        dropout=dropout_rate\n",
        "    ).to(device)\n",
        "    \n",
        "    criterion = nn.MSELoss().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "    best_model = None\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 15\n",
        "    patience_counter = 0\n",
        "    \n",
        "    print(f\"[Info] Starting training for LSTM with {epochs} epochs (early stopping patience={patience})...\")\n",
        "    print(f\"[Info] Train size: {len(X_tr)}, Val size: {len(X_val)}, Test size: {len(X_test)}\")\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss, predictions, targets = validate(model, val_loader, criterion, device)\n",
        "        \n",
        "        pred_std = np.std(predictions)\n",
        "        positive_ratio = (np.sign(predictions) > 0).mean()\n",
        "        \n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model = model.state_dict().copy()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            \n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"[LSTM] Epoch {epoch+1}/{epochs}, \"\n",
        "                  f\"Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}, \"\n",
        "                  f\"PredStd: {pred_std:.6f}, PosRatio: {positive_ratio:.3f}\")\n",
        "        \n",
        "        if patience_counter >= patience:\n",
        "            print(f\"[LSTM] Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "    \n",
        "    if best_model is not None:\n",
        "        model.load_state_dict(best_model)\n",
        "    \n",
        "    _, y_pred, y_true = validate(model, test_loader, criterion, device)\n",
        "    \n",
        "    print(\"\\n=== Directional Sanity Check ===\")\n",
        "    print(\"Pos ratio (y_test):\", (y_test > 0).mean())\n",
        "    print(\"Neg ratio (y_test):\", (y_test < 0).mean())\n",
        "    sign_pred = np.sign(y_pred)\n",
        "    print(\"Pred +1 ratio:\", (sign_pred > 0).mean())\n",
        "    print(\"Pred -1 ratio:\", (sign_pred < 0).mean())\n",
        "    \n",
        "    conf = confusion_matrix(np.sign(y_test), sign_pred, labels=[1, -1])\n",
        "    print(\"      Pred+  Pred-\")\n",
        "    print(\"+1 |\", conf[0])\n",
        "    print(\"-1 |\", conf[1])\n",
        "    \n",
        "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    metrics = regression_metrics(y_true, y_pred, k=window_size, meta=meta, permnos=permnos_test)\n",
        "    print(f\"[Info] Model has {num_params} trainable parameters, using window_size={window_size} for Adjusted R²\")\n",
        "    \n",
        "    save_model(model, \"LSTM\", window_size)\n",
        "    save_metrics(metrics, \"LSTM\", window_size)\n",
        "    save_predictions(\"LSTM\", window_size, y_true, y_pred, permnos_test)\n",
        "    \n",
        "    print(f\"[Info] Training completed for LSTM\")\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== 8. Main Dispatcher Function ==========\n",
        "\n",
        "def load_datasets(npz_path=\"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/all_window_datasets_unscaled.npz\"):\n",
        "    \"\"\"Load dataset\"\"\"\n",
        "    data = np.load(npz_path, allow_pickle=True)\n",
        "    datasets = {}\n",
        "    for key in data.files:\n",
        "        datasets[key] = data[key]\n",
        "    return datasets\n",
        "\n",
        "def loop_all_windows():\n",
        "    \"\"\"Train LSTM on different window sizes\"\"\"\n",
        "    datasets = load_datasets()\n",
        "    \n",
        "    window_sizes = [5, 21, 252, 512]\n",
        "    shared_params = None\n",
        "\n",
        "    for window in window_sizes:\n",
        "        print(f\"\\n=== Processing Window Size: {window} ===\")\n",
        "        \n",
        "        X_train = datasets[f\"X_train_{window}\"]\n",
        "        y_train = datasets[f\"y_train_{window}\"]\n",
        "        X_test = datasets[f\"X_test_{window}\"]\n",
        "        y_test = datasets[f\"y_test_{window}\"]\n",
        "        \n",
        "        meta_train_dict = datasets[f\"meta_train_{window}\"].item()\n",
        "        meta_test_dict = datasets[f\"meta_test_{window}\"].item()\n",
        "\n",
        "        meta_train = pd.DataFrame.from_dict(meta_train_dict)\n",
        "        meta_test = pd.DataFrame.from_dict(meta_test_dict)\n",
        "\n",
        "        permnos_train = meta_train[\"PERMNO\"].values\n",
        "        permnos_test = meta_test[\"PERMNO\"].values\n",
        "\n",
        "        if window == 5:\n",
        "            print(\"\\nTraining LSTM to get shared parameters...\")\n",
        "            shared_params = optimize_lstm_hyperparameters(\n",
        "                X_train, y_train, permnos_train, window, n_trials=15\n",
        "            )\n",
        "            print(f\"Shared parameters from LSTM: {shared_params}\")\n",
        "\n",
        "        if window == 5:\n",
        "            print(\"\\nTraining LSTM with optimized parameters...\")\n",
        "        else:\n",
        "            print(f\"\\nTraining LSTM with shared parameters...\")\n",
        "            \n",
        "        train_and_evaluate_lstm(\n",
        "            window, X_train, y_train, X_test, y_test,\n",
        "            permnos_train, permnos_test, meta_test, shared_params\n",
        "        )\n",
        "        \n",
        "        clear_memory()\n",
        "        print(f\"Window {window} completed and memory cleared.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Processing Window Size: 5 ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-02 19:27:19,106] A new study created in memory with name: no-name-85eff0be-0980-4a90-849b-511603efbcd7\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training LSTM to get shared parameters...\n",
            "[Optuna] Starting hyperparameter optimization with 15 trials on CPU...\n",
            "[Optuna] Using MedianPruner for early trial termination\n",
            "[Optuna] Fixed parameters: hidden_size=128, num_layers=2\n",
            "[Optuna] Searching: batch_size, learning_rate, dropout_rate\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1725b3c41d5a46a28a07e56f90a1dfd6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[I 2025-08-02 19:30:18,312] Trial 0 finished with value: 0.00016400192423913853 and parameters: {'batch_size': 64, 'learning_rate': 0.00041282053438262235, 'dropout_rate': 0.031203728088487304}. Best is trial 0 with value: 0.00016400192423913853.\n",
            "[I 2025-08-02 19:32:34,330] Trial 1 finished with value: 0.00016334617800106121 and parameters: {'batch_size': 128, 'learning_rate': 0.00041917115166952007, 'dropout_rate': 0.1416145155592091}. Best is trial 1 with value: 0.00016334617800106121.\n",
            "[I 2025-08-02 19:35:23,771] Trial 2 finished with value: 0.0001636845359152849 and parameters: {'batch_size': 64, 'learning_rate': 3.7419406111184946e-05, 'dropout_rate': 0.03636499344142013}. Best is trial 1 with value: 0.00016334617800106121.\n",
            "[I 2025-08-02 19:36:03,759] Trial 3 pruned. \n",
            "[I 2025-08-02 19:37:16,473] Trial 4 pruned. \n",
            "[I 2025-08-02 19:38:30,570] Trial 5 pruned. \n",
            "[I 2025-08-02 19:39:50,720] Trial 6 pruned. \n",
            "[I 2025-08-02 19:41:09,023] Trial 7 pruned. \n",
            "[I 2025-08-02 19:42:04,099] Trial 8 pruned. \n",
            "[I 2025-08-02 19:43:20,924] Trial 9 pruned. \n",
            "[I 2025-08-02 19:45:35,183] Trial 10 finished with value: 0.00016325460356628437 and parameters: {'batch_size': 128, 'learning_rate': 1.0379733829293226e-05, 'dropout_rate': 0.15990303248508864}. Best is trial 10 with value: 0.00016325460356628437.\n",
            "[I 2025-08-02 19:47:50,258] Trial 11 finished with value: 0.0001632710291705262 and parameters: {'batch_size': 128, 'learning_rate': 1.8563021749927446e-05, 'dropout_rate': 0.16043997363603446}. Best is trial 10 with value: 0.00016325460356628437.\n",
            "[I 2025-08-02 19:50:06,113] Trial 12 finished with value: 0.00016328729927479727 and parameters: {'batch_size': 128, 'learning_rate': 1.0368237437467533e-05, 'dropout_rate': 0.15672592422980336}. Best is trial 10 with value: 0.00016325460356628437.\n",
            "[I 2025-08-02 19:51:13,159] Trial 13 pruned. \n",
            "[I 2025-08-02 19:52:06,202] Trial 14 pruned. \n",
            "[Optuna] Best parameters: {'batch_size': 128, 'learning_rate': 1.0379733829293226e-05, 'dropout_rate': 0.15990303248508864, 'hidden_size': 128, 'num_layers': 2, 'epochs': 50}\n",
            "[Optuna] Best validation loss: 0.000163\n",
            "[Optuna] Number of pruned trials: 9\n",
            "[Optuna] Number of completed trials: 6\n",
            "Shared parameters from LSTM: {'batch_size': 128, 'learning_rate': 1.0379733829293226e-05, 'dropout_rate': 0.15990303248508864, 'hidden_size': 128, 'num_layers': 2, 'epochs': 50}\n",
            "\n",
            "Training LSTM with optimized parameters...\n",
            "\n",
            "▶ Training LSTM on Window = 5\n",
            "[Info] Input shape: (196920, 5, 1), Input size: 1\n",
            "[Info] Using shared parameters for LSTM\n",
            "[Info] Starting training for LSTM with 50 epochs (early stopping patience=15)...\n",
            "[Info] Train size: 157536, Val size: 39384, Test size: 110850\n",
            "[LSTM] Epoch 10/50, Train Loss: 0.000449, Val Loss: 0.000168, PredStd: 0.002343, PosRatio: 0.638\n",
            "[LSTM] Epoch 20/50, Train Loss: 0.000441, Val Loss: 0.000164, PredStd: 0.001313, PosRatio: 0.716\n",
            "[LSTM] Epoch 30/50, Train Loss: 0.000438, Val Loss: 0.000163, PredStd: 0.000928, PosRatio: 0.792\n",
            "[LSTM] Epoch 40/50, Train Loss: 0.000437, Val Loss: 0.000163, PredStd: 0.000770, PosRatio: 0.850\n",
            "[LSTM] Epoch 50/50, Train Loss: 0.000437, Val Loss: 0.000163, PredStd: 0.000694, PosRatio: 0.894\n",
            "\\n=== Directional Sanity Check ===\n",
            "Pos ratio (y_test): 0.5225259359494813\n",
            "Neg ratio (y_test): 0.47667117726657643\n",
            "Pred +1 ratio: 0.8905638249887236\n",
            "Pred -1 ratio: 0.1094361750112765\n",
            "      Pred+  Pred-\n",
            "+1 | [51673  6249]\n",
            "-1 | [46963  5876]\n",
            "[Info] Model has 199297 trainable parameters, using window_size=5 for Adjusted R²\n",
            "[Update] Metrics updated for LSTM w=5\n",
            "[Save] LSTM_w5.csv\n",
            "[Info] Training completed for LSTM\n",
            "Window 5 completed and memory cleared.\n",
            "\n",
            "=== Processing Window Size: 21 ===\n",
            "\n",
            "Training LSTM with shared parameters...\n",
            "\n",
            "▶ Training LSTM on Window = 21\n",
            "[Info] Input shape: (196120, 21, 1), Input size: 1\n",
            "[Info] Using shared parameters for LSTM\n",
            "[Info] Starting training for LSTM with 50 epochs (early stopping patience=15)...\n",
            "[Info] Train size: 156896, Val size: 39224, Test size: 110850\n",
            "[LSTM] Epoch 10/50, Train Loss: 0.000465, Val Loss: 0.000187, PredStd: 0.003695, PosRatio: 0.556\n",
            "[LSTM] Epoch 20/50, Train Loss: 0.000453, Val Loss: 0.000181, PredStd: 0.002808, PosRatio: 0.594\n",
            "[LSTM] Epoch 30/50, Train Loss: 0.000449, Val Loss: 0.000178, PredStd: 0.002407, PosRatio: 0.616\n",
            "[LSTM] Epoch 40/50, Train Loss: 0.000446, Val Loss: 0.000176, PredStd: 0.002131, PosRatio: 0.675\n",
            "[LSTM] Epoch 50/50, Train Loss: 0.000445, Val Loss: 0.000174, PredStd: 0.001924, PosRatio: 0.686\n",
            "\\n=== Directional Sanity Check ===\n",
            "Pos ratio (y_test): 0.5225259359494813\n",
            "Neg ratio (y_test): 0.47667117726657643\n",
            "Pred +1 ratio: 0.6924943617501128\n",
            "Pred -1 ratio: 0.30749661705006764\n",
            "      Pred+  Pred-\n",
            "+1 | [40218 17703]\n",
            "-1 | [36488 16351]\n",
            "[Info] Model has 199297 trainable parameters, using window_size=21 for Adjusted R²\n",
            "[Update] Metrics updated for LSTM w=21\n",
            "[Save] LSTM_w21.csv\n",
            "[Info] Training completed for LSTM\n",
            "Window 21 completed and memory cleared.\n",
            "\n",
            "=== Processing Window Size: 252 ===\n",
            "\n",
            "Training LSTM with shared parameters...\n",
            "\n",
            "▶ Training LSTM on Window = 252\n",
            "[Info] Input shape: (184570, 252, 1), Input size: 1\n",
            "[Info] Using shared parameters for LSTM\n",
            "[Info] Starting training for LSTM with 50 epochs (early stopping patience=15)...\n",
            "[Info] Train size: 147656, Val size: 36914, Test size: 110850\n",
            "[LSTM] Epoch 10/50, Train Loss: 0.000404, Val Loss: 0.000168, PredStd: 0.002231, PosRatio: 0.637\n",
            "[LSTM] Epoch 20/50, Train Loss: 0.000395, Val Loss: 0.000164, PredStd: 0.001301, PosRatio: 0.748\n",
            "[LSTM] Epoch 30/50, Train Loss: 0.000393, Val Loss: 0.000164, PredStd: 0.001012, PosRatio: 0.801\n",
            "[LSTM] Epoch 40/50, Train Loss: 0.000392, Val Loss: 0.000164, PredStd: 0.000864, PosRatio: 0.830\n",
            "[LSTM] Epoch 50/50, Train Loss: 0.000391, Val Loss: 0.000163, PredStd: 0.000767, PosRatio: 0.860\n",
            "\\n=== Directional Sanity Check ===\n",
            "Pos ratio (y_test): 0.5225259359494813\n",
            "Neg ratio (y_test): 0.47667117726657643\n",
            "Pred +1 ratio: 0.8576274244474515\n",
            "Pred -1 ratio: 0.14237257555254848\n",
            "      Pred+  Pred-\n",
            "+1 | [49789  8133]\n",
            "-1 | [45198  7641]\n",
            "[Info] Model has 199297 trainable parameters, using window_size=252 for Adjusted R²\n",
            "[Update] Metrics updated for LSTM w=252\n",
            "[Save] LSTM_w252.csv\n",
            "[Info] Training completed for LSTM\n",
            "Window 252 completed and memory cleared.\n",
            "\n",
            "=== Processing Window Size: 512 ===\n",
            "\n",
            "Training LSTM with shared parameters...\n",
            "\n",
            "▶ Training LSTM on Window = 512\n",
            "[Info] Input shape: (171570, 512, 1), Input size: 1\n",
            "[Info] Using shared parameters for LSTM\n",
            "[Info] Starting training for LSTM with 50 epochs (early stopping patience=15)...\n",
            "[Info] Train size: 137256, Val size: 34314, Test size: 110850\n",
            "[LSTM] Epoch 10/50, Train Loss: 0.000369, Val Loss: 0.000172, PredStd: 0.002081, PosRatio: 0.803\n",
            "[LSTM] Epoch 20/50, Train Loss: 0.000361, Val Loss: 0.000168, PredStd: 0.001238, PosRatio: 0.920\n",
            "[LSTM] Epoch 30/50, Train Loss: 0.000359, Val Loss: 0.000167, PredStd: 0.000940, PosRatio: 0.956\n",
            "[LSTM] Epoch 40/50, Train Loss: 0.000357, Val Loss: 0.000167, PredStd: 0.000781, PosRatio: 0.973\n",
            "[LSTM] Epoch 50/50, Train Loss: 0.000356, Val Loss: 0.000166, PredStd: 0.000678, PosRatio: 0.982\n",
            "\\n=== Directional Sanity Check ===\n",
            "Pos ratio (y_test): 0.5225259359494813\n",
            "Neg ratio (y_test): 0.47667117726657643\n",
            "Pred +1 ratio: 0.9790888588182228\n",
            "Pred -1 ratio: 0.020911141181777178\n",
            "      Pred+  Pred-\n",
            "+1 | [56734  1188]\n",
            "-1 | [51710  1129]\n",
            "[Info] Model has 199297 trainable parameters, using window_size=512 for Adjusted R²\n",
            "[Update] Metrics updated for LSTM w=512\n",
            "[Save] LSTM_w512.csv\n",
            "[Info] Training completed for LSTM\n",
            "Window 512 completed and memory cleared.\n"
          ]
        }
      ],
      "source": [
        "# ========== 9. Entry Point ==========\n",
        "if __name__ == \"__main__\":\n",
        "    loop_all_windows()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (tf-mac)",
      "language": "python",
      "name": "tf-mac"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
