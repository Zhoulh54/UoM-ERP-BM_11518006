{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: install [-bCcpSsUv] [-f flags] [-g group] [-m mode] [-o owner]\n",
      "               [-M log] [-D dest] [-h hash] [-T tags]\n",
      "               [-B suffix] [-l linkflags] [-N dbdir]\n",
      "               file1 file2\n",
      "       install [-bCcpSsUv] [-f flags] [-g group] [-m mode] [-o owner]\n",
      "               [-M log] [-D dest] [-h hash] [-T tags]\n",
      "               [-B suffix] [-l linkflags] [-N dbdir]\n",
      "               file1 ... fileN directory\n",
      "       install -dU [-vU] [-g group] [-m mode] [-N dbdir] [-o owner]\n",
      "               [-M log] [-D dest] [-h hash] [-T tags]\n",
      "               directory ...\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# ========== Basic Libraries ==========\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ========== Model and Preprocessing ==========\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# ========== Evaluation Metrics ==========\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    ")\n",
    "\n",
    "# ========== Visualization and Hyperparameter Tuning ==========\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "!install optuna\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# ========== Global Configuration ==========\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "plt.rcdefaults()\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 1. Data Loading and Preprocessing ==========\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, X, y, device):\n",
    "        self.X = torch.FloatTensor(X).to(device)\n",
    "        self.y = torch.FloatTensor(y).to(device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def load_datasets(npz_path=\"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/all_window_datasets_unscaled.npz\"):\n",
    "    data = np.load(npz_path, allow_pickle=True)\n",
    "    datasets = {}\n",
    "    for key in data.files:\n",
    "        datasets[key] = data[key]\n",
    "    return datasets\n",
    "\n",
    "def prepare_data(X, y, batch_size=128, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    dataset = StockDataset(X, y, device)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ========== 2. Evaluation Metrics ==========\n",
    "def r2_zero(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate zero-based RÂ² (baseline is zero)\n",
    "    y_true: true values (N,)\n",
    "    y_pred: predicted values (N,)\n",
    "    \"\"\"\n",
    "    rss = np.sum((y_true - y_pred)**2)  \n",
    "    tss = np.sum(y_true**2)            \n",
    "    return 1 - rss / tss\n",
    "\n",
    "def calc_directional_metrics(y_true, y_pred, permnos=None):\n",
    "    \"\"\"\n",
    "    Calculate sign prediction and up/down accuracy.\n",
    "    If permnos is provided, calculate metrics per group and average.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    if permnos is None:\n",
    "        s_true = np.sign(y_true)\n",
    "        s_pred = np.sign(y_pred)\n",
    "        mask = s_true != 0\n",
    "        s_true = s_true[mask]\n",
    "        s_pred = s_pred[mask]\n",
    "\n",
    "        overall_acc = np.mean(s_true == s_pred)\n",
    "\n",
    "        up_mask = s_true > 0\n",
    "        down_mask = s_true < 0\n",
    "        up_acc = np.mean(s_true[up_mask] == s_pred[up_mask]) if np.any(up_mask) else 0\n",
    "        down_acc = np.mean(s_true[down_mask] == s_pred[down_mask]) if np.any(down_mask) else 0\n",
    "\n",
    "    else:\n",
    "        df = pd.DataFrame({\"permno\": permnos, \"yt\": y_true, \"yp\": y_pred})\n",
    "        overall_accs = []\n",
    "        up_accs = []\n",
    "        down_accs = []\n",
    "\n",
    "        for _, g in df.groupby(\"permno\"):\n",
    "            s_true = np.sign(g[\"yt\"].values)\n",
    "            s_pred = np.sign(g[\"yp\"].values)\n",
    "            mask = s_true != 0\n",
    "            s_true = s_true[mask]\n",
    "            s_pred = s_pred[mask]\n",
    "            if len(s_true) == 0:\n",
    "                continue\n",
    "            overall_accs.append(np.mean(s_true == s_pred))\n",
    "\n",
    "            up_mask = s_true > 0\n",
    "            down_mask = s_true < 0\n",
    "            up_accs.append(np.mean(s_true[up_mask] == s_pred[up_mask]) if np.any(up_mask) else np.nan)\n",
    "            down_accs.append(np.mean(s_true[down_mask] == s_pred[down_mask]) if np.any(down_mask) else np.nan)\n",
    "\n",
    "        overall_acc = np.nanmean(overall_accs)\n",
    "        up_acc = np.nanmean(up_accs)\n",
    "        down_acc = np.nanmean(down_accs)\n",
    "\n",
    "    return overall_acc, up_acc, down_acc\n",
    "\n",
    "def regression_metrics(y_true, y_pred, k, meta=None, permnos=None):\n",
    "    \"\"\"\n",
    "    Calculate regression metrics and directional accuracy.\n",
    "    If meta is provided and contains MKTCAP_PERCENTILE, also calculate metrics for top and bottom market cap groups.\n",
    "    \"\"\"\n",
    "    if isinstance(y_true, torch.Tensor):\n",
    "        y_true = y_true.detach().cpu().numpy()\n",
    "    if isinstance(y_pred, torch.Tensor):\n",
    "        y_pred = y_pred.detach().cpu().numpy()\n",
    "    \n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    n = len(y_true)\n",
    "\n",
    "    r2 = r2_zero(y_true, y_pred)  \n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "\n",
    "    dir_acc, up_acc, down_acc = calc_directional_metrics(y_true, y_pred, permnos)\n",
    "\n",
    "    metrics = {\n",
    "        \"R2_zero\": r2,\n",
    "        \"RMSE\": rmse,\n",
    "        \"MAE\": mae,\n",
    "        \"MSE\": mse,\n",
    "        \"Directional Accuracy\": dir_acc,\n",
    "        \"Up_Directional_Acc\": up_acc,\n",
    "        \"Down_Directional_Acc\": down_acc\n",
    "    }\n",
    "\n",
    "    if meta is not None and \"MKTCAP_PERCENTILE\" in meta:\n",
    "        top_mask = meta[\"MKTCAP_PERCENTILE\"] >= 0.75\n",
    "        bottom_mask = meta[\"MKTCAP_PERCENTILE\"] <= 0.25\n",
    "\n",
    "        if np.any(top_mask):\n",
    "            yt_top = y_true[top_mask]\n",
    "            yp_top = y_pred[top_mask]\n",
    "            perm_top = permnos[top_mask] if permnos is not None else None\n",
    "            r2_top = r2_zero(yt_top, yp_top)\n",
    "            rmse_top = np.sqrt(mean_squared_error(yt_top, yp_top))\n",
    "            mae_top = mean_absolute_error(yt_top, yp_top)\n",
    "            mse_top = mean_squared_error(yt_top, yp_top)\n",
    "            dir_top, up_top, down_top = calc_directional_metrics(yt_top, yp_top, perm_top)\n",
    "            metrics.update({\n",
    "                \"Top25_R2_zero\": r2_top,\n",
    "                \"Top25_MSE\": mse_top,\n",
    "                \"Top25_RMSE\": rmse_top,\n",
    "                \"Top25_MAE\": mae_top,\n",
    "                \"Top25_Dir_Acc\": dir_top,\n",
    "                \"Top25_Up_Acc\": up_top,\n",
    "                \"Top25_Down_Acc\": down_top\n",
    "            })\n",
    "\n",
    "        if np.any(bottom_mask):\n",
    "            yt_bot = y_true[bottom_mask]\n",
    "            yp_bot = y_pred[bottom_mask]\n",
    "            perm_bot = permnos[bottom_mask] if permnos is not None else None\n",
    "            r2_bot = r2_zero(yt_bot, yp_bot)\n",
    "            rmse_bot = np.sqrt(mean_squared_error(yt_bot, yp_bot))\n",
    "            mae_bot = mean_absolute_error(yt_bot, yp_bot)\n",
    "            mse_bot = mean_squared_error(yt_bot, yp_bot)\n",
    "            dir_bot, up_bot, down_bot = calc_directional_metrics(yt_bot, yp_bot, perm_bot)\n",
    "            metrics.update({\n",
    "                \"Bottom25_R2_zero\": r2_bot,\n",
    "                \"Bottom25_MSE\": mse_bot,\n",
    "                \"Bottom25_RMSE\": rmse_bot,\n",
    "                \"Bottom25_MAE\": mae_bot,\n",
    "                \"Bottom25_Dir_Acc\": dir_bot,\n",
    "                \"Bottom25_Up_Acc\": up_bot,\n",
    "                \"Bottom25_Down_Acc\": down_bot\n",
    "            })\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Model Definition\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, dropout_rate=0.1):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # Build hidden layers\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze()\n",
    "\n",
    "# Predefined network structures\n",
    "MLP_CONFIGS = {\n",
    "    \"NN1\": [64],                    \n",
    "    \"NN2\": [64, 32],               \n",
    "    \"NN3\": [128, 64, 32],          \n",
    "    \"NN4\": [128, 64, 32, 16],      \n",
    "    \"NN5\": [256, 128, 64, 32, 16]  \n",
    "}\n",
    "\n",
    "# Default hyperparameters\n",
    "DEFAULT_PARAMS = {\n",
    "    \"batch_size\": 128,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"epochs\": 50\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Training functions\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            output = model(X)\n",
    "            loss = criterion(output, y)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            predictions.extend(output.cpu().numpy())\n",
    "            targets.extend(y.cpu().numpy())\n",
    "    \n",
    "    return (total_loss / len(val_loader), \n",
    "            np.array(predictions), \n",
    "            np.array(targets))\n",
    "\n",
    "def create_train_val_split(X_train, y_train, permnos_train, val_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Create a validation set from the training set in chronological order.\n",
    "    The last val_ratio proportion of the data is used as the validation set.\n",
    "    \"\"\"\n",
    "    split_idx = int(len(X_train) * (1 - val_ratio))\n",
    "    \n",
    "    X_tr = X_train[:split_idx]\n",
    "    X_val = X_train[split_idx:]\n",
    "    y_tr = y_train[:split_idx]\n",
    "    y_val = y_train[split_idx:]\n",
    "    \n",
    "    if permnos_train is not None:\n",
    "        perm_tr = permnos_train[:split_idx]\n",
    "        perm_val = permnos_train[split_idx:]\n",
    "        return X_tr, X_val, y_tr, y_val, perm_tr, perm_val\n",
    "    else:\n",
    "        return X_tr, X_val, y_tr, y_val, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 5. Hyperparameter Tuning ==========\n",
    "\n",
    "def tune_model_with_optuna(model_name, X, y, permnos=None, n_trials=10):\n",
    "    \"\"\"Hyperparameter tuning using Optuna, pure MSE loss, reduced learning rate, less regularization\"\"\"\n",
    "    input_dim = X.shape[1]\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"batch_size\": trial.suggest_categorical(\"batch_size\", [64, 128]),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True),\n",
    "            \"dropout_rate\": trial.suggest_float(\"dropout_rate\", 0.05, 0.15),\n",
    "            \"epochs\": 20\n",
    "        }\n",
    "        \n",
    "        cv_scores = []\n",
    "        for train_idx, val_idx in tscv.split(X):\n",
    "            X_tr, X_val = X[train_idx], X[val_idx]\n",
    "            y_tr, y_val = y[train_idx], y[val_idx]\n",
    "            \n",
    "            train_loader = prepare_data(X_tr, y_tr, batch_size=params[\"batch_size\"])\n",
    "            val_loader = prepare_data(X_val, y_val, batch_size=params[\"batch_size\"])\n",
    "            \n",
    "            model = MLP(\n",
    "                input_dim=input_dim,\n",
    "                hidden_dims=MLP_CONFIGS[model_name],\n",
    "                dropout_rate=params[\"dropout_rate\"]\n",
    "            ).to(device)\n",
    "            \n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=params[\"learning_rate\"])\n",
    "            \n",
    "            best_val_loss = float('inf')\n",
    "            \n",
    "            for epoch in range(params[\"epochs\"]):\n",
    "                train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "                val_loss, _, _ = validate(model, val_loader, criterion, device)\n",
    "                \n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "            \n",
    "            cv_scores.append(best_val_loss)\n",
    "        \n",
    "        return np.mean(cv_scores)\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        sampler=optuna.samplers.TPESampler(seed=42),\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=5)\n",
    "    )\n",
    "    \n",
    "    study.optimize(objective, n_trials=n_trials, n_jobs=1)\n",
    "    \n",
    "    if len(study.trials) == 0 or study.best_trial is None:\n",
    "        print(f\"[Skip Model] {model_name} failed to complete any trial. Skipping.\")\n",
    "        return None\n",
    "    \n",
    "    best_params = study.best_params\n",
    "    best_params[\"epochs\"] = 50  \n",
    "    best_score = study.best_value\n",
    "    print(f\"[Optuna] {model_name} best_MSE={best_score:.6f}, best_params={best_params}\")\n",
    "    \n",
    "    return best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 6. Save Functions ==========\n",
    "def save_model(model, name, window, path=\"models/\"):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join(path, f\"{name}_w{window}.pth\"))\n",
    "\n",
    "def save_metrics(metrics_dict, name, window, path=\"results.csv\"):\n",
    "    row = pd.DataFrame([metrics_dict])\n",
    "    row.insert(0, \"Model\", name)\n",
    "    row.insert(1, \"Window\", window)\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path)\n",
    "        df = df[~((df[\"Model\"] == name) & (df[\"Window\"] == window))]\n",
    "        df = pd.concat([df, row], ignore_index=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        print(f\"[Update] Metrics updated for {name} w={window}\")\n",
    "    else:\n",
    "        row.to_csv(path, index=False)\n",
    "        print(f\"[Create] New metrics file created with {name} w={window}\")\n",
    "\n",
    "def save_predictions(model_name, window_size, y_true, y_pred, permnos, path=\"predictions/\"):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        \"PERMNO\": permnos,\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred\n",
    "    })\n",
    "\n",
    "    filename = f\"{model_name}_w{window_size}.csv\"\n",
    "    df.to_csv(os.path.join(path, filename), index=False)\n",
    "    print(f\"[Save] {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 7. Main training and evaluation function ==========\n",
    "def train_and_evaluate(model_name, window_size,\n",
    "                       X_train, y_train, X_test, y_test,\n",
    "                       permnos_train, permnos_test, meta=None, shared_params=None):\n",
    "    print(f\"\\nTraining {model_name} on Window = {window_size}\")\n",
    "    input_dim = X_train.shape[1]\n",
    "    \n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f\"[Info] Using device: {device}\")\n",
    "    \n",
    "    if model_name == \"NN1\":\n",
    "        best_params = tune_model_with_optuna(model_name, X_train, y_train, permnos=permnos_train)\n",
    "        if best_params is None:\n",
    "            print(f\"[Skip] {model_name} tuning failed, using default parameters\")\n",
    "            best_params = DEFAULT_PARAMS.copy()\n",
    "    else:\n",
    "        if shared_params is None:\n",
    "            print(f\"[Warning] No shared parameters provided for {model_name}, using default\")\n",
    "            best_params = DEFAULT_PARAMS.copy()\n",
    "        else:\n",
    "            print(f\"[Info] Using shared parameters for {model_name}\")\n",
    "            best_params = shared_params.copy()\n",
    "    \n",
    "    X_tr, X_val, y_tr, y_val, perm_tr, perm_val = create_train_val_split(\n",
    "        X_train, y_train, permnos_train, val_ratio=0.2\n",
    "    )\n",
    "    \n",
    "    train_loader = prepare_data(X_tr, y_tr, batch_size=best_params[\"batch_size\"], device=device)\n",
    "    val_loader = prepare_data(X_val, y_val, batch_size=best_params[\"batch_size\"], device=device)\n",
    "    test_loader = prepare_data(X_test, y_test, batch_size=best_params[\"batch_size\"], device=device)\n",
    "    \n",
    "    model = MLP(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dims=MLP_CONFIGS[model_name],\n",
    "        dropout_rate=best_params[\"dropout_rate\"]\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_params[\"learning_rate\"])\n",
    "    \n",
    "    best_model = None\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 20\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(f\"[Info] Starting training for {model_name} with {best_params['epochs']} epochs (early stopping patience={patience})...\")\n",
    "    print(f\"[Info] Train size: {len(X_tr)}, Val size: {len(X_val)}, Test size: {len(X_test)}\")\n",
    "    \n",
    "    for epoch in range(best_params[\"epochs\"]):\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, predictions, targets = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        pred_std = np.std(predictions)\n",
    "        positive_ratio = (np.sign(predictions) > 0).mean()\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"[{model_name}] Epoch {epoch+1}/{best_params['epochs']}, \"\n",
    "                  f\"Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}, \"\n",
    "                  f\"PredStd: {pred_std:.6f}, PosRatio: {positive_ratio:.3f}\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"[{model_name}] Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    if best_model is not None:\n",
    "        model.load_state_dict(best_model)\n",
    "    \n",
    "    _, y_pred, y_true = validate(model, test_loader, criterion, device)\n",
    "    \n",
    "    y_pred = y_pred.cpu() if isinstance(y_pred, torch.Tensor) else y_pred\n",
    "    y_true = y_true.cpu() if isinstance(y_true, torch.Tensor) else y_true\n",
    "    \n",
    "    print(\"\\n=== Directional Sanity Check ===\")\n",
    "    print(\"Pos ratio (y_test):\", (y_test > 0).mean())\n",
    "    print(\"Neg ratio (y_test):\", (y_test < 0).mean())\n",
    "    sign_pred = np.sign(y_pred)\n",
    "    print(\"Pred +1 ratio:\", (sign_pred > 0).mean())\n",
    "    print(\"Pred -1 ratio:\", (sign_pred < 0).mean())\n",
    "    \n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    conf = confusion_matrix(np.sign(y_test), sign_pred, labels=[1, -1])\n",
    "    print(\"      Pred+  Pred-\")\n",
    "    print(\"+1 |\", conf[0])\n",
    "    print(\"-1 |\", conf[1])\n",
    "    \n",
    "    metrics = regression_metrics(y_true, y_pred, k=X_test.shape[1], meta=meta, permnos=permnos_test)\n",
    "    \n",
    "    save_model(model, model_name, window_size)\n",
    "    save_metrics(metrics, model_name, window_size)\n",
    "    save_predictions(model_name, window_size, y_true, y_pred, permnos_test)\n",
    "    \n",
    "    print(f\"[Info] Training completed for {model_name}\")\n",
    "    return metrics, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 8. Main dispatcher function: loop through all models and windows ==========\n",
    "def loop_all_models(run_test_first=False):\n",
    "    \"\"\"Loop to train all models on different window sizes\"\"\"\n",
    "    datasets = load_datasets()\n",
    "    \n",
    "    model_list = [\"NN1\", \"NN2\", \"NN3\", \"NN4\", \"NN5\"]\n",
    "    window_sizes = [5, 21, 252, 512]\n",
    "\n",
    "    for window in window_sizes:\n",
    "        print(f\"\\n=== Processing Window Size: {window} ===\")\n",
    "        \n",
    "        X_train = datasets[f\"X_train_{window}\"]\n",
    "        y_train = datasets[f\"y_train_{window}\"]\n",
    "        X_test = datasets[f\"X_test_{window}\"]\n",
    "        y_test = datasets[f\"y_test_{window}\"]\n",
    "        \n",
    "        meta_train_dict = datasets[f\"meta_train_{window}\"].item()\n",
    "        meta_test_dict = datasets[f\"meta_test_{window}\"].item()\n",
    "\n",
    "        meta_train = pd.DataFrame.from_dict(meta_train_dict)\n",
    "        meta_test = pd.DataFrame.from_dict(meta_test_dict)\n",
    "\n",
    "        permnos_train = meta_train[\"PERMNO\"].values\n",
    "        permnos_test = meta_test[\"PERMNO\"].values\n",
    "\n",
    "        print(f\"\\nTraining NN1 to get shared parameters...\")\n",
    "        _, shared_params = train_and_evaluate(\n",
    "            \"NN1\", window,\n",
    "            X_train, y_train, X_test, y_test,\n",
    "            permnos_train, permnos_test,\n",
    "            meta_test\n",
    "        )\n",
    "        print(f\"Shared parameters from NN1: {shared_params}\")\n",
    "\n",
    "        for model_name in model_list[1:]:\n",
    "            print(f\"\\nTraining {model_name} with shared parameters...\")\n",
    "            train_and_evaluate(\n",
    "                model_name, window,\n",
    "                X_train, y_train, X_test, y_test,\n",
    "                permnos_train, permnos_test,\n",
    "                meta_test, shared_params\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Window Size: 5 ===\n",
      "\n",
      "Training NN1 to get shared parameters...\n",
      "\n",
      "â¶ Training NN1 on Window = 5\n",
      "[Info] Using device: mps\n",
      "[Optuna] NN1 best_MSE=0.000304, best_params={'batch_size': 64, 'learning_rate': 3.8396292998041685e-05, 'dropout_rate': 0.08663618432936918, 'epochs': 50}\n",
      "[Info] Starting training for NN1 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 157536, Val size: 39384, Test size: 110850\n",
      "[NN1] Epoch 10/50, Train Loss: 0.000444, Val Loss: 0.000164, PredStd: 0.000921, PosRatio: 0.661\n",
      "[NN1] Epoch 20/50, Train Loss: 0.000438, Val Loss: 0.000164, PredStd: 0.000924, PosRatio: 0.624\n",
      "[NN1] Epoch 30/50, Train Loss: 0.000436, Val Loss: 0.000164, PredStd: 0.001002, PosRatio: 0.610\n",
      "[NN1] Epoch 40/50, Train Loss: 0.000435, Val Loss: 0.000164, PredStd: 0.000977, PosRatio: 0.642\n",
      "[NN1] Epoch 50/50, Train Loss: 0.000434, Val Loss: 0.000164, PredStd: 0.000969, PosRatio: 0.630\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5225259359494813\n",
      "Neg ratio (y_test): 0.47667117726657643\n",
      "Pred +1 ratio: 0.6098601714027966\n",
      "Pred -1 ratio: 0.3901398285972034\n",
      "      Pred+  Pred-\n",
      "+1 | [35746 22176]\n",
      "-1 | [31801 21038]\n",
      "[Create] New metrics file created with NN1 w=5\n",
      "[Save] NN1_w5.csv\n",
      "[Info] Training completed for NN1\n",
      "Shared parameters from NN1: {'batch_size': 64, 'learning_rate': 3.8396292998041685e-05, 'dropout_rate': 0.08663618432936918, 'epochs': 50}\n",
      "\n",
      "Training NN2 with shared parameters...\n",
      "\n",
      "â¶ Training NN2 on Window = 5\n",
      "[Info] Using device: mps\n",
      "[Info] Using shared parameters for NN2\n",
      "[Info] Starting training for NN2 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 157536, Val size: 39384, Test size: 110850\n",
      "[NN2] Epoch 10/50, Train Loss: 0.000435, Val Loss: 0.000164, PredStd: 0.000814, PosRatio: 0.843\n",
      "[NN2] Epoch 20/50, Train Loss: 0.000434, Val Loss: 0.000165, PredStd: 0.001093, PosRatio: 0.747\n",
      "[NN2] Epoch 30/50, Train Loss: 0.000433, Val Loss: 0.000165, PredStd: 0.001216, PosRatio: 0.750\n",
      "[NN2] Early stopping at epoch 30\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5225259359494813\n",
      "Neg ratio (y_test): 0.47667117726657643\n",
      "Pred +1 ratio: 0.7347586829048264\n",
      "Pred -1 ratio: 0.2652413170951737\n",
      "      Pred+  Pred-\n",
      "+1 | [42695 15227]\n",
      "-1 | [38679 14160]\n",
      "[Update] Metrics updated for NN2 w=5\n",
      "[Save] NN2_w5.csv\n",
      "[Info] Training completed for NN2\n",
      "\n",
      "Training NN3 with shared parameters...\n",
      "\n",
      "â¶ Training NN3 on Window = 5\n",
      "[Info] Using device: mps\n",
      "[Info] Using shared parameters for NN3\n",
      "[Info] Starting training for NN3 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 157536, Val size: 39384, Test size: 110850\n",
      "[NN3] Epoch 10/50, Train Loss: 0.000435, Val Loss: 0.000164, PredStd: 0.000911, PosRatio: 0.831\n",
      "[NN3] Epoch 20/50, Train Loss: 0.000433, Val Loss: 0.000164, PredStd: 0.000986, PosRatio: 0.808\n",
      "[NN3] Epoch 30/50, Train Loss: 0.000432, Val Loss: 0.000165, PredStd: 0.001141, PosRatio: 0.762\n",
      "[NN3] Early stopping at epoch 32\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5225259359494813\n",
      "Neg ratio (y_test): 0.47667117726657643\n",
      "Pred +1 ratio: 0.6993234100135318\n",
      "Pred -1 ratio: 0.3006765899864682\n",
      "      Pred+  Pred-\n",
      "+1 | [40703 17219]\n",
      "-1 | [36750 16089]\n",
      "[Update] Metrics updated for NN3 w=5\n",
      "[Save] NN3_w5.csv\n",
      "[Info] Training completed for NN3\n",
      "\n",
      "Training NN4 with shared parameters...\n",
      "\n",
      "â¶ Training NN4 on Window = 5\n",
      "[Info] Using device: mps\n",
      "[Info] Using shared parameters for NN4\n",
      "[Info] Starting training for NN4 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 157536, Val size: 39384, Test size: 110850\n",
      "[NN4] Epoch 10/50, Train Loss: 0.000434, Val Loss: 0.000164, PredStd: 0.000628, PosRatio: 0.869\n",
      "[NN4] Epoch 20/50, Train Loss: 0.000432, Val Loss: 0.000164, PredStd: 0.000682, PosRatio: 0.871\n",
      "[NN4] Epoch 30/50, Train Loss: 0.000432, Val Loss: 0.000164, PredStd: 0.000854, PosRatio: 0.803\n",
      "[NN4] Early stopping at epoch 32\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5225259359494813\n",
      "Neg ratio (y_test): 0.47667117726657643\n",
      "Pred +1 ratio: 0.7479115922417682\n",
      "Pred -1 ratio: 0.2520884077582318\n",
      "      Pred+  Pred-\n",
      "+1 | [43577 14345]\n",
      "-1 | [39257 13582]\n",
      "[Update] Metrics updated for NN4 w=5\n",
      "[Save] NN4_w5.csv\n",
      "[Info] Training completed for NN4\n",
      "\n",
      "Training NN5 with shared parameters...\n",
      "\n",
      "â¶ Training NN5 on Window = 5\n",
      "[Info] Using device: mps\n",
      "[Info] Using shared parameters for NN5\n",
      "[Info] Starting training for NN5 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 157536, Val size: 39384, Test size: 110850\n",
      "[NN5] Epoch 10/50, Train Loss: 0.000434, Val Loss: 0.000163, PredStd: 0.000797, PosRatio: 0.789\n",
      "[NN5] Epoch 20/50, Train Loss: 0.000432, Val Loss: 0.000163, PredStd: 0.000555, PosRatio: 0.922\n",
      "[NN5] Epoch 30/50, Train Loss: 0.000431, Val Loss: 0.000163, PredStd: 0.000653, PosRatio: 0.865\n",
      "[NN5] Epoch 40/50, Train Loss: 0.000430, Val Loss: 0.000164, PredStd: 0.000906, PosRatio: 0.777\n",
      "[NN5] Early stopping at epoch 40\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5225259359494813\n",
      "Neg ratio (y_test): 0.47667117726657643\n",
      "Pred +1 ratio: 0.7289941362201173\n",
      "Pred -1 ratio: 0.2710058637798827\n",
      "      Pred+  Pred-\n",
      "+1 | [42388 15534]\n",
      "-1 | [38351 14488]\n",
      "[Update] Metrics updated for NN5 w=5\n",
      "[Save] NN5_w5.csv\n",
      "[Info] Training completed for NN5\n",
      "\n",
      "=== Processing Window Size: 21 ===\n",
      "\n",
      "Training NN1 to get shared parameters...\n",
      "\n",
      "â¶ Training NN1 on Window = 21\n",
      "[Info] Using device: mps\n",
      "[Optuna] NN1 best_MSE=0.000302, best_params={'batch_size': 64, 'learning_rate': 3.8396292998041685e-05, 'dropout_rate': 0.08663618432936918, 'epochs': 50}\n",
      "[Info] Starting training for NN1 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 156896, Val size: 39224, Test size: 110850\n",
      "[NN1] Epoch 10/50, Train Loss: 0.000451, Val Loss: 0.000166, PredStd: 0.001598, PosRatio: 0.442\n",
      "[NN1] Epoch 20/50, Train Loss: 0.000441, Val Loss: 0.000166, PredStd: 0.001474, PosRatio: 0.428\n",
      "[NN1] Epoch 30/50, Train Loss: 0.000436, Val Loss: 0.000165, PredStd: 0.001470, PosRatio: 0.444\n",
      "[NN1] Epoch 40/50, Train Loss: 0.000433, Val Loss: 0.000165, PredStd: 0.001472, PosRatio: 0.424\n",
      "[NN1] Early stopping at epoch 46\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5225259359494813\n",
      "Neg ratio (y_test): 0.47667117726657643\n",
      "Pred +1 ratio: 0.43720342805593143\n",
      "Pred -1 ratio: 0.5627965719440685\n",
      "      Pred+  Pred-\n",
      "+1 | [25731 32191]\n",
      "-1 | [22693 30146]\n",
      "[Update] Metrics updated for NN1 w=21\n",
      "[Save] NN1_w21.csv\n",
      "[Info] Training completed for NN1\n",
      "Shared parameters from NN1: {'batch_size': 64, 'learning_rate': 3.8396292998041685e-05, 'dropout_rate': 0.08663618432936918, 'epochs': 50}\n",
      "\n",
      "Training NN2 with shared parameters...\n",
      "\n",
      "â¶ Training NN2 on Window = 21\n",
      "[Info] Using device: mps\n",
      "[Info] Using shared parameters for NN2\n",
      "[Info] Starting training for NN2 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 156896, Val size: 39224, Test size: 110850\n",
      "[NN2] Epoch 10/50, Train Loss: 0.000432, Val Loss: 0.000164, PredStd: 0.001277, PosRatio: 0.605\n",
      "[NN2] Epoch 20/50, Train Loss: 0.000431, Val Loss: 0.000164, PredStd: 0.001341, PosRatio: 0.597\n",
      "[NN2] Early stopping at epoch 28\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5225259359494813\n",
      "Neg ratio (y_test): 0.47667117726657643\n",
      "Pred +1 ratio: 0.6143707713125846\n",
      "Pred -1 ratio: 0.38562922868741545\n",
      "      Pred+  Pred-\n",
      "+1 | [35909 22013]\n",
      "-1 | [32153 20686]\n",
      "[Update] Metrics updated for NN2 w=21\n",
      "[Save] NN2_w21.csv\n",
      "[Info] Training completed for NN2\n",
      "\n",
      "Training NN3 with shared parameters...\n",
      "\n",
      "â¶ Training NN3 on Window = 21\n",
      "[Info] Using device: mps\n",
      "[Info] Using shared parameters for NN3\n",
      "[Info] Starting training for NN3 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 156896, Val size: 39224, Test size: 110850\n",
      "[NN3] Epoch 10/50, Train Loss: 0.000432, Val Loss: 0.000164, PredStd: 0.001257, PosRatio: 0.704\n",
      "[NN3] Epoch 20/50, Train Loss: 0.000429, Val Loss: 0.000164, PredStd: 0.001167, PosRatio: 0.719\n",
      "[NN3] Epoch 30/50, Train Loss: 0.000427, Val Loss: 0.000164, PredStd: 0.001180, PosRatio: 0.696\n",
      "[NN3] Early stopping at epoch 37\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5225259359494813\n",
      "Neg ratio (y_test): 0.47667117726657643\n",
      "Pred +1 ratio: 0.6389715832205684\n",
      "Pred -1 ratio: 0.36102841677943165\n",
      "      Pred+  Pred-\n",
      "+1 | [37310 20612]\n",
      "-1 | [33461 19378]\n",
      "[Update] Metrics updated for NN3 w=21\n",
      "[Save] NN3_w21.csv\n",
      "[Info] Training completed for NN3\n",
      "\n",
      "Training NN4 with shared parameters...\n",
      "\n",
      "â¶ Training NN4 on Window = 21\n",
      "[Info] Using device: mps\n",
      "[Info] Using shared parameters for NN4\n",
      "[Info] Starting training for NN4 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 156896, Val size: 39224, Test size: 110850\n",
      "[NN4] Epoch 10/50, Train Loss: 0.000433, Val Loss: 0.000164, PredStd: 0.000875, PosRatio: 0.821\n",
      "[NN4] Epoch 20/50, Train Loss: 0.000430, Val Loss: 0.000164, PredStd: 0.001093, PosRatio: 0.745\n",
      "[NN4] Epoch 30/50, Train Loss: 0.000428, Val Loss: 0.000164, PredStd: 0.001045, PosRatio: 0.704\n",
      "[NN4] Epoch 40/50, Train Loss: 0.000426, Val Loss: 0.000164, PredStd: 0.001293, PosRatio: 0.653\n",
      "[NN4] Epoch 50/50, Train Loss: 0.000425, Val Loss: 0.000164, PredStd: 0.001169, PosRatio: 0.681\n",
      "[NN4] Early stopping at epoch 50\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5225259359494813\n",
      "Neg ratio (y_test): 0.47667117726657643\n",
      "Pred +1 ratio: 0.6399548940009021\n",
      "Pred -1 ratio: 0.3600451059990979\n",
      "      Pred+  Pred-\n",
      "+1 | [37419 20503]\n",
      "-1 | [33459 19380]\n",
      "[Update] Metrics updated for NN4 w=21\n",
      "[Save] NN4_w21.csv\n",
      "[Info] Training completed for NN4\n",
      "\n",
      "Training NN5 with shared parameters...\n",
      "\n",
      "â¶ Training NN5 on Window = 21\n",
      "[Info] Using device: mps\n",
      "[Info] Using shared parameters for NN5\n",
      "[Info] Starting training for NN5 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 156896, Val size: 39224, Test size: 110850\n",
      "[NN5] Epoch 10/50, Train Loss: 0.000431, Val Loss: 0.000163, PredStd: 0.000620, PosRatio: 0.839\n",
      "[NN5] Epoch 20/50, Train Loss: 0.000427, Val Loss: 0.000163, PredStd: 0.000809, PosRatio: 0.808\n",
      "[NN5] Epoch 30/50, Train Loss: 0.000423, Val Loss: 0.000164, PredStd: 0.000998, PosRatio: 0.730\n",
      "[NN5] Early stopping at epoch 34\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5225259359494813\n",
      "Neg ratio (y_test): 0.47667117726657643\n",
      "Pred +1 ratio: 0.7160036084799278\n",
      "Pred -1 ratio: 0.28399639152007217\n",
      "      Pred+  Pred-\n",
      "+1 | [41788 16134]\n",
      "-1 | [37514 15325]\n",
      "[Update] Metrics updated for NN5 w=21\n",
      "[Save] NN5_w21.csv\n",
      "[Info] Training completed for NN5\n",
      "\n",
      "=== Processing Window Size: 252 ===\n",
      "\n",
      "Training NN1 to get shared parameters...\n",
      "\n",
      "â¶ Training NN1 on Window = 252\n",
      "[Info] Using device: mps\n",
      "[Optuna] NN1 best_MSE=0.000294, best_params={'batch_size': 64, 'learning_rate': 0.000164092867306479, 'dropout_rate': 0.06705241236872915, 'epochs': 50}\n",
      "[Info] Starting training for NN1 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 147656, Val size: 36914, Test size: 110850\n",
      "[NN1] Epoch 10/50, Train Loss: 0.000377, Val Loss: 0.000166, PredStd: 0.001928, PosRatio: 0.769\n",
      "[NN1] Epoch 20/50, Train Loss: 0.000362, Val Loss: 0.000165, PredStd: 0.001754, PosRatio: 0.748\n",
      "[NN1] Early stopping at epoch 23\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5225259359494813\n",
      "Neg ratio (y_test): 0.47667117726657643\n",
      "Pred +1 ratio: 0.7238700947225981\n",
      "Pred -1 ratio: 0.2761299052774019\n",
      "      Pred+  Pred-\n",
      "+1 | [42182 15740]\n",
      "-1 | [37996 14843]\n",
      "[Update] Metrics updated for NN1 w=252\n",
      "[Save] NN1_w252.csv\n",
      "[Info] Training completed for NN1\n",
      "Shared parameters from NN1: {'batch_size': 64, 'learning_rate': 0.000164092867306479, 'dropout_rate': 0.06705241236872915, 'epochs': 50}\n",
      "\n",
      "Training NN2 with shared parameters...\n",
      "\n",
      "â¶ Training NN2 on Window = 252\n",
      "[Info] Using device: mps\n",
      "[Info] Using shared parameters for NN2\n",
      "[Info] Starting training for NN2 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 147656, Val size: 36914, Test size: 110850\n",
      "[NN2] Epoch 10/50, Train Loss: 0.000383, Val Loss: 0.000166, PredStd: 0.001574, PosRatio: 0.694\n",
      "[NN2] Epoch 20/50, Train Loss: 0.000358, Val Loss: 0.000167, PredStd: 0.002235, PosRatio: 0.558\n",
      "[NN2] Early stopping at epoch 25\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5225259359494813\n",
      "Neg ratio (y_test): 0.47667117726657643\n",
      "Pred +1 ratio: 0.6033017591339648\n",
      "Pred -1 ratio: 0.39669824086603517\n",
      "      Pred+  Pred-\n",
      "+1 | [35105 22817]\n",
      "-1 | [31717 21122]\n",
      "[Update] Metrics updated for NN2 w=252\n",
      "[Save] NN2_w252.csv\n",
      "[Info] Training completed for NN2\n",
      "\n",
      "Training NN3 with shared parameters...\n",
      "\n",
      "â¶ Training NN3 on Window = 252\n",
      "[Info] Using device: mps\n",
      "[Info] Using shared parameters for NN3\n",
      "[Info] Starting training for NN3 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 147656, Val size: 36914, Test size: 110850\n",
      "[NN3] Epoch 10/50, Train Loss: 0.000378, Val Loss: 0.000165, PredStd: 0.001483, PosRatio: 0.676\n",
      "[NN3] Epoch 20/50, Train Loss: 0.000343, Val Loss: 0.000167, PredStd: 0.001968, PosRatio: 0.630\n",
      "[NN3] Early stopping at epoch 22\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5225259359494813\n",
      "Neg ratio (y_test): 0.47667117726657643\n",
      "Pred +1 ratio: 0.6406765899864681\n",
      "Pred -1 ratio: 0.3593234100135318\n",
      "      Pred+  Pred-\n",
      "+1 | [37241 20681]\n",
      "-1 | [33718 19121]\n",
      "[Update] Metrics updated for NN3 w=252\n",
      "[Save] NN3_w252.csv\n",
      "[Info] Training completed for NN3\n",
      "\n",
      "Training NN4 with shared parameters...\n",
      "\n",
      "â¶ Training NN4 on Window = 252\n",
      "[Info] Using device: mps\n",
      "[Info] Using shared parameters for NN4\n",
      "[Info] Starting training for NN4 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 147656, Val size: 36914, Test size: 110850\n",
      "[NN4] Epoch 10/50, Train Loss: 0.000384, Val Loss: 0.000164, PredStd: 0.001297, PosRatio: 0.777\n",
      "[NN4] Epoch 20/50, Train Loss: 0.000349, Val Loss: 0.000165, PredStd: 0.001368, PosRatio: 0.724\n",
      "[NN4] Early stopping at epoch 29\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5225259359494813\n",
      "Neg ratio (y_test): 0.47667117726657643\n",
      "Pred +1 ratio: 0.7719170049616599\n",
      "Pred -1 ratio: 0.2280829950383401\n",
      "      Pred+  Pred-\n",
      "+1 | [44873 13049]\n",
      "-1 | [40619 12220]\n",
      "[Update] Metrics updated for NN4 w=252\n",
      "[Save] NN4_w252.csv\n",
      "[Info] Training completed for NN4\n",
      "\n",
      "Training NN5 with shared parameters...\n",
      "\n",
      "â¶ Training NN5 on Window = 252\n",
      "[Info] Using device: mps\n",
      "[Info] Using shared parameters for NN5\n",
      "[Info] Starting training for NN5 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 147656, Val size: 36914, Test size: 110850\n",
      "[NN5] Epoch 10/50, Train Loss: 0.000383, Val Loss: 0.000164, PredStd: 0.001214, PosRatio: 0.858\n",
      "[NN5] Epoch 20/50, Train Loss: 0.000337, Val Loss: 0.000164, PredStd: 0.001170, PosRatio: 0.840\n",
      "[NN5] Early stopping at epoch 28\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5225259359494813\n",
      "Neg ratio (y_test): 0.47667117726657643\n",
      "Pred +1 ratio: 0.7763644564727109\n",
      "Pred -1 ratio: 0.22363554352728912\n",
      "      Pred+  Pred-\n",
      "+1 | [45198 12724]\n",
      "-1 | [40800 12039]\n",
      "[Update] Metrics updated for NN5 w=252\n",
      "[Save] NN5_w252.csv\n",
      "[Info] Training completed for NN5\n",
      "\n",
      "=== Processing Window Size: 512 ===\n",
      "\n",
      "Training NN1 to get shared parameters...\n",
      "\n",
      "â¶ Training NN1 on Window = 512\n",
      "[Info] Using device: mps\n",
      "[Optuna] NN1 best_MSE=0.000291, best_params={'batch_size': 64, 'learning_rate': 0.000233596350262616, 'dropout_rate': 0.09401524937396014, 'epochs': 50}\n",
      "[Info] Starting training for NN1 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 137256, Val size: 34314, Test size: 110850\n",
      "[NN1] Epoch 10/50, Train Loss: 0.000341, Val Loss: 0.000169, PredStd: 0.001756, PosRatio: 0.815\n",
      "[NN1] Epoch 20/50, Train Loss: 0.000325, Val Loss: 0.000168, PredStd: 0.001655, PosRatio: 0.825\n",
      "[NN1] Early stopping at epoch 22\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5225259359494813\n",
      "Neg ratio (y_test): 0.47667117726657643\n",
      "Pred +1 ratio: 0.8074064050518719\n",
      "Pred -1 ratio: 0.1925935949481281\n",
      "      Pred+  Pred-\n",
      "+1 | [46977 10945]\n",
      "-1 | [42456 10383]\n",
      "[Update] Metrics updated for NN1 w=512\n",
      "[Save] NN1_w512.csv\n",
      "[Info] Training completed for NN1\n",
      "Shared parameters from NN1: {'batch_size': 64, 'learning_rate': 0.000233596350262616, 'dropout_rate': 0.09401524937396014, 'epochs': 50}\n",
      "\n",
      "Training NN2 with shared parameters...\n",
      "\n",
      "â¶ Training NN2 on Window = 512\n",
      "[Info] Using device: mps\n",
      "[Info] Using shared parameters for NN2\n",
      "[Info] Starting training for NN2 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 137256, Val size: 34314, Test size: 110850\n",
      "[NN2] Epoch 10/50, Train Loss: 0.000341, Val Loss: 0.000168, PredStd: 0.001276, PosRatio: 0.860\n",
      "[NN2] Epoch 20/50, Train Loss: 0.000315, Val Loss: 0.000168, PredStd: 0.001318, PosRatio: 0.923\n",
      "[NN2] Early stopping at epoch 22\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5225259359494813\n",
      "Neg ratio (y_test): 0.47667117726657643\n",
      "Pred +1 ratio: 0.8447992783040145\n",
      "Pred -1 ratio: 0.15520072169598556\n",
      "      Pred+  Pred-\n",
      "+1 | [49200  8722]\n",
      "-1 | [44373  8466]\n",
      "[Update] Metrics updated for NN2 w=512\n",
      "[Save] NN2_w512.csv\n",
      "[Info] Training completed for NN2\n",
      "\n",
      "Training NN3 with shared parameters...\n",
      "\n",
      "â¶ Training NN3 on Window = 512\n",
      "[Info] Using device: mps\n",
      "[Info] Using shared parameters for NN3\n",
      "[Info] Starting training for NN3 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 137256, Val size: 34314, Test size: 110850\n",
      "[NN3] Epoch 10/50, Train Loss: 0.000343, Val Loss: 0.000169, PredStd: 0.001495, PosRatio: 0.893\n",
      "[NN3] Epoch 20/50, Train Loss: 0.000299, Val Loss: 0.000172, PredStd: 0.001927, PosRatio: 0.901\n",
      "[NN3] Early stopping at epoch 22\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5225259359494813\n",
      "Neg ratio (y_test): 0.47667117726657643\n",
      "Pred +1 ratio: 0.8286332882273343\n",
      "Pred -1 ratio: 0.17136671177266577\n",
      "      Pred+  Pred-\n",
      "+1 | [48226  9696]\n",
      "-1 | [43561  9278]\n",
      "[Update] Metrics updated for NN3 w=512\n",
      "[Save] NN3_w512.csv\n",
      "[Info] Training completed for NN3\n",
      "\n",
      "Training NN4 with shared parameters...\n",
      "\n",
      "â¶ Training NN4 on Window = 512\n",
      "[Info] Using device: mps\n",
      "[Info] Using shared parameters for NN4\n",
      "[Info] Starting training for NN4 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 137256, Val size: 34314, Test size: 110850\n",
      "[NN4] Epoch 10/50, Train Loss: 0.000349, Val Loss: 0.000168, PredStd: 0.000981, PosRatio: 0.953\n",
      "[NN4] Epoch 20/50, Train Loss: 0.000306, Val Loss: 0.000167, PredStd: 0.000903, PosRatio: 0.949\n",
      "[NN4] Early stopping at epoch 23\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5225259359494813\n",
      "Neg ratio (y_test): 0.47667117726657643\n",
      "Pred +1 ratio: 0.9129273793414524\n",
      "Pred -1 ratio: 0.08707262065854758\n",
      "      Pred+  Pred-\n",
      "+1 | [53125  4797]\n",
      "-1 | [47992  4847]\n",
      "[Update] Metrics updated for NN4 w=512\n",
      "[Save] NN4_w512.csv\n",
      "[Info] Training completed for NN4\n",
      "\n",
      "Training NN5 with shared parameters...\n",
      "\n",
      "â¶ Training NN5 on Window = 512\n",
      "[Info] Using device: mps\n",
      "[Info] Using shared parameters for NN5\n",
      "[Info] Starting training for NN5 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 137256, Val size: 34314, Test size: 110850\n",
      "[NN5] Epoch 10/50, Train Loss: 0.000352, Val Loss: 0.000167, PredStd: 0.000797, PosRatio: 0.961\n",
      "[NN5] Epoch 20/50, Train Loss: 0.000293, Val Loss: 0.000166, PredStd: 0.000570, PosRatio: 0.984\n",
      "[NN5] Epoch 30/50, Train Loss: 0.000241, Val Loss: 0.000172, PredStd: 0.002531, PosRatio: 0.821\n",
      "[NN5] Early stopping at epoch 37\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5225259359494813\n",
      "Neg ratio (y_test): 0.47667117726657643\n",
      "Pred +1 ratio: 0.7026702751465945\n",
      "Pred -1 ratio: 0.29732972485340553\n",
      "      Pred+  Pred-\n",
      "+1 | [41011 16911]\n",
      "-1 | [36816 16023]\n",
      "[Update] Metrics updated for NN5 w=512\n",
      "[Save] NN5_w512.csv\n",
      "[Info] Training completed for NN5\n"
     ]
    }
   ],
   "source": [
    "# ========== 9. Entry point ==========\n",
    "if __name__ == \"__main__\":\n",
    "    loop_all_models()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-mac)",
   "language": "python",
   "name": "tf-mac"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
