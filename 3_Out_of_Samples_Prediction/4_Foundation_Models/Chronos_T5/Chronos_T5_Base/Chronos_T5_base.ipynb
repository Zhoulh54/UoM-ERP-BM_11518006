{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "! torch.compile not available, continuing with standard mode\n",
            "Dynamic batch size configuration:\n",
            "  Window 5: batch size = 256\n",
            "  Window 21: batch size = 256\n",
            "  Window 252: batch size = 64\n",
            "  Window 512: batch size = 64\n",
            "Device: mps:0\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import random\n",
        "import os\n",
        "def set_seed(seed=42):\n",
        "    \"\"\"Set all random seeds for reproducibility\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    if torch.backends.mps.is_available():\n",
        "        torch.mps.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "sys.path.append(\"src\")\n",
        "\n",
        "from chronos import BaseChronosPipeline \n",
        "\n",
        "pipeline = BaseChronosPipeline.from_pretrained(\n",
        "    \"amazon/chronos-t5-base\",\n",
        "    device_map=\"mps\",\n",
        "    torch_dtype=torch.float32\n",
        ")\n",
        "\n",
        "try:\n",
        "    pipeline.model = torch.compile(pipeline.model, backend=\"mps\")\n",
        "    print(\"torch.compile optimization enabled\")\n",
        "except:\n",
        "    print(\"torch.compile not available, continuing with standard mode\")\n",
        "\n",
        "data = np.load(\"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/all_window_datasets_unscaled.npz\", \n",
        "               allow_pickle=True)\n",
        "\n",
        "window_sizes = [5, 21, 252, 512]\n",
        "results = {}\n",
        "\n",
        "def get_batch_size(window_size, base_batch=256):\n",
        "    \"\"\"\n",
        "    Dynamically calculate batch size based on window size.\n",
        "    Larger window sizes require smaller batch sizes to avoid memory issues.\n",
        "    \"\"\"\n",
        "    if window_size <= 5:\n",
        "        return base_batch\n",
        "    elif window_size <= 21:\n",
        "        return base_batch // 1  \n",
        "    elif window_size <= 252:\n",
        "        return base_batch // 4  \n",
        "    elif window_size <= 512:\n",
        "        return base_batch // 4  \n",
        "    else:\n",
        "        return max(base_batch // (window_size // 10), 64)\n",
        "\n",
        "print(\"Dynamic batch size configuration:\")\n",
        "for ws in window_sizes:\n",
        "    batch_size = get_batch_size(ws)\n",
        "    print(f\"  Window {ws}: batch size = {batch_size}\")\n",
        "\n",
        "print(f\"Device: {pipeline.model.device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Processing Window Size: 5 ===\n",
            "Test samples: 110,850\n",
            "Sequence length: 5\n",
            "Batch size for window 5: 256\n",
            "Converting data to Tensor...\n",
            "Starting batch inference...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batch Inference: 100%|██████████| 434/434 [02:18<00:00,  3.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating evaluation metrics...\n",
            "\n",
            "  Window 5 processing completed:\n",
            "   Total time: 139.52 seconds\n",
            "   Processing speed: 794 samples/second\n",
            "   Estimated speedup vs sequential: 7.9x\n",
            "\n",
            " Window 5 evaluation metrics:\n",
            "   R²: -0.6636\n",
            "   MAE: 0.0145\n",
            "   MSE: 0.0004\n",
            "   Directional Accuracy: 0.4984\n",
            "   Up_Directional_Acc: 0.5885\n",
            "   Down_Directional_Acc: 0.3989\n",
            "\n",
            "=== Processing Window Size: 21 ===\n",
            "Test samples: 110,850\n",
            "Sequence length: 21\n",
            "Batch size for window 21: 256\n",
            "Converting data to Tensor...\n",
            "Starting batch inference...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batch Inference: 100%|██████████| 434/434 [06:01<00:00,  1.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating evaluation metrics...\n",
            "\n",
            "  Window 21 processing completed:\n",
            "   Total time: 362.44 seconds\n",
            "   Processing speed: 306 samples/second\n",
            "   Estimated speedup vs sequential: 3.1x\n",
            "\n",
            " Window 21 evaluation metrics:\n",
            "   R²: -0.1389\n",
            "   MAE: 0.0126\n",
            "   MSE: 0.0003\n",
            "   Directional Accuracy: 0.5002\n",
            "   Up_Directional_Acc: 0.5904\n",
            "   Down_Directional_Acc: 0.3994\n",
            "\n",
            "=== Processing Window Size: 252 ===\n",
            "Test samples: 110,850\n",
            "Sequence length: 252\n",
            "Batch size for window 252: 64\n",
            "Converting data to Tensor...\n",
            "Starting batch inference...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batch Inference: 100%|██████████| 1733/1733 [1:05:04<00:00,  2.25s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating evaluation metrics...\n",
            "\n",
            "  Window 252 processing completed:\n",
            "   Total time: 3905.06 seconds\n",
            "   Processing speed: 28 samples/second\n",
            "   Estimated speedup vs sequential: 0.3x\n",
            "\n",
            " Window 252 evaluation metrics:\n",
            "   R²: -0.0239\n",
            "   MAE: 0.0120\n",
            "   MSE: 0.0003\n",
            "   Directional Accuracy: 0.4984\n",
            "   Up_Directional_Acc: 0.4651\n",
            "   Down_Directional_Acc: 0.5313\n",
            "\n",
            "=== Processing Window Size: 512 ===\n",
            "Test samples: 110,850\n",
            "Sequence length: 512\n",
            "Batch size for window 512: 64\n",
            "Converting data to Tensor...\n",
            "Starting batch inference...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batch Inference: 100%|██████████| 1733/1733 [2:10:05<00:00,  4.50s/it] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating evaluation metrics...\n",
            "\n",
            "  Window 512 processing completed:\n",
            "   Total time: 7806.34 seconds\n",
            "   Processing speed: 14 samples/second\n",
            "   Estimated speedup vs sequential: 0.1x\n",
            "\n",
            " Window 512 evaluation metrics:\n",
            "   R²: -0.0264\n",
            "   MAE: 0.0120\n",
            "   MSE: 0.0003\n",
            "   Directional Accuracy: 0.4943\n",
            "   Up_Directional_Acc: 0.3742\n",
            "   Down_Directional_Acc: 0.6221\n"
          ]
        }
      ],
      "source": [
        "def r2_zero(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate zero-based R² (baseline is 0).\n",
        "    y_true: true values array (N,)\n",
        "    y_pred: predicted values array (N,)\n",
        "    \"\"\"\n",
        "    rss = np.sum((y_true - y_pred)**2)  \n",
        "    tss = np.sum(y_true**2)            \n",
        "    return 1 - rss / tss\n",
        "\n",
        "def calc_directional_metrics(y_true, y_pred, permnos=None):\n",
        "    \"\"\"\n",
        "    Calculate directional accuracy metrics.\n",
        "    If permnos is provided, calculate metrics per group and average.\n",
        "    \"\"\"\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "\n",
        "    if permnos is None:\n",
        "        s_true = np.sign(y_true)\n",
        "        s_pred = np.sign(y_pred)\n",
        "        mask = s_true != 0\n",
        "        s_true = s_true[mask]\n",
        "        s_pred = s_pred[mask]\n",
        "\n",
        "        overall_acc = np.mean(s_true == s_pred)\n",
        "\n",
        "        up_mask = s_true > 0\n",
        "        down_mask = s_true < 0\n",
        "        up_acc = np.mean(s_true[up_mask] == s_pred[up_mask]) if np.any(up_mask) else 0\n",
        "        down_acc = np.mean(s_true[down_mask] == s_pred[down_mask]) if np.any(down_mask) else 0\n",
        "\n",
        "    else:\n",
        "        df = pd.DataFrame({\"permno\": permnos, \"yt\": y_true, \"yp\": y_pred})\n",
        "        overall_accs = []\n",
        "        up_accs = []\n",
        "        down_accs = []\n",
        "\n",
        "        for _, g in df.groupby(\"permno\"):\n",
        "            s_true = np.sign(g[\"yt\"].values)\n",
        "            s_pred = np.sign(g[\"yp\"].values)\n",
        "            mask = s_true != 0\n",
        "            s_true = s_true[mask]\n",
        "            s_pred = s_pred[mask]\n",
        "            if len(s_true) == 0:\n",
        "                continue\n",
        "            overall_accs.append(np.mean(s_true == s_pred))\n",
        "\n",
        "            up_mask = s_true > 0\n",
        "            down_mask = s_true < 0\n",
        "            up_accs.append(np.mean(s_true[up_mask] == s_pred[up_mask]) if np.any(up_mask) else np.nan)\n",
        "            down_accs.append(np.mean(s_true[down_mask] == s_pred[down_mask]) if np.any(down_mask) else np.nan)\n",
        "\n",
        "        overall_acc = np.nanmean(overall_accs)\n",
        "        up_acc = np.nanmean(up_accs)\n",
        "        down_acc = np.nanmean(down_accs)\n",
        "\n",
        "    return overall_acc, up_acc, down_acc\n",
        "\n",
        "def calculate_metrics(y_true, y_pred, k=1, meta=None, permnos=None):\n",
        "    \"\"\"\n",
        "    Calculate regression and directional metrics.\n",
        "    If meta is provided, also calculate metrics for market cap groups.\n",
        "    \"\"\"\n",
        "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "    \n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    n = len(y_true)\n",
        "\n",
        "    r2 = r2_zero(y_true, y_pred)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)  \n",
        "\n",
        "    dir_acc, up_acc, down_acc = calc_directional_metrics(y_true, y_pred, permnos)\n",
        "\n",
        "    metrics = {\n",
        "        \"R²\": r2,\n",
        "        \"MAE\": mae,\n",
        "        \"MSE\": mse,\n",
        "        \"RMSE\": rmse,  \n",
        "        \"Directional Accuracy\": dir_acc,\n",
        "        \"Up_Directional_Acc\": up_acc,\n",
        "        \"Down_Directional_Acc\": down_acc\n",
        "    }\n",
        "\n",
        "    if meta is not None and \"MKTCAP_PERCENTILE\" in meta.columns:\n",
        "        top_mask = meta[\"MKTCAP_PERCENTILE\"] >= 0.75\n",
        "        bottom_mask = meta[\"MKTCAP_PERCENTILE\"] <= 0.25\n",
        "\n",
        "        if np.any(top_mask):\n",
        "            yt_top = y_true[top_mask]\n",
        "            yp_top = y_pred[top_mask]\n",
        "            perm_top = permnos[top_mask] if permnos is not None else None\n",
        "            r2_top = r2_zero(yt_top, yp_top)\n",
        "            mae_top = mean_absolute_error(yt_top, yp_top)\n",
        "            mse_top = mean_squared_error(yt_top, yp_top)\n",
        "            rmse_top = np.sqrt(mse_top)  \n",
        "            dir_top, up_top, down_top = calc_directional_metrics(yt_top, yp_top, perm_top)\n",
        "            metrics.update({\n",
        "                \"Top25_R2\": r2_top,\n",
        "                \"Top25_MAE\": mae_top,\n",
        "                \"Top25_MSE\": mse_top,\n",
        "                \"Top25_RMSE\": rmse_top,  \n",
        "                \"Top25_Dir_Acc\": dir_top,\n",
        "                \"Top25_Up_Acc\": up_top,\n",
        "                \"Top25_Down_Acc\": down_top\n",
        "            })\n",
        "\n",
        "        if np.any(bottom_mask):\n",
        "            yt_bot = y_true[bottom_mask]\n",
        "            yp_bot = y_pred[bottom_mask]\n",
        "            perm_bot = permnos[bottom_mask] if permnos is not None else None\n",
        "            r2_bot = r2_zero(yt_bot, yp_bot)\n",
        "            mae_bot = mean_absolute_error(yt_bot, yp_bot)\n",
        "            mse_bot = mean_squared_error(yt_bot, yp_bot)\n",
        "            rmse_bot = np.sqrt(mse_bot)  \n",
        "            dir_bot, up_bot, down_bot = calc_directional_metrics(yt_bot, yp_bot, perm_bot)\n",
        "            metrics.update({\n",
        "                \"Bottom25_R2\": r2_bot,\n",
        "                \"Bottom25_MAE\": mae_bot,\n",
        "                \"Bottom25_MSE\": mse_bot,\n",
        "                \"Bottom25_RMSE\": rmse_bot,  \n",
        "                \"Bottom25_Dir_Acc\": dir_bot,\n",
        "                \"Bottom25_Up_Acc\": up_bot,\n",
        "                \"Bottom25_Down_Acc\": down_bot\n",
        "            })\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def batch_predict(X_test_tensor, pipeline, batch_size=4096):\n",
        "    \"\"\"\n",
        "    Batch inference for faster prediction.\n",
        "    \"\"\"\n",
        "    all_predictions = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(X_test_tensor), batch_size), desc=\"Batch Inference\"):\n",
        "            batch_end = min(i + batch_size, len(X_test_tensor))\n",
        "            batch_ctx = X_test_tensor[i:batch_end]\n",
        "            context_list = [seq for seq in batch_ctx]\n",
        "            forecasts = pipeline.predict(\n",
        "                context=context_list,\n",
        "                prediction_length=1,\n",
        "                num_samples=10\n",
        "            )\n",
        "            batch_means = np.array([f[0].mean() for f in forecasts])\n",
        "            all_predictions.extend(batch_means.ravel())\n",
        "    \n",
        "    return np.array(all_predictions)\n",
        "\n",
        "for window_size in window_sizes:\n",
        "    print(f\"\\n=== Processing Window Size: {window_size} ===\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    X_test = data[f\"X_test_{window_size}\"]\n",
        "    y_test = data[f\"y_test_{window_size}\"]\n",
        "    meta_test = pd.DataFrame(data[f\"meta_test_{window_size}\"].item())\n",
        "    \n",
        "    current_batch_size = get_batch_size(window_size)\n",
        "    \n",
        "    print(f\"Test samples: {len(X_test):,}\")\n",
        "    print(f\"Sequence length: {X_test.shape[1]}\")\n",
        "    print(f\"Batch size for window {window_size}: {current_batch_size}\")\n",
        "    \n",
        "    print(\"Converting data to Tensor...\")\n",
        "    X_test_tensor = torch.from_numpy(X_test.astype(np.float32))\n",
        "    \n",
        "    print(\"Starting batch inference...\")\n",
        "    all_predictions = batch_predict(X_test_tensor, pipeline, current_batch_size)\n",
        "    \n",
        "    print(\"Calculating evaluation metrics...\")\n",
        "    metrics = calculate_metrics(y_test, all_predictions, k=window_size, meta=meta_test, permnos=meta_test[\"PERMNO\"].values)\n",
        "    \n",
        "    results[window_size] = {\n",
        "        'predictions': all_predictions,\n",
        "        'true_values': y_test,\n",
        "        'metrics': metrics,\n",
        "        'meta': meta_test\n",
        "    }\n",
        "    \n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    samples_per_second = len(X_test) / elapsed_time\n",
        "    \n",
        "    print(f\"\\n  Window {window_size} processing completed:\")\n",
        "    print(f\"   Total time: {elapsed_time:.2f} seconds\")\n",
        "    print(f\"   Processing speed: {samples_per_second:.0f} samples/second\")\n",
        "    print(f\"   Estimated speedup vs sequential: {samples_per_second/100:.1f}x\")\n",
        "    \n",
        "    print(f\"\\n Window {window_size} evaluation metrics:\")\n",
        "    main_metrics = [\"R²\", \"MAE\", \"MSE\", \"Directional Accuracy\", \"Up_Directional_Acc\", \"Down_Directional_Acc\"]\n",
        "    for metric_name in main_metrics:\n",
        "        if metric_name in metrics:\n",
        "            value = metrics[metric_name]\n",
        "            print(f\"   {metric_name}: {value:.4f}\" if value is not None else f\"   {metric_name}: N/A\")\n",
        "    \n",
        "    del X_test_tensor\n",
        "    torch.mps.empty_cache() if torch.backends.mps.is_available() else None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Save] results_base.pkl saved successfully (base)\n"
          ]
        }
      ],
      "source": [
        "# Save results\n",
        "import os\n",
        "import joblib\n",
        "\n",
        "# Create directories for saving results\n",
        "os.makedirs(\"chronos_t5_base_results\", exist_ok=True)\n",
        "os.makedirs(\"chronos_t5_base_predictions\", exist_ok=True)\n",
        "\n",
        "# Save the complete results to a pkl file\n",
        "results_dict = {\n",
        "    'window_sizes': window_sizes,\n",
        "    'results': results,\n",
        "    'model_name': 'Chronos-T5-Base'\n",
        "}\n",
        "joblib.dump(results_dict, \"chronos_t5_base_results/results_base.pkl\")\n",
        "print(\"[Save] results_base.pkl saved successfully (base)\")\n",
        "\n",
        "# Save evaluation metrics\n",
        "metrics_df = pd.DataFrame([\n",
        "    {**{\"Window\": window_size}, **results[window_size][\"metrics\"]}\n",
        "    for window_size in window_sizes\n",
        "])\n",
        "metrics_df.to_csv(\"chronos_t5_base_results/chronos_t5_base_metrics.csv\", index=False)\n",
        "\n",
        "# Save prediction results\n",
        "for window_size in window_sizes:\n",
        "    df = pd.DataFrame({\n",
        "        \"PERMNO\": results[window_size][\"meta\"][\"PERMNO\"],\n",
        "        \"y_true\": results[window_size][\"true_values\"],\n",
        "        \"y_pred\": results[window_size][\"predictions\"]\n",
        "    })\n",
        "    df.to_csv(f\"chronos_t5_base_predictions/chronos_t5_base_w{window_size}.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Chronos-T5-Base Results ===\n",
            "Window sizes: [5, 21, 252, 512]\n",
            "\n",
            "Metrics for window 5:\n",
            "R²: -0.6636\n",
            "MAE: 0.0145\n",
            "MSE: 0.0004\n",
            "Directional Accuracy: 0.4984\n",
            "Up_Directional_Acc: 0.5885\n",
            "Down_Directional_Acc: 0.3989\n",
            "\n",
            "Metrics for window 21:\n",
            "R²: -0.1389\n",
            "MAE: 0.0126\n",
            "MSE: 0.0003\n",
            "Directional Accuracy: 0.5002\n",
            "Up_Directional_Acc: 0.5904\n",
            "Down_Directional_Acc: 0.3994\n",
            "\n",
            "Metrics for window 252:\n",
            "R²: -0.0239\n",
            "MAE: 0.0120\n",
            "MSE: 0.0003\n",
            "Directional Accuracy: 0.4984\n",
            "Up_Directional_Acc: 0.4651\n",
            "Down_Directional_Acc: 0.5313\n",
            "\n",
            "Metrics for window 512:\n",
            "R²: -0.0264\n",
            "MAE: 0.0120\n",
            "MSE: 0.0003\n",
            "Directional Accuracy: 0.4943\n",
            "Up_Directional_Acc: 0.3742\n",
            "Down_Directional_Acc: 0.6221\n"
          ]
        }
      ],
      "source": [
        "# Load and display saved results\n",
        "def load_results(file_path=\"chronos_t5_base_results/results_base.pkl\"):\n",
        "    \"\"\"Load saved results\"\"\"\n",
        "    loaded_results = joblib.load(file_path)\n",
        "    print(f\"\\n=== {loaded_results['model_name']} Results ===\")\n",
        "    print(f\"Window sizes: {loaded_results['window_sizes']}\")\n",
        "    \n",
        "    for window_size in loaded_results['window_sizes']:\n",
        "        print(f\"\\nMetrics for window {window_size}:\")\n",
        "        metrics = loaded_results['results'][window_size]['metrics']\n",
        "        main_metrics = [\"R²\", \"MAE\", \"MSE\", \"Directional Accuracy\", \"Up_Directional_Acc\", \"Down_Directional_Acc\"]\n",
        "        for metric_name in main_metrics:\n",
        "            if metric_name in metrics:\n",
        "                value = metrics[metric_name]\n",
        "                print(f\"{metric_name}: {value:.4f}\" if value is not None else f\"{metric_name}: N/A\")\n",
        "    \n",
        "    return loaded_results\n",
        "\n",
        "loaded_results = load_results()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (tf-mac)",
      "language": "python",
      "name": "tf-mac"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
