{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: install [-bCcpSsUv] [-f flags] [-g group] [-m mode] [-o owner]\n",
      "               [-M log] [-D dest] [-h hash] [-T tags]\n",
      "               [-B suffix] [-l linkflags] [-N dbdir]\n",
      "               file1 file2\n",
      "       install [-bCcpSsUv] [-f flags] [-g group] [-m mode] [-o owner]\n",
      "               [-M log] [-D dest] [-h hash] [-T tags]\n",
      "               [-B suffix] [-l linkflags] [-N dbdir]\n",
      "               file1 ... fileN directory\n",
      "       install -dU [-vU] [-g group] [-m mode] [-N dbdir] [-o owner]\n",
      "               [-M log] [-D dest] [-h hash] [-T tags]\n",
      "               directory ...\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# ========== Basic Libraries ==========\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ========== Model and Preprocessing ==========\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# ========== Evaluation Metrics ==========\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    ")\n",
    "\n",
    "# ========== Visualization and Hyperparameter Tuning ==========\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "!install optuna\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# ========== Global Configuration ==========\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "plt.rcdefaults()\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 1. Data Loading and Preprocessing ==========\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, X, y, device):\n",
    "        self.X = torch.FloatTensor(X).to(device)\n",
    "        self.y = torch.FloatTensor(y).to(device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def load_datasets(npz_path=\"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/all_window_datasets_scaled.npz\"):\n",
    "    data = np.load(npz_path, allow_pickle=True)\n",
    "    datasets = {}\n",
    "    for key in data.files:\n",
    "        datasets[key] = data[key]\n",
    "    return datasets\n",
    "\n",
    "def load_y_scaler(window_size, scaler_path=\"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/\"):\n",
    "    scaler_file = os.path.join(scaler_path, f\"scaler_y_window_{window_size}.pkl\")\n",
    "    if os.path.exists(scaler_file):\n",
    "        scaler_y = joblib.load(scaler_file)\n",
    "        print(f\"[Info] Loaded y scaler for window {window_size}\")\n",
    "        return scaler_y\n",
    "    else:\n",
    "        print(f\"[Warning] Y scaler file not found: {scaler_file}\")\n",
    "        return None\n",
    "\n",
    "def prepare_data(X, y, batch_size=128, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    dataset = StockDataset(X, y, device)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ========== 2. Evaluation Metrics ==========\n",
    "def r2_zero(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate zero-based RÂ² (baseline is zero)\n",
    "    y_true: true values (N,)\n",
    "    y_pred: predicted values (N,)\n",
    "    \"\"\"\n",
    "    rss = np.sum((y_true - y_pred)**2)  \n",
    "    tss = np.sum(y_true**2)            \n",
    "    return 1 - rss / tss\n",
    "\n",
    "def calc_directional_metrics(y_true, y_pred, permnos=None):\n",
    "    \"\"\"\n",
    "    Directional metrics:\n",
    "    - Sign prediction at sample level\n",
    "    - If grouped by stock, calculate Overall, Up, Down for each stock and average\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    if permnos is None:\n",
    "        s_true = np.sign(y_true)\n",
    "        s_pred = np.sign(y_pred)\n",
    "        mask = s_true != 0\n",
    "        s_true = s_true[mask]\n",
    "        s_pred = s_pred[mask]\n",
    "\n",
    "        overall_acc = np.mean(s_true == s_pred)\n",
    "\n",
    "        up_mask = s_true > 0\n",
    "        down_mask = s_true < 0\n",
    "        up_acc = np.mean(s_true[up_mask] == s_pred[up_mask]) if np.any(up_mask) else 0\n",
    "        down_acc = np.mean(s_true[down_mask] == s_pred[down_mask]) if np.any(down_mask) else 0\n",
    "\n",
    "    else:\n",
    "        df = pd.DataFrame({\"permno\": permnos, \"yt\": y_true, \"yp\": y_pred})\n",
    "        overall_accs = []\n",
    "        up_accs = []\n",
    "        down_accs = []\n",
    "\n",
    "        for _, g in df.groupby(\"permno\"):\n",
    "            s_true = np.sign(g[\"yt\"].values)\n",
    "            s_pred = np.sign(g[\"yp\"].values)\n",
    "            mask = s_true != 0\n",
    "            s_true = s_true[mask]\n",
    "            s_pred = s_pred[mask]\n",
    "            if len(s_true) == 0:\n",
    "                continue\n",
    "            overall_accs.append(np.mean(s_true == s_pred))\n",
    "\n",
    "            up_mask = s_true > 0\n",
    "            down_mask = s_true < 0\n",
    "            up_accs.append(np.mean(s_true[up_mask] == s_pred[up_mask]) if np.any(up_mask) else np.nan)\n",
    "            down_accs.append(np.mean(s_true[down_mask] == s_pred[down_mask]) if np.any(down_mask) else np.nan)\n",
    "\n",
    "        overall_acc = np.nanmean(overall_accs)\n",
    "        up_acc = np.nanmean(up_accs)\n",
    "        down_acc = np.nanmean(down_accs)\n",
    "\n",
    "    return overall_acc, up_acc, down_acc\n",
    "\n",
    "def regression_metrics(y_true, y_pred, k, meta=None, permnos=None):\n",
    "    \"\"\"\n",
    "    Combined regression metrics:\n",
    "    - Regression metrics\n",
    "    - Pointwise directional accuracy\n",
    "    - Market cap group metrics\n",
    "    \"\"\"\n",
    "    if isinstance(y_true, torch.Tensor):\n",
    "        y_true = y_true.detach().cpu().numpy()\n",
    "    if isinstance(y_pred, torch.Tensor):\n",
    "        y_pred = y_pred.detach().cpu().numpy()\n",
    "    \n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    n = len(y_true)\n",
    "\n",
    "    r2 = r2_zero(y_true, y_pred)  \n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "\n",
    "    dir_acc, up_acc, down_acc = calc_directional_metrics(y_true, y_pred, permnos)\n",
    "\n",
    "    metrics = {\n",
    "        \"R2_zero\": r2,\n",
    "        \"RMSE\": rmse,\n",
    "        \"MAE\": mae,\n",
    "        \"MSE\": mse,\n",
    "        \"Directional Accuracy\": dir_acc,\n",
    "        \"Up_Directional_Acc\": up_acc,\n",
    "        \"Down_Directional_Acc\": down_acc\n",
    "    }\n",
    "\n",
    "    if meta is not None and \"MKTCAP_PERCENTILE\" in meta:\n",
    "        top_mask = meta[\"MKTCAP_PERCENTILE\"] >= 0.75\n",
    "        bottom_mask = meta[\"MKTCAP_PERCENTILE\"] <= 0.25\n",
    "\n",
    "        if np.any(top_mask):\n",
    "            yt_top = y_true[top_mask]\n",
    "            yp_top = y_pred[top_mask]\n",
    "            perm_top = permnos[top_mask] if permnos is not None else None\n",
    "            r2_top = r2_zero(yt_top, yp_top)\n",
    "            rmse_top = np.sqrt(mean_squared_error(yt_top, yp_top))\n",
    "            mae_top = mean_absolute_error(yt_top, yp_top)\n",
    "            mse_top = mean_squared_error(yt_top, yp_top)\n",
    "            dir_top, up_top, down_top = calc_directional_metrics(yt_top, yp_top, perm_top)\n",
    "            metrics.update({\n",
    "                \"Top25_R2_zero\": r2_top,\n",
    "                \"Top25_MSE\": mse_top,\n",
    "                \"Top25_RMSE\": rmse_top,\n",
    "                \"Top25_MAE\": mae_top,\n",
    "                \"Top25_Dir_Acc\": dir_top,\n",
    "                \"Top25_Up_Acc\": up_top,\n",
    "                \"Top25_Down_Acc\": down_top\n",
    "            })\n",
    "\n",
    "        if np.any(bottom_mask):\n",
    "            yt_bot = y_true[bottom_mask]\n",
    "            yp_bot = y_pred[bottom_mask]\n",
    "            perm_bot = permnos[bottom_mask] if permnos is not None else None\n",
    "            r2_bot = r2_zero(yt_bot, yp_bot)\n",
    "            rmse_bot = np.sqrt(mean_squared_error(yt_bot, yp_bot))\n",
    "            mae_bot = mean_absolute_error(yt_bot, yp_bot)\n",
    "            mse_bot = mean_squared_error(yt_bot, yp_bot)\n",
    "            dir_bot, up_bot, down_bot = calc_directional_metrics(yt_bot, yp_bot, perm_bot)\n",
    "            metrics.update({\n",
    "                \"Bottom25_R2_zero\": r2_bot,\n",
    "                \"Bottom25_MSE\": mse_bot,\n",
    "                \"Bottom25_RMSE\": rmse_bot,\n",
    "                \"Bottom25_MAE\": mae_bot,\n",
    "                \"Bottom25_Dir_Acc\": dir_bot,\n",
    "                \"Bottom25_Up_Acc\": up_bot,\n",
    "                \"Bottom25_Down_Acc\": down_bot\n",
    "            })\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 3. Model Definition =====\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, dropout_rate=0.1):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze()\n",
    "\n",
    "MLP_CONFIGS = {\n",
    "    \"NN1\": [64],                    \n",
    "    \"NN2\": [64, 32],               \n",
    "    \"NN3\": [128, 64, 32],          \n",
    "    \"NN4\": [128, 64, 32, 16],      \n",
    "    \"NN5\": [256, 128, 64, 32, 16]  \n",
    "}\n",
    "\n",
    "DEFAULT_PARAMS = {\n",
    "    \"batch_size\": 128,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"epochs\": 50\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 4. Training Functions ==========\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            output = model(X)\n",
    "            loss = criterion(output, y)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            predictions.extend(output.cpu().numpy())\n",
    "            targets.extend(y.cpu().numpy())\n",
    "    \n",
    "    return (total_loss / len(val_loader), \n",
    "            np.array(predictions), \n",
    "            np.array(targets))\n",
    "\n",
    "def create_train_val_split(X_train, y_train, permnos_train, val_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Create a validation set from the training set in chronological order.\n",
    "    The last val_ratio proportion of the data is used as the validation set.\n",
    "    \"\"\"\n",
    "    split_idx = int(len(X_train) * (1 - val_ratio))\n",
    "    \n",
    "    X_tr = X_train[:split_idx]\n",
    "    X_val = X_train[split_idx:]\n",
    "    y_tr = y_train[:split_idx]\n",
    "    y_val = y_train[split_idx:]\n",
    "    \n",
    "    if permnos_train is not None:\n",
    "        perm_tr = permnos_train[:split_idx]\n",
    "        perm_val = permnos_train[split_idx:]\n",
    "        return X_tr, X_val, y_tr, y_val, perm_tr, perm_val\n",
    "    else:\n",
    "        return X_tr, X_val, y_tr, y_val, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 5. Hyperparameter Tuning ==========\n",
    "\n",
    "def tune_model_with_optuna(model_name, X, y, permnos=None, n_trials=10):\n",
    "    \"\"\"Hyperparameter tuning using Optuna with pure MSE loss, reduced learning rate, and less regularization\"\"\"\n",
    "    input_dim = X.shape[1]\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"batch_size\": trial.suggest_categorical(\"batch_size\", [64, 128]),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True),\n",
    "            \"dropout_rate\": trial.suggest_float(\"dropout_rate\", 0.05, 0.15),\n",
    "            \"epochs\": 20\n",
    "        }\n",
    "        \n",
    "        cv_scores = []\n",
    "        for train_idx, val_idx in tscv.split(X):\n",
    "            X_tr, X_val = X[train_idx], X[val_idx]\n",
    "            y_tr, y_val = y[train_idx], y[val_idx]\n",
    "            \n",
    "            train_loader = prepare_data(X_tr, y_tr, batch_size=params[\"batch_size\"])\n",
    "            val_loader = prepare_data(X_val, y_val, batch_size=params[\"batch_size\"])\n",
    "            \n",
    "            model = MLP(\n",
    "                input_dim=input_dim,\n",
    "                hidden_dims=MLP_CONFIGS[model_name],\n",
    "                dropout_rate=params[\"dropout_rate\"]\n",
    "            ).to(device)\n",
    "            \n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=params[\"learning_rate\"])\n",
    "            \n",
    "            best_val_loss = float('inf')\n",
    "            \n",
    "            for epoch in range(params[\"epochs\"]):\n",
    "                train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "                val_loss, _, _ = validate(model, val_loader, criterion, device)\n",
    "                \n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "            \n",
    "            cv_scores.append(best_val_loss)\n",
    "        \n",
    "        return np.mean(cv_scores)\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        sampler=optuna.samplers.TPESampler(seed=42),\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=5)\n",
    "    )\n",
    "    \n",
    "    study.optimize(objective, n_trials=n_trials, n_jobs=1)\n",
    "    \n",
    "    if len(study.trials) == 0 or study.best_trial is None:\n",
    "        print(f\"[Skip Model] {model_name} failed to complete any trial. Skipping.\")\n",
    "        return None\n",
    "    \n",
    "    best_params = study.best_params\n",
    "    best_params[\"epochs\"] = 50  \n",
    "    best_score = study.best_value\n",
    "    print(f\"[Optuna] {model_name} best_MSE={best_score:.6f}, best_params={best_params}\")\n",
    "    \n",
    "    return best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, name, window, path=\"models/\"):\n",
    "    \"\"\"Save model to file\"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join(path, f\"{name}_w{window}.pth\"))\n",
    "\n",
    "def save_metrics(metrics_dict, name, window, path=\"results.csv\"):\n",
    "    \"\"\"Save evaluation metrics\"\"\"\n",
    "    row = pd.DataFrame([metrics_dict])\n",
    "    row.insert(0, \"Model\", name)\n",
    "    row.insert(1, \"Window\", window)\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path)\n",
    "        df = df[~((df[\"Model\"] == name) & (df[\"Window\"] == window))]\n",
    "        df = pd.concat([df, row], ignore_index=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        print(f\"[Update] Metrics updated for {name} w={window}\")\n",
    "    else:\n",
    "        row.to_csv(path, index=False)\n",
    "        print(f\"[Create] New metrics file created with {name} w={window}\")\n",
    "\n",
    "def save_predictions(model_name, window_size, y_true, y_pred, permnos, path=\"predictions/\"):\n",
    "    \"\"\"Save prediction results\"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        \"PERMNO\": permnos,\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred\n",
    "    })\n",
    "\n",
    "    filename = f\"{model_name}_w{window_size}.csv\"\n",
    "    df.to_csv(os.path.join(path, filename), index=False)\n",
    "    print(f\"[Save] {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 7. Main Training and Evaluation Function ==========\n",
    "def train_and_evaluate(model_name, window_size,\n",
    "                       X_train, y_train, X_test, y_test,\n",
    "                       permnos_train, permnos_test, meta=None, shared_params=None, scaler_y=None):\n",
    "    print(f\"\\nTraining {model_name} on Window = {window_size}\")\n",
    "    input_dim = X_train.shape[1]\n",
    "    \n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f\"[Info] Using device: {device}\")\n",
    "    \n",
    "    if model_name == \"NN1\":\n",
    "        best_params = tune_model_with_optuna(model_name, X_train, y_train, permnos=permnos_train)\n",
    "        if best_params is None:\n",
    "            print(f\"[Skip] {model_name} tuning failed, using default parameters\")\n",
    "            best_params = DEFAULT_PARAMS.copy()\n",
    "    else:\n",
    "        if shared_params is None:\n",
    "            print(f\"[Warning] No shared parameters provided for {model_name}, using default\")\n",
    "            best_params = DEFAULT_PARAMS.copy()\n",
    "        else:\n",
    "            print(f\"[Info] Using shared parameters for {model_name}\")\n",
    "            best_params = shared_params.copy()\n",
    "    \n",
    "    X_tr, X_val, y_tr, y_val, perm_tr, perm_val = create_train_val_split(\n",
    "        X_train, y_train, permnos_train, val_ratio=0.2\n",
    "    )\n",
    "    \n",
    "    train_loader = prepare_data(X_tr, y_tr, batch_size=best_params[\"batch_size\"], device=device)\n",
    "    val_loader = prepare_data(X_val, y_val, batch_size=best_params[\"batch_size\"], device=device)\n",
    "    test_loader = prepare_data(X_test, y_test, batch_size=best_params[\"batch_size\"], device=device)\n",
    "    \n",
    "    model = MLP(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dims=MLP_CONFIGS[model_name],\n",
    "        dropout_rate=best_params[\"dropout_rate\"]\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_params[\"learning_rate\"])\n",
    "    \n",
    "    best_model = None\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 20\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(f\"[Info] Starting training for {model_name} with {best_params['epochs']} epochs (early stopping patience={patience})...\")\n",
    "    print(f\"[Info] Train size: {len(X_tr)}, Val size: {len(X_val)}, Test size: {len(X_test)}\")\n",
    "    \n",
    "    for epoch in range(best_params[\"epochs\"]):\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, predictions, targets = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        pred_std = np.std(predictions)\n",
    "        positive_ratio = (np.sign(predictions) > 0).mean()\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"[{model_name}] Epoch {epoch+1}/{best_params['epochs']}, \"\n",
    "                  f\"Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}, \"\n",
    "                  f\"PredStd: {pred_std:.6f}, PosRatio: {positive_ratio:.3f}\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"[{model_name}] Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    if best_model is not None:\n",
    "        model.load_state_dict(best_model)\n",
    "    \n",
    "    _, y_pred, y_true = validate(model, test_loader, criterion, device)\n",
    "    \n",
    "    y_pred = y_pred.cpu() if isinstance(y_pred, torch.Tensor) else y_pred\n",
    "    y_true = y_true.cpu() if isinstance(y_true, torch.Tensor) else y_true\n",
    "    \n",
    "    if scaler_y is not None:\n",
    "        print(f\"[Info] Applying inverse transform using y scaler for window {window_size}\")\n",
    "        y_true_original = scaler_y.inverse_transform(y_true.reshape(-1, 1)).flatten()\n",
    "        y_pred_original = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "        print(f\"[Info] Original scale - y_true range: [{y_true_original.min():.6f}, {y_true_original.max():.6f}]\")\n",
    "        print(f\"[Info] Original scale - y_pred range: [{y_pred_original.min():.6f}, {y_pred_original.max():.6f}]\")\n",
    "        y_true_eval = y_true_original\n",
    "        y_pred_eval = y_pred_original\n",
    "    else:\n",
    "        print(f\"[Warning] No y scaler provided, using standardized values for evaluation\")\n",
    "        y_true_eval = y_true\n",
    "        y_pred_eval = y_pred\n",
    "    \n",
    "    print(\"\\n=== Directional Sanity Check ===\")\n",
    "    print(\"Pos ratio (y_test):\", (y_true_eval > 0).mean())\n",
    "    print(\"Neg ratio (y_test):\", (y_true_eval < 0).mean())\n",
    "    sign_pred = np.sign(y_pred_eval)\n",
    "    print(\"Pred +1 ratio:\", (sign_pred > 0).mean())\n",
    "    print(\"Pred -1 ratio:\", (sign_pred < 0).mean())\n",
    "    \n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    conf = confusion_matrix(np.sign(y_true_eval), sign_pred, labels=[1, -1])\n",
    "    print(\"      Pred+  Pred-\")\n",
    "    print(\"+1 |\", conf[0])\n",
    "    print(\"-1 |\", conf[1])\n",
    "    \n",
    "    metrics = regression_metrics(y_true_eval, y_pred_eval, k=X_test.shape[1], meta=meta, permnos=permnos_test)\n",
    "    \n",
    "    save_model(model, model_name, window_size)\n",
    "    save_metrics(metrics, model_name, window_size)\n",
    "    save_predictions(model_name, window_size, y_true_eval, y_pred_eval, permnos_test)\n",
    "    \n",
    "    print(f\"[Info] Training completed for {model_name}\")\n",
    "    return metrics, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 8. ä¸»è°åº¦å½æ°ï¼å¾ªç¯æææ¨¡åä¸çªå£ ==========\n",
    "def loop_all_models(run_test_first=False):\n",
    "    \"\"\"å¾ªç¯è®­ç»æææ¨¡åå¨ä¸åçªå£ä¸\"\"\"\n",
    "    # å è½½æ°æ®é\n",
    "    datasets = load_datasets()\n",
    "    \n",
    "    # å®ä¹æ¨¡ååè¡¨åçªå£å¤§å°\n",
    "    model_list = [\"NN1\", \"NN2\", \"NN3\", \"NN4\", \"NN5\"]\n",
    "    window_sizes = [5, 21, 252, 512]\n",
    "\n",
    "    # å¾ªç¯è®­ç»æ¯ä¸ªæ¨¡åå¨æ¯ä¸ªçªå£ä¸\n",
    "    for window in window_sizes:\n",
    "        print(f\"\\n=== Processing Window Size: {window} ===\")\n",
    "        \n",
    "        # å è½½å¯¹åºçªå£çæ°æ®\n",
    "        X_train = datasets[f\"X_train_{window}\"]\n",
    "        y_train = datasets[f\"y_train_{window}\"]\n",
    "        X_test = datasets[f\"X_test_{window}\"]\n",
    "        y_test = datasets[f\"y_test_{window}\"]\n",
    "        \n",
    "        # åæ¶å è½½ train & test ç meta\n",
    "        meta_train_dict = datasets[f\"meta_train_{window}\"].item()\n",
    "        meta_test_dict = datasets[f\"meta_test_{window}\"].item()\n",
    "\n",
    "        meta_train = pd.DataFrame.from_dict(meta_train_dict)\n",
    "        meta_test = pd.DataFrame.from_dict(meta_test_dict)\n",
    "\n",
    "        # åå«æå permnos\n",
    "        permnos_train = meta_train[\"PERMNO\"].values\n",
    "        permnos_test = meta_test[\"PERMNO\"].values\n",
    "\n",
    "        # å è½½å¯¹åºçªå£çyæ ååå¨\n",
    "        scaler_y = load_y_scaler(window)\n",
    "\n",
    "        # é¦åè®­ç»NN1è·åå±äº«åæ°\n",
    "        print(f\"\\nTraining NN1 to get shared parameters...\")\n",
    "        _, shared_params = train_and_evaluate(\n",
    "            \"NN1\", window,\n",
    "            X_train, y_train, X_test, y_test,\n",
    "            permnos_train, permnos_test,\n",
    "            meta_test, None, scaler_y\n",
    "        )\n",
    "        print(f\"Shared parameters from NN1: {shared_params}\")\n",
    "\n",
    "        # ä½¿ç¨NN1çåæ°è®­ç»å¶ä»æ¨¡å\n",
    "        for model_name in model_list[1:]:  # è·³è¿NN1\n",
    "            print(f\"\\nTraining {model_name} with shared parameters...\")\n",
    "            train_and_evaluate(\n",
    "                model_name, window,\n",
    "                X_train, y_train, X_test, y_test,\n",
    "                permnos_train, permnos_test,\n",
    "                meta_test, shared_params, scaler_y\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Window Size: 5 ===\n",
      "[Info] Loaded y scaler for window 5\n",
      "\n",
      "Training NN1 to get shared parameters...\n",
      "\n",
      "â¶ Training NN1 on Window = 5\n",
      "[Info] Using device: mps\n",
      "[Optuna] NN1 best_MSE=0.788915, best_params={'batch_size': 64, 'learning_rate': 0.000164092867306479, 'dropout_rate': 0.06705241236872915, 'epochs': 50}\n",
      "[Info] Starting training for NN1 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 157536, Val size: 39384, Test size: 110850\n",
      "[NN1] Epoch 10/50, Train Loss: 1.133244, Val Loss: 0.437505, PredStd: 0.098423, PosRatio: 0.360\n",
      "[NN1] Epoch 20/50, Train Loss: 1.131761, Val Loss: 0.438108, PredStd: 0.099188, PosRatio: 0.367\n",
      "[NN1] Early stopping at epoch 29\n",
      "[Info] Applying inverse transform using y scaler for window 5\n",
      "[Info] Original scale - y_true range: [-0.100574, 0.108696]\n",
      "[Info] Original scale - y_pred range: [-0.027309, 0.023656]\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5225259359494813\n",
      "Neg ratio (y_test): 0.47747406405051873\n",
      "Pred +1 ratio: 0.4816508795669824\n",
      "Pred -1 ratio: 0.5183491204330176\n",
      "      Pred+  Pred-\n",
      "+1 | [28335 29587]\n",
      "-1 | [25056 27872]\n",
      "[Update] Metrics updated for NN1 w=5\n",
      "[Save] NN1_w5.csv\n",
      "[Info] Training completed for NN1\n",
      "Shared parameters from NN1: {'batch_size': 64, 'learning_rate': 0.000164092867306479, 'dropout_rate': 0.06705241236872915, 'epochs': 50}\n",
      "\n",
      "Training NN2 with shared parameters...\n",
      "\n",
      "â¶ Training NN2 on Window = 5\n",
      "[Info] Using device: mps\n",
      "[Info] Using shared parameters for NN2\n",
      "[Info] Starting training for NN2 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 157536, Val size: 39384, Test size: 110850\n",
      "[NN2] Epoch 10/50, Train Loss: 1.129520, Val Loss: 0.435666, PredStd: 0.087227, PosRatio: 0.396\n",
      "[NN2] Epoch 20/50, Train Loss: 1.125453, Val Loss: 0.436324, PredStd: 0.087556, PosRatio: 0.414\n",
      "[NN2] Early stopping at epoch 25\n",
      "[Info] Applying inverse transform using y scaler for window 5\n",
      "[Info] Original scale - y_true range: [-0.100574, 0.108696]\n",
      "[Info] Original scale - y_pred range: [-0.035249, 0.033847]\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5225259359494813\n",
      "Neg ratio (y_test): 0.47747406405051873\n",
      "Pred +1 ratio: 0.5103112313937753\n",
      "Pred -1 ratio: 0.4896887686062246\n",
      "      Pred+  Pred-\n",
      "+1 | [29988 27934]\n",
      "-1 | [26580 26348]\n",
      "[Update] Metrics updated for NN2 w=5\n",
      "[Save] NN2_w5.csv\n",
      "[Info] Training completed for NN2\n",
      "\n",
      "Training NN3 with shared parameters...\n",
      "\n",
      "â¶ Training NN3 on Window = 5\n",
      "[Info] Using device: mps\n",
      "[Info] Using shared parameters for NN3\n",
      "[Info] Starting training for NN3 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 157536, Val size: 39384, Test size: 110850\n",
      "[NN3] Epoch 10/50, Train Loss: 1.126186, Val Loss: 0.434442, PredStd: 0.079679, PosRatio: 0.399\n",
      "[NN3] Epoch 20/50, Train Loss: 1.118861, Val Loss: 0.435118, PredStd: 0.083376, PosRatio: 0.446\n",
      "[NN3] Early stopping at epoch 21\n",
      "[Info] Applying inverse transform using y scaler for window 5\n",
      "[Info] Original scale - y_true range: [-0.100574, 0.108696]\n",
      "[Info] Original scale - y_pred range: [-0.027742, 0.042144]\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5225259359494813\n",
      "Neg ratio (y_test): 0.47747406405051873\n",
      "Pred +1 ratio: 0.54106450157871\n",
      "Pred -1 ratio: 0.45893549842129006\n",
      "      Pred+  Pred-\n",
      "+1 | [31646 26276]\n",
      "-1 | [28331 24597]\n",
      "[Update] Metrics updated for NN3 w=5\n",
      "[Save] NN3_w5.csv\n",
      "[Info] Training completed for NN3\n",
      "\n",
      "Training NN4 with shared parameters...\n",
      "\n",
      "â¶ Training NN4 on Window = 5\n",
      "[Info] Using device: mps\n",
      "[Info] Using shared parameters for NN4\n",
      "[Info] Starting training for NN4 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 157536, Val size: 39384, Test size: 110850\n",
      "[NN4] Epoch 10/50, Train Loss: 1.128361, Val Loss: 0.435933, PredStd: 0.086725, PosRatio: 0.379\n",
      "[NN4] Epoch 20/50, Train Loss: 1.121471, Val Loss: 0.438016, PredStd: 0.090800, PosRatio: 0.395\n",
      "[NN4] Early stopping at epoch 24\n",
      "[Info] Applying inverse transform using y scaler for window 5\n",
      "[Info] Original scale - y_true range: [-0.100574, 0.108696]\n",
      "[Info] Original scale - y_pred range: [-0.027711, 0.038808]\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5225259359494813\n",
      "Neg ratio (y_test): 0.47747406405051873\n",
      "Pred +1 ratio: 0.5134866937302661\n",
      "Pred -1 ratio: 0.4865133062697339\n",
      "      Pred+  Pred-\n",
      "+1 | [30079 27843]\n",
      "-1 | [26841 26087]\n",
      "[Update] Metrics updated for NN4 w=5\n",
      "[Save] NN4_w5.csv\n",
      "[Info] Training completed for NN4\n",
      "\n",
      "Training NN5 with shared parameters...\n",
      "\n",
      "â¶ Training NN5 on Window = 5\n",
      "[Info] Using device: mps\n",
      "[Info] Using shared parameters for NN5\n",
      "[Info] Starting training for NN5 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 157536, Val size: 39384, Test size: 110850\n",
      "[NN5] Epoch 10/50, Train Loss: 1.128706, Val Loss: 0.434709, PredStd: 0.088426, PosRatio: 0.458\n",
      "[NN5] Epoch 20/50, Train Loss: 1.117919, Val Loss: 0.435502, PredStd: 0.088364, PosRatio: 0.414\n",
      "[NN5] Early stopping at epoch 23\n",
      "[Info] Applying inverse transform using y scaler for window 5\n",
      "[Info] Original scale - y_true range: [-0.100574, 0.108696]\n",
      "[Info] Original scale - y_pred range: [-0.033948, 0.041025]\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5225259359494813\n",
      "Neg ratio (y_test): 0.47747406405051873\n",
      "Pred +1 ratio: 0.5433558863328822\n",
      "Pred -1 ratio: 0.4566441136671177\n",
      "      Pred+  Pred-\n",
      "+1 | [31807 26115]\n",
      "-1 | [28424 24504]\n",
      "[Update] Metrics updated for NN5 w=5\n",
      "[Save] NN5_w5.csv\n",
      "[Info] Training completed for NN5\n",
      "\n",
      "=== Processing Window Size: 21 ===\n",
      "[Info] Loaded y scaler for window 21\n",
      "\n",
      "Training NN1 to get shared parameters...\n",
      "\n",
      "â¶ Training NN1 on Window = 21\n",
      "[Info] Using device: mps\n",
      "[Optuna] NN1 best_MSE=0.791087, best_params={'batch_size': 64, 'learning_rate': 0.000164092867306479, 'dropout_rate': 0.06705241236872915, 'epochs': 50}\n",
      "[Info] Starting training for NN1 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 156896, Val size: 39224, Test size: 110850\n",
      "[NN1] Epoch 10/50, Train Loss: 1.132025, Val Loss: 0.442658, PredStd: 0.113337, PosRatio: 0.362\n",
      "[NN1] Epoch 20/50, Train Loss: 1.128221, Val Loss: 0.444315, PredStd: 0.120017, PosRatio: 0.377\n",
      "[NN1] Early stopping at epoch 25\n",
      "[Info] Applying inverse transform using y scaler for window 21\n",
      "[Info] Original scale - y_true range: [-0.100574, 0.108696]\n",
      "[Info] Original scale - y_pred range: [-0.024968, 0.026627]\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5233288227334235\n",
      "Neg ratio (y_test): 0.47667117726657643\n",
      "Pred +1 ratio: 0.48573748308525033\n",
      "Pred -1 ratio: 0.5142625169147497\n",
      "      Pred+  Pred-\n",
      "+1 | [28594 29417]\n",
      "-1 | [25250 27589]\n",
      "[Update] Metrics updated for NN1 w=21\n",
      "[Save] NN1_w21.csv\n",
      "[Info] Training completed for NN1\n",
      "Shared parameters from NN1: {'batch_size': 64, 'learning_rate': 0.000164092867306479, 'dropout_rate': 0.06705241236872915, 'epochs': 50}\n",
      "\n",
      "Training NN2 with shared parameters...\n",
      "\n",
      "â¶ Training NN2 on Window = 21\n",
      "[Info] Using device: mps\n",
      "[Info] Using shared parameters for NN2\n",
      "[Info] Starting training for NN2 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 156896, Val size: 39224, Test size: 110850\n",
      "[NN2] Epoch 10/50, Train Loss: 1.128463, Val Loss: 0.441893, PredStd: 0.107821, PosRatio: 0.347\n",
      "[NN2] Epoch 20/50, Train Loss: 1.121039, Val Loss: 0.445628, PredStd: 0.119071, PosRatio: 0.349\n",
      "[NN2] Early stopping at epoch 25\n",
      "[Info] Applying inverse transform using y scaler for window 21\n",
      "[Info] Original scale - y_true range: [-0.100574, 0.108696]\n",
      "[Info] Original scale - y_pred range: [-0.030434, 0.035091]\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5233288227334235\n",
      "Neg ratio (y_test): 0.47667117726657643\n",
      "Pred +1 ratio: 0.4757780784844384\n",
      "Pred -1 ratio: 0.5242219215155616\n",
      "      Pred+  Pred-\n",
      "+1 | [27933 30078]\n",
      "-1 | [24807 28032]\n",
      "[Update] Metrics updated for NN2 w=21\n",
      "[Save] NN2_w21.csv\n",
      "[Info] Training completed for NN2\n",
      "\n",
      "Training NN3 with shared parameters...\n",
      "\n",
      "â¶ Training NN3 on Window = 21\n",
      "[Info] Using device: mps\n",
      "[Info] Using shared parameters for NN3\n",
      "[Info] Starting training for NN3 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 156896, Val size: 39224, Test size: 110850\n",
      "[NN3] Epoch 10/50, Train Loss: 1.121616, Val Loss: 0.441179, PredStd: 0.102735, PosRatio: 0.383\n",
      "[NN3] Epoch 20/50, Train Loss: 1.105381, Val Loss: 0.447674, PredStd: 0.132119, PosRatio: 0.402\n",
      "[NN3] Early stopping at epoch 25\n",
      "[Info] Applying inverse transform using y scaler for window 21\n",
      "[Info] Original scale - y_true range: [-0.100574, 0.108696]\n",
      "[Info] Original scale - y_pred range: [-0.046612, 0.050797]\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5233288227334235\n",
      "Neg ratio (y_test): 0.47667117726657643\n",
      "Pred +1 ratio: 0.5108795669824087\n",
      "Pred -1 ratio: 0.48912043301759134\n",
      "      Pred+  Pred-\n",
      "+1 | [29917 28094]\n",
      "-1 | [26714 26125]\n",
      "[Update] Metrics updated for NN3 w=21\n",
      "[Save] NN3_w21.csv\n",
      "[Info] Training completed for NN3\n",
      "\n",
      "Training NN4 with shared parameters...\n",
      "\n",
      "â¶ Training NN4 on Window = 21\n",
      "[Info] Using device: mps\n",
      "[Info] Using shared parameters for NN4\n",
      "[Info] Starting training for NN4 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 156896, Val size: 39224, Test size: 110850\n",
      "[NN4] Epoch 10/50, Train Loss: 1.124250, Val Loss: 0.438970, PredStd: 0.101307, PosRatio: 0.433\n",
      "[NN4] Epoch 20/50, Train Loss: 1.109919, Val Loss: 0.442264, PredStd: 0.119713, PosRatio: 0.480\n",
      "[NN4] Early stopping at epoch 24\n",
      "[Info] Applying inverse transform using y scaler for window 21\n",
      "[Info] Original scale - y_true range: [-0.100574, 0.108696]\n",
      "[Info] Original scale - y_pred range: [-0.080937, 0.040738]\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5233288227334235\n",
      "Neg ratio (y_test): 0.47667117726657643\n",
      "Pred +1 ratio: 0.5821019395579612\n",
      "Pred -1 ratio: 0.4178980604420388\n",
      "      Pred+  Pred-\n",
      "+1 | [34137 23874]\n",
      "-1 | [30389 22450]\n",
      "[Update] Metrics updated for NN4 w=21\n",
      "[Save] NN4_w21.csv\n",
      "[Info] Training completed for NN4\n",
      "\n",
      "Training NN5 with shared parameters...\n",
      "\n",
      "â¶ Training NN5 on Window = 21\n",
      "[Info] Using device: mps\n",
      "[Info] Using shared parameters for NN5\n",
      "[Info] Starting training for NN5 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 156896, Val size: 39224, Test size: 110850\n",
      "[NN5] Epoch 10/50, Train Loss: 1.117668, Val Loss: 0.440258, PredStd: 0.108248, PosRatio: 0.401\n",
      "[NN5] Epoch 20/50, Train Loss: 1.076233, Val Loss: 0.450679, PredStd: 0.142796, PosRatio: 0.398\n",
      "[NN5] Early stopping at epoch 24\n",
      "[Info] Applying inverse transform using y scaler for window 21\n",
      "[Info] Original scale - y_true range: [-0.100574, 0.108696]\n",
      "[Info] Original scale - y_pred range: [-0.087991, 0.083998]\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5233288227334235\n",
      "Neg ratio (y_test): 0.47667117726657643\n",
      "Pred +1 ratio: 0.5029409111411818\n",
      "Pred -1 ratio: 0.4970590888588182\n",
      "      Pred+  Pred-\n",
      "+1 | [29316 28695]\n",
      "-1 | [26435 26404]\n",
      "[Update] Metrics updated for NN5 w=21\n",
      "[Save] NN5_w21.csv\n",
      "[Info] Training completed for NN5\n",
      "\n",
      "=== Processing Window Size: 252 ===\n",
      "[Info] Loaded y scaler for window 252\n",
      "\n",
      "Training NN1 to get shared parameters...\n",
      "\n",
      "â¶ Training NN1 on Window = 252\n",
      "[Info] Using device: mps\n",
      "[Optuna] NN1 best_MSE=0.837319, best_params={'batch_size': 128, 'learning_rate': 0.0008536189862866829, 'dropout_rate': 0.1308397348116461, 'epochs': 50}\n",
      "[Info] Starting training for NN1 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 147656, Val size: 36914, Test size: 110850\n",
      "[NN1] Epoch 10/50, Train Loss: 1.053923, Val Loss: 0.505682, PredStd: 0.183911, PosRatio: 0.485\n",
      "[NN1] Epoch 20/50, Train Loss: 1.014956, Val Loss: 0.519650, PredStd: 0.218034, PosRatio: 0.469\n",
      "[NN1] Early stopping at epoch 21\n",
      "[Info] Applying inverse transform using y scaler for window 252\n",
      "[Info] Original scale - y_true range: [-0.100574, 0.108696]\n",
      "[Info] Original scale - y_pred range: [-0.056633, 0.058570]\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5233288227334235\n",
      "Neg ratio (y_test): 0.47667117726657643\n",
      "Pred +1 ratio: 0.5375552548488949\n",
      "Pred -1 ratio: 0.4624447451511051\n",
      "      Pred+  Pred-\n",
      "+1 | [31428 26583]\n",
      "-1 | [28160 24679]\n",
      "[Update] Metrics updated for NN1 w=252\n",
      "[Save] NN1_w252.csv\n",
      "[Info] Training completed for NN1\n",
      "Shared parameters from NN1: {'batch_size': 128, 'learning_rate': 0.0008536189862866829, 'dropout_rate': 0.1308397348116461, 'epochs': 50}\n",
      "\n",
      "Training NN2 with shared parameters...\n",
      "\n",
      "â¶ Training NN2 on Window = 252\n",
      "[Info] Using device: mps\n",
      "[Info] Using shared parameters for NN2\n",
      "[Info] Starting training for NN2 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 147656, Val size: 36914, Test size: 110850\n",
      "[NN2] Epoch 10/50, Train Loss: 1.051118, Val Loss: 0.489334, PredStd: 0.128536, PosRatio: 0.508\n",
      "[NN2] Epoch 20/50, Train Loss: 1.009083, Val Loss: 0.494758, PredStd: 0.146107, PosRatio: 0.390\n",
      "[NN2] Early stopping at epoch 21\n",
      "[Info] Applying inverse transform using y scaler for window 252\n",
      "[Info] Original scale - y_true range: [-0.100574, 0.108696]\n",
      "[Info] Original scale - y_pred range: [-0.064978, 0.061954]\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5233288227334235\n",
      "Neg ratio (y_test): 0.47667117726657643\n",
      "Pred +1 ratio: 0.5369418132611637\n",
      "Pred -1 ratio: 0.46305818673883625\n",
      "      Pred+  Pred-\n",
      "+1 | [31231 26780]\n",
      "-1 | [28289 24550]\n",
      "[Update] Metrics updated for NN2 w=252\n",
      "[Save] NN2_w252.csv\n",
      "[Info] Training completed for NN2\n",
      "\n",
      "Training NN3 with shared parameters...\n",
      "\n",
      "â¶ Training NN3 on Window = 252\n",
      "[Info] Using device: mps\n",
      "[Info] Using shared parameters for NN3\n",
      "[Info] Starting training for NN3 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 147656, Val size: 36914, Test size: 110850\n",
      "[NN3] Epoch 10/50, Train Loss: 1.011085, Val Loss: 0.486507, PredStd: 0.115445, PosRatio: 0.459\n",
      "[NN3] Epoch 20/50, Train Loss: 0.936185, Val Loss: 0.494760, PredStd: 0.143979, PosRatio: 0.437\n",
      "[NN3] Early stopping at epoch 22\n",
      "[Info] Applying inverse transform using y scaler for window 252\n",
      "[Info] Original scale - y_true range: [-0.100574, 0.108696]\n",
      "[Info] Original scale - y_pred range: [-0.072524, 0.074331]\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5233288227334235\n",
      "Neg ratio (y_test): 0.47667117726657643\n",
      "Pred +1 ratio: 0.5721425349571493\n",
      "Pred -1 ratio: 0.4278574650428507\n",
      "      Pred+  Pred-\n",
      "+1 | [33414 24597]\n",
      "-1 | [30008 22831]\n",
      "[Update] Metrics updated for NN3 w=252\n",
      "[Save] NN3_w252.csv\n",
      "[Info] Training completed for NN3\n",
      "\n",
      "Training NN4 with shared parameters...\n",
      "\n",
      "â¶ Training NN4 on Window = 252\n",
      "[Info] Using device: mps\n",
      "[Info] Using shared parameters for NN4\n",
      "[Info] Starting training for NN4 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 147656, Val size: 36914, Test size: 110850\n",
      "[NN4] Epoch 10/50, Train Loss: 1.026877, Val Loss: 0.486073, PredStd: 0.112186, PosRatio: 0.439\n",
      "[NN4] Epoch 20/50, Train Loss: 0.953678, Val Loss: 0.492189, PredStd: 0.132835, PosRatio: 0.448\n",
      "[NN4] Early stopping at epoch 21\n",
      "[Info] Applying inverse transform using y scaler for window 252\n",
      "[Info] Original scale - y_true range: [-0.100574, 0.108696]\n",
      "[Info] Original scale - y_pred range: [-0.053933, 0.074767]\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5233288227334235\n",
      "Neg ratio (y_test): 0.47667117726657643\n",
      "Pred +1 ratio: 0.590509697789806\n",
      "Pred -1 ratio: 0.40949030221019395\n",
      "      Pred+  Pred-\n",
      "+1 | [34644 23367]\n",
      "-1 | [30814 22025]\n",
      "[Update] Metrics updated for NN4 w=252\n",
      "[Save] NN4_w252.csv\n",
      "[Info] Training completed for NN4\n",
      "\n",
      "Training NN5 with shared parameters...\n",
      "\n",
      "â¶ Training NN5 on Window = 252\n",
      "[Info] Using device: mps\n",
      "[Info] Using shared parameters for NN5\n",
      "[Info] Starting training for NN5 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 147656, Val size: 36914, Test size: 110850\n",
      "[NN5] Epoch 10/50, Train Loss: 0.983806, Val Loss: 0.488278, PredStd: 0.127328, PosRatio: 0.443\n",
      "[NN5] Epoch 20/50, Train Loss: 0.866497, Val Loss: 0.502921, PredStd: 0.175737, PosRatio: 0.498\n",
      "[NN5] Early stopping at epoch 21\n",
      "[Info] Applying inverse transform using y scaler for window 252\n",
      "[Info] Original scale - y_true range: [-0.100574, 0.108696]\n",
      "[Info] Original scale - y_pred range: [-0.072076, 0.098881]\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5233288227334235\n",
      "Neg ratio (y_test): 0.47667117726657643\n",
      "Pred +1 ratio: 0.5395489400090212\n",
      "Pred -1 ratio: 0.4604510599909788\n",
      "      Pred+  Pred-\n",
      "+1 | [31629 26382]\n",
      "-1 | [28180 24659]\n",
      "[Update] Metrics updated for NN5 w=252\n",
      "[Save] NN5_w252.csv\n",
      "[Info] Training completed for NN5\n",
      "\n",
      "=== Processing Window Size: 512 ===\n",
      "[Info] Loaded y scaler for window 512\n",
      "\n",
      "Training NN1 to get shared parameters...\n",
      "\n",
      "â¶ Training NN1 on Window = 512\n",
      "[Info] Using device: mps\n",
      "[Optuna] NN1 best_MSE=0.907149, best_params={'batch_size': 128, 'learning_rate': 0.0008536189862866829, 'dropout_rate': 0.1308397348116461, 'epochs': 50}\n",
      "[Info] Starting training for NN1 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 137256, Val size: 34314, Test size: 110850\n",
      "[NN1] Epoch 10/50, Train Loss: 0.977610, Val Loss: 0.569118, PredStd: 0.223503, PosRatio: 0.523\n",
      "[NN1] Epoch 20/50, Train Loss: 0.904689, Val Loss: 0.601272, PredStd: 0.282263, PosRatio: 0.493\n",
      "[NN1] Early stopping at epoch 21\n",
      "[Info] Applying inverse transform using y scaler for window 512\n",
      "[Info] Original scale - y_true range: [-0.100574, 0.108696]\n",
      "[Info] Original scale - y_pred range: [-0.054538, 0.061359]\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5233288227334235\n",
      "Neg ratio (y_test): 0.47667117726657643\n",
      "Pred +1 ratio: 0.563256653134867\n",
      "Pred -1 ratio: 0.43674334686513305\n",
      "      Pred+  Pred-\n",
      "+1 | [32753 25258]\n",
      "-1 | [29684 23155]\n",
      "[Update] Metrics updated for NN1 w=512\n",
      "[Save] NN1_w512.csv\n",
      "[Info] Training completed for NN1\n",
      "Shared parameters from NN1: {'batch_size': 128, 'learning_rate': 0.0008536189862866829, 'dropout_rate': 0.1308397348116461, 'epochs': 50}\n",
      "\n",
      "Training NN2 with shared parameters...\n",
      "\n",
      "â¶ Training NN2 on Window = 512\n",
      "[Info] Using device: mps\n",
      "[Info] Using shared parameters for NN2\n",
      "[Info] Starting training for NN2 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 137256, Val size: 34314, Test size: 110850\n",
      "[NN2] Epoch 10/50, Train Loss: 0.981531, Val Loss: 0.548668, PredStd: 0.161938, PosRatio: 0.483\n",
      "[NN2] Epoch 20/50, Train Loss: 0.919146, Val Loss: 0.557333, PredStd: 0.181945, PosRatio: 0.478\n",
      "[NN2] Early stopping at epoch 21\n",
      "[Info] Applying inverse transform using y scaler for window 512\n",
      "[Info] Original scale - y_true range: [-0.100574, 0.108696]\n",
      "[Info] Original scale - y_pred range: [-0.053440, 0.087743]\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5233288227334235\n",
      "Neg ratio (y_test): 0.47667117726657643\n",
      "Pred +1 ratio: 0.6007126747857465\n",
      "Pred -1 ratio: 0.3992873252142535\n",
      "      Pred+  Pred-\n",
      "+1 | [35181 22830]\n",
      "-1 | [31408 21431]\n",
      "[Update] Metrics updated for NN2 w=512\n",
      "[Save] NN2_w512.csv\n",
      "[Info] Training completed for NN2\n",
      "\n",
      "Training NN3 with shared parameters...\n",
      "\n",
      "â¶ Training NN3 on Window = 512\n",
      "[Info] Using device: mps\n",
      "[Info] Using shared parameters for NN3\n",
      "[Info] Starting training for NN3 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 137256, Val size: 34314, Test size: 110850\n",
      "[NN3] Epoch 10/50, Train Loss: 0.940351, Val Loss: 0.536778, PredStd: 0.122734, PosRatio: 0.419\n",
      "[NN3] Epoch 20/50, Train Loss: 0.823269, Val Loss: 0.550294, PredStd: 0.166326, PosRatio: 0.434\n",
      "[NN3] Early stopping at epoch 21\n",
      "[Info] Applying inverse transform using y scaler for window 512\n",
      "[Info] Original scale - y_true range: [-0.100574, 0.108696]\n",
      "[Info] Original scale - y_pred range: [-0.057213, 0.075586]\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5233288227334235\n",
      "Neg ratio (y_test): 0.47667117726657643\n",
      "Pred +1 ratio: 0.6015516463689671\n",
      "Pred -1 ratio: 0.3984483536310329\n",
      "      Pred+  Pred-\n",
      "+1 | [35043 22968]\n",
      "-1 | [31639 21200]\n",
      "[Update] Metrics updated for NN3 w=512\n",
      "[Save] NN3_w512.csv\n",
      "[Info] Training completed for NN3\n",
      "\n",
      "Training NN4 with shared parameters...\n",
      "\n",
      "â¶ Training NN4 on Window = 512\n",
      "[Info] Using device: mps\n",
      "[Info] Using shared parameters for NN4\n",
      "[Info] Starting training for NN4 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 137256, Val size: 34314, Test size: 110850\n",
      "[NN4] Epoch 10/50, Train Loss: 0.959510, Val Loss: 0.532735, PredStd: 0.102146, PosRatio: 0.321\n",
      "[NN4] Epoch 20/50, Train Loss: 0.841794, Val Loss: 0.547954, PredStd: 0.157096, PosRatio: 0.351\n",
      "[NN4] Early stopping at epoch 21\n",
      "[Info] Applying inverse transform using y scaler for window 512\n",
      "[Info] Original scale - y_true range: [-0.100574, 0.108696]\n",
      "[Info] Original scale - y_pred range: [-0.046156, 0.060249]\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5233288227334235\n",
      "Neg ratio (y_test): 0.47667117726657643\n",
      "Pred +1 ratio: 0.5759585024808299\n",
      "Pred -1 ratio: 0.42404149751917003\n",
      "      Pred+  Pred-\n",
      "+1 | [33680 24331]\n",
      "-1 | [30165 22674]\n",
      "[Update] Metrics updated for NN4 w=512\n",
      "[Save] NN4_w512.csv\n",
      "[Info] Training completed for NN4\n",
      "\n",
      "Training NN5 with shared parameters...\n",
      "\n",
      "â¶ Training NN5 on Window = 512\n",
      "[Info] Using device: mps\n",
      "[Info] Using shared parameters for NN5\n",
      "[Info] Starting training for NN5 with 50 epochs (early stopping patience=20)...\n",
      "[Info] Train size: 137256, Val size: 34314, Test size: 110850\n",
      "[NN5] Epoch 10/50, Train Loss: 0.903646, Val Loss: 0.538931, PredStd: 0.128230, PosRatio: 0.509\n",
      "[NN5] Epoch 20/50, Train Loss: 0.723982, Val Loss: 0.570022, PredStd: 0.220125, PosRatio: 0.494\n",
      "[NN5] Early stopping at epoch 21\n",
      "[Info] Applying inverse transform using y scaler for window 512\n",
      "[Info] Original scale - y_true range: [-0.100574, 0.108696]\n",
      "[Info] Original scale - y_pred range: [-0.052850, 0.092993]\n",
      "\n",
      "=== Directional Sanity Check ===\n",
      "Pos ratio (y_test): 0.5233288227334235\n",
      "Neg ratio (y_test): 0.47667117726657643\n",
      "Pred +1 ratio: 0.5284709066305818\n",
      "Pred -1 ratio: 0.4715290933694181\n",
      "      Pred+  Pred-\n",
      "+1 | [30859 27152]\n",
      "-1 | [27722 25117]\n",
      "[Update] Metrics updated for NN5 w=512\n",
      "[Save] NN5_w512.csv\n",
      "[Info] Training completed for NN5\n"
     ]
    }
   ],
   "source": [
    "# ========== 9. Entry Point ==========\n",
    "if __name__ == \"__main__\":\n",
    "    loop_all_models()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-mac)",
   "language": "python",
   "name": "tf-mac"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
