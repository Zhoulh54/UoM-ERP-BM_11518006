{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optuna is available for hyperparameter tuning\n",
            "Using device: mps\n"
          ]
        }
      ],
      "source": [
        "# ========== Basic Libraries ==========\n",
        "import os\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gc\n",
        "import joblib\n",
        "import random\n",
        "\n",
        "# ========== Evaluation Metrics ==========\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error, mean_absolute_error, r2_score, confusion_matrix\n",
        ")\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "# ========== Hyperparameter Tuning ==========\n",
        "try:\n",
        "    import optuna\n",
        "    OPTUNA_AVAILABLE = True\n",
        "    print(\"Optuna is available for hyperparameter tuning\")\n",
        "except ImportError:\n",
        "    OPTUNA_AVAILABLE = False\n",
        "    print(\"Optuna not available, will use default parameters\")\n",
        "\n",
        "# ========== Global Configuration ==========\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def set_random_seeds(seed=42):\n",
        "    \"\"\"Set all relevant random seeds for reproducibility\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    if torch.backends.mps.is_available():\n",
        "        torch.mps.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_random_seeds(42)\n",
        "\n",
        "device = (\n",
        "    \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "def clear_memory():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    if device == \"mps\":\n",
        "        torch.mps.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkHmGLL_E_dl",
        "outputId": "0eef771b-7434-42ba-ced9-78648d558836"
      },
      "outputs": [],
      "source": [
        "# ========== 2. Evaluation Metrics ==========\n",
        "\n",
        "def r2_zero(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute zero-based R² (baseline is 0)\n",
        "    y_true: true values (N,)\n",
        "    y_pred: predicted values (N,)\n",
        "    \"\"\"\n",
        "    rss = np.sum((y_true - y_pred)**2)  \n",
        "    tss = np.sum(y_true**2)            \n",
        "    return 1 - rss / tss\n",
        "\n",
        "def calc_directional_metrics(y_true, y_pred, permnos=None):\n",
        "    \"\"\"\n",
        "    Improved version:\n",
        "    - Sign prediction at sample level\n",
        "    - If grouped by stock, calculate Overall, Up, Down for each stock and then average\n",
        "    \"\"\"\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "\n",
        "    if permnos is None:\n",
        "        s_true = np.sign(y_true)\n",
        "        s_pred = np.sign(y_pred)\n",
        "        mask = s_true != 0\n",
        "        s_true = s_true[mask]\n",
        "        s_pred = s_pred[mask]\n",
        "\n",
        "        overall_acc = np.mean(s_true == s_pred)\n",
        "\n",
        "        up_mask = s_true > 0\n",
        "        down_mask = s_true < 0\n",
        "        up_acc = np.mean(s_true[up_mask] == s_pred[up_mask]) if np.any(up_mask) else 0\n",
        "        down_acc = np.mean(s_true[down_mask] == s_pred[down_mask]) if np.any(down_mask) else 0\n",
        "\n",
        "    else:\n",
        "        df = pd.DataFrame({\"permno\": permnos, \"yt\": y_true, \"yp\": y_pred})\n",
        "        overall_accs = []\n",
        "        up_accs = []\n",
        "        down_accs = []\n",
        "\n",
        "        for _, g in df.groupby(\"permno\"):\n",
        "            s_true = np.sign(g[\"yt\"].values)\n",
        "            s_pred = np.sign(g[\"yp\"].values)\n",
        "            mask = s_true != 0\n",
        "            s_true = s_true[mask]\n",
        "            s_pred = s_pred[mask]\n",
        "            if len(s_true) == 0:\n",
        "                continue\n",
        "            overall_accs.append(np.mean(s_true == s_pred))\n",
        "\n",
        "            up_mask = s_true > 0\n",
        "            down_mask = s_true < 0\n",
        "            up_accs.append(np.mean(s_true[up_mask] == s_pred[up_mask]) if np.any(up_mask) else np.nan)\n",
        "            down_accs.append(np.mean(s_true[down_mask] == s_pred[down_mask]) if np.any(down_mask) else np.nan)\n",
        "\n",
        "        overall_acc = np.nanmean(overall_accs)\n",
        "        up_acc = np.nanmean(up_accs)\n",
        "        down_acc = np.nanmean(down_accs)\n",
        "\n",
        "    return overall_acc, up_acc, down_acc\n",
        "\n",
        "def regression_metrics(y_true, y_pred, k, meta=None, permnos=None):\n",
        "    \"\"\"\n",
        "    Includes:\n",
        "    - Regression metrics\n",
        "    - Pointwise directional accuracy\n",
        "    - Market cap group metrics\n",
        "    \"\"\"\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    n = len(y_true)\n",
        "\n",
        "    r2 = r2_zero(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "\n",
        "    dir_acc, up_acc, down_acc = calc_directional_metrics(y_true, y_pred, permnos)\n",
        "\n",
        "    metrics = {\n",
        "        \"R2_zero\": r2,\n",
        "        \"RMSE\": rmse,\n",
        "        \"MAE\": mae,\n",
        "        \"MSE\": mse,\n",
        "        \"Directional Accuracy\": dir_acc,\n",
        "        \"Up_Directional_Acc\": up_acc,\n",
        "        \"Down_Directional_Acc\": down_acc\n",
        "    }\n",
        "\n",
        "    if meta is not None and \"MKTCAP_PERCENTILE\" in meta:\n",
        "        top_mask = meta[\"MKTCAP_PERCENTILE\"] >= 0.75\n",
        "        bottom_mask = meta[\"MKTCAP_PERCENTILE\"] <= 0.25\n",
        "\n",
        "        if np.any(top_mask):\n",
        "            yt_top = y_true[top_mask]\n",
        "            yp_top = y_pred[top_mask]\n",
        "            perm_top = permnos[top_mask] if permnos is not None else None\n",
        "            r2_top = r2_zero(yt_top, yp_top)\n",
        "            rmse_top = np.sqrt(mean_squared_error(yt_top, yp_top))\n",
        "            mae_top = mean_absolute_error(yt_top, yp_top)\n",
        "            mse_top = mean_squared_error(yt_top, yp_top)\n",
        "            dir_top, up_top, down_top = calc_directional_metrics(yt_top, yp_top, perm_top)\n",
        "            metrics.update({\n",
        "                \"Top25_R2_zero\": r2_top,\n",
        "                \"Top25_RMSE\": rmse_top,\n",
        "                \"Top25_MAE\": mae_top,\n",
        "                \"Top25_MSE\": mse_top,\n",
        "                \"Top25_Dir_Acc\": dir_top,\n",
        "                \"Top25_Up_Acc\": up_top,\n",
        "                \"Top25_Down_Acc\": down_top\n",
        "            })\n",
        "\n",
        "        if np.any(bottom_mask):\n",
        "            yt_bot = y_true[bottom_mask]\n",
        "            yp_bot = y_pred[bottom_mask]\n",
        "            perm_bot = permnos[bottom_mask] if permnos is not None else None\n",
        "            r2_bot = r2_zero(yt_bot, yp_bot)\n",
        "            rmse_bot = np.sqrt(mean_squared_error(yt_bot, yp_bot))\n",
        "            mae_bot = mean_absolute_error(yt_bot, yp_bot)\n",
        "            mse_bot = mean_squared_error(yt_bot, yp_bot)\n",
        "            dir_bot, up_bot, down_bot = calc_directional_metrics(yt_bot, yp_bot, perm_bot)\n",
        "            metrics.update({\n",
        "                \"Bottom25_R2_zero\": r2_bot,\n",
        "                \"Bottom25_RMSE\": rmse_bot,\n",
        "                \"Bottom25_MAE\": mae_bot,\n",
        "                \"Bottom25_MSE\": mse_bot,\n",
        "                \"Bottom25_Dir_Acc\": dir_bot,\n",
        "                \"Bottom25_Up_Acc\": up_bot,\n",
        "                \"Bottom25_Down_Acc\": down_bot\n",
        "            })\n",
        "\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6Fw6M08OE_dm"
      },
      "outputs": [],
      "source": [
        "# ========== 3. 保存函数（与Linear_Models保持一致）==========\n",
        "\n",
        "def save_model(model, name, window, path=\"models/\"):\n",
        "    \"\"\"保存模型\"\"\"\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    torch.save(model.state_dict(), os.path.join(path, f\"{name}_w{window}.pth\"))\n",
        "\n",
        "def save_metrics(metrics_dict, name, window, path=\"results.csv\"):\n",
        "    \"\"\"保存评估指标\"\"\"\n",
        "    row = pd.DataFrame([metrics_dict])\n",
        "    row.insert(0, \"Model\", name)\n",
        "    row.insert(1, \"Window\", window)\n",
        "\n",
        "    if os.path.exists(path):\n",
        "        df = pd.read_csv(path)\n",
        "        # 删除已存在的相同模型和窗口的记录\n",
        "        df = df[~((df[\"Model\"] == name) & (df[\"Window\"] == window))]\n",
        "        # 添加新记录\n",
        "        df = pd.concat([df, row], ignore_index=True)\n",
        "        df.to_csv(path, index=False)\n",
        "        print(f\"[Update] Metrics updated for {name} w={window}\")\n",
        "    else:\n",
        "        row.to_csv(path, index=False)\n",
        "        print(f\"[Create] New metrics file created with {name} w={window}\")\n",
        "\n",
        "def save_predictions(model_name, window_size, y_true, y_pred, permnos, path=\"predictions/\"):\n",
        "    \"\"\"保存预测结果\"\"\"\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    \n",
        "    df = pd.DataFrame({\n",
        "        \"PERMNO\": permnos,\n",
        "        \"y_true\": y_true,\n",
        "        \"y_pred\": y_pred\n",
        "    })\n",
        "\n",
        "    filename = f\"{model_name}_w{window_size}.csv\"\n",
        "    df.to_csv(os.path.join(path, filename), index=False)\n",
        "    print(f\"[Save] {filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VeMzH06WE_dm"
      },
      "outputs": [],
      "source": [
        "# ========== 4. Dataset and Model Definition ==========\n",
        "\n",
        "class StockDataset(Dataset):\n",
        "    def __init__(self, X, y, device):\n",
        "        \"\"\"Custom dataset class\"\"\"\n",
        "        self.X = torch.FloatTensor(X).to(device)\n",
        "        self.y = torch.FloatTensor(y).to(device)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, dropout=0.2):\n",
        "        super(LSTMModel, self).__init__()\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, input_size)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        # Only use the output of the last time step\n",
        "        last_output = lstm_out[:, -1, :]\n",
        "        last_output = self.dropout(last_output)\n",
        "        out = self.fc(last_output)\n",
        "        return out.squeeze()\n",
        "\n",
        "def prepare_data(X, y, batch_size=32, device=None):\n",
        "    \"\"\"Prepare data loader\"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "    dataset = StockDataset(X, y, device)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1AlUtoL9E_dn",
        "outputId": "f520b088-ef55-46de-c990-8c9866ce19d7"
      },
      "outputs": [],
      "source": [
        "# ========== 5. Training Functions ==========\n",
        "\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for X, y in train_loader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(X)\n",
        "        loss = criterion(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    \"\"\"Validate the model\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    predictions = []\n",
        "    targets = []\n",
        "    with torch.no_grad():\n",
        "        for X, y in val_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            output = model(X)\n",
        "            loss = criterion(output, y)\n",
        "            total_loss += loss.item()\n",
        "            predictions.extend(output.cpu().numpy())\n",
        "            targets.extend(y.cpu().numpy())\n",
        "    return (total_loss / len(val_loader), \n",
        "            np.array(predictions), \n",
        "            np.array(targets))\n",
        "\n",
        "def create_train_val_split_timeseries(X_train, y_train, permnos_train, n_splits=3, test_size_ratio=0.2):\n",
        "    \"\"\"\n",
        "    Create time series cross-validation splits using TimeSeriesSplit.\n",
        "    Returns the last split as the train-validation split.\n",
        "    \"\"\"\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits, test_size=int(len(X_train) * test_size_ratio))\n",
        "    splits = list(tscv.split(X_train))\n",
        "    train_idx, val_idx = splits[-1]\n",
        "    X_tr = X_train[train_idx]\n",
        "    X_val = X_train[val_idx]\n",
        "    y_tr = y_train[train_idx]\n",
        "    y_val = y_train[val_idx]\n",
        "    if permnos_train is not None:\n",
        "        perm_tr = permnos_train[train_idx]\n",
        "        perm_val = permnos_train[val_idx]\n",
        "        return X_tr, X_val, y_tr, y_val, perm_tr, perm_val\n",
        "    else:\n",
        "        return X_tr, X_val, y_tr, y_val, None, None\n",
        "\n",
        "def create_train_val_split(X_train, y_train, permnos_train, val_ratio=0.2):\n",
        "    \"\"\"\n",
        "    Create a validation set from the last val_ratio portion of the training set in chronological order.\n",
        "    \"\"\"\n",
        "    split_idx = int(len(X_train) * (1 - val_ratio))\n",
        "    X_tr = X_train[:split_idx]\n",
        "    X_val = X_train[split_idx:]\n",
        "    y_tr = y_train[:split_idx]\n",
        "    y_val = y_train[split_idx:]\n",
        "    if permnos_train is not None:\n",
        "        perm_tr = permnos_train[:split_idx]\n",
        "        perm_val = permnos_train[split_idx:]\n",
        "        return X_tr, X_val, y_tr, y_val, perm_tr, perm_val\n",
        "    else:\n",
        "        return X_tr, X_val, y_tr, y_val, None, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== 6. Hyperparameter Optimization Function ==========\n",
        "\n",
        "def optimize_lstm_hyperparameters(X_train, y_train, permnos_train, window_size, n_trials=15):\n",
        "    \"\"\"\n",
        "    Optimize LSTM hyperparameters using Optuna (with pruning, some parameters fixed)\n",
        "    \"\"\"\n",
        "    if not OPTUNA_AVAILABLE:\n",
        "        print(\"[Warning] Optuna not available, using default parameters\")\n",
        "        return {\n",
        "            'batch_size': 32,\n",
        "            'learning_rate': 0.0005,\n",
        "            'dropout_rate': 0.1,\n",
        "            'hidden_size': 128,\n",
        "            'num_layers': 2,\n",
        "            'epochs': 50\n",
        "        }\n",
        "    \n",
        "    X_train_reshaped = X_train.reshape(X_train.shape[0], window_size, -1)\n",
        "    input_size = X_train_reshaped.shape[2]\n",
        "    \n",
        "    X_tr, X_val, y_tr, y_val, _, _ = create_train_val_split_timeseries(\n",
        "        X_train_reshaped, y_train, permnos_train, n_splits=4, test_size_ratio=0.2\n",
        "    )\n",
        "    \n",
        "    cpu_device = torch.device(\"cpu\")\n",
        "    \n",
        "    def objective(trial):\n",
        "        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
        "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 5e-3, log=True)\n",
        "        dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.2)\n",
        "        \n",
        "        hidden_size = 128\n",
        "        num_layers = 2\n",
        "        \n",
        "        try:\n",
        "            train_loader = prepare_data(X_tr, y_tr, batch_size=batch_size, device=cpu_device)\n",
        "            val_loader = prepare_data(X_val, y_val, batch_size=batch_size, device=cpu_device)\n",
        "            \n",
        "            model = LSTMModel(\n",
        "                input_size=input_size,\n",
        "                hidden_size=hidden_size,\n",
        "                num_layers=num_layers,\n",
        "                dropout=dropout_rate\n",
        "            ).to(cpu_device)\n",
        "            \n",
        "            criterion = nn.MSELoss()\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "            \n",
        "            best_val_loss = float('inf')\n",
        "            patience = 10\n",
        "            patience_counter = 0\n",
        "            max_epochs = 20\n",
        "            \n",
        "            for epoch in range(max_epochs):\n",
        "                model.train()\n",
        "                train_loss = 0\n",
        "                for X_batch, y_batch in train_loader:\n",
        "                    X_batch, y_batch = X_batch.to(cpu_device), y_batch.to(cpu_device)\n",
        "                    optimizer.zero_grad()\n",
        "                    output = model(X_batch)\n",
        "                    loss = criterion(output, y_batch)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    train_loss += loss.item()\n",
        "                \n",
        "                model.eval()\n",
        "                val_loss = 0\n",
        "                with torch.no_grad():\n",
        "                    for X_batch, y_batch in val_loader:\n",
        "                        X_batch, y_batch = X_batch.to(cpu_device), y_batch.to(cpu_device)\n",
        "                        output = model(X_batch)\n",
        "                        loss = criterion(output, y_batch)\n",
        "                        val_loss += loss.item()\n",
        "                \n",
        "                val_loss /= len(val_loader)\n",
        "                \n",
        "                trial.report(val_loss, epoch)\n",
        "                \n",
        "                if trial.should_prune():\n",
        "                    del model, train_loader, val_loader\n",
        "                    gc.collect()\n",
        "                    raise optuna.exceptions.TrialPruned()\n",
        "                \n",
        "                if val_loss < best_val_loss:\n",
        "                    best_val_loss = val_loss\n",
        "                    patience_counter = 0\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "                    if patience_counter >= patience:\n",
        "                        break\n",
        "            \n",
        "            del model, train_loader, val_loader\n",
        "            gc.collect()\n",
        "            \n",
        "            return best_val_loss\n",
        "            \n",
        "        except optuna.exceptions.TrialPruned:\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            print(f\"Trial failed: {e}\")\n",
        "            return float('inf')\n",
        "    \n",
        "    pruner = optuna.pruners.MedianPruner(\n",
        "        n_startup_trials=3,\n",
        "        n_warmup_steps=5,\n",
        "        interval_steps=2\n",
        "    )\n",
        "    \n",
        "    print(f\"[Optuna] Starting hyperparameter optimization with {n_trials} trials on CPU...\")\n",
        "    print(\"[Optuna] Using MedianPruner for early trial termination\")\n",
        "    print(\"[Optuna] Fixed parameters: hidden_size=128, num_layers=2\")\n",
        "    print(\"[Optuna] Searching: batch_size, learning_rate, dropout_rate\")\n",
        "    \n",
        "    study = optuna.create_study(\n",
        "        direction='minimize', \n",
        "        sampler=optuna.samplers.TPESampler(seed=42),\n",
        "        pruner=pruner\n",
        "    )\n",
        "    \n",
        "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
        "    \n",
        "    best_params = study.best_params\n",
        "    best_params['hidden_size'] = 128\n",
        "    best_params['num_layers'] = 2\n",
        "    best_params['epochs'] = 50\n",
        "    \n",
        "    print(f\"[Optuna] Best parameters: {best_params}\")\n",
        "    print(f\"[Optuna] Best validation loss: {study.best_value:.6f}\")\n",
        "    print(f\"[Optuna] Number of pruned trials: {len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])}\")\n",
        "    print(f\"[Optuna] Number of completed trials: {len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])}\")\n",
        "    \n",
        "    return best_params\n",
        "\n",
        "# ========== 7. Training and Evaluation Main Function ==========\n",
        "\n",
        "def train_and_evaluate_lstm(window_size, X_train, y_train, X_test, y_test,\n",
        "                           permnos_train, permnos_test, meta=None, shared_params=None, scaler_y=None):\n",
        "    \"\"\"Train and evaluate LSTM model\"\"\"\n",
        "    print(f\"\\nTraining LSTM on Window = {window_size}\")\n",
        "    \n",
        "    X_train_reshaped = X_train.reshape(X_train.shape[0], window_size, -1)\n",
        "    X_test_reshaped = X_test.reshape(X_test.shape[0], window_size, -1)\n",
        "    \n",
        "    input_size = X_train_reshaped.shape[2]\n",
        "    print(f\"[Info] Input shape: {X_train_reshaped.shape}, Input size: {input_size}\")\n",
        "    \n",
        "    X_tr, X_val, y_tr, y_val, perm_tr, perm_val = create_train_val_split_timeseries(\n",
        "        X_train_reshaped, y_train, permnos_train, n_splits=4, test_size_ratio=0.2\n",
        "    )\n",
        "    \n",
        "    if shared_params is not None:\n",
        "        print(f\"[Info] Using shared parameters for LSTM\")\n",
        "        batch_size = shared_params['batch_size']\n",
        "        learning_rate = shared_params['learning_rate']\n",
        "        dropout_rate = shared_params['dropout_rate']\n",
        "        hidden_size = shared_params['hidden_size']\n",
        "        num_layers = shared_params['num_layers']\n",
        "        epochs = shared_params['epochs']\n",
        "    else:\n",
        "        print(f\"[Info] Using default parameters for LSTM\")\n",
        "        batch_size = 32\n",
        "        learning_rate = 0.0005\n",
        "        dropout_rate = 0.1\n",
        "        hidden_size = 128\n",
        "        num_layers = 2\n",
        "        epochs = 50\n",
        "    \n",
        "    train_loader = prepare_data(X_tr, y_tr, batch_size=batch_size, device=device)\n",
        "    val_loader = prepare_data(X_val, y_val, batch_size=batch_size, device=device)\n",
        "    test_loader = prepare_data(X_test_reshaped, y_test, batch_size=batch_size, device=device)\n",
        "    \n",
        "    model = LSTMModel(\n",
        "        input_size=input_size,\n",
        "        hidden_size=hidden_size,\n",
        "        num_layers=num_layers,\n",
        "        dropout=dropout_rate\n",
        "    ).to(device)\n",
        "    \n",
        "    criterion = nn.MSELoss().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "    best_model = None\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 15\n",
        "    patience_counter = 0\n",
        "    \n",
        "    print(f\"[Info] Starting training for LSTM with {epochs} epochs (early stopping patience={patience})...\")\n",
        "    print(f\"[Info] Train size: {len(X_tr)}, Val size: {len(X_val)}, Test size: {len(X_test)}\")\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss, predictions, targets = validate(model, val_loader, criterion, device)\n",
        "        \n",
        "        pred_std = np.std(predictions)\n",
        "        positive_ratio = (np.sign(predictions) > 0).mean()\n",
        "        \n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model = model.state_dict().copy()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            \n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"[LSTM] Epoch {epoch+1}/{epochs}, \"\n",
        "                  f\"Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}, \"\n",
        "                  f\"PredStd: {pred_std:.6f}, PosRatio: {positive_ratio:.3f}\")\n",
        "        \n",
        "        if patience_counter >= patience:\n",
        "            print(f\"[LSTM] Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "    \n",
        "    if best_model is not None:\n",
        "        model.load_state_dict(best_model)\n",
        "    \n",
        "    _, y_pred, y_true = validate(model, test_loader, criterion, device)\n",
        "    \n",
        "    if scaler_y is not None:\n",
        "        print(f\"[Info] Applying inverse transform using y scaler for window {window_size}\")\n",
        "        y_true_original = scaler_y.inverse_transform(y_true.reshape(-1, 1)).flatten()\n",
        "        y_pred_original = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
        "        print(f\"[Info] Original scale - y_true range: [{y_true_original.min():.6f}, {y_true_original.max():.6f}]\")\n",
        "        print(f\"[Info] Original scale - y_pred range: [{y_pred_original.min():.6f}, {y_pred_original.max():.6f}]\")\n",
        "        y_true_eval = y_true_original\n",
        "        y_pred_eval = y_pred_original\n",
        "    else:\n",
        "        print(f\"[Warning] No y scaler provided, using standardized values for evaluation\")\n",
        "        y_true_eval = y_true\n",
        "        y_pred_eval = y_pred\n",
        "    \n",
        "    print(\"\\n=== Directional Sanity Check ===\")\n",
        "    print(\"Pos ratio (y_test):\", (y_true_eval > 0).mean())\n",
        "    print(\"Neg ratio (y_test):\", (y_true_eval < 0).mean())\n",
        "    sign_pred = np.sign(y_pred_eval)\n",
        "    print(\"Pred +1 ratio:\", (sign_pred > 0).mean())\n",
        "    print(\"Pred -1 ratio:\", (sign_pred < 0).mean())\n",
        "    \n",
        "    conf = confusion_matrix(np.sign(y_true_eval), sign_pred, labels=[1, -1])\n",
        "    print(\"      Pred+  Pred-\")\n",
        "    print(\"+1 |\", conf[0])\n",
        "    print(\"-1 |\", conf[1])\n",
        "    \n",
        "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    metrics = regression_metrics(y_true_eval, y_pred_eval, k=window_size, meta=meta, permnos=permnos_test)\n",
        "    print(f\"[Info] Model has {num_params} trainable parameters, using window_size={window_size} for Adjusted R²\")\n",
        "    \n",
        "    save_model(model, \"LSTM\", window_size)\n",
        "    save_metrics(metrics, \"LSTM\", window_size)\n",
        "    save_predictions(\"LSTM\", window_size, y_true_eval, y_pred_eval, permnos_test)\n",
        "    \n",
        "    print(f\"[Info] Training completed for LSTM\")\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== 8. Main Scheduler Function ==========\n",
        "\n",
        "def load_datasets(npz_path=\"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/all_window_datasets_scaled.npz\"):\n",
        "    \"\"\"Load dataset\"\"\"\n",
        "    data = np.load(npz_path, allow_pickle=True)\n",
        "    datasets = {}\n",
        "    for key in data.files:\n",
        "        datasets[key] = data[key]\n",
        "    return datasets\n",
        "\n",
        "def load_y_scaler(window_size, scaler_path=\"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/\"):\n",
        "    \"\"\"Load y scaler for the corresponding window\"\"\"\n",
        "    scaler_file = os.path.join(scaler_path, f\"scaler_y_window_{window_size}.pkl\")\n",
        "    if os.path.exists(scaler_file):\n",
        "        scaler_y = joblib.load(scaler_file)\n",
        "        print(f\"[Info] Loaded y scaler for window {window_size}\")\n",
        "        return scaler_y\n",
        "    else:\n",
        "        print(f\"[Warning] Y scaler file not found: {scaler_file}\")\n",
        "        return None\n",
        "\n",
        "def loop_all_windows():\n",
        "    \"\"\"Train LSTM on different window sizes\"\"\"\n",
        "    datasets = load_datasets()\n",
        "    \n",
        "    window_sizes = [5, 21, 252, 512]\n",
        "    shared_params = None\n",
        "\n",
        "    for window in window_sizes:\n",
        "        print(f\"\\n=== Processing Window Size: {window} ===\")\n",
        "        \n",
        "        X_train = datasets[f\"X_train_{window}\"]\n",
        "        y_train = datasets[f\"y_train_{window}\"]\n",
        "        X_test = datasets[f\"X_test_{window}\"]\n",
        "        y_test = datasets[f\"y_test_{window}\"]\n",
        "        \n",
        "        meta_train_dict = datasets[f\"meta_train_{window}\"].item()\n",
        "        meta_test_dict = datasets[f\"meta_test_{window}\"].item()\n",
        "\n",
        "        meta_train = pd.DataFrame.from_dict(meta_train_dict)\n",
        "        meta_test = pd.DataFrame.from_dict(meta_test_dict)\n",
        "\n",
        "        permnos_train = meta_train[\"PERMNO\"].values\n",
        "        permnos_test = meta_test[\"PERMNO\"].values\n",
        "\n",
        "        scaler_y = load_y_scaler(window)\n",
        "\n",
        "        if window == 5:\n",
        "            print(\"\\nTraining LSTM to get shared parameters...\")\n",
        "            shared_params = optimize_lstm_hyperparameters(\n",
        "                X_train, y_train, permnos_train, window, n_trials=15\n",
        "            )\n",
        "            print(f\"Shared parameters from LSTM: {shared_params}\")\n",
        "\n",
        "        if window == 5:\n",
        "            print(\"\\nTraining LSTM with optimized parameters...\")\n",
        "        else:\n",
        "            print(f\"\\nTraining LSTM with shared parameters...\")\n",
        "            \n",
        "        train_and_evaluate_lstm(\n",
        "            window, X_train, y_train, X_test, y_test,\n",
        "            permnos_train, permnos_test, meta_test, shared_params, scaler_y\n",
        "        )\n",
        "        \n",
        "        clear_memory()\n",
        "        print(f\"Window {window} completed and memory cleared.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Processing Window Size: 5 ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-02 21:36:42,804] A new study created in memory with name: no-name-e6bc1e52-4b17-48d0-b215-57aebddd82ae\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Info] Loaded y scaler for window 5\n",
            "\n",
            "Training LSTM to get shared parameters...\n",
            "[Optuna] Starting hyperparameter optimization with 15 trials on CPU...\n",
            "[Optuna] Using MedianPruner for early trial termination\n",
            "[Optuna] Fixed parameters: hidden_size=128, num_layers=2\n",
            "[Optuna] Searching: batch_size, learning_rate, dropout_rate\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8305b0058db244f7b599f1c8c2119f3a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[I 2025-08-02 21:39:42,500] Trial 0 finished with value: 0.4279136874096154 and parameters: {'batch_size': 64, 'learning_rate': 0.00041282053438262235, 'dropout_rate': 0.031203728088487304}. Best is trial 0 with value: 0.4279136874096154.\n",
            "[I 2025-08-02 21:41:08,435] Trial 1 finished with value: 0.42772007709393256 and parameters: {'batch_size': 128, 'learning_rate': 0.00041917115166952007, 'dropout_rate': 0.1416145155592091}. Best is trial 1 with value: 0.42772007709393256.\n",
            "[I 2025-08-02 21:43:12,220] Trial 2 finished with value: 0.42778563010808707 and parameters: {'batch_size': 64, 'learning_rate': 3.7419406111184946e-05, 'dropout_rate': 0.03636499344142013}. Best is trial 1 with value: 0.42772007709393256.\n",
            "[I 2025-08-02 21:44:38,915] Trial 3 finished with value: 0.42770433786337253 and parameters: {'batch_size': 128, 'learning_rate': 0.0001464895513280072, 'dropout_rate': 0.058245828039608386}. Best is trial 3 with value: 0.42770433786337253.\n",
            "[I 2025-08-02 21:47:17,008] Trial 4 finished with value: 0.4273294554183533 and parameters: {'batch_size': 32, 'learning_rate': 9.745399020374078e-05, 'dropout_rate': 0.0912139968434072}. Best is trial 4 with value: 0.4273294554183533.\n",
            "[I 2025-08-02 21:50:35,576] Trial 5 finished with value: 0.4273221507081406 and parameters: {'batch_size': 32, 'learning_rate': 0.00039710847107924746, 'dropout_rate': 0.009290082543999545}. Best is trial 5 with value: 0.4273221507081406.\n",
            "[I 2025-08-02 21:51:43,739] Trial 6 pruned. \n",
            "[I 2025-08-02 21:55:05,718] Trial 7 finished with value: 0.42734653273011786 and parameters: {'batch_size': 32, 'learning_rate': 0.0007026263205443051, 'dropout_rate': 0.08803049874792027}. Best is trial 5 with value: 0.4273221507081406.\n",
            "[I 2025-08-02 21:55:55,389] Trial 8 pruned. \n",
            "[I 2025-08-02 21:58:30,515] Trial 9 finished with value: 0.42736229823734295 and parameters: {'batch_size': 32, 'learning_rate': 0.00029891977384598987, 'dropout_rate': 0.03697089110510541}. Best is trial 5 with value: 0.4273221507081406.\n",
            "[I 2025-08-02 22:02:09,720] Trial 10 finished with value: 0.4273635848817982 and parameters: {'batch_size': 32, 'learning_rate': 1.0379733829293226e-05, 'dropout_rate': 0.0015094638632586338}. Best is trial 5 with value: 0.4273221507081406.\n",
            "[I 2025-08-02 22:04:39,990] Trial 11 finished with value: 0.42727011105667567 and parameters: {'batch_size': 32, 'learning_rate': 7.852821475632365e-05, 'dropout_rate': 0.11983077739393332}. Best is trial 11 with value: 0.42727011105667567.\n",
            "[I 2025-08-02 22:07:17,004] Trial 12 finished with value: 0.42728833047405285 and parameters: {'batch_size': 32, 'learning_rate': 4.263484370318412e-05, 'dropout_rate': 0.1375702279733455}. Best is trial 11 with value: 0.42727011105667567.\n",
            "[I 2025-08-02 22:10:04,153] Trial 13 finished with value: 0.4272445194865028 and parameters: {'batch_size': 32, 'learning_rate': 2.88133536306097e-05, 'dropout_rate': 0.13943117966801888}. Best is trial 13 with value: 0.4272445194865028.\n",
            "[I 2025-08-02 22:13:17,708] Trial 14 finished with value: 0.42732624433008665 and parameters: {'batch_size': 32, 'learning_rate': 1.3526274653413606e-05, 'dropout_rate': 0.14152970828551334}. Best is trial 13 with value: 0.4272445194865028.\n",
            "[Optuna] Best parameters: {'batch_size': 32, 'learning_rate': 2.88133536306097e-05, 'dropout_rate': 0.13943117966801888, 'hidden_size': 128, 'num_layers': 2, 'epochs': 50}\n",
            "[Optuna] Best validation loss: 0.427245\n",
            "[Optuna] Number of pruned trials: 2\n",
            "[Optuna] Number of completed trials: 13\n",
            "Shared parameters from LSTM: {'batch_size': 32, 'learning_rate': 2.88133536306097e-05, 'dropout_rate': 0.13943117966801888, 'hidden_size': 128, 'num_layers': 2, 'epochs': 50}\n",
            "\n",
            "Training LSTM with optimized parameters...\n",
            "\n",
            "▶ Training LSTM on Window = 5\n",
            "[Info] Input shape: (196920, 5, 1), Input size: 1\n",
            "[Info] Using shared parameters for LSTM\n",
            "[Info] Starting training for LSTM with 50 epochs (early stopping patience=15)...\n",
            "[Info] Train size: 157536, Val size: 39384, Test size: 110850\n",
            "[LSTM] Epoch 10/50, Train Loss: 1.142185, Val Loss: 0.427439, PredStd: 0.025477, PosRatio: 0.709\n",
            "[LSTM] Early stopping at epoch 17\n",
            "[Info] Applying inverse transform using y scaler for window 5\n",
            "[Info] Original scale - y_true range: [-0.100574, 0.108696]\n",
            "[Info] Original scale - y_pred range: [-0.006937, 0.005693]\n",
            "\\n=== Directional Sanity Check ===\n",
            "Pos ratio (y_test): 0.5225259359494813\n",
            "Neg ratio (y_test): 0.47747406405051873\n",
            "Pred +1 ratio: 0.9073071718538566\n",
            "Pred -1 ratio: 0.09269282814614344\n",
            "      Pred+  Pred-\n",
            "+1 | [52890  5032]\n",
            "-1 | [47685  5243]\n",
            "[Info] Model has 199297 trainable parameters, using window_size=5 for Adjusted R²\n",
            "[Create] New metrics file created with LSTM w=5\n",
            "[Save] LSTM_w5.csv\n",
            "[Info] Training completed for LSTM\n",
            "Window 5 completed and memory cleared.\n",
            "\n",
            "=== Processing Window Size: 21 ===\n",
            "[Info] Loaded y scaler for window 21\n",
            "\n",
            "Training LSTM with shared parameters...\n",
            "\n",
            "▶ Training LSTM on Window = 21\n",
            "[Info] Input shape: (196120, 21, 1), Input size: 1\n",
            "[Info] Using shared parameters for LSTM\n",
            "[Info] Starting training for LSTM with 50 epochs (early stopping patience=15)...\n",
            "[Info] Train size: 156896, Val size: 39224, Test size: 110850\n",
            "[LSTM] Epoch 10/50, Train Loss: 1.141227, Val Loss: 0.430428, PredStd: 0.027060, PosRatio: 0.677\n",
            "[LSTM] Early stopping at epoch 18\n",
            "[Info] Applying inverse transform using y scaler for window 21\n",
            "[Info] Original scale - y_true range: [-0.100574, 0.108696]\n",
            "[Info] Original scale - y_pred range: [-0.009618, 0.005936]\n",
            "\\n=== Directional Sanity Check ===\n",
            "Pos ratio (y_test): 0.5233288227334235\n",
            "Neg ratio (y_test): 0.47667117726657643\n",
            "Pred +1 ratio: 0.8905999097880019\n",
            "Pred -1 ratio: 0.1094000902119982\n",
            "      Pred+  Pred-\n",
            "+1 | [52013  5998]\n",
            "-1 | [46710  6129]\n",
            "[Info] Model has 199297 trainable parameters, using window_size=21 for Adjusted R²\n",
            "[Update] Metrics updated for LSTM w=21\n",
            "[Save] LSTM_w21.csv\n",
            "[Info] Training completed for LSTM\n",
            "Window 21 completed and memory cleared.\n",
            "\n",
            "=== Processing Window Size: 252 ===\n",
            "[Info] Loaded y scaler for window 252\n",
            "\n",
            "Training LSTM with shared parameters...\n",
            "\n",
            "▶ Training LSTM on Window = 252\n",
            "[Info] Input shape: (184570, 252, 1), Input size: 1\n",
            "[Info] Using shared parameters for LSTM\n",
            "[Info] Starting training for LSTM with 50 epochs (early stopping patience=15)...\n",
            "[Info] Train size: 147656, Val size: 36914, Test size: 110850\n",
            "[LSTM] Epoch 10/50, Train Loss: 1.130257, Val Loss: 0.474431, PredStd: 0.030176, PosRatio: 0.701\n",
            "[LSTM] Early stopping at epoch 16\n",
            "[Info] Applying inverse transform using y scaler for window 252\n",
            "[Info] Original scale - y_true range: [-0.100574, 0.108696]\n",
            "[Info] Original scale - y_pred range: [-0.011167, 0.006462]\n",
            "\\n=== Directional Sanity Check ===\n",
            "Pos ratio (y_test): 0.5233288227334235\n",
            "Neg ratio (y_test): 0.47667117726657643\n",
            "Pred +1 ratio: 0.8893640054127199\n",
            "Pred -1 ratio: 0.11063599458728011\n",
            "      Pred+  Pred-\n",
            "+1 | [51953  6058]\n",
            "-1 | [46633  6206]\n",
            "[Info] Model has 199297 trainable parameters, using window_size=252 for Adjusted R²\n",
            "[Update] Metrics updated for LSTM w=252\n",
            "[Save] LSTM_w252.csv\n",
            "[Info] Training completed for LSTM\n",
            "Window 252 completed and memory cleared.\n",
            "\n",
            "=== Processing Window Size: 512 ===\n",
            "[Info] Loaded y scaler for window 512\n",
            "\n",
            "Training LSTM with shared parameters...\n",
            "\n",
            "▶ Training LSTM on Window = 512\n",
            "[Info] Input shape: (171570, 512, 1), Input size: 1\n",
            "[Info] Using shared parameters for LSTM\n",
            "[Info] Starting training for LSTM with 50 epochs (early stopping patience=15)...\n",
            "[Info] Train size: 137256, Val size: 34314, Test size: 110850\n",
            "[LSTM] Epoch 10/50, Train Loss: 1.118552, Val Loss: 0.521893, PredStd: 0.031359, PosRatio: 0.779\n",
            "[LSTM] Epoch 20/50, Train Loss: 1.117853, Val Loss: 0.522402, PredStd: 0.034117, PosRatio: 0.712\n",
            "[LSTM] Early stopping at epoch 21\n",
            "[Info] Applying inverse transform using y scaler for window 512\n",
            "[Info] Original scale - y_true range: [-0.100574, 0.108696]\n",
            "[Info] Original scale - y_pred range: [-0.014680, 0.007364]\n",
            "\\n=== Directional Sanity Check ===\n",
            "Pos ratio (y_test): 0.5233288227334235\n",
            "Neg ratio (y_test): 0.47667117726657643\n",
            "Pred +1 ratio: 0.9068019846639603\n",
            "Pred -1 ratio: 0.09319801533603969\n",
            "      Pred+  Pred-\n",
            "+1 | [52884  5127]\n",
            "-1 | [47635  5204]\n",
            "[Info] Model has 199297 trainable parameters, using window_size=512 for Adjusted R²\n",
            "[Update] Metrics updated for LSTM w=512\n",
            "[Save] LSTM_w512.csv\n",
            "[Info] Training completed for LSTM\n",
            "Window 512 completed and memory cleared.\n"
          ]
        }
      ],
      "source": [
        "# ========== 9. 执行入口点 ==========\n",
        "if __name__ == \"__main__\":\n",
        "    # 训练所有窗口的LSTM模型\n",
        "    loop_all_windows()\n",
        "    \n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (tf-mac)",
      "language": "python",
      "name": "tf-mac"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
