{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Metal Performance Shaders)\n"
     ]
    }
   ],
   "source": [
    "# ========== N-BEATS 标准化版本 ==========\n",
    "# 使用标准化数据进行训练，在预测后自动进行y的反标准化\n",
    "# 需要对应的 scaler_y_window_*.pkl 文件进行反标准化处理\n",
    "\n",
    "# ========== 基础库 ==========\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ========== 评估指标 ==========\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# ========== 超参数调优 ==========\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# ========== 可视化 ==========\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ========== 设置全局配置 ==========\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# ========== MPS 加速配置 ==========\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Metal Performance Shaders)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# 内存优化\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision('medium')  # 提升性能\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 数据加载 ==========\n",
    "def load_datasets(npz_path=\"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/all_window_datasets_scaled.npz\"):\n",
    "    data = np.load(npz_path, allow_pickle=True) \n",
    "    datasets = {}\n",
    "    for key in data.files:\n",
    "        datasets[key] = data[key]\n",
    "    return datasets\n",
    "\n",
    "def load_y_scaler(window_size, scaler_dir=\"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/\"):\n",
    "    \"\"\"加载对应窗口大小的y标准化器\"\"\"\n",
    "    scaler_path = os.path.join(scaler_dir, f\"scaler_y_window_{window_size}.pkl\")\n",
    "    if os.path.exists(scaler_path):\n",
    "        scaler = joblib.load(scaler_path)\n",
    "        print(f\"Loaded y scaler for window {window_size}\")\n",
    "        return scaler\n",
    "    else:\n",
    "        print(f\"Warning: Y scaler not found for window {window_size}: {scaler_path}\")\n",
    "        return None\n",
    "\n",
    "def prepare_sequences(X, y, lookback):\n",
    "    \"\"\"准备序列数据用于N-BEATS训练\"\"\"\n",
    "    if len(X) < lookback:\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    # 对于N-BEATS，我们使用滑动窗口方法\n",
    "    X_seq, y_seq = [], []\n",
    "    \n",
    "    for i in range(lookback, len(X)):\n",
    "        # 这里直接取第 i 行作为 1D 向量（与Portfolio版本保持一致）\n",
    "        X_seq.append(X[i])\n",
    "        # 只有在 y 不为 None 时才记录目标\n",
    "        if y is not None:\n",
    "            y_seq.append(y[i])\n",
    "    \n",
    "    # y 为 None 时返回空数组即可\n",
    "    if y is None:\n",
    "        return np.array(X_seq), np.array([])\n",
    "    return np.array(X_seq), np.array(y_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 评估指标 ==========\n",
    "def r2_zero(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    计算老师要求的 R² (zero-based, 基准为0)\n",
    "    y_true: 实际值数组 (N,)\n",
    "    y_pred: 预测值数组 (N,)\n",
    "    \"\"\"\n",
    "    rss = np.sum((y_true - y_pred)**2)  \n",
    "    tss = np.sum(y_true**2)            \n",
    "    return 1 - rss / tss\n",
    "\n",
    "def calc_directional_metrics(y_true, y_pred, permnos=None):\n",
    "    \"\"\"计算方向性指标\"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    if permnos is None:\n",
    "        s_true = np.sign(y_true)\n",
    "        s_pred = np.sign(y_pred)\n",
    "        mask = s_true != 0\n",
    "        s_true = s_true[mask]\n",
    "        s_pred = s_pred[mask]\n",
    "\n",
    "        overall_acc = np.mean(s_true == s_pred)\n",
    "        up_mask = s_true > 0\n",
    "        down_mask = s_true < 0\n",
    "        up_acc = np.mean(s_true[up_mask] == s_pred[up_mask]) if np.any(up_mask) else 0\n",
    "        down_acc = np.mean(s_true[down_mask] == s_pred[down_mask]) if np.any(down_mask) else 0\n",
    "    else:\n",
    "        df = pd.DataFrame({\"permno\": permnos, \"yt\": y_true, \"yp\": y_pred})\n",
    "        overall_accs = []\n",
    "        up_accs = []\n",
    "        down_accs = []\n",
    "\n",
    "        for _, g in df.groupby(\"permno\"):\n",
    "            s_true = np.sign(g[\"yt\"].values)\n",
    "            s_pred = np.sign(g[\"yp\"].values)\n",
    "            mask = s_true != 0\n",
    "            s_true = s_true[mask]\n",
    "            s_pred = s_pred[mask]\n",
    "            if len(s_true) == 0:\n",
    "                continue\n",
    "            overall_accs.append(np.mean(s_true == s_pred))\n",
    "\n",
    "            up_mask = s_true > 0\n",
    "            down_mask = s_true < 0\n",
    "            up_accs.append(np.mean(s_true[up_mask] == s_pred[up_mask]) if np.any(up_mask) else np.nan)\n",
    "            down_accs.append(np.mean(s_true[down_mask] == s_pred[down_mask]) if np.any(down_mask) else np.nan)\n",
    "\n",
    "        overall_acc = np.nanmean(overall_accs)\n",
    "        up_acc = np.nanmean(up_accs)\n",
    "        down_acc = np.nanmean(down_accs)\n",
    "\n",
    "    return overall_acc, up_acc, down_acc\n",
    "\n",
    "def regression_metrics(y_true, y_pred, k, meta=None, permnos=None):\n",
    "    \"\"\"深度学习模型评估指标\"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    n = len(y_true)\n",
    "\n",
    "    # 使用r2_zero替换原来的r2_score\n",
    "    r2 = r2_zero(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "\n",
    "    dir_acc, up_acc, down_acc = calc_directional_metrics(y_true, y_pred, permnos)\n",
    "\n",
    "    metrics = {\n",
    "        \"R2_zero\": r2,\n",
    "        \"RMSE\": rmse,\n",
    "        \"MAE\": mae,\n",
    "        \"MSE\": mse,\n",
    "        \"Directional Accuracy\": dir_acc,\n",
    "        \"Up_Directional_Acc\": up_acc,\n",
    "        \"Down_Directional_Acc\": down_acc\n",
    "    }\n",
    "\n",
    "    # 市值组分析\n",
    "    if meta is not None and \"MKTCAP_PERCENTILE\" in meta:\n",
    "        top_mask = meta[\"MKTCAP_PERCENTILE\"] >= 0.75\n",
    "        bottom_mask = meta[\"MKTCAP_PERCENTILE\"] <= 0.25\n",
    "\n",
    "        if np.any(top_mask):\n",
    "            yt_top = y_true[top_mask]\n",
    "            yp_top = y_pred[top_mask]\n",
    "            perm_top = permnos[top_mask] if permnos is not None else None\n",
    "            r2_top = r2_zero(yt_top, yp_top)\n",
    "            rmse_top = np.sqrt(mean_squared_error(yt_top, yp_top))\n",
    "            mae_top = mean_absolute_error(yt_top, yp_top)\n",
    "            mse_top = mean_squared_error(yt_top, yp_top)\n",
    "            dir_top, up_top, down_top = calc_directional_metrics(yt_top, yp_top, perm_top)\n",
    "            metrics.update({\n",
    "                \"Top25_R2_zero\": r2_top,\n",
    "                \"Top25_RMSE\": rmse_top,\n",
    "                \"Top25_MAE\": mae_top,\n",
    "                \"Top25_MSE\": mse_top,\n",
    "                \"Top25_Dir_Acc\": dir_top,\n",
    "                \"Top25_Up_Acc\": up_top,\n",
    "                \"Top25_Down_Acc\": down_top\n",
    "            })\n",
    "\n",
    "        if np.any(bottom_mask):\n",
    "            yt_bot = y_true[bottom_mask]\n",
    "            yp_bot = y_pred[bottom_mask]\n",
    "            perm_bot = permnos[bottom_mask] if permnos is not None else None\n",
    "            r2_bot = r2_zero(yt_bot, yp_bot)\n",
    "            rmse_bot = np.sqrt(mean_squared_error(yt_bot, yp_bot))\n",
    "            mae_bot = mean_absolute_error(yt_bot, yp_bot)\n",
    "            mse_bot = mean_squared_error(yt_bot, yp_bot)\n",
    "            dir_bot, up_bot, down_bot = calc_directional_metrics(yt_bot, yp_bot, perm_bot)\n",
    "            metrics.update({\n",
    "                \"Bottom25_R2_zero\": r2_bot,\n",
    "                \"Bottom25_RMSE\": rmse_bot,\n",
    "                \"Bottom25_MAE\": mae_bot,\n",
    "                \"Bottom25_MSE\": mse_bot,\n",
    "                \"Bottom25_Dir_Acc\": dir_bot,\n",
    "                \"Bottom25_Up_Acc\": up_bot,\n",
    "                \"Bottom25_Down_Acc\": down_bot\n",
    "            })\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 保存模型和指标 ==========\n",
    "def save_model(model, name, window, path=\"models/\"):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join(path, f\"{name}_w{window}.pth\"))\n",
    "\n",
    "def save_metrics(metrics_dict, name, window, path=\"results.csv\"):\n",
    "    \"\"\"保存评估指标\"\"\"\n",
    "    row = pd.DataFrame([metrics_dict])\n",
    "    row.insert(0, \"Model\", name)\n",
    "    row.insert(1, \"Window\", window)\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path)\n",
    "        df = df[~((df[\"Model\"] == name) & (df[\"Window\"] == window))]\n",
    "        df = pd.concat([df, row], ignore_index=True)\n",
    "        df.to_csv(path, index=False)\n",
    "    else:\n",
    "        row.to_csv(path, index=False)\n",
    "\n",
    "def save_predictions(model_name, window_size, y_true, y_pred, permnos, path=\"predictions/\"):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        \"PERMNO\": permnos,\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred\n",
    "    })\n",
    "\n",
    "    filename = f\"{model_name}_w{window_size}.csv\"\n",
    "    df.to_csv(os.path.join(path, filename), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== N-BEATS 模型架构 ==========\n",
    "class NBeatsBlock(nn.Module):\n",
    "    def __init__(self, input_size, theta_size, basis_size, layers, layer_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(input_size, layer_size)] + \n",
    "                                   [nn.Linear(layer_size, layer_size) for _ in range(layers-1)])\n",
    "        self.basis_parameters = nn.Linear(layer_size, theta_size)\n",
    "        self.input_size = input_size\n",
    "        self.theta_size = theta_size\n",
    "        self.basis_size = basis_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 前向传播\n",
    "        for layer in self.layers:\n",
    "            x = torch.relu(layer(x))\n",
    "        \n",
    "        # 生成基函数参数\n",
    "        theta = self.basis_parameters(x)\n",
    "        \n",
    "        # 线性基函数（简化版本）\n",
    "        backcast = theta[:, :self.input_size]\n",
    "        forecast = theta[:, self.input_size:self.input_size+1]  # 预测1步\n",
    "        \n",
    "        return backcast, forecast\n",
    "\n",
    "class NBeatsNet(nn.Module):\n",
    "    def __init__(self, input_size, stacks=2, blocks_per_stack=2, layers=4, layer_size=128):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.stacks = nn.ModuleList()\n",
    "        \n",
    "        # 创建堆栈和块\n",
    "        for _ in range(stacks):\n",
    "            stack = nn.ModuleList()\n",
    "            for _ in range(blocks_per_stack):\n",
    "                stack.append(NBeatsBlock(input_size, input_size + 1, input_size, layers, layer_size))\n",
    "            self.stacks.append(stack)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        forecast = 0\n",
    "        \n",
    "        for stack in self.stacks:\n",
    "            for block in stack:\n",
    "                backcast, block_forecast = block(residual)\n",
    "                residual = residual - backcast\n",
    "                forecast = forecast + block_forecast\n",
    "        \n",
    "        return forecast\n",
    "\n",
    "# 快速训练函数\n",
    "def train_step(model, criterion, optimizer, X_batch, y_batch):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(X_batch).squeeze()\n",
    "    loss = criterion(predictions, y_batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# 条件性编译加速（仅CUDA支持）\n",
    "if device.type == \"cuda\":\n",
    "    train_step = torch.compile(train_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 超参数调优 ==========\n",
    "TUNED_MODELS = {\"NBEATS\"}\n",
    "\n",
    "def tune_nbeats_with_optuna(X, y, window, n_trials=10):\n",
    "    \"\"\"N-BEATS模型超参数调优 - 使用TimeSeriesSplit\"\"\"\n",
    "    if len(y) < 100:\n",
    "        return {\n",
    "            'stacks': 2, \n",
    "            'blocks_per_stack': 2, \n",
    "            'layers': 2, \n",
    "            'layer_size': 128, \n",
    "            'learning_rate': 0.001, \n",
    "            'batch_size': 32,\n",
    "            'max_epochs': 25,\n",
    "            'warm_start_epochs': 15\n",
    "        }\n",
    "    \n",
    "    print(f\"    [Hyperparameter Tuning] Running Optuna optimization for window={window}\")\n",
    "    print(f\"    [Device Setting] Using CPU for hyperparameter tuning (faster for small windows)\")\n",
    "    \n",
    "    # 强制在CPU上进行调优，小窗口CPU更快\n",
    "    tuning_device = torch.device(\"cpu\")\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    features_per_timestep = X.shape[1] // window\n",
    "    \n",
    "    def objective(trial):\n",
    "        try:\n",
    "            # 超参数搜索空间\n",
    "            stacks = trial.suggest_int(\"stacks\", 1, 3)\n",
    "            blocks_per_stack = trial.suggest_int(\"blocks_per_stack\", 1, 3)\n",
    "            layers = 2  # 固定为2\n",
    "            layer_size = 128  # 固定为128\n",
    "            lr = trial.suggest_float(\"lr\", 1e-5, 5e-4, log=True)\n",
    "            batch_size = trial.suggest_categorical(\"batch_size\", [32, 64])\n",
    "            \n",
    "            cv_scores = []\n",
    "            for train_idx, val_idx in tscv.split(X):\n",
    "                X_tr, X_val = X[train_idx], X[val_idx]\n",
    "                y_tr, y_val = y[train_idx], y[val_idx]\n",
    "                \n",
    "                if len(X_tr) < 50 or len(X_val) < 10:\n",
    "                    continue\n",
    "                \n",
    "                # 准备序列数据\n",
    "                lookback = min(20, len(X_tr)//4)\n",
    "                X_seq_tr, y_seq_tr = prepare_sequences(X_tr, y_tr, lookback)\n",
    "                X_seq_val, y_seq_val = prepare_sequences(X_val, y_val, lookback)\n",
    "                \n",
    "                if len(X_seq_tr) == 0 or len(X_seq_val) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # 转换为张量（使用CPU进行调优）\n",
    "                X_tensor_tr = torch.FloatTensor(X_seq_tr).to(tuning_device)\n",
    "                y_tensor_tr = torch.FloatTensor(y_seq_tr).to(tuning_device)\n",
    "                X_tensor_val = torch.FloatTensor(X_seq_val).to(tuning_device)\n",
    "                y_tensor_val = torch.FloatTensor(y_seq_val).to(tuning_device)\n",
    "                \n",
    "                # 创建模型（在CPU上）\n",
    "                input_size = X_seq_tr.shape[-1]\n",
    "                model = NBeatsNet(input_size, stacks, blocks_per_stack, layers, layer_size).to(tuning_device)\n",
    "                criterion = nn.MSELoss()\n",
    "                optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "                \n",
    "                # 快速训练（少量epoch，在CPU上不需要pin_memory）\n",
    "                dataset = TensorDataset(X_tensor_tr, y_tensor_tr)\n",
    "                dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=False, num_workers=0)\n",
    "                \n",
    "                model.train()\n",
    "                for epoch in range(5):  # 轻量化训练\n",
    "                    for X_batch, y_batch in dataloader:\n",
    "                        train_step(model, criterion, optimizer, X_batch, y_batch)\n",
    "                \n",
    "                # 验证\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_pred = model(X_tensor_val).squeeze().cpu().numpy()\n",
    "                    mse = mean_squared_error(y_seq_val, val_pred)\n",
    "                    cv_scores.append(mse)\n",
    "                \n",
    "                # 清理内存\n",
    "                del model, optimizer, criterion, X_tensor_tr, y_tensor_tr, X_tensor_val, y_tensor_val\n",
    "                if tuning_device.type == 'cuda':\n",
    "                    torch.cuda.empty_cache()\n",
    "                elif tuning_device.type == 'mps':\n",
    "                    torch.mps.empty_cache()\n",
    "            \n",
    "            return np.mean(cv_scores) if cv_scores else float('inf')\n",
    "        except Exception as e:\n",
    "            return float('inf')\n",
    "    \n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    \n",
    "    print(f\"    [Tuning Completed] Switching back to {device} for model training\")\n",
    "    \n",
    "    if study.best_trial is None:\n",
    "        return {\n",
    "            'stacks': 2, \n",
    "            'blocks_per_stack': 2, \n",
    "            'layers':2, \n",
    "            'layer_size': 128, \n",
    "            'learning_rate': 0.001, \n",
    "            'batch_size': 32,\n",
    "            'max_epochs': 25,\n",
    "            'warm_start_epochs': 15\n",
    "        }\n",
    "    \n",
    "    # 构建完整的参数字典\n",
    "    best_params = study.best_params.copy()\n",
    "    best_params['learning_rate'] = best_params.pop('lr')  # 重命名键\n",
    "    best_params['max_epochs'] = 25\n",
    "    best_params['warm_start_epochs'] = 15\n",
    "    best_params['layers']             = 2                         \n",
    "    best_params['layer_size']         = 128  \n",
    "    \n",
    "    print(f\"    [Optuna] NBEATS best_params={best_params}\")\n",
    "    return best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 模型训练和预测 ==========\n",
    "def train_nbeats_model(X_train, y_train, X_test, y_test, best_params, max_epochs=50, y_scaler=None):\n",
    "    \"\"\"训练N-BEATS模型 - 修正：返回对齐的测试数据，支持y反标准化\"\"\"\n",
    "    try:\n",
    "        # 准备序列数据\n",
    "        lookback = min(20, len(X_train)//4)\n",
    "        X_seq_train, y_seq_train = prepare_sequences(X_train, y_train, lookback)\n",
    "        X_seq_test, y_seq_test = prepare_sequences(X_test, y_test, lookback)\n",
    "        \n",
    "        if len(X_seq_train) == 0:\n",
    "            return None, None, None\n",
    "        \n",
    "        # 转换为张量\n",
    "        X_tensor_train = torch.FloatTensor(X_seq_train).to(device)\n",
    "        y_tensor_train = torch.FloatTensor(y_seq_train).to(device)\n",
    "        X_tensor_test = torch.FloatTensor(X_seq_test).to(device)\n",
    "        \n",
    "        # 创建模型\n",
    "        input_size = X_seq_train.shape[-1]\n",
    "        model = NBeatsNet(\n",
    "            input_size=input_size,\n",
    "            stacks=best_params['stacks'],\n",
    "            blocks_per_stack=best_params['blocks_per_stack'],\n",
    "            layers=best_params['layers'],\n",
    "            layer_size=best_params['layer_size']\n",
    "        ).to(device)\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        # 适配新的参数键名\n",
    "        lr_key = 'learning_rate' if 'learning_rate' in best_params else 'lr'\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=best_params[lr_key], weight_decay=1e-5)\n",
    "        \n",
    "        # 数据加载器\n",
    "        dataset = TensorDataset(X_tensor_train, y_tensor_train)\n",
    "        pin_memory = device.type == \"cuda\"  # 仅CUDA支持pin_memory\n",
    "        dataloader = DataLoader(\n",
    "            dataset, \n",
    "            batch_size=best_params['batch_size'], \n",
    "            shuffle=True, \n",
    "            pin_memory=pin_memory,\n",
    "            num_workers=0  # MPS 兼容性\n",
    "        )\n",
    "        \n",
    "        # 训练\n",
    "        model.train()\n",
    "        for epoch in range(max_epochs):\n",
    "            epoch_loss = 0\n",
    "            for X_batch, y_batch in dataloader:\n",
    "                loss = train_step(model, criterion, optimizer, X_batch, y_batch)\n",
    "                epoch_loss += loss\n",
    "            \n",
    "            # 早停检查（简化版）\n",
    "            if epoch > 10 and epoch_loss < 1e-6:\n",
    "                break\n",
    "        \n",
    "        # 预测\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            if len(X_seq_test) > 0:\n",
    "                y_pred_tensor = model(X_tensor_test).squeeze()\n",
    "                y_pred = y_pred_tensor.cpu().numpy()\n",
    "                \n",
    "                # 修正：对齐评估数据，避免填充偏差\n",
    "                # 只对y_test中有对应序列预测的部分进行评估\n",
    "                if len(y_pred) < len(y_test):\n",
    "                    # 截取y_test的后len(y_pred)部分进行对齐\n",
    "                    y_test_aligned = y_test[-len(y_pred):]\n",
    "                else:\n",
    "                    # 如果预测长度>=测试长度，直接截取\n",
    "                    y_pred = y_pred[:len(y_test)]\n",
    "                    y_test_aligned = y_test\n",
    "                \n",
    "                # 反标准化处理\n",
    "                if y_scaler is not None:\n",
    "                    # 对预测值进行反标准化\n",
    "                    y_pred_reshaped = y_pred.reshape(-1, 1)\n",
    "                    y_pred_inverse = y_scaler.inverse_transform(y_pred_reshaped).flatten()\n",
    "                    \n",
    "                    # 对测试值进行反标准化\n",
    "                    y_test_reshaped = y_test_aligned.reshape(-1, 1)\n",
    "                    y_test_inverse = y_scaler.inverse_transform(y_test_reshaped).flatten()\n",
    "                    \n",
    "                    print(f\"Applied inverse scaling: pred range [{y_pred_inverse.min():.6f}, {y_pred_inverse.max():.6f}], test range [{y_test_inverse.min():.6f}, {y_test_inverse.max():.6f}]\")\n",
    "                    return model, y_pred_inverse, y_test_inverse\n",
    "                else:\n",
    "                    print(\"No y_scaler provided, using standardized values\")\n",
    "                    return model, y_pred, y_test_aligned\n",
    "            else:\n",
    "                # 如果没有测试序列，返回空预测\n",
    "                return model, np.array([]), np.array([])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Training failed: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "def get_model(name: str, best_params=None):\n",
    "    \"\"\"获取默认模型参数\"\"\"\n",
    "    if name == \"NBEATS\":\n",
    "        if best_params:\n",
    "            return best_params\n",
    "        else:\n",
    "            return {\n",
    "                'stacks': 2, \n",
    "                'blocks_per_stack': 2, \n",
    "                'layers': 2, \n",
    "                'layer_size': 128, \n",
    "                'learning_rate': 0.001, \n",
    "                'batch_size': 32,\n",
    "                'max_epochs': 25,\n",
    "                'warm_start_epochs': 15\n",
    "            }\n",
    "    \n",
    "    raise ValueError(f\"Unexpected model: {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 训练和评估主逻辑 ==========\n",
    "def train_and_evaluate(model_name, window_size,\n",
    "                       X_train, y_train, X_test, y_test,\n",
    "                       permnos_train, permnos_test, meta=None, shared_params=None):\n",
    "    \"\"\"训练和评估N-BEATS模型\"\"\"\n",
    "    \n",
    "    # 加载对应窗口的y标准化器\n",
    "    y_scaler = load_y_scaler(window_size)\n",
    "    \n",
    "    if model_name in TUNED_MODELS:\n",
    "        # 超参数调优：仅在window=5时进行，其他窗口使用共享参数\n",
    "        if shared_params is None:\n",
    "            print(f\"[Hyperparameter Tuning] Running Optuna optimization for window={window_size}\")\n",
    "            best_params = tune_nbeats_with_optuna(X_train, y_train, window_size, n_trials=10)\n",
    "            print(f\"[Optuna] {model_name} best_params={best_params}\")\n",
    "            model_params = get_model(model_name, best_params)\n",
    "        else:\n",
    "            print(f\"[Shared Parameters] Using optimized params from window=5 for window={window_size}\")\n",
    "            model_params = get_model(model_name, shared_params)\n",
    "    else:\n",
    "        model_params = get_model(model_name)\n",
    "\n",
    "    # 训练模型并预测\n",
    "    fitted_model, y_pred, y_test_aligned = train_nbeats_model(X_train, y_train, X_test, y_test, model_params, y_scaler=y_scaler)\n",
    "    \n",
    "    if fitted_model is None or y_pred is None or y_test_aligned is None:\n",
    "        print(f\"[Skip Model] {model_name} failed to fit. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    # 如果预测为空，跳过\n",
    "    if len(y_pred) == 0:\n",
    "        print(f\"[Skip Model] {model_name} no valid predictions. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    # 对齐元数据和PERMNO\n",
    "    if len(y_test_aligned) < len(y_test):\n",
    "        # 截取对应的元数据和PERMNO\n",
    "        offset = len(y_test) - len(y_test_aligned)\n",
    "        meta_aligned = meta.iloc[offset:] if meta is not None else None\n",
    "        permnos_test_aligned = permnos_test[offset:] if permnos_test is not None else None\n",
    "    else:\n",
    "        meta_aligned = meta\n",
    "        permnos_test_aligned = permnos_test\n",
    "\n",
    "    # 计算评估指标\n",
    "    k = X_test.shape[1]  # 特征数量\n",
    "    metrics = regression_metrics(y_test_aligned, y_pred, k, meta=meta_aligned, permnos=permnos_test_aligned)\n",
    "\n",
    "    # 保存结果\n",
    "    save_model(fitted_model, model_name, window_size)\n",
    "    save_metrics(metrics, model_name, window_size)\n",
    "    save_predictions(model_name, window_size, y_test_aligned, y_pred, permnos_test_aligned)\n",
    "\n",
    "    print(f\"Completed {model_name} w={window_size}: MSE={metrics['MSE']:.6f}, Dir_Acc={metrics['Directional Accuracy']:.4f}\")\n",
    "    \n",
    "    # 清理GPU内存\n",
    "    if device.type == 'mps':\n",
    "        torch.mps.empty_cache()\n",
    "    elif device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # 返回指标和参数（如果是新调优的话）\n",
    "    if shared_params is None and model_name in TUNED_MODELS:\n",
    "        return metrics, best_params\n",
    "    else:\n",
    "        return metrics, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training NBEATS on Window = 5\n",
      "Loaded y scaler for window 5\n",
      "[Hyperparameter Tuning] Running Optuna optimization for window=5\n",
      "    [Hyperparameter Tuning] Running Optuna optimization for window=5\n",
      "    [Device Setting] Using CPU for hyperparameter tuning (faster for small windows)\n",
      "    [Tuning Completed] Switching back to mps for model training\n",
      "    [Optuna] NBEATS best_params={'stacks': 2, 'blocks_per_stack': 1, 'batch_size': 64, 'learning_rate': 1.289795048085554e-05, 'max_epochs': 25, 'warm_start_epochs': 15, 'layers': 2, 'layer_size': 128}\n",
      "[Optuna] NBEATS best_params={'stacks': 2, 'blocks_per_stack': 1, 'batch_size': 64, 'learning_rate': 1.289795048085554e-05, 'max_epochs': 25, 'warm_start_epochs': 15, 'layers': 2, 'layer_size': 128}\n",
      "Applied inverse scaling: pred range [-0.016936, 0.015704], test range [-0.100574, 0.108696]\n",
      "Completed NBEATS w=5: MSE=0.000268, Dir_Acc=0.5235\n",
      "[Shared Hyperparams] Optimized on window=5: {'stacks': 2, 'blocks_per_stack': 1, 'batch_size': 64, 'learning_rate': 1.289795048085554e-05, 'max_epochs': 25, 'warm_start_epochs': 15, 'layers': 2, 'layer_size': 128}\n",
      "Training NBEATS on Window = 21\n",
      "Loaded y scaler for window 21\n",
      "[Shared Parameters] Using optimized params from window=5 for window=21\n",
      "Applied inverse scaling: pred range [-0.018712, 0.022931], test range [-0.100574, 0.108696]\n",
      "Completed NBEATS w=21: MSE=0.000271, Dir_Acc=0.5102\n",
      "Training NBEATS on Window = 252\n",
      "Loaded y scaler for window 252\n",
      "[Shared Parameters] Using optimized params from window=5 for window=252\n",
      "Applied inverse scaling: pred range [-0.061074, 0.070591], test range [-0.100574, 0.108696]\n",
      "Completed NBEATS w=252: MSE=0.000323, Dir_Acc=0.5027\n",
      "Training NBEATS on Window = 512\n",
      "Loaded y scaler for window 512\n",
      "[Shared Parameters] Using optimized params from window=5 for window=512\n",
      "Applied inverse scaling: pred range [-0.087534, 0.095442], test range [-0.100574, 0.108696]\n",
      "Completed NBEATS w=512: MSE=0.000371, Dir_Acc=0.5002\n"
     ]
    }
   ],
   "source": [
    "# ========== 主调度函数 ==========\n",
    "def loop_all_models():\n",
    "    \"\"\"循环所有N-BEATS模型与窗口 - 使用标准化数据训练，自动进行y的反标准化\n",
    "    只在window=5上调参，然后共享到其他窗口\"\"\"\n",
    "    datasets = load_datasets()\n",
    "    model_list = [\"NBEATS\"]\n",
    "    window_sizes = [5, 21, 252, 512]\n",
    "    \n",
    "    # 存储从window=5优化得到的超参数\n",
    "    shared_hyperparams = None\n",
    "\n",
    "    for window in window_sizes:\n",
    "        X_train = datasets[f\"X_train_{window}\"]\n",
    "        y_train = datasets[f\"y_train_{window}\"]\n",
    "        X_test = datasets[f\"X_test_{window}\"]\n",
    "        y_test = datasets[f\"y_test_{window}\"]\n",
    "\n",
    "        # 加载元数据\n",
    "        meta_train_dict = datasets[f\"meta_train_{window}\"].item()\n",
    "        meta_test_dict = datasets[f\"meta_test_{window}\"].item()\n",
    "\n",
    "        meta_train = pd.DataFrame.from_dict(meta_train_dict)\n",
    "        meta_test = pd.DataFrame.from_dict(meta_test_dict)\n",
    "\n",
    "        permnos_train = meta_train[\"PERMNO\"].values\n",
    "        permnos_test = meta_test[\"PERMNO\"].values\n",
    "\n",
    "        for model_name in model_list:\n",
    "            print(f\"Training {model_name} on Window = {window}\")\n",
    "            \n",
    "            # 仅在window=5时进行超参数调优，其他窗口共享参数\n",
    "            if window == 5:\n",
    "                result = train_and_evaluate(\n",
    "                    model_name, window,\n",
    "                    X_train, y_train, X_test, y_test,\n",
    "                    permnos_train, permnos_test,  \n",
    "                    meta_test, shared_params=None\n",
    "                )\n",
    "                if result is not None:\n",
    "                    metrics, optimized_params = result\n",
    "                    shared_hyperparams = optimized_params\n",
    "                    print(f\"[Shared Hyperparams] Optimized on window=5: {shared_hyperparams}\")\n",
    "                else:\n",
    "                    print(f\"[Error] Failed to train {model_name} on window={window}\")\n",
    "                    continue\n",
    "            else:\n",
    "                result = train_and_evaluate(\n",
    "                    model_name, window,\n",
    "                    X_train, y_train, X_test, y_test,\n",
    "                    permnos_train, permnos_test,  \n",
    "                    meta_test, shared_params=shared_hyperparams\n",
    "                )\n",
    "                if result is None:\n",
    "                    print(f\"[Error] Failed to train {model_name} on window={window}\")\n",
    "                    continue\n",
    "# ========== 执行模型训练 ==========\n",
    "if __name__ == \"__main__\":\n",
    "    loop_all_models()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-mac)",
   "language": "python",
   "name": "tf-mac"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
