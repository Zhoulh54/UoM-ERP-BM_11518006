{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Metal Performance Shaders)\n"
     ]
    }
   ],
   "source": [
    "# ========== N-BEATS Standardized Version ==========\n",
    "# Train with standardized data, automatically inverse-transform y after prediction\n",
    "# Requires corresponding scaler_y_window_*.pkl file for inverse transformation\n",
    "\n",
    "# ========== Basic Libraries ==========\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ========== Evaluation Metrics ==========\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# ========== Hyperparameter Tuning ==========\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# ========== Visualization ==========\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ========== Global Configuration ==========\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# ========== MPS Acceleration Configuration ==========\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Metal Performance Shaders)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision('medium')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Data Loading ==========\n",
    "def load_datasets(npz_path=\"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/all_window_datasets_scaled.npz\"):\n",
    "    data = np.load(npz_path, allow_pickle=True) \n",
    "    datasets = {}\n",
    "    for key in data.files:\n",
    "        datasets[key] = data[key]\n",
    "    return datasets\n",
    "\n",
    "def load_y_scaler(window_size, scaler_dir=\"/Users/june/Documents/University of Manchester/Data Science/ERP/Project code/1_Data_Preprocessing/\"):\n",
    "    \"\"\"Load the y scaler for the corresponding window size\"\"\"\n",
    "    scaler_path = os.path.join(scaler_dir, f\"scaler_y_window_{window_size}.pkl\")\n",
    "    if os.path.exists(scaler_path):\n",
    "        scaler = joblib.load(scaler_path)\n",
    "        print(f\"Loaded y scaler for window {window_size}\")\n",
    "        return scaler\n",
    "    else:\n",
    "        print(f\"Warning: Y scaler not found for window {window_size}: {scaler_path}\")\n",
    "        return None\n",
    "\n",
    "def prepare_sequences(X, y, lookback):\n",
    "    \"\"\"Prepare sequence data for N-BEATS training\"\"\"\n",
    "    if len(X) < lookback:\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    X_seq, y_seq = [], []\n",
    "    \n",
    "    for i in range(lookback, len(X)):\n",
    "        X_seq.append(X[i])\n",
    "        if y is not None:\n",
    "            y_seq.append(y[i])\n",
    "    \n",
    "    if y is None:\n",
    "        return np.array(X_seq), np.array([])\n",
    "    return np.array(X_seq), np.array(y_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Evaluation Metrics ==========\n",
    "def r2_zero(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate zero-based RÂ² (baseline is 0)\n",
    "    y_true: true values array (N,)\n",
    "    y_pred: predicted values array (N,)\n",
    "    \"\"\"\n",
    "    rss = np.sum((y_true - y_pred)**2)  \n",
    "    tss = np.sum(y_true**2)            \n",
    "    return 1 - rss / tss\n",
    "\n",
    "def calc_directional_metrics(y_true, y_pred, permnos=None):\n",
    "    \"\"\"Calculate directional metrics\"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    if permnos is None:\n",
    "        s_true = np.sign(y_true)\n",
    "        s_pred = np.sign(y_pred)\n",
    "        mask = s_true != 0\n",
    "        s_true = s_true[mask]\n",
    "        s_pred = s_pred[mask]\n",
    "\n",
    "        overall_acc = np.mean(s_true == s_pred)\n",
    "        up_mask = s_true > 0\n",
    "        down_mask = s_true < 0\n",
    "        up_acc = np.mean(s_true[up_mask] == s_pred[up_mask]) if np.any(up_mask) else 0\n",
    "        down_acc = np.mean(s_true[down_mask] == s_pred[down_mask]) if np.any(down_mask) else 0\n",
    "    else:\n",
    "        df = pd.DataFrame({\"permno\": permnos, \"yt\": y_true, \"yp\": y_pred})\n",
    "        overall_accs = []\n",
    "        up_accs = []\n",
    "        down_accs = []\n",
    "\n",
    "        for _, g in df.groupby(\"permno\"):\n",
    "            s_true = np.sign(g[\"yt\"].values)\n",
    "            s_pred = np.sign(g[\"yp\"].values)\n",
    "            mask = s_true != 0\n",
    "            s_true = s_true[mask]\n",
    "            s_pred = s_pred[mask]\n",
    "            if len(s_true) == 0:\n",
    "                continue\n",
    "            overall_accs.append(np.mean(s_true == s_pred))\n",
    "\n",
    "            up_mask = s_true > 0\n",
    "            down_mask = s_true < 0\n",
    "            up_accs.append(np.mean(s_true[up_mask] == s_pred[up_mask]) if np.any(up_mask) else np.nan)\n",
    "            down_accs.append(np.mean(s_true[down_mask] == s_pred[down_mask]) if np.any(down_mask) else np.nan)\n",
    "\n",
    "        overall_acc = np.nanmean(overall_accs)\n",
    "        up_acc = np.nanmean(up_accs)\n",
    "        down_acc = np.nanmean(down_accs)\n",
    "\n",
    "    return overall_acc, up_acc, down_acc\n",
    "\n",
    "def regression_metrics(y_true, y_pred, k, meta=None, permnos=None):\n",
    "    \"\"\"Evaluation metrics for deep learning models\"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    n = len(y_true)\n",
    "\n",
    "    r2 = r2_zero(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "\n",
    "    dir_acc, up_acc, down_acc = calc_directional_metrics(y_true, y_pred, permnos)\n",
    "\n",
    "    metrics = {\n",
    "        \"R2_zero\": r2,\n",
    "        \"RMSE\": rmse,\n",
    "        \"MAE\": mae,\n",
    "        \"MSE\": mse,\n",
    "        \"Directional Accuracy\": dir_acc,\n",
    "        \"Up_Directional_Acc\": up_acc,\n",
    "        \"Down_Directional_Acc\": down_acc\n",
    "    }\n",
    "\n",
    "    # Market cap group analysis\n",
    "    if meta is not None and \"MKTCAP_PERCENTILE\" in meta:\n",
    "        top_mask = meta[\"MKTCAP_PERCENTILE\"] >= 0.75\n",
    "        bottom_mask = meta[\"MKTCAP_PERCENTILE\"] <= 0.25\n",
    "\n",
    "        if np.any(top_mask):\n",
    "            yt_top = y_true[top_mask]\n",
    "            yp_top = y_pred[top_mask]\n",
    "            perm_top = permnos[top_mask] if permnos is not None else None\n",
    "            r2_top = r2_zero(yt_top, yp_top)\n",
    "            rmse_top = np.sqrt(mean_squared_error(yt_top, yp_top))\n",
    "            mae_top = mean_absolute_error(yt_top, yp_top)\n",
    "            mse_top = mean_squared_error(yt_top, yp_top)\n",
    "            dir_top, up_top, down_top = calc_directional_metrics(yt_top, yp_top, perm_top)\n",
    "            metrics.update({\n",
    "                \"Top25_R2_zero\": r2_top,\n",
    "                \"Top25_RMSE\": rmse_top,\n",
    "                \"Top25_MAE\": mae_top,\n",
    "                \"Top25_MSE\": mse_top,\n",
    "                \"Top25_Dir_Acc\": dir_top,\n",
    "                \"Top25_Up_Acc\": up_top,\n",
    "                \"Top25_Down_Acc\": down_top\n",
    "            })\n",
    "\n",
    "        if np.any(bottom_mask):\n",
    "            yt_bot = y_true[bottom_mask]\n",
    "            yp_bot = y_pred[bottom_mask]\n",
    "            perm_bot = permnos[bottom_mask] if permnos is not None else None\n",
    "            r2_bot = r2_zero(yt_bot, yp_bot)\n",
    "            rmse_bot = np.sqrt(mean_squared_error(yt_bot, yp_bot))\n",
    "            mae_bot = mean_absolute_error(yt_bot, yp_bot)\n",
    "            mse_bot = mean_squared_error(yt_bot, yp_bot)\n",
    "            dir_bot, up_bot, down_bot = calc_directional_metrics(yt_bot, yp_bot, perm_bot)\n",
    "            metrics.update({\n",
    "                \"Bottom25_R2_zero\": r2_bot,\n",
    "                \"Bottom25_RMSE\": rmse_bot,\n",
    "                \"Bottom25_MAE\": mae_bot,\n",
    "                \"Bottom25_MSE\": mse_bot,\n",
    "                \"Bottom25_Dir_Acc\": dir_bot,\n",
    "                \"Bottom25_Up_Acc\": up_bot,\n",
    "                \"Bottom25_Down_Acc\": down_bot\n",
    "            })\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Save model and metrics ==========\n",
    "def save_model(model, name, window, path=\"models/\"):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join(path, f\"{name}_w{window}.pth\"))\n",
    "\n",
    "def save_metrics(metrics_dict, name, window, path=\"results.csv\"):\n",
    "    row = pd.DataFrame([metrics_dict])\n",
    "    row.insert(0, \"Model\", name)\n",
    "    row.insert(1, \"Window\", window)\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path)\n",
    "        df = df[~((df[\"Model\"] == name) & (df[\"Window\"] == window))]\n",
    "        df = pd.concat([df, row], ignore_index=True)\n",
    "        df.to_csv(path, index=False)\n",
    "    else:\n",
    "        row.to_csv(path, index=False)\n",
    "\n",
    "def save_predictions(model_name, window_size, y_true, y_pred, permnos, path=\"predictions/\"):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        \"PERMNO\": permnos,\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred\n",
    "    })\n",
    "\n",
    "    filename = f\"{model_name}_w{window_size}.csv\"\n",
    "    df.to_csv(os.path.join(path, filename), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-BEATS model architecture\n",
    "class NBeatsBlock(nn.Module):\n",
    "    def __init__(self, input_size, theta_size, basis_size, layers, layer_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(input_size, layer_size)] + \n",
    "                                   [nn.Linear(layer_size, layer_size) for _ in range(layers-1)])\n",
    "        self.basis_parameters = nn.Linear(layer_size, theta_size)\n",
    "        self.input_size = input_size\n",
    "        self.theta_size = theta_size\n",
    "        self.basis_size = basis_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = torch.relu(layer(x))\n",
    "        theta = self.basis_parameters(x)\n",
    "        backcast = theta[:, :self.input_size]\n",
    "        forecast = theta[:, self.input_size:self.input_size+1]\n",
    "        return backcast, forecast\n",
    "\n",
    "class NBeatsNet(nn.Module):\n",
    "    def __init__(self, input_size, stacks=2, blocks_per_stack=2, layers=4, layer_size=128):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.stacks = nn.ModuleList()\n",
    "        for _ in range(stacks):\n",
    "            stack = nn.ModuleList()\n",
    "            for _ in range(blocks_per_stack):\n",
    "                stack.append(NBeatsBlock(input_size, input_size + 1, input_size, layers, layer_size))\n",
    "            self.stacks.append(stack)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        forecast = 0\n",
    "        for stack in self.stacks:\n",
    "            for block in stack:\n",
    "                backcast, block_forecast = block(residual)\n",
    "                residual = residual - backcast\n",
    "                forecast = forecast + block_forecast\n",
    "        return forecast\n",
    "\n",
    "def train_step(model, criterion, optimizer, X_batch, y_batch):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(X_batch).squeeze()\n",
    "    loss = criterion(predictions, y_batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    train_step = torch.compile(train_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Hyperparameter Tuning ==========\n",
    "TUNED_MODELS = {\"NBEATS\"}\n",
    "\n",
    "def tune_nbeats_with_optuna(X, y, window, n_trials=10):\n",
    "    \"\"\"N-BEATS hyperparameter tuning using TimeSeriesSplit\"\"\"\n",
    "    if len(y) < 100:\n",
    "        return {\n",
    "            'stacks': 2, \n",
    "            'blocks_per_stack': 2, \n",
    "            'layers': 2, \n",
    "            'layer_size': 128, \n",
    "            'learning_rate': 0.001, \n",
    "            'batch_size': 32,\n",
    "            'max_epochs': 25,\n",
    "            'warm_start_epochs': 15\n",
    "        }\n",
    "    \n",
    "    print(f\"    [Hyperparameter Tuning] Running Optuna optimization for window={window}\")\n",
    "    print(f\"    [Device Setting] Using CPU for hyperparameter tuning (faster for small windows)\")\n",
    "    \n",
    "    tuning_device = torch.device(\"cpu\")\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    features_per_timestep = X.shape[1] // window\n",
    "    \n",
    "    def objective(trial):\n",
    "        try:\n",
    "            stacks = trial.suggest_int(\"stacks\", 1, 3)\n",
    "            blocks_per_stack = trial.suggest_int(\"blocks_per_stack\", 1, 3)\n",
    "            layers = 2\n",
    "            layer_size = 128\n",
    "            lr = trial.suggest_float(\"lr\", 1e-5, 5e-4, log=True)\n",
    "            batch_size = trial.suggest_categorical(\"batch_size\", [32, 64])\n",
    "            \n",
    "            cv_scores = []\n",
    "            for train_idx, val_idx in tscv.split(X):\n",
    "                X_tr, X_val = X[train_idx], X[val_idx]\n",
    "                y_tr, y_val = y[train_idx], y[val_idx]\n",
    "                \n",
    "                if len(X_tr) < 50 or len(X_val) < 10:\n",
    "                    continue\n",
    "                \n",
    "                lookback = min(20, len(X_tr)//4)\n",
    "                X_seq_tr, y_seq_tr = prepare_sequences(X_tr, y_tr, lookback)\n",
    "                X_seq_val, y_seq_val = prepare_sequences(X_val, y_val, lookback)\n",
    "                \n",
    "                if len(X_seq_tr) == 0 or len(X_seq_val) == 0:\n",
    "                    continue\n",
    "                \n",
    "                X_tensor_tr = torch.FloatTensor(X_seq_tr).to(tuning_device)\n",
    "                y_tensor_tr = torch.FloatTensor(y_seq_tr).to(tuning_device)\n",
    "                X_tensor_val = torch.FloatTensor(X_seq_val).to(tuning_device)\n",
    "                y_tensor_val = torch.FloatTensor(y_seq_val).to(tuning_device)\n",
    "                \n",
    "                input_size = X_seq_tr.shape[-1]\n",
    "                model = NBeatsNet(input_size, stacks, blocks_per_stack, layers, layer_size).to(tuning_device)\n",
    "                criterion = nn.MSELoss()\n",
    "                optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "                \n",
    "                dataset = TensorDataset(X_tensor_tr, y_tensor_tr)\n",
    "                dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=False, num_workers=0)\n",
    "                \n",
    "                model.train()\n",
    "                for epoch in range(5):\n",
    "                    for X_batch, y_batch in dataloader:\n",
    "                        train_step(model, criterion, optimizer, X_batch, y_batch)\n",
    "                \n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_pred = model(X_tensor_val).squeeze().cpu().numpy()\n",
    "                    mse = mean_squared_error(y_seq_val, val_pred)\n",
    "                    cv_scores.append(mse)\n",
    "                \n",
    "                del model, optimizer, criterion, X_tensor_tr, y_tensor_tr, X_tensor_val, y_tensor_val\n",
    "                if tuning_device.type == 'cuda':\n",
    "                    torch.cuda.empty_cache()\n",
    "                elif tuning_device.type == 'mps':\n",
    "                    torch.mps.empty_cache()\n",
    "            \n",
    "            return np.mean(cv_scores) if cv_scores else float('inf')\n",
    "        except Exception as e:\n",
    "            return float('inf')\n",
    "    \n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    \n",
    "    print(f\"    [Tuning Completed] Switching back to {device} for model training\")\n",
    "    \n",
    "    if study.best_trial is None:\n",
    "        return {\n",
    "            'stacks': 2, \n",
    "            'blocks_per_stack': 2, \n",
    "            'layers':2, \n",
    "            'layer_size': 128, \n",
    "            'learning_rate': 0.001, \n",
    "            'batch_size': 32,\n",
    "            'max_epochs': 25,\n",
    "            'warm_start_epochs': 15\n",
    "        }\n",
    "    \n",
    "    best_params = study.best_params.copy()\n",
    "    best_params['learning_rate'] = best_params.pop('lr')\n",
    "    best_params['max_epochs'] = 25\n",
    "    best_params['warm_start_epochs'] = 15\n",
    "    best_params['layers']             = 2                         \n",
    "    best_params['layer_size']         = 128  \n",
    "    \n",
    "    print(f\"    [Optuna] NBEATS best_params={best_params}\")\n",
    "    return best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Model Training and Prediction ==========\n",
    "def train_nbeats_model(X_train, y_train, X_test, y_test, best_params, max_epochs=50, y_scaler=None):\n",
    "    \"\"\"Train N-BEATS model and return aligned test data, support inverse scaling for y\"\"\"\n",
    "    try:\n",
    "        lookback = min(20, len(X_train)//4)\n",
    "        X_seq_train, y_seq_train = prepare_sequences(X_train, y_train, lookback)\n",
    "        X_seq_test, y_seq_test = prepare_sequences(X_test, y_test, lookback)\n",
    "        \n",
    "        if len(X_seq_train) == 0:\n",
    "            return None, None, None\n",
    "        \n",
    "        X_tensor_train = torch.FloatTensor(X_seq_train).to(device)\n",
    "        y_tensor_train = torch.FloatTensor(y_seq_train).to(device)\n",
    "        X_tensor_test = torch.FloatTensor(X_seq_test).to(device)\n",
    "        \n",
    "        input_size = X_seq_train.shape[-1]\n",
    "        model = NBeatsNet(\n",
    "            input_size=input_size,\n",
    "            stacks=best_params['stacks'],\n",
    "            blocks_per_stack=best_params['blocks_per_stack'],\n",
    "            layers=best_params['layers'],\n",
    "            layer_size=best_params['layer_size']\n",
    "        ).to(device)\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        lr_key = 'learning_rate' if 'learning_rate' in best_params else 'lr'\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=best_params[lr_key], weight_decay=1e-5)\n",
    "        \n",
    "        dataset = TensorDataset(X_tensor_train, y_tensor_train)\n",
    "        pin_memory = device.type == \"cuda\"\n",
    "        dataloader = DataLoader(\n",
    "            dataset, \n",
    "            batch_size=best_params['batch_size'], \n",
    "            shuffle=True, \n",
    "            pin_memory=pin_memory,\n",
    "            num_workers=0\n",
    "        )\n",
    "        \n",
    "        model.train()\n",
    "        for epoch in range(max_epochs):\n",
    "            epoch_loss = 0\n",
    "            for X_batch, y_batch in dataloader:\n",
    "                loss = train_step(model, criterion, optimizer, X_batch, y_batch)\n",
    "                epoch_loss += loss\n",
    "            \n",
    "            if epoch > 10 and epoch_loss < 1e-6:\n",
    "                break\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            if len(X_seq_test) > 0:\n",
    "                y_pred_tensor = model(X_tensor_test).squeeze()\n",
    "                y_pred = y_pred_tensor.cpu().numpy()\n",
    "                \n",
    "                if len(y_pred) < len(y_test):\n",
    "                    y_test_aligned = y_test[-len(y_pred):]\n",
    "                else:\n",
    "                    y_pred = y_pred[:len(y_test)]\n",
    "                    y_test_aligned = y_test\n",
    "                \n",
    "                if y_scaler is not None:\n",
    "                    y_pred_reshaped = y_pred.reshape(-1, 1)\n",
    "                    y_pred_inverse = y_scaler.inverse_transform(y_pred_reshaped).flatten()\n",
    "                    \n",
    "                    y_test_reshaped = y_test_aligned.reshape(-1, 1)\n",
    "                    y_test_inverse = y_scaler.inverse_transform(y_test_reshaped).flatten()\n",
    "                    \n",
    "                    print(f\"Applied inverse scaling: pred range [{y_pred_inverse.min():.6f}, {y_pred_inverse.max():.6f}], test range [{y_test_inverse.min():.6f}, {y_test_inverse.max():.6f}]\")\n",
    "                    return model, y_pred_inverse, y_test_inverse\n",
    "                else:\n",
    "                    print(\"No y_scaler provided, using standardized values\")\n",
    "                    return model, y_pred, y_test_aligned\n",
    "            else:\n",
    "                return model, np.array([]), np.array([])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Training failed: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "def get_model(name: str, best_params=None):\n",
    "    \"\"\"Get default model parameters\"\"\"\n",
    "    if name == \"NBEATS\":\n",
    "        if best_params:\n",
    "            return best_params\n",
    "        else:\n",
    "            return {\n",
    "                'stacks': 2, \n",
    "                'blocks_per_stack': 2, \n",
    "                'layers': 2, \n",
    "                'layer_size': 128, \n",
    "                'learning_rate': 0.001, \n",
    "                'batch_size': 32,\n",
    "                'max_epochs': 25,\n",
    "                'warm_start_epochs': 15\n",
    "            }\n",
    "    \n",
    "    raise ValueError(f\"Unexpected model: {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Main training and evaluation logic ==========\n",
    "def train_and_evaluate(model_name, window_size,\n",
    "                       X_train, y_train, X_test, y_test,\n",
    "                       permnos_train, permnos_test, meta=None, shared_params=None):\n",
    "    \"\"\"Train and evaluate N-BEATS model\"\"\"\n",
    "    \n",
    "    y_scaler = load_y_scaler(window_size)\n",
    "    \n",
    "    if model_name in TUNED_MODELS:\n",
    "        if shared_params is None:\n",
    "            print(f\"[Hyperparameter Tuning] Running Optuna optimization for window={window_size}\")\n",
    "            best_params = tune_nbeats_with_optuna(X_train, y_train, window_size, n_trials=10)\n",
    "            print(f\"[Optuna] {model_name} best_params={best_params}\")\n",
    "            model_params = get_model(model_name, best_params)\n",
    "        else:\n",
    "            print(f\"[Shared Parameters] Using optimized params from window=5 for window={window_size}\")\n",
    "            model_params = get_model(model_name, shared_params)\n",
    "    else:\n",
    "        model_params = get_model(model_name)\n",
    "\n",
    "    fitted_model, y_pred, y_test_aligned = train_nbeats_model(X_train, y_train, X_test, y_test, model_params, y_scaler=y_scaler)\n",
    "    \n",
    "    if fitted_model is None or y_pred is None or y_test_aligned is None:\n",
    "        print(f\"[Skip Model] {model_name} failed to fit. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    if len(y_pred) == 0:\n",
    "        print(f\"[Skip Model] {model_name} no valid predictions. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    if len(y_test_aligned) < len(y_test):\n",
    "        offset = len(y_test) - len(y_test_aligned)\n",
    "        meta_aligned = meta.iloc[offset:] if meta is not None else None\n",
    "        permnos_test_aligned = permnos_test[offset:] if permnos_test is not None else None\n",
    "    else:\n",
    "        meta_aligned = meta\n",
    "        permnos_test_aligned = permnos_test\n",
    "\n",
    "    k = X_test.shape[1]\n",
    "    metrics = regression_metrics(y_test_aligned, y_pred, k, meta=meta_aligned, permnos=permnos_test_aligned)\n",
    "\n",
    "    save_model(fitted_model, model_name, window_size)\n",
    "    save_metrics(metrics, model_name, window_size)\n",
    "    save_predictions(model_name, window_size, y_test_aligned, y_pred, permnos_test_aligned)\n",
    "\n",
    "    print(f\"Completed {model_name} w={window_size}: MSE={metrics['MSE']:.6f}, Dir_Acc={metrics['Directional Accuracy']:.4f}\")\n",
    "    \n",
    "    if device.type == 'mps':\n",
    "        torch.mps.empty_cache()\n",
    "    elif device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    if shared_params is None and model_name in TUNED_MODELS:\n",
    "        return metrics, best_params\n",
    "    else:\n",
    "        return metrics, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training NBEATS on Window = 5\n",
      "Loaded y scaler for window 5\n",
      "[Hyperparameter Tuning] Running Optuna optimization for window=5\n",
      "    [Hyperparameter Tuning] Running Optuna optimization for window=5\n",
      "    [Device Setting] Using CPU for hyperparameter tuning (faster for small windows)\n",
      "    [Tuning Completed] Switching back to mps for model training\n",
      "    [Optuna] NBEATS best_params={'stacks': 2, 'blocks_per_stack': 1, 'batch_size': 64, 'learning_rate': 1.289795048085554e-05, 'max_epochs': 25, 'warm_start_epochs': 15, 'layers': 2, 'layer_size': 128}\n",
      "[Optuna] NBEATS best_params={'stacks': 2, 'blocks_per_stack': 1, 'batch_size': 64, 'learning_rate': 1.289795048085554e-05, 'max_epochs': 25, 'warm_start_epochs': 15, 'layers': 2, 'layer_size': 128}\n",
      "Applied inverse scaling: pred range [-0.016936, 0.015704], test range [-0.100574, 0.108696]\n",
      "Completed NBEATS w=5: MSE=0.000268, Dir_Acc=0.5235\n",
      "[Shared Hyperparams] Optimized on window=5: {'stacks': 2, 'blocks_per_stack': 1, 'batch_size': 64, 'learning_rate': 1.289795048085554e-05, 'max_epochs': 25, 'warm_start_epochs': 15, 'layers': 2, 'layer_size': 128}\n",
      "Training NBEATS on Window = 21\n",
      "Loaded y scaler for window 21\n",
      "[Shared Parameters] Using optimized params from window=5 for window=21\n",
      "Applied inverse scaling: pred range [-0.018712, 0.022931], test range [-0.100574, 0.108696]\n",
      "Completed NBEATS w=21: MSE=0.000271, Dir_Acc=0.5102\n",
      "Training NBEATS on Window = 252\n",
      "Loaded y scaler for window 252\n",
      "[Shared Parameters] Using optimized params from window=5 for window=252\n",
      "Applied inverse scaling: pred range [-0.061074, 0.070591], test range [-0.100574, 0.108696]\n",
      "Completed NBEATS w=252: MSE=0.000323, Dir_Acc=0.5027\n",
      "Training NBEATS on Window = 512\n",
      "Loaded y scaler for window 512\n",
      "[Shared Parameters] Using optimized params from window=5 for window=512\n",
      "Applied inverse scaling: pred range [-0.087534, 0.095442], test range [-0.100574, 0.108696]\n",
      "Completed NBEATS w=512: MSE=0.000371, Dir_Acc=0.5002\n"
     ]
    }
   ],
   "source": [
    "# Main dispatcher function\n",
    "def loop_all_models():\n",
    "    \"\"\"\n",
    "    Loop through all N-BEATS models and window sizes.\n",
    "    Use standardized data for training, automatically inverse-transform y.\n",
    "    Hyperparameter tuning is only performed on window=5, and the parameters are shared with other windows.\n",
    "    \"\"\"\n",
    "    datasets = load_datasets()\n",
    "    model_list = [\"NBEATS\"]\n",
    "    window_sizes = [5, 21, 252, 512]\n",
    "    \n",
    "    shared_hyperparams = None\n",
    "\n",
    "    for window in window_sizes:\n",
    "        X_train = datasets[f\"X_train_{window}\"]\n",
    "        y_train = datasets[f\"y_train_{window}\"]\n",
    "        X_test = datasets[f\"X_test_{window}\"]\n",
    "        y_test = datasets[f\"y_test_{window}\"]\n",
    "\n",
    "        meta_train_dict = datasets[f\"meta_train_{window}\"].item()\n",
    "        meta_test_dict = datasets[f\"meta_test_{window}\"].item()\n",
    "\n",
    "        meta_train = pd.DataFrame.from_dict(meta_train_dict)\n",
    "        meta_test = pd.DataFrame.from_dict(meta_test_dict)\n",
    "\n",
    "        permnos_train = meta_train[\"PERMNO\"].values\n",
    "        permnos_test = meta_test[\"PERMNO\"].values\n",
    "\n",
    "        for model_name in model_list:\n",
    "            print(f\"Training {model_name} on Window = {window}\")\n",
    "            \n",
    "            if window == 5:\n",
    "                result = train_and_evaluate(\n",
    "                    model_name, window,\n",
    "                    X_train, y_train, X_test, y_test,\n",
    "                    permnos_train, permnos_test,  \n",
    "                    meta_test, shared_params=None\n",
    "                )\n",
    "                if result is not None:\n",
    "                    metrics, optimized_params = result\n",
    "                    shared_hyperparams = optimized_params\n",
    "                    print(f\"[Shared Hyperparams] Optimized on window=5: {shared_hyperparams}\")\n",
    "                else:\n",
    "                    print(f\"[Error] Failed to train {model_name} on window={window}\")\n",
    "                    continue\n",
    "            else:\n",
    "                result = train_and_evaluate(\n",
    "                    model_name, window,\n",
    "                    X_train, y_train, X_test, y_test,\n",
    "                    permnos_train, permnos_test,  \n",
    "                    meta_test, shared_params=shared_hyperparams\n",
    "                )\n",
    "                if result is None:\n",
    "                    print(f\"[Error] Failed to train {model_name} on window={window}\")\n",
    "                    continue\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    loop_all_models()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-mac)",
   "language": "python",
   "name": "tf-mac"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
